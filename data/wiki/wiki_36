<doc id="4802" url="https://en.wikipedia.org/wiki?curid=4802" title="Biome">
Biome

A biome is a community of plants and animals that have common characteristics for the environment they exist in. They can be found over a range of continents. Biomes are distinct biological communities that have formed in response to a shared physical climate. "Biome" is a broader term than "habitat"; any biome can comprise a variety of habitats.
While a biome can cover large areas, a microbiome is a mix of organisms that coexist in a defined space on a much smaller scale. For example, the human microbiome is the collection of bacteria, viruses, and other microorganisms that are present on or in a human body.
A 'biota' is the total collection of organisms of a geographic region or a time period, from local geographic scales and instantaneous temporal scales all the way up to whole-planet and whole-timescale spatiotemporal scales. The biotas of the Earth make up the biosphere.
The term was suggested in 1916 by Clements, originally as a synonym for biotic community of Möbius (1877). Later, it gained its current definition, based on earlier concepts of phytophysiognomy, formation and vegetation (used in opposition to flora), with the inclusion of the animal element and the exclusion of the taxonomic element of species composition. In 1935, Tansley added the climatic and soil aspects to the idea, calling it ecosystem. The International Biological Program (1964–74) projects popularized the concept of biome.
However, in some contexts, the term biome is used in a different manner. In German literature, particularly in the Walter terminology, the term is used similarly as biotope (a concrete geographical unit), while the biome definition used in this article is used as an international, non-regional, terminology—irrespectively of the continent in which an area is present, it takes the same biome name—and corresponds to his "zonobiome", "orobiome" and "pedobiome" (biomes determined by climate zone, altitude or soil).
In Brazilian literature, the term "biome" is sometimes used as synonym of "biogeographic province", an area based on species composition (the term "floristic province" being used when plant species are considered), or also as synonym of the "morphoclimatic and phytogeographical domain" of Ab'Sáber, a geographic space with subcontinental dimensions, with the predominance of similar geomorphologic and climatic characteristics, and of a certain vegetation form. Both include many biomes in fact.
To divide the world in a few ecological zones is a difficult attempt, notably because of the small-scale variations that exist everywhere on earth and because of the gradual changeover from one biome to the other. Their boundaries must therefore be drawn arbitrarily and their characterization made according to the average conditions that predominate in them.
A 1978 study on North American grasslands found a positive logistic correlation between evapotranspiration in mm/yr and above-ground net primary production in g/m/yr. The general results from the study were that precipitation and water use led to above-ground primary production, while solar irradiation and temperature lead to below-ground primary production (roots), and temperature and water lead to cool and warm season growth habit. These findings help explain the categories used in Holdridge's bioclassification scheme (see below), which were then later simplified by Whittaker. The number of classification schemes and the variety of determinants used in those schemes, however, should be taken as strong indicators that biomes do not fit perfectly into the classification schemes created.
Holdridge classified climates based on the biological effects of temperature and rainfall on vegetation under the assumption that these two abiotic factors are the largest determinants of the types of vegetation found in a habitat. Holdridge uses the four axes to define 30 so-called "humidity provinces", which are clearly visible in his diagram. While this scheme largely ignores soil and sun exposure, Holdridge acknowledged that these were important.
The principal biome-types by Allee (1949):
The principal biomes of the world by Kendeigh (1961):
Whittaker classified biomes using two abiotic factors: precipitation and temperature. His scheme can be seen as a simplification of Holdridge's; more readily accessible, but missing Holdridge's greater specificity.
Whittaker based his approach on theoretical assertions and empirical sampling. He was in a unique position to make such a holistic assertion because he had previously compiled a review of biome classifications.
Whittaker's distinction between biome and formation can be simplified: formation is used when applied to plant communities only, while biome is used when concerned with both plants and animals. Whittaker's convention of biome-type or formation-type is simply a broader method to categorize similar communities. 
Whittaker, seeing the need for a simpler way to express the relationship of community structure to the environment, used what he called "gradient analysis" of ecocline patterns to relate communities to climate on a worldwide scale. Whittaker considered four main ecoclines in the terrestrial realm.
Along these gradients, Whittaker noted several trends that allowed him to qualitatively establish biome-types:
Whittaker summed the effects of gradients (3) and (4) to get an overall temperature gradient and combined this with a gradient (2), the moisture gradient, to express the above conclusions in what is known as the Whittaker classification scheme. The scheme graphs average annual precipitation (x-axis) versus average annual temperature (y-axis) to classify biome-types.
The multiauthored series "Ecosystems of the world", edited by David W. Goodall, provides a comprehensive coverage of the major "ecosystem types or biomes" on earth:
The eponymously-named Heinrich Walter classification scheme considers the seasonality of temperature and precipitation. The system, also assessing precipitation and temperature, finds nine major biome types, with the important climate traits and vegetation types. The boundaries of each biome correlate to the conditions of moisture and cold stress that are strong determinants of plant form, and therefore the vegetation that defines the region. Extreme conditions, such as flooding in a swamp, can create different kinds of communities within the same biome.
Schultz (1988) defined nine ecozones (note that his concept of ecozone is more similar to the concept of biome used in this article than to the concept of ecozone of BBC):
Robert G. Bailey nearly developed a biogeographical classification system of ecoregions for the United States in a map published in 1976. He subsequently expanded the system to include the rest of North America in 1981, and the world in 1989. The Bailey system, based on climate, is divided into seven domains (polar, humid temperate, dry, humid, and humid tropical), with further divisions based on other climate characteristics (subarctic, warm temperate, hot temperate, and subtropical; marine and continental; lowland and mountain).
A team of biologists convened by the World Wildlife Fund (WWF) developed a scheme that divided the world's land area into biogeographic realms (called "ecozones" in a BBC scheme), and these into ecoregions (Olson & Dinerstein, 1998, etc.). Each ecoregion is characterized by a main biome (also called major habitat type).
This classification is used to define the Global 200 list of ecoregions identified by the WWF as priorities for conservation.
For the terrestrial ecoregions, there is a specific EcoID, format XXnnNN (XX is the biogeographic realm, nn is the biome number, NN is the individual number).
The applicability of the realms scheme above - based on Udvardy (1975)—to most freshwater taxa is unresolved.
According to the WWF, the following are classified as freshwater biomes:
Biomes of the coastal and continental shelf areas (neritic zone):
Example:
Pruvot (1896) zones or "systems":
Longhurst (1998) biomes:
Other marine habitat types (not covered yet by the Global 200/WWF scheme):
Humans have altered global patterns of biodiversity and ecosystem processes. As a result, vegetation forms predicted by conventional biome systems can no longer be observed across much of Earth's land surface as they have been replaced by crop and rangelands or cities. Anthropogenic biomes provide an alternative view of the terrestrial biosphere based on global patterns of sustained direct human interaction with ecosystems, including agriculture, human settlements, urbanization, forestry and other uses of land. Anthropogenic biomes offer a new way forward in ecology and conservation by recognizing the irreversible coupling of human and ecological systems at global scales and moving us toward an understanding of how best to live in and manage our biosphere and the anthropogenic biomes we live in.
Major anthropogenic biomes:
The endolithic biome, consisting entirely of microscopic life in rock pores and cracks, kilometers beneath the surface, has only recently been discovered, and does not fit well into most classification schemes.

</doc>
<doc id="4805" url="https://en.wikipedia.org/wiki?curid=4805" title="Behavior">
Behavior

Behavior (American English) or behaviour (British English; see spelling differences) is the actions and mannerisms made by individuals, organisms, systems or artificial entities in conjunction with themselves or their environment, which includes the other systems or organisms around as well as the (inanimate) physical environment. It is the computed response of the system or organism to various stimuli or inputs, whether internal or external, conscious or subconscious, overt or covert, and voluntary or involuntary.
Taking a behavior informatics perspective, a behavior consists of behavior actor, operation, interactions, and their properties. A behavior can be represented as a behavior vector.
Although there is some disagreement as to how to precisely define behavior in a biological context, one common interpretation based on a meta-analysis of scientific literature states that "behavior is the internally coordinated responses (actions or inactions) of whole living organisms (individuals or groups) to internal and/or external stimuli".
A broader definition of behavior, applicable to plants and other organisms, is similar to the concept of phenotypic plasticity. It describes behavior as a response to an event or environment change during the course of the lifetime of an individual, differing from other physiological or biochemical changes that occur more rapidly, and excluding changes that are result of development (ontogeny).
Behaviors can be either innate or learned from the environment.
Behavior can be regarded as any action of an organism that changes its relationship to its environment. Behavior provides outputs from the organism to the environment.
Human behavior is believed to be influenced by the endocrine system and the nervous system. It is most commonly believed that complexity in the behavior of an organism is correlated to the complexity of its nervous system. Generally, organisms with more complex nervous systems have a greater capacity to learn new responses and thus adjust their behavior.
Ethology is the scientific and objective study of animal behavior, usually with a focus on behavior under natural conditions, and viewing behavior as an evolutionarily adaptive trait. behaviorism is a term that also describes the scientific and objective study of animal behavior, usually referring to measured responses to stimuli or trained behavioral responses in a laboratory context, without a particular emphasis on evolutionary adaptivity.
Consumer behavior refers to the processes consumers go through, and reactions they have towards products or services (Dowhan, 2013). It is to do with consumption, and the processes consumers go through around purchasing and consuming goods and services (Szwacka-Mokrzycka, 2015). Consumers recognise needs or wants, and go through a process to satisfy these needs. Consumer behavior is the process they go through as customers, which includes types of products purchased, amount spent, frequency of purchases and what influences them to make the purchase decision or not. There is a lot that influences consumer behavior, with contributions from both internal and external factors (Szwacka-Mokrzycka, 2015). Internal factors include attitudes, needs, motives, preferences and perceptual processes, whilst external factors include marketing activities, social and economic factors, and cultural aspects (Szwacka-Mokrzycka, 2015). Doctor Lars Perner of the University of Southern California claims that there are also physical factors that influence consumer behavior, for example if a consumer is hungry, then this physical feeling of hunger will influence them so that they go and purchase a sandwich to satisfy the hunger (Perner, 2008).
There is a model described by Lars Perner which illustrates the decision making process with regards to consumer behavior. It begins with recognition of a problem, the consumer recognises a need or want which has not been satisfied. This leads the consumer to search for information, if it is a low involvement product then the search will be internal, identifying alternatives purely from memory. If the product is high involvement then the search be more thorough, such as reading reviews or reports or asking friends. The consumer will then evaluate his or her alternatives, comparing price, quality, doing trade-offs between products and narrowing down the choice by eliminating the less appealing products until there is one left. After this has been identified, the consumer will purchase the product. Finally the consumer will evaluate the purchase decision, and the purchased product, bringing in factors such as value for money, quality of goods and purchase experience (Model taken from Perner, 2008). However, this logical process does not always happen this way, people are emotional and irrational creatures. People make decisions with emotion and then justify it with logic according to Robert Caladini Ph.D Psychology.
The 4 P's are a marketing tool, and stand for Price, Promotion, Product and Place or Product Placement (Clemons, 2008). Consumer behavior is influenced greatly by business to consumer marketing, so being a prominent marketing tool, the 4 P's will have an effect on consumer's behavior. The price of a good or service is largely determined by the market, as businesses will set their prices to be similar to that of other business so as to remain competitive whilst making a profit (Clemons, 2008). When market prices for a product are high, it will cause consumers to purchase less and use purchased goods for longer periods of time, meaning they are purchasing the product less often. Alternatively, when market prices for a product are low, consumers are more likely to purchase more of the product, and more often. The way that promotion influences consumer behavior has changed over time. In the past, large promotional campaigns and heavy advertising would convert into sales for a business, but nowadays businesses can have success on products with little or no advertising (Clemons, 2008). This is due to the Internet, and in particular social media. They rely on word of mouth from consumers using social media, and as products trend online, so sales increase as products effectively promote themselves (Clemons, 2008). Thus, promotion by businesses does not necessarily result in consumer behavior trending towards purchasing products. The way that product influences consumer behavior is through consumer willingness to pay, and consumer preferences (Clemons, 2008). This means that even if a company were to have a long history of products in the market, consumers will still pick a cheaper product over the company in question's product if it means they will pay less for something that is very similar (Clemons, 2008). This is due to consumer willingness to pay, or their willingness to part with their money they have earned. Product also influences consumer behavior through customer preferences. For example, take Pepsi vs Coca-Cola, a Pepsi-drinker is less likely to purchase Coca-Cola, even if it is cheaper and more convenient. This is due to the preference of the consumer, and no matter how hard the opposing company tries they will not be able to force the customer to change their mind. Product placement in the modern era has little influence on consumer behavior, due to the availability of goods online (Clemons, 2008). If a customer can purchase a good from the comfort of their home instead of purchasing in-store, then the placement of products is not going to influence their purchase decision.
In management, behaviors are associated with desired or undesired focuses. Managers generally note what the desired outcome is, but behavioral patterns can take over. These patterns are the reference to how often the desired behavior actually occurs. Before a behavior actually occurs, antecedents focus on the stimuli that influence the behavior that is about to happen. After the behavior occurs, consequences fall into place. Consequences consist of rewards or punishments.
Social behavior is behavior among two or more organisms within the same species, and encompasses any behavior in which one member affects the other. This is due to an interaction among those members. Social behavior can be seen as similar to an exchange of goods, with the expectation that when one gives, one will receive the same. This behavior can be affected by both the qualities of the individual and the environmental (situational) factors. Therefore, social behavior arises as a result of an interaction between the two—the organism and its environment. This means that, in regards to humans, social behavior can be determined by both the individual characteristics of the person, and the situation they are in.
Behavior informatics also called behavior computing, explores behavior intelligence and behavior insights from the informatics and computing perspectives.
Different from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations.
Health behavior refers to a person's beliefs and actions regarding their health and well-being. Health behaviors are direct factors in maintaining a healthy lifestyle. Health behaviors are influenced by the social, cultural and physical environments in which we live and work. They are shaped by individual choices and external constraints. Positive behaviors help promote health and prevent disease, while the opposite is true for risk behaviors. Health behaviors are early indicators of population health. Because of the time lag that often occurs between certain behaviors and the development of disease, these indicators may foreshadow the future burdens and benefits of health-risk and health-promoting behaviors. Health behaviors do not occur in isolation—they are influenced and constrained by social and cultural norms.
A variety of studies have examined the relationship between health behaviors and health outcomes (e.g., Blaxter 1990) and have demonstrated their role in both morbidity and mortality.
These studies have identified seven features of lifestyle which were associated with lower morbidity and higher subsequent long-term survival (Belloc and Breslow 1972):
Health behaviors impact upon individuals' quality of life, by delaying the onset of chronic disease and extending active lifespan. Smoking, alcohol consumption, diet, gaps in primary care services and low screening uptake are all significant determinants of poor health, and changing such behaviors should lead to improved health.
For example, in USA, Healthy People 2000, United States Department of Health and Human Services, lists increased physical activity, changes in nutrition and reductions in tobacco, alcohol and drug use as important for health promotion and disease prevention.
Any interventions done are matched with the needs of each individual in an ethical and respected manner. Health belief model encourages increasing individuals' perceived susceptibility to negative health outcomes and making individuals aware of the severity of such negative health behavior outcomes. E.g. through health promotion messages. In addition, the health belief model suggests the need to focus on the benefits of health behaviors and the fact that barriers to action are easily overcome. The theory of planned behavior suggests using persuasive messages for tackling behavioral beliefs to increase the readiness to perform a behavior, called "intentions". The theory of planned behavior advocates the need to tackle normative beliefs and control beliefs in any attempt to change behavior. Challenging the normative beliefs is not enough but to follow through the "intention" with self-efficacy from individual's mastery in problem solving and task completion is important to bring about a positive change. Self efficacy is often cemented through standard persuasive techniques.

</doc>
<doc id="4806" url="https://en.wikipedia.org/wiki?curid=4806" title="Battle of Marathon">
Battle of Marathon

The Battle of Marathon () took place in 490 BC during the first Persian invasion of Greece. It was fought between the citizens of Athens, aided by Plataea, and a Persian force commanded by Datis and Artaphernes. The battle was the culmination of the first attempt by Persia, under King Darius I, to subjugate Greece. The Greek army decisively defeated the more numerous Persians, marking a turning point in the Greco-Persian Wars.
The first Persian invasion was a response to Athenian involvement in the Ionian Revolt, when Athens and Eretria sent a force to support the cities of Ionia in their attempt to overthrow Persian rule. The Athenians and Eretrians had succeeded in capturing and burning Sardis, but they were then forced to retreat with heavy losses. In response to this raid, Darius swore to burn down Athens and Eretria. According to Herodotus, Darius had his bow brought to him and then shot an arrow "upwards towards heaven", saying as he did so: "Zeus, that it may be granted me to take vengeance upon the Athenians!" Herodotus further writes that Darius charged one of his servants to say "Master, remember the Athenians" three times before dinner each day.
At the time of the battle, Sparta and Athens were the two largest city-states in Greece. Once the Ionian revolt was finally crushed by the Persian victory at the Battle of Lade in 494 BC, Darius began plans to subjugate Greece. In 490 BC, he sent a naval task force under Datis and Artaphernes across the Aegean, to subjugate the Cyclades, and then to make punitive attacks on Athens and Eretria. Reaching Euboea in mid-summer after a successful campaign in the Aegean, the Persians proceeded to besiege and capture Eretria. The Persian force then sailed for Attica, landing in the bay near the town of Marathon. The Athenians, joined by a small force from Plataea, marched to Marathon, and succeeded in blocking the two exits from the plain of Marathon. The Athenians also sent a message to the Spartans asking for support. When the messenger arrived in Sparta, the Spartans were involved in a religious festival and gave this as a reason for not coming to help the Athenians.
The Athenians and their allies chose a location for the battle, with marshes and mountainous terrain, that prevented the Persian cavalry from joining the Persian infantry. Miltiades, the Athenian general, ordered a general attack against the Persian forces, composed primarily of missile troops. He reinforced his flanks, luring the Persians' best fighters into his centre. The inward wheeling flanks enveloped the Persians, routing them. The Persian army broke in panic towards their ships, and large numbers were slaughtered. The defeat at Marathon marked the end of the first Persian invasion of Greece, and the Persian force retreated to Asia. Darius then began raising a huge new army with which he meant to completely subjugate Greece; however, in 486 BC, his Egyptian subjects revolted, indefinitely postponing any Greek expedition. After Darius died, his son Xerxes I restarted the preparations for a second invasion of Greece, which finally began in 480 BC.
The Battle of Marathon was a watershed in the Greco-Persian wars, showing the Greeks that the Persians could be beaten; the eventual Greek triumph in these wars can be seen to have begun at Marathon. The battle also showed the Greeks that they were able to win battles without the Spartans, as they had heavily relied on Sparta previously. This victory was largely due to the Athenians, and Marathon raised Greek esteem of them. The following two hundred years saw the rise of the Classical Greek civilization, which has been enduringly influential in western society and so the Battle of Marathon is often seen as a pivotal moment in Mediterranean and European history.
The first Persian invasion of Greece had its immediate roots in the Ionian Revolt, the earliest phase of the Greco-Persian Wars. However, it was also the result of the longer-term interaction between the Greeks and Persians. In 500 BC the Persian Empire was still relatively young and highly expansionistic, but prone to revolts amongst its subject peoples. Moreover, the Persian King Darius was a usurper, and had spent considerable time extinguishing revolts against his rule. Even before the Ionian Revolt, Darius had begun to expand the empire into Europe, subjugating Thrace, and forcing Macedon to become a vassal of Persia. Attempts at further expansion into the politically fractious world of ancient Greece may have been inevitable. However, the Ionian Revolt had directly threatened the integrity of the Persian empire, and the states of mainland Greece remained a potential menace to its future stability. Darius thus resolved to subjugate and pacify Greece and the Aegean, and to punish those involved in the Ionian Revolt.
The Ionian Revolt had begun with an unsuccessful expedition against Naxos, a joint venture between the Persian satrap Artaphernes and the Milesian tyrant Aristagoras. In the aftermath, Artaphernes decided to remove Aristagoras from power, but before he could do so, Aristagoras abdicated, and declared Miletus a democracy. The other Ionian cities followed suit, ejecting their Persian-appointed tyrants, and declaring themselves democracies. Aristagoras then appealed to the states of mainland Greece for support, but only Athens and Eretria offered to send troops.
The involvement of Athens in the Ionian Revolt arose from a complex set of circumstances, beginning with the establishment of the Athenian Democracy in the late 6th century BC.
In 510 BC, with the aid of Cleomenes I, King of Sparta, the Athenian people had expelled Hippias, the tyrant ruler of Athens. With Hippias's father Peisistratus, the family had ruled for 36 out of the previous 50 years and fully intended to continue Hippias's rule. Hippias fled to Sardis to the court of the Persian satrap, Artaphernes and promised control of Athens to the Persians if they were to help restore him. In the meantime, Cleomenes helped install a pro-Spartan tyranny under Isagoras in Athens, in opposition to Cleisthenes, the leader of the traditionally powerful Alcmaeonidae family, who considered themselves the natural heirs to the rule of Athens. Cleisthenes, however, found himself being politically defeated by a coalition led by Isagoras and decided to change the rules of the game by appealing to the "demos" (the people), in effect making them a new faction in the political arena. This tactic succeeded, but the Spartan King, Cleomenes I, returned at the request of Isagoras and so Cleisthenes, the Alcmaeonids and other prominent Athenian families were exiled from Athens. When Isagoras attempted to create a narrow oligarchic government, the Athenian people, in a spontaneous and unprecedented move, expelled Cleomenes and Isagoras. Cleisthenes was thus restored to Athens (507 BC), and at breakneck speed began to reform the state with the aim of securing his position. The result was not actually a democracy or a real civic state, but he enabled the development of a fully democratic government, which would emerge in the next generation as the demos realized its power. The new-found freedom and self-governance of the Athenians meant that they were thereafter exceptionally hostile to the return of the tyranny of Hippias, or any form of outside subjugation, by Sparta, Persia, or anyone else.
Cleomenes was not pleased with events, and marched on Athens with the Spartan army. Cleomenes's attempts to restore Isagoras to Athens ended in a debacle, but fearing the worst, the Athenians had by this point already sent an embassy to Artaphernes in Sardis, to request aid from the Persian empire. Artaphernes requested that the Athenians give him an 'earth and water', a traditional token of submission, to which the Athenian ambassadors acquiesced. They were, however, severely censured for this when they returned to Athens. At some later point Cleomenes instigated a plot to restore Hippias to the rule of Athens. This failed and Hippias again fled to Sardis and tried to persuade the Persians to subjugate Athens. The Athenians dispatched ambassadors to Artaphernes to dissuade him from taking action, but Artaphernes merely instructed the Athenians to take Hippias back as tyrant. The Athenians indignantly declined, and instead resolved to open war with Persia. Having thus become the enemy of Persia, Athens was already in a position to support the Ionian cities when they began their revolt. The fact that the Ionian democracies were inspired by the example the Athenians had set no doubt further persuaded the Athenians to support the Ionian Revolt, especially since the cities of Ionia were originally Athenian colonies.
The Athenians and Eretrians sent a task force of 25 triremes to Asia Minor to aid the revolt. Whilst there, the Greek army surprised and outmaneuvered Artaphernes, marching to Sardis and burning the lower city. This was, however, as much as the Greeks achieved, and they were then repelled and pursued back to the coast by Persian horsemen, losing many men in the process. Despite the fact that their actions were ultimately fruitless, the Eretrians and in particular the Athenians had earned Darius's lasting enmity, and he vowed to punish both cities. The Persian naval victory at the Battle of Lade (494 BC) all but ended the Ionian Revolt, and by 493 BC, the last hold-outs were vanquished by the Persian fleet. The revolt was used as an opportunity by Darius to extend the empire's border to the islands of the eastern Aegean and the Propontis, which had not been part of the Persian dominions before. The pacification of Ionia allowed the Persians to begin planning their next moves; to extinguish the threat to the empire from Greece and to punish Athens and Eretria.
In 492 BC, after the Ionian Revolt had finally been crushed, Darius dispatched an to Greece under the command of his son-in-law, Mardonius. Mardonius re-subjugated Thrace and made Macedonia a fully subordinate part of the Persians; they had been vassals of the Persians since the late 6th century BC, but retained their general autonomy. Not long after however, his fleet became wrecked by a violent storm, which brought a premature end to the campaign.
However, in 490 BC, following the successes of the previous campaign, Darius decided to send a maritime expedition led by Artaphernes, (son of the satrap to whom Hippias had fled) and Datis, a Median admiral. Mardonius had been injured in the prior campaign and had fallen out of favor. The was intended to bring the Cyclades into the Persian empire, to punish Naxos (which had resisted a Persian assault in 499 BC) and then to head to Greece to force Eretria and Athens to submit to Darius or be destroyed. After island-hopping across the Aegean, including successfully attacking Naxos, the Persian task force arrived off Euboea in mid summer. The Persians then proceeded to besiege, capture and burn Eretria. They then headed south down the coast of Attica, en route to complete the final objective of the campaign—punish Athens.
The Persians sailed down the coast of Attica, and landed at the bay of Marathon, from Athens, on the advice of the exiled Athenian tyrant Hippias (who had accompanied the expedition). Under the guidance of Miltiades, the Athenian general with the greatest experience of fighting the Persians, the Athenian army marched quickly to block the two exits from the plain of Marathon, and prevent the Persians moving inland. At the same time, Athens's greatest runner, Pheidippides (or Philippides in some accounts) had been sent to Sparta to request that the Spartan army march to the aid of Athens. Pheidippides arrived during the festival of "Carneia", a sacrosanct period of peace, and was informed that the Spartan army could not march to war until the full moon rose; Athens could not expect reinforcement for at least ten days. The Athenians would have to hold out at Marathon for the time being, although they were reinforced by the full muster of 1,000 hoplites from the small city of Plataea, a gesture which did much to steady the nerves of the Athenians and won unending Athenian gratitude to Plataea.
For approximately five days the armies therefore confronted each other across the plain of Marathon in stalemate. The flanks of the Athenian camp were protected either by a grove of trees, or an "abbatis" of stakes (depending on the exact reading). Since every day brought the arrival of the Spartans closer, the delay worked in favor of the Athenians. There were ten Athenian "strategoi" (generals) at Marathon, elected by each of the ten tribes that the Athenians were divided into; Miltiades was one of these. In addition, in overall charge, was the War-Archon (polemarch), Callimachus, who had been elected by the whole citizen body. Herodotus suggests that command rotated between the "strategoi", each taking in turn a day to command the army. He further suggests that each "strategos", on his day in command, instead deferred to Miltiades. In Herodotus's account, Miltiades is keen to attack the Persians (despite knowing that the Spartans are coming to aid the Athenians), but strangely, chooses to wait until his actual day of command to attack. This passage is undoubtedly problematic; the Athenians had little to gain by attacking before the Spartans arrived, and there is no real evidence of this rotating generalship. There does, however, seem to have been a delay between the Athenian arrival at Marathon and the battle; Herodotus, who evidently believed that Miltiades was eager to attack, may have made a mistake while seeking to explain this delay.
As is discussed below, the reason for the delay was probably simply that neither the Athenians nor the Persians were willing to risk battle initially. This then raises the question of why the battle occurred when it did. Herodotus explicitly tells us that the Greeks attacked the Persians (and the other sources confirm this), but it is not clear why they did this before the arrival of the Spartans. There are two main theories to explain this.
The first theory is that the Persian cavalry left Marathon for an unspecified reason, and that the Greeks moved to take advantage of this by attacking. This theory is based on the absence of any mention of cavalry in Herodotus' account of the battle, and an entry in the "Suda" dictionary. The entry "χωρίς ἱππέων" ("without cavalry") is explained thus: The cavalry left. When Datis surrendered and was ready for retreat, the Ionians climbed the trees and gave the Athenians the signal that the cavalry had left. And when Miltiades realized that, he attacked and thus won. From there comes the above-mentioned quote, which is used when someone breaks ranks before battle. There are many variations of this theory, but perhaps the most prevalent is that the cavalry were completing the time-consuming process of re-embarking on the ships, and were to be sent by sea to attack (undefended) Athens in the rear, whilst the rest of the Persians pinned down the Athenian army at Marathon. This theory therefore utilises Herodotus' suggestion that after Marathon, the Persian army began to re-embark, intending to sail around Cape Sounion to attack Athens directly. Thus, this re-embarcation would have occurred "before" the battle (and indeed have triggered the battle).
The second theory is simply that the battle occurred because the Persians finally moved to attack the Athenians. Although this theory has the Persians moving to the "strategic" offensive, this can be reconciled with the traditional account of the Athenians attacking the Persians by assuming that, seeing the Persians advancing, the Athenians took the "tactical" offensive, and attacked them. Obviously, it cannot be firmly established which theory (if either) is correct. However, both theories imply that there was some kind of Persian activity which occurred on or about the fifth day which ultimately triggered the battle. It is also possible that both theories are correct: when the Persians sent the cavalry by ship to attack Athens, they simultaneously sent their infantry to attack at Marathon, triggering the Greek counterattack.
Herodotus mentions for several events a date in the lunisolar calendar, of which each Greek city-state used a variant. Astronomical computation allows us to derive an absolute date in the proleptic Julian calendar which is much used by historians as the chronological frame. Philipp August Böckh in 1855 concluded that the battle took place on September 12, 490 BC in the Julian calendar, and this is the conventionally accepted date. However, this depends on when exactly the Spartans held their festival and it is possible that the Spartan calendar was one month ahead of that of Athens. In that case the battle took place on August 12, 490 BC.
Herodotus does not give a figure for the size of the Athenian army. However, Cornelius Nepos, Pausanias and Plutarch all give the figure of 9,000 Athenians and 1,000 Plataeans; while Justin suggests that there were 10,000 Athenians and 1,000 Plataeans. These numbers are highly comparable to the number of troops Herodotus says that the Athenians and Plataeans sent to the Battle of Plataea 11 years later. Pausanias noticed on the monument to the battle the names of former slaves who were freed in exchange for military services. Modern historians generally accept these numbers as reasonable. The areas ruled by Athens (Attica) had a population of 315,000 at this time including slaves, which implies the full Athenian army at the times of both Marathon and Plataea numbered about 3% of the population.
According to Herodotus, the fleet sent by Darius consisted of 600 triremes. Herodotus does not estimate the size of the Persian army, only saying that they were a "large infantry that was well packed". Among ancient sources, the poet Simonides, another near-contemporary, says the campaign force numbered 200,000; while a later writer, the Roman Cornelius Nepos estimates 200,000 infantry and 10,000 cavalry, of which only 100,000 fought in the battle, while the rest were loaded into the fleet that was rounding Cape Sounion; Plutarch and Pausanias both independently give 300,000, as does the Suda dictionary. Plato and Lysias give 500,000; and Justinus 600,000.
Modern historians have proposed wide-ranging numbers for the infantry, from 20,000–100,000 with a consensus of perhaps 25,000; estimates for the cavalry are in the range of 1,000.
The fleet included various contingents from different parts of the Achaemenid Empire, particularly Ionians and Aeolians, although they are not mentioned as participating directly to the battle and may have remained on the ships:
Regarding the ethnicities involved in the battle, Herodotus specifically mentions the presence of the Persians and the Sakae at the center of the Achaemenid line:
From a strategic point of view, the Athenians had some disadvantages at Marathon. In order to face the Persians in battle, the Athenians had to summon all available hoplites; and even then they were still probably outnumbered at least 2 to 1. Furthermore, raising such a large army had denuded Athens of defenders, and thus any secondary attack in the Athenian rear would cut the army off from the city; and any direct attack on the city could not be defended against. Still further, defeat at Marathon would mean the complete defeat of Athens, since no other Athenian army existed. The Athenian strategy was therefore to keep the Persian army pinned down at Marathon, blocking both exits from the plain, and thus preventing themselves from being outmaneuvered. However, these disadvantages were balanced by some advantages. The Athenians initially had no need to seek battle, since they had managed to confine the Persians to the plain of Marathon. Furthermore, time worked in their favour, as every day brought the arrival of the Spartans closer. Having everything to lose by attacking, and much to gain by waiting, the Athenians remained on the defensive in the run up to the battle. Tactically, hoplites were vulnerable to attacks by cavalry, and since the Persians had substantial numbers of cavalry, this made any offensive maneuver by the Athenians even more of a risk, and thus reinforced the defensive strategy of the Athenians.
The Persian strategy, on the other hand, was probably principally determined by tactical considerations. The Persian infantry was evidently lightly armoured, and no match for hoplites in a head-on confrontation (as would be demonstrated at the later battles of Thermopylae and Plataea.) Since the Athenians seem to have taken up a strong defensive position at Marathon, the Persian hesitance was probably a reluctance to attack the Athenians head-on. The camp of the Athenians was located on a spur of mount Agrieliki next to the plain of Marathon; remains of its fortifications are still visible.
Whatever event eventually triggered the battle, it obviously altered the strategic or tactical balance sufficiently to induce the Athenians to attack the Persians. If the first theory is correct (see above), then the absence of cavalry removed the main Athenian tactical disadvantage, and the threat of being outflanked made it imperative to attack. Conversely, if the second theory is correct, then the Athenians were merely reacting to the Persians attacking them. Since the Persian force obviously contained a high proportion of missile troops, a static defensive position would have made little sense for the Athenians; the strength of the hoplite was in the melee, and the sooner that could be brought about, the better, from the Athenian point of view. If the second theory is correct, this raises the further question of why the Persians, having hesitated for several days, then attacked. There may have been several strategic reasons for this; perhaps they were aware (or suspected) that the Athenians were expecting reinforcements. Alternatively, they may have felt the need to force some kind of victory—they could hardly remain at Marathon indefinitely.
The distance between the two armies at the point of battle had narrowed to "a distance not less than 8 stadia" or about 1,500 meters. Miltiades ordered the two tribes forming the center of the Greek formation, the Leontis tribe led by Themistocles and the Antiochis tribe led by Aristides, to be arranged in the depth of four ranks while the rest of the tribes at their flanks were in ranks of eight. Some modern commentators have suggested this was a deliberate ploy to encourage a double envelopment of the Persian centre. However, this suggests a level of training that the Greeks are thought not to have possessed. There is little evidence for any such tactical thinking in Greek battles until Leuctra in 371 BC. It is therefore possible that this arrangement was made, perhaps at the last moment, so that the Athenian line was as long as the Persian line, and would not therefore be outflanked.
When the Athenian line was ready, according to one source, the simple signal to advance was given by Miltiades: "At them". Herodotus implies the Athenians ran the whole distance to the Persian lines, a feat under the weight of hoplite armory generally thought to be physically impossible. More likely, they marched until they reached the limit of the archers' effectiveness, the "beaten zone" (roughly 200 meters), and then broke into a run towards their enemy. Another possibility is that they ran "up to" the 200 meter-mark in broken ranks, and then reformed for the march into battle from there. Herodotus suggests that this was the first time a Greek army ran into battle in this way; this was probably because it was the first time that a Greek army had faced an enemy composed primarily of missile troops. All this was evidently much to the surprise of the Persians; "... in their minds they charged the Athenians with madness which must be fatal, seeing that they were few and yet were pressing forwards at a run, having neither cavalry nor archers". Indeed, based on their previous experience of the Greeks, the Persians might be excused for this; Herodotus tells us that the Athenians at Marathon were "first to endure looking at Median dress and men wearing it, for up until then just hearing the name of the Medes caused the Hellenes to panic". Passing through the hail of arrows launched by the Persian army, protected for the most part by their armour, the Greek line finally made contact with the enemy army. The Athenian wings quickly routed the inferior Persian levies on the flanks, before turning inwards to surround the Persian centre, which had been more successful against the thin Greek centre. The battle ended when the Persian centre then broke in panic towards their ships, pursued by the Greeks. Some, unaware of the local terrain, ran towards the swamps where unknown numbers drowned. The Athenians pursued the Persians back to their ships, and managed to capture seven ships, though the majority were able to launch successfully. Herodotus recounts the story that Cynaegirus, brother of the playwright Aeschylus, who was also among the fighters, charged into the sea, grabbed one Persian trireme, and started pulling it towards shore. A member of the crew saw him, cut off his hand, and Cynaegirus died.
Herodotus records that 6,400 Persian bodies were counted on the battlefield, and it is unknown how many more perished in the swamps. He also reported that the Athenians lost 192 men and the Plataeans 11. Among the dead were the war archon Callimachus and the general Stesilaos.
There are several explanations of the Greek success. Most scholars believe that the Greeks had better equipment and used superior tactics. According to Herodotus, the Greeks were better equipped. They did not use bronze upper body armour at this time, but that of leather or linen. The phalanx formation proved successful, because the hoplites had a long tradition in hand-to-hand combat, whereas the Persian soldiers were accustomed to a very different kind of conflict. At Marathon, the Athenians thinned their centre in order to make their army equal in length to the Persian army, not as a result of a tactical planning. It seems that the Persian centre tried to return, realizing that their wings had broken, and was caught in the flanks by the victorious Greek wings. Lazenby (1993) believes that the ultimate reason for the Greek success was the courage the Greeks displayed:
According to Vic Hurley, the Persian defeat is explained by the "complete failure ... to field a representative army", calling the battle the "most convincing" example of the fact that infantry-bowmen cannot defend any position while stationed in close-quarters and unsupported (i.e. by fortifications, or failing to support them by cavalry and chariots, as was the common Persian tactic).
In the immediate aftermath of the battle, Herodotus says that the Persian fleet sailed around Cape Sounion to attack Athens directly. As has been discussed above, some modern historians place this attempt just before the battle. Either way, the Athenians evidently realised that their city was still under threat, and marched as quickly as possible back to Athens.
The two tribes which had been in the centre of the Athenian line stayed to guard the battlefield under the command of Aristides. The Athenians arrived in time to prevent the Persians from securing a landing, and seeing that the opportunity was lost, the Persians turned about and returned to Asia. Connected with this episode, Herodotus recounts a rumour that this manoeuver by the Persians had been planned in conjunction with the Alcmaeonids, the prominent Athenian aristocratic family, and that a "shield-signal" had been given after the battle. Although many interpretations of this have been offered, it is impossible to tell whether this was true, and if so, what exactly the signal meant. On the next day, the Spartan army arrived at Marathon, having covered the in only three days. The Spartans toured the battlefield at Marathon, and agreed that the Athenians had won a great victory.
The Athenian and Plataean dead of Marathon were buried on the battlefield in two tumuli. On the tomb of the Athenians this epigram composed by Simonides was written:
Meanwhile, Darius began raising a huge new army with which he meant to completely subjugate Greece; however, in 486 BC, his Egyptian subjects revolted, indefinitely postponing any Greek expedition. Darius then died whilst preparing to march on Egypt, and the throne of Persia passed to his son Xerxes I. Xerxes crushed the Egyptian revolt, and very quickly restarted the preparations for the invasion of Greece. The epic second Persian invasion of Greece finally began in 480 BC, and the Persians met with initial success at the battles of Thermopylae and Artemisium. However, defeat at the Battle of Salamis would be the turning point in the campaign, and the next year the expedition was ended by the decisive Greek victory at the Battle of Plataea.
The defeat at Marathon barely touched the vast resources of the Persian empire, yet for the Greeks it was an enormously significant victory. It was the first time the Greeks had beaten the Persians, proving that the Persians were not invincible, and that resistance, rather than subjugation, was possible.
The battle was a defining moment for the young Athenian democracy, showing what might be achieved through unity and self-belief; indeed, the battle effectively marks the start of a "golden age" for Athens. This was also applicable to Greece as a whole; "their victory endowed the Greeks with a faith in their destiny that was to endure for three centuries, during which western culture was born". John Stuart Mill's famous opinion was that "the Battle of Marathon, even as an event in British history, is more important than the Battle of Hastings". According to Isaac Asimov,"if the Athenians had lost in Marathon, . . . Greece might have never gone to develop the peak of its civilization, a peak whose fruits we moderns have inherited."
It seems that the Athenian playwright Aeschylus considered his participation at Marathon to be his greatest achievement in life (rather than his plays) since on his gravestone there was the following epigram:
Militarily, a major lesson for the Greeks was the potential of the hoplite phalanx. This style had developed during internecine warfare amongst the Greeks; since each city-state fought in the same way, the advantages and disadvantages of the hoplite phalanx had not been obvious. Marathon was the first time a phalanx faced more lightly armed troops, and revealed how effective the hoplites could be in battle. The phalanx formation was still vulnerable to cavalry (the cause of much caution by the Greek forces at the Battle of Plataea), but used in the right circumstances, it was now shown to be a potentially devastating weapon.
The main source for the Greco-Persian Wars is the Greek historian Herodotus. Herodotus, who has been called the "Father of History", was born in 484 BC in Halicarnassus, Asia Minor (then under Persian overlordship). He wrote his "Enquiries" (Greek – "Historiai"; English – "(The) Histories") around 440–430 BC, trying to trace the origins of the Greco-Persian Wars, which would still have been relatively recent history (the wars finally ended in 450 BC). Herodotus's approach was entirely novel, and at least in Western society, he does seem to have invented "history" as we know it. As Holland has it: "For the first time, a chronicler set himself to trace the origins of a conflict not to a past so remote so as to be utterly fabulous, nor to the whims and wishes of some god, nor to a people's claim to manifest destiny, but rather explanations he could verify personally."
Some subsequent ancient historians, despite following in his footsteps, criticised Herodotus, starting with Thucydides. Nevertheless, Thucydides chose to begin his history where Herodotus left off (at the Siege of Sestos), and may therefore have felt that Herodotus's history was accurate enough not to need re-writing or correcting. Plutarch criticised Herodotus in his essay "On the malice of Herodotus", describing Herodotus as "Philobarbaros" (barbarian-lover), for not being pro-Greek enough, which suggests that Herodotus might actually have done a reasonable job of being even-handed. A negative view of Herodotus was passed on to Renaissance Europe, though he remained well read. However, since the 19th century his reputation has been dramatically rehabilitated by archaeological finds which have repeatedly confirmed his version of events. The prevailing modern view is that Herodotus generally did a remarkable job in his "Historiai", but that some of his specific details (particularly troop numbers and dates) should be viewed with skepticism. Nevertheless, there are still some historians who believe Herodotus made up much of his story.
The Sicilian historian Diodorus Siculus, writing in the 1st century BC in his "Bibliotheca Historica", also provides an account of the Greco-Persian wars, partially derived from the earlier Greek historian Ephorus. This account is fairly consistent with Herodotus's. The Greco-Persian wars are also described in less detail by a number of other ancient historians including Plutarch, Ctesias of Cnidus, and are alluded by other authors, such as the playwright Aeschylus. Archaeological evidence, such as the Serpent Column, also supports some of Herodotus's specific claims.
The most famous legend associated with Marathon is that of the runner Pheidippides (or Philippides) bringing news to Athens of the battle, which is described below.
Pheidippides' run to Sparta to bring aid has other legends associated with it. Herodotus mentions that Pheidippides was visited by the god Pan on his way to Sparta (or perhaps on his return journey). Pan asked why the Athenians did not honor him and the awed Pheidippides promised that they would do so from then on. The god apparently felt that the promise would be kept, so he appeared in battle and at the crucial moment he instilled the Persians with his own brand of fear, the mindless, frenzied fear that bore his name: "panic". After the battle, a sacred precinct was established for Pan in a grotto on the north slope of the Acropolis, and a sacrifice was annually offered.
Similarly, after the victory the festival of the "Agroteras Thysia" ("sacrifice to the Agrotéra") was held at Agrae near Athens, in honor of Artemis Agrotera ("Artemis the Huntress"). This was in fulfillment of a vow made by the city before the battle, to offer in sacrifice a number of goats equal to that of the Persians slain in the conflict. The number was so great, it was decided to offer 500 goats yearly until the number was filled. Xenophon notes that at his time, 90 years after the battle, goats were still offered yearly.
Plutarch mentions that the Athenians saw the phantom of King Theseus, the mythical hero of Athens, leading the army in full battle gear in the charge against the Persians, and indeed he was depicted in the mural of the Stoa Poikile fighting for the Athenians, along with the twelve Olympian gods and other heroes. Pausanias also tells us that: They say too that there chanced to be present in the battle a man of rustic appearance and dress. Having slaughtered many of the foreigners with a plough he was seen no more after the engagement. When the Athenians made enquiries at the oracle, the god merely ordered them to honor Echetlaeus ("he of the Plough-tail") as a hero.
Another tale from the conflict is of the dog of Marathon. Aelian relates that one hoplite brought his dog to the Athenian encampment. The dog followed his master to battle and attacked the Persians at his master's side. He also informs us that this dog is depicted in the mural of the Stoa Poikile.
According to Herodotus, an Athenian runner named Pheidippides was sent to run from Athens to Sparta to ask for assistance before the battle. He ran a distance of over 225 kilometers (140 miles), arriving in Sparta the day after he left. Then, following the battle, the Athenian army marched the 40 kilometers (25 miles) or so back to Athens at a very high pace (considering the quantity of armour, and the fatigue after the battle), in order to head off the Persian force sailing around Cape Sounion. They arrived back in the late afternoon, in time to see the Persian ships turn away from Athens, thus completing the Athenian victory.
Later, in popular imagination, these two events were conflated, leading to a legendary but inaccurate version of events. This myth has Pheidippides running from Marathon to Athens after the battle, to announce the Greek victory with the word "nenikēkamen!" (Attic: ; we've won!), whereupon he promptly died of exhaustion. Most accounts incorrectly attribute this story to Herodotus; actually, the story first appears in Plutarch's "On the Glory of Athens" in the 1st century AD, who quotes from Heracleides of Pontus's lost work, giving the runner's name as either Thersipus of Erchius or Eucles. Lucian of Samosata (2nd century AD) gives the same story but names the runner Philippides (not Pheidippides). In some medieval codices of Herodotus, the name of the runner between Athens and Sparta before the battle is given as Philippides, and this name is also preferred in a few modern editions.
When the idea of a modern Olympics became a reality at the end of the 19th century, the initiators and organizers were looking for a great popularizing event, recalling the ancient glory of Greece. The idea of organizing a "marathon race" came from Michel Bréal, who wanted the event to feature in the first modern Olympic Games in 1896 in Athens. This idea was heavily supported by Pierre de Coubertin, the founder of the modern Olympics, as well as the Greeks. This would echo the legendary version of events, with the competitors running from Marathon to Athens. So popular was this event that it quickly caught on, becoming a fixture at the Olympic games, with major cities staging their own annual events. The distance eventually became fixed at 26 miles 385 yards, or 42.195 km, though for the first years it was variable, being around —the approximate distance from Marathon to Athens.

</doc>
<doc id="4810" url="https://en.wikipedia.org/wiki?curid=4810" title="Balance of trade">
Balance of trade

The balance of trade, commercial balance, or net exports (sometimes symbolized as NX), is the difference between the monetary value of a nation's exports and imports over a certain time period. Sometimes a distinction is made between a balance of trade for goods versus one for services. The balance of trade measures a flow of exports and imports over a given period of time. The notion of the balance of trade does not mean that exports and imports are "in balance" with each other.
If a country exports a greater value than it imports, it has a trade surplus or positive trade balance, and conversely, if a country imports a greater value than it exports, it has a trade deficit or negative trade balance. As of 2016, about 60 out of 200 countries have a trade surplus. The notion that bilateral trade deficits are bad in and of themselves is overwhelmingly rejected by trade experts and economists.
The balance of trade forms part of the current account, which includes other transactions such as income from the net international investment position as well as international aid. If the current account is in surplus, the country's net international asset position increases correspondingly. Equally, a deficit decreases the net international asset position.
The trade balance is identical to the difference between a country's output and its domestic demand (the difference between what goods a country produces and how many goods it buys from abroad; this does not include money re-spent on foreign stock, nor does it factor in the concept of importing goods to produce for the domestic market).
Measuring the balance of trade can be problematic because of problems with recording and collecting data. As an illustration of this problem, when official data for all the world's countries are added up, exports exceed imports by almost 1%; it appears the world is running a positive balance of trade with itself. This cannot be true, because all transactions involve an equal credit or debit in the account of each nation. The discrepancy is widely believed to be explained by transactions intended to launder money or evade taxes, smuggling and other visibility problems. While the accuracy of developing countries statistics would be suspicious, most of the discrepancy actually occurs between developed countries of trusted statistics,
Factors that can affect the balance of trade include:
In addition, the trade balance is likely to differ across the business cycle. In export-led growth (such as oil and early industrial goods), the balance of trade will shift towards exports during an economic expansion. However, with domestic demand-led growth (as in the United States and Australia) the trade balance will shift towards imports at the same stage in the business cycle.
The monetary balance of trade is different from the physical balance of trade (which is expressed in amount of raw materials, known also as Total Material Consumption). Developed countries usually import a substantial amount of raw materials from developing countries. Typically, these imported materials are transformed into finished products and might be exported after adding value. Financial trade balance statistics conceal material flow. Most developed countries have a large physical trade deficit because they consume more raw materials than they produce. Many civil society organisations claim this imbalance is predatory and campaign for ecological debt repayment.
Many countries in early modern Europe adopted a policy of mercantilism, which theorized that a trade surplus was beneficial to a country, among other elements such as colonialism and trade barriers with other countries and their colonies. (Bullionism was an early philosophy supporting mercantilism.)
The practices and abuses of mercantilism led the natural resources and cash crops of British North America to be exported in exchange for finished goods from Great Britain, a factor leading to the American Revolution. An early statement appeared in "Discourse of the Common Wealth of this Realm of England", 1549: "We must always take heed that we buy no more from strangers than we sell them, for so should we impoverish ourselves and enrich them." Similarly, a systematic and coherent explanation of balance of trade was made public through Thomas Mun's 1630 "England's treasure by foreign trade, or, The balance of our foreign trade is the rule of our treasure"
Since the mid-1980s, the United States has had a growing deficit in tradeable goods, especially with Asian nations (China and Japan) which now hold large sums of U.S debt that has in part funded the consumption. The U.S. has a trade surplus with nations such as Australia. The issue of trade deficits can be complex. Trade deficits generated in tradeable goods such as manufactured goods or software may impact domestic employment to different degrees than do trade deficits in raw materials.
Economies that have savings surpluses, such as Japan and Germany, typically run trade surpluses. China, a high-growth economy, has tended to run trade surpluses. A higher savings rate generally corresponds to a trade surplus. Correspondingly, the U.S. with its lower savings rate has tended to run high trade deficits, especially with Asian nations.
Some have said that China pursues a mercantilist economic policy. Russia pursues a policy based on protectionism, according to which international trade is not a "win-win" game but a zero-sum game: surplus countries get richer at the expense of deficit countries.
In March 2019, Armenia recorded a trade deficit of US$203.9 million. For the last two decades, the Armenian trade balance has been negative, reaching an all-time high of –33.98 USD million in August 2003. The reason for the trade deficit is that Armenia's foreign trade is limited by its landlocked location and border disputes with Turkey and Azerbaijan, to the west and east respectively. The situation results in the country's typically reporting large trade deficits.
The notion that bilateral trade deficits are bad in and of themselves is overwhelmingly rejected by trade experts and economists. According to the IMF trade deficits can cause a balance of payments problem, which can affect foreign exchange shortages and hurt countries. On the other hand, Joseph Stiglitz points out that countries running surpluses exert a "negative externality" on trading partners, and pose a threat to global prosperity, far more than those in deficit. Ben Bernanke argues that "persistent imbalances within the euro zone are... unhealthy, as they lead to financial imbalances as well as to unbalanced growth. The fact that Germany is selling so much more than it is buying redirects demand from its neighbors (as well as from other countries around the world), reducing output and employment outside Germany."
A 2018 National Bureau of Economic Research paper by economists at the International Monetary Fund and University of California, Berkeley, found in a study of 151 countries over 1963-2014 that the imposition of tariffs had little effect on the trade balance.
In the last few years of his life, John Maynard Keynes was much preoccupied with the question of balance in international trade. He was the leader of the British delegation to the United Nations Monetary and Financial Conference in 1944 that established the Bretton Woods system of international currency management.
He was the principal author of a proposal – the so-called Keynes Plan – for an International Clearing Union. The two governing principles of the plan were that the problem of settling outstanding balances should be solved by 'creating' additional 'international money', and that debtor and creditor should be treated almost alike as disturbers of equilibrium. In the event, though, the plans were rejected, in part because "American opinion was naturally reluctant to accept the principle of equality of treatment so novel in debtor-creditor relationships".
The new system is not founded on free-trade (liberalisation of foreign trade) but rather on the regulation of international trade, in order to eliminate trade imbalances: the nations with a surplus would have a powerful incentive to get rid of it, and in doing so they would automatically clear other nations deficits. He proposed a global bank that would issue its own currency – the bancor – which was exchangeable with national currencies at fixed rates of exchange and would become the unit of account between nations, which means it would be used to measure a country's trade deficit or trade surplus. Every country would have an overdraft facility in its bancor account at the International Clearing Union. He pointed out that surpluses lead to weak global aggregate demand – countries running surpluses exert a "negative externality" on trading partners, and posed far more than those in deficit, a threat to global prosperity.
In ""National Self-Sufficiency" The Yale Review, Vol. 22, no. 4 (June 1933)", he already highlighted the problems created by free trade.
His view, supported by many economists and commentators at the time, was that creditor nations may be just as responsible as debtor nations for disequilibrium in exchanges and that both should be under an obligation to bring trade back into a state of balance. Failure for them to do so could have serious consequences. In the words of Geoffrey Crowther, then editor of The Economist, "If the economic relationships between nations are not, by one means or another, brought fairly close to balance, then there is no set of financial arrangements that can rescue the world from the impoverishing results of chaos."
These ideas were informed by events prior to the Great Depression when – in the opinion of Keynes and others – international lending, primarily by the U.S., exceeded the capacity of sound investment and so got diverted into non-productive and speculative uses, which in turn invited default and a sudden stop to the process of lending.
Influenced by Keynes, economics texts in the immediate post-war period put a significant emphasis on balance in trade. For example, the second edition of the popular introductory textbook, "An Outline of Money", devoted the last three of its ten chapters to questions of foreign exchange management and in particular the 'problem of balance'. However, in more recent years, since the end of the Bretton Woods system in 1971, with the increasing influence of monetarist schools of thought in the 1980s, and particularly in the face of large sustained trade imbalances, these concerns – and particularly concerns about the destabilising effects of large trade surpluses – have largely disappeared from mainstream economics discourse and Keynes' insights have slipped from view. They are receiving some attention again in the wake of the financial crisis of 2007–08.
Prior to 20th-century monetarist theory, the 19th-century economist and philosopher Frédéric Bastiat expressed the idea that trade deficits actually were a manifestation of profit, rather than a loss. He proposed as an example to suppose that he, a Frenchman, exported French wine and imported British coal, turning a profit. He supposed he was in France and sent a cask of wine which was worth 50 francs to England. The customhouse would record an export of 50 francs. If in England, the wine sold for 70 francs (or the pound equivalent), which he then used to buy coal, which he imported into France, and was found to be worth 90 francs in France, he would have made a profit of 40 francs. But the customhouse would say that the value of imports exceeded that of exports and was trade deficit against the ledger of France.
By "reductio ad absurdum", Bastiat argued that the national trade deficit was an indicator of a successful economy, rather than a failing one. Bastiat predicted that a successful, growing economy would result in greater trade deficits, and an unsuccessful, shrinking economy would result in lower trade deficits. This was later, in the 20th century, echoed by economist Milton Friedman.
In the 1980s, Milton Friedman, a Nobel Memorial Prize-winning economist and a proponent of monetarism, contended that some of the concerns of trade deficits are unfair criticisms in an attempt to push macroeconomic policies favorable to exporting industries.
Friedman argued that trade deficits are not necessarily important, as high exports raise the value of the currency, reducing aforementioned exports, and vice versa for imports, thus naturally removing trade deficits "not due to investment". Since 1971, when the Nixon administration decided to abolish fixed exchange rates, America's Current Account accumulated trade deficits have totaled $7.75 trillion as of 2010. This deficit exists as it is matched by investment coming into the United States – purely by the definition of the balance of payments, any current account deficit that exists is matched by an inflow of foreign investment.
In the late 1970s and early 1980s, the U.S. had experienced high inflation and Friedman's policy positions tended to defend the stronger dollar at that time. He stated his belief that these trade deficits were not necessarily harmful to the economy at the time since the currency comes back to the country (country A sells to country B, country B sells to country C who buys from country A, but the trade deficit only includes A and B). However, it may be in one form or another including the possible tradeoff of foreign control of assets. In his view, the "worst-case scenario" of the currency never returning to the country of origin was actually the best possible outcome: the country actually purchased its goods by exchanging them for pieces of cheaply made paper. As Friedman put it, this would be the same result as if the exporting country burned the dollars it earned, never returning it to market circulation.
This position is a more refined version of the theorem first discovered by David Hume. Hume argued that England could not permanently gain from exports, because hoarding gold (i.e., currency) would make gold more plentiful in England; therefore, the prices of English goods would rise, making them less attractive exports and making foreign goods more attractive imports. In this way, countries' trade balances would balance out.
Friedman presented his analysis of the balance of trade in "Free to Choose", widely considered his most significant popular work.
Exports directly increase and imports directly reduce a nation's balance of trade (i.e. net exports). A trade surplus is a positive net balance of trade, and a trade deficit is a negative net balance of trade. Due to the balance of trade being explicitly added to the calculation of the nation's gross domestic product using the expenditure method of calculating gross domestic product (i.e. GDP), trade surpluses are contributions and trade deficits are "drags" upon their nation's GDP; however, foreign made goods sold (e.g., retail) contribute to total GDP.

</doc>
<doc id="4816" url="https://en.wikipedia.org/wiki?curid=4816" title="Biosphere">
Biosphere

The biosphere (from Greek βίος "bíos" "life" and σφαῖρα "sphaira" "sphere"), also known as the ecosphere (from Greek οἶκος "oîkos" "environment" and σφαῖρα), is the worldwide sum of all ecosystems. It can also be termed the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior 
of the Earth), and largely self-regulating. By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere. The biosphere is postulated to have evolved, beginning with a process of biopoiesis (life created naturally from non-living matter, such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago.
In a general sense, biospheres are any closed, self-regulating systems containing ecosystems. This includes artificial biospheres such as Biosphere 2 and BIOS-3, and potentially ones on other planets or moons.
The term "biosphere" was coined by geologist Eduard Suess in 1875, which he defined as the place on Earth's surface where life dwells.
While the concept has a geological origin, it is an indication of the effect of both Charles Darwin and Matthew F. Maury on the Earth sciences. The biosphere's ecological context comes from the 1920s (see Vladimir I. Vernadsky), preceding the 1935 introduction of the term "ecosystem" by Sir Arthur Tansley (see ecology history). Vernadsky defined ecology as the science of the biosphere. It is an interdisciplinary concept for integrating astronomy, geophysics, meteorology, biogeography, evolution, geology, geochemistry, hydrology and, generally speaking, all life and Earth sciences.
Geochemists define the biosphere as being the total sum of living organisms (the "biomass" or "biota" as referred to by biologists and ecologists). In this sense, the biosphere is but one of four separate components of the geochemical model, the other three being "geosphere", "hydrosphere", and "atmosphere". When these four component spheres are combined into one system, it is known as the Ecosphere. This term was coined during the 1960s and encompasses both biological and physical components of the planet.
The Second International Conference on Closed Life Systems defined "biospherics" as the science and technology of analogs and models of Earth's biosphere; i.e., artificial Earth-like biospheres. Others may include the creation of artificial non-Earth biospheres—for example, human-centered biospheres or a native Martian biosphere—as part of the topic of biospherics.
The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia. More recently, in 2015, "remains of biotic life" were found in 4.1 billion-year-old rocks in Western Australia. In 2017, putative fossilized microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on earth, suggesting "an almost instantaneous emergence of life" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. According to biologist Stephen Blair Hedges, "If life arose relatively quickly on Earth ... then it could be common in the universe."
Every part of the planet, from the polar ice caps to the equator, features life of some kind. Recent advances in microbiology have demonstrated that microbes live deep beneath the Earth's terrestrial surface, and that the total mass of microbial life in so-called "uninhabitable zones" may, in biomass, exceed all animal and plant life on the surface. The actual thickness of the biosphere on earth is difficult to measure. Birds typically fly at altitudes as high as and fish live as much as underwater in the Puerto Rico Trench.
There are more extreme examples for life on the planet: Rüppell's vulture has been found at altitudes of ; bar-headed geese migrate at altitudes of at least ; yaks live at elevations as high as above sea level; mountain goats live up to . Herbivorous animals at these elevations depend on lichens, grasses, and herbs.
Life forms live in every part of the Earth's biosphere, including soil, hot springs, inside rocks at least deep underground, the deepest parts of the ocean, and at least high in the atmosphere. Microorganisms, under certain test conditions, have been observed to survive the vacuum of outer space. The total amount of soil and subsurface bacterial carbon is estimated as 5 × 10 g, or the "weight of the United Kingdom". The mass of prokaryote microorganisms—which includes bacteria and archaea, but not the nucleated eukaryote microorganisms—may be as much as 0.8 trillion tons of carbon (of the total biosphere mass, estimated at between 1 and 4 trillion tons). Barophilic marine microbes have been found at more than a depth of in the Mariana Trench, the deepest spot in the Earth's oceans. In fact, single-celled life forms have been found in the deepest part of the Mariana Trench, by the Challenger Deep, at depths of . Other researchers reported related studies that microorganisms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. Culturable thermophilic microbes have been extracted from cores drilled more than into the Earth's crust in Sweden, from rocks between . Temperature increases with increasing depth into the Earth's crust. The rate at which the temperature increases depends on many factors, including type of crust (continental vs. oceanic), rock type, geographic location, etc. The greatest known temperature at which microbial life can exist is ("Methanopyrus kandleri" Strain 116), and it is likely that the limit of life in the "deep biosphere" is defined by temperature rather than absolute depth. On 20 August 2014, scientists confirmed the existence of microorganisms living below the ice of Antarctica. According to one researcher, "You can find microbes everywhere – they're extremely adaptable to conditions, and survive wherever they are."
Our biosphere is divided into a number of biomes, inhabited by fairly similar flora and fauna. On land, biomes are separated primarily by latitude. Terrestrial biomes lying within the Arctic and Antarctic Circles are relatively barren of plant and animal life, while most of the more populous biomes lie near the equator. 
Experimental biospheres, also called closed ecological systems, have been created to study ecosystems and the potential for supporting life outside the Earth. These include spacecraft and the following terrestrial laboratories:
No biospheres have been detected beyond the Earth; therefore, the existence of extraterrestrial biospheres remains hypothetical. The rare Earth hypothesis suggests they should be very rare, save ones composed of microbial life only. On the other hand, Earth analogs may be quite numerous, at least in the Milky Way galaxy, given the large number of planets. Three of the planets discovered orbiting TRAPPIST-1 could possibly contain biospheres. Given limited understanding of abiogenesis, it is currently unknown what percentage of these planets actually develop biospheres.
Based on observations by the Kepler Space Telescope team, it has been calculated that provided the probability of abiogenesis is higher than 1 to 1000, the closest alien biosphere should be within 100 light-years from the Earth.
It is also possible that artificial biospheres will be created in the future, for example with the terraforming of Mars.

</doc>
<doc id="4817" url="https://en.wikipedia.org/wiki?curid=4817" title="Biological membrane">
Biological membrane

A biological membrane, biomembrane or cell membrane is a selectively permeable membrane that separates cell from the external environment or creates intracellular compartments. Biological membranes, in the form of eukaryotic cell membranes, consist of a phospholipid bilayer with embedded, integral and peripheral proteins used in communication and transportation of chemicals and ions. The bulk of lipid in a cell membrane provides a fluid matrix for proteins to rotate and laterally diffuse for physiological functioning. Proteins are adapted to high membrane fluidity environment of lipid bilayer with the presence of an annular lipid shell, consisting of lipid molecules bound tightly to surface of integral membrane proteins. The cell membranes are different from the isolating tissues formed by layers of cells, such as mucous membranes, basement membranes, and serous membranes.
The lipid bilayer consists of two layers- an outer leaflet and an inner leaflet. The components of bilayers are distributed unequally between the two surfaces to create asymmetry between the outer and inner surfaces. This asymmetric organization is important for cell functions such as cell signaling. The asymmetry of the biological membrane reflects the different functions of the two leaflets of the membrane. As seen in the fluid membrane model of the phospholipid bilayer, the outer leaflet and inner leaflet of the membrane are asymmetrical in their composition. Certain proteins and lipids rest only on one surface of the membrane and not the other.
• Both the plasma membrane and internal membranes have cytosolic and exoplasmic faces
• This orientation is maintained during membrane trafficking – proteins, lipids, glycoconjugates facing the lumen of the ER and Golgi get expressed on the extracellular side of the plasma membrane. In eucaryotic cells, new phospholipids are manufactured by enzymes bound to the part of the endoplasmic reticulum membrane that faces the cytosol. These enzymes, which use free fatty acids as substrates, deposit all newly made phospholipids into the cytosolic half of the bilayer. To enable the membrane as a whole to grow evenly, half of the new phospholipid molecules then have to be transferred to the opposite monolayer. This transfer is catalyzed by enzymes called flippases. In the plasma membrane, flippases transfer specific phospholipids selectively, so that different types become concentrated in each monolayer.
Using selective flippases is not the only way to produce asymmetry in lipid bilayers, however. In particular, a different mechanism operates for glycolipids—the lipids that show the most striking and consistent asymmetric distribution in animal cells.
The biological membrane is made up of lipids with hydrophobic tails and hydrophilic heads. The hydrophobic tails are hydrocarbon tails whose length and saturation is important in characterizing the cell. Lipid rafts occur when lipid species and proteins aggregate in domains in the membrane. These help organize membrane components into localized areas that are involved in specific processes, such as signal transduction.
Red blood cells, or erythrocytes, have a unique lipid composition. The bilayer of red blood cells is composed of cholesterol and phospholipids in equal proportions by weight. Erythrocyte membrane plays a crucial role in blood clotting. In the bilayer of red blood cells is phosphatidylserine. This is usually in the cytoplasmic side of the membrane. However, it is flipped to the outer membrane to be used during blood clotting.
Phospholipid bilayers contain different proteins. These membrane proteins have various functions and characteristics and catalyze different chemical reactions. Integral proteins span the membranes with different domains on either side. Integral proteins hold strong association with the lipid bilayer and cannot easily become detached. They will dissociate only with chemical treatment that breaks the membrane. Peripheral proteins are unlike integral proteins in that they hold weak interactions with the surface of the bilayer and can easily become dissociated from the membrane. Peripheral proteins are located on only one face of a membrane and create membrane asymmetry.
Oligosaccharides are sugar containing polymers. In the membrane, they can be covalently bound to lipids to form glycolipids or covalently bound to proteins to form glycoproteins. Membranes contain sugar-containing lipid molecules known as glycolipids. In the bilayer, the sugar groups of glycolipids are exposed at the cell surface, where they can form hydrogen bonds. Glycolipids provide the most extreme example of asymmetry in the lipid bilayer. Glycolipids perform a vast number of functions in the biological membrane that are mainly communicative, including cell recognition and cell-cell adhesion. Glycoproteins are integral proteins. They play an important role in the immune response and protection.
The phospholipid bilayer is formed due to the aggregation of membrane lipids in aqueous solutions. Aggregation is caused by the hydrophobic effect, where hydrophobic ends come into contact with each other and are sequestered away from water. This arrangement maximises hydrogen bonding between hydrophilic heads and water while minimising unfavorable contact between hydrophobic tails and water. The increase in available hydrogen bonding increases the entropy of the system, creating a spontaneous process.
Biological molecules are amphiphilic or amphipathic, i.e. are simultaneously hydrophobic and hydrophilic. The phospholipid bilayer contains charged hydrophilic headgroups, which interact with polar water. The layers also contain hydrophobic tails, which meet with the hydrophobic tails of the complementary layer. The hydrophobic tails are usually fatty acids that differ in lengths. The interactions of lipids, especially the hydrophobic tails, determine the lipid bilayer physical properties such as fluidity.
Membranes in cells typically define enclosed spaces or compartments in which cells may maintain a chemical or biochemical environment that differs from the outside. For example, the membrane around peroxisomes shields the rest of the cell from peroxides, chemicals that can be toxic to the cell, and the cell membrane separates a cell from its surrounding medium. Peroxisomes are one form of vacuole found in the cell that contain by-products of chemical reactions within the cell. Most organelles are defined by such membranes, and are called "membrane-bound" organelles.
Probably the most important feature of a biomembrane is that it is a selectively permeable structure. This means that the size, charge, and other chemical properties of the atoms and molecules attempting to cross it will determine whether they succeed in doing so. Selective permeability is essential for effective separation of a cell or organelle from its surroundings. Biological membranes also have certain mechanical or elastic properties that allow them to change shape and move as required.
Generally, small hydrophobic molecules can readily cross phospholipid bilayers by simple diffusion.
Particles that are required for cellular function but are unable to diffuse freely across a membrane enter through a membrane transport protein or are taken in by means of endocytosis, where the membrane allows for a vacuole to join onto it and push its contents into the cell. Many types of specialized plasma membranes can separate cell from external environment: apical, basolateral, presynaptic and postsynaptic ones, membranes of flagella, cilia, microvillus, filopodia and lamellipodia, the sarcolemma of muscle cells, as well as specialized myelin and dendritic spine membranes of neurons. Plasma membranes can also form different types of "supramembrane" structures such as caveolae, postsynaptic density, podosome, invadopodium, desmosome, hemidesmosome, focal adhesion, and cell junctions. These types of membranes differ in lipid and protein composition.
Distinct types of membranes also create intracellular organelles: endosome; smooth and rough endoplasmic reticulum; sarcoplasmic reticulum; Golgi apparatus; lysosome; mitochondrion (inner and outer membranes); nucleus (inner and outer membranes); peroxisome; vacuole; cytoplasmic granules; cell vesicles (phagosome, autophagosome, clathrin-coated vesicles, COPI-coated and COPII-coated vesicles) and secretory vesicles (including synaptosome, acrosomes, melanosomes, and chromaffin granules).
Different types of biological membranes have diverse lipid and protein compositions. The content of membranes defines their physical and biological properties. Some components of membranes play a key role in medicine, such as the efflux pumps that pump drugs out of a cell.
The hydrophobic core of the phospholipid bilayer is constantly in motion because of rotations around the bonds of lipid tails. Hydrophobic tails of a bilayer bend and lock together. However, because of hydrogen bonding with water, the hydrophilic head groups exhibit less movement as their rotation and mobility are constrained. This results in increasing viscosity of the lipid bilayer closer to the hydrophilic heads.
Below a transition temperature, a lipid bilayer loses fluidity when the highly mobile lipids exhibits less movement becoming a gel-like solid. The transition temperature depends on such components of the lipid bilayer as the hydrocarbon chain length and the saturation of its fatty acids. Temperature-dependence fluidity constitutes an important physiological attribute for bacteria and cold-blooded organisms. These organisms maintain a constant fluidity by modifying membrane lipid fatty acid composition in accordance with differing temperatures.
In animal cells, membrane fluidity is modulated by the inclusion of the sterol cholesterol. This molecule is present in especially large amounts in the plasma membrane, where it constitutes approximately 20% of the lipids in the membrane by weight. Because cholesterol molecules are short and rigid, they fill the spaces between neighboring phospholipid molecules left by the kinks in their unsaturated hydrocarbon tails. In this way, cholesterol tends to stiffen the bilayer, making it more rigid and less permeable.
For all cells, membrane fluidity is important for many reasons. It enables membrane proteins to diffuse rapidly in the plane of the bilayer and to interact with one another, as is crucial, for example, in cell signaling. It permits membrane lipids and proteins to diffuse from sites where they are inserted into the bilayer after their synthesis to other regions of the cell. It allows membranes to fuse with one another and mix their molecules, and it ensures that membrane molecules are distributed evenly between daughter cells when a cell divides. If biological membranes were not fluid, it is hard to imagine how cells could live, grow, and reproduce.

</doc>
<doc id="4819" url="https://en.wikipedia.org/wiki?curid=4819" title="Balfour Declaration of 1926">
Balfour Declaration of 1926

The Balfour Declaration of 1926, issued by the 1926 Imperial Conference of British Empire leaders in London, was named after Lord President of the Council (and former Prime Minister of the United Kingdom) Arthur Balfour. It declared the United Kingdom and the Dominions to be:
The Inter-Imperial Relations Committee, chaired by Balfour, drew up the document preparatory to its unanimous approval by the imperial premiers on 15 November 1926. It was first proposed by South African Prime Minister J. B. M. Hertzog and Canadian Prime Minister William Lyon Mackenzie King.
The Declaration accepted the growing political and diplomatic independence of the Dominions in the years after World War I. It also recommended that the governors-general, the representatives of the King who acted for the Crown as "de facto" head of state in each dominion, should no longer also serve automatically as the representative of the British government in diplomatic relations between the countries. In following years, High Commissioners were gradually appointed, whose duties were soon recognised to be virtually identical to those of an ambassador. The first such British High Commissioner was appointed to Ottawa in 1928.
The conclusions of the imperial premiers conference of 1926 were re-stated by the 1930 conference and incorporated in the Statute of Westminster of December 1931. In the Statute, the British Parliament provided that it would not enact a law which applied to a Dominion as part of the law of that Dominion, unless the law expressly stated that the Dominion government had requested and consented to the enactment of that law.

</doc>
<doc id="4820" url="https://en.wikipedia.org/wiki?curid=4820" title="Balfour Declaration">
Balfour Declaration

The Balfour Declaration was a public statement issued by the British government in 1917 during the First World War announcing support for the establishment of a "national home for the Jewish people" in Palestine, then an Ottoman region with a small minority Jewish population. The declaration was contained in a letter dated 2November 1917 from the United Kingdom's Foreign Secretary Arthur Balfour to Lord Rothschild, a leader of the British Jewish community, for transmission to the Zionist Federation of Great Britain and Ireland. The text of the declaration was published in the press on 9November 1917.
Immediately following their declaration of war on the Ottoman Empire in November 1914, the British War Cabinet began to consider the future of Palestine; within two months a memorandum was circulated to the Cabinet by a Zionist Cabinet member, Herbert Samuel, proposing the support of Zionist ambitions in order to enlist the support of Jews in the wider war. A committee was established in April 1915 by British Prime Minister H. H. Asquith to determine their policy toward the Ottoman Empire including Palestine. Asquith, who had favoured post-war reform of the Ottoman Empire, resigned in December 1916; his replacement David Lloyd George, favoured partition of the Empire. The first negotiations between the British and the Zionists took place at a conference on 7 February 1917 that included Sir Mark Sykes and the Zionist leadership. Subsequent discussions led to Balfour's request, on 19 June, that Rothschild and Chaim Weizmann submit a draft of a public declaration. Further drafts were discussed by the British Cabinet during September and October, with input from Zionist and anti-Zionist Jews but with no representation from the local population in Palestine.
By late 1917, in the lead up to the Balfour Declaration, the wider war had reached a stalemate, with two of Britain's allies not fully engaged: the United States had yet to suffer a casualty, and the Russians were in the midst of a revolution with Bolsheviks taking over the government. A stalemate in southern Palestine was broken by the Battle of Beersheba on 31 October 1917. The release of the final declaration was authorised on 31 October; the preceding Cabinet discussion had referenced perceived propaganda benefits amongst the worldwide Jewish community for the Allied war effort.
The opening words of the declaration represented the first public expression of support for Zionism by a major political power. The term "national home" had no precedent in international law, and was intentionally vague as to whether a Jewish state was contemplated. The intended boundaries of Palestine were not specified, and the British government later confirmed that the words "in Palestine" meant that the Jewish national home was not intended to cover all of Palestine. The second half of the declaration was added to satisfy opponents of the policy, who had claimed that it would otherwise prejudice the position of the local population of Palestine and encourage antisemitism worldwide by "stamping the Jews as strangers in their native lands". The declaration called for safeguarding the civil and religious rights for the Palestinian Arabs, who composed the vast majority of the local population, and also the rights and political status of the Jewish communities in other countries outside of Palestine. The British government acknowledged in 1939 that the local population's views should have been taken into account, and recognised in 2017 that the declaration should have called for protection of the Palestinian Arabs' political rights.
The declaration had many long-lasting consequences. It greatly increased popular support for Zionism within Jewish communities worldwide, and became a core component of the British Mandate for Palestine, the founding document of Mandatory Palestine, which later became Israel and the Palestinian territories. As a result, it is considered a principal cause of the ongoing Israeli–Palestinian conflict, often described as the world's most intractable conflict. Controversy remains over a number of areas, such as whether the declaration contradicted earlier promises the British made to the Sharif of Mecca in the McMahon–Hussein correspondence.
Early British political support for an increased Jewish presence in the region of Palestine was based upon geopolitical calculations. This support began in the early 1840s and was led by Lord Palmerston, following the occupation of Syria and Palestine by separatist Ottoman governor Muhammad Ali of Egypt. French influence had grown in Palestine and the wider Middle East, and its role as protector of the Catholic communities began to grow, just as Russian influence had grown as protector of the Eastern Orthodox in the same regions. This left Britain without a sphere of influence, and thus a need to find or create their own regional "protégés". These political considerations were supported by a sympathetic evangelical Christian sentiment towards the "restoration of the Jews" to Palestine among elements of the mid-19th-century British political elite – most notably Lord Shaftesbury. The British Foreign Office actively encouraged Jewish emigration to Palestine, exemplified by Charles Henry Churchill's 1841–1842 exhortations to Moses Montefiore, the leader of the British Jewish community.
Such efforts were premature, and did not succeed; only 24,000 Jews were living in Palestine on the eve of the emergence of Zionism within the world's Jewish communities in the last two decades of the 19th century. With the geopolitical shakeup occasioned by the outbreak of the First World War, the earlier calculations, which had lapsed for some time, led to a renewal of strategic assessments and political bargaining over the Middle and Far East.
Zionism arose in the late 19th century in reaction to anti-Semitic and exclusionary nationalist movements in Europe. Romantic nationalism in Central and Eastern Europe had helped to set off the Haskalah, or "Jewish Enlightenment", creating a split in the Jewish community between those who saw Judaism as their religion and those who saw it as their ethnicity or nation. The 1881–1884 anti-Jewish pogroms in the Russian Empire encouraged the growth of the latter identity, resulting in the formation of the Hovevei Zion pioneer organizations, the publication of Leon Pinsker's "Autoemancipation", and the first major wave of Jewish immigration to Palestine – retrospectively named the "First Aliyah".
In 1896, Theodor Herzl, a Jewish journalist living in Austria-Hungary, published the foundational text of political Zionism, "Der Judenstaat" ("The Jews' State" or "The State of the Jews"), in which he asserted that the only solution to the "Jewish Question" in Europe, including growing anti-Semitism, was the establishment of a state for the Jews. A year later, Herzl founded the Zionist Organization, which at its first congress called for the establishment of "a home for the Jewish people in Palestine secured under public law". Proposed measures to attain that goal included the promotion of Jewish settlement there, the organisation of Jews in the diaspora, the strengthening of Jewish feeling and consciousness, and preparatory steps to attain necessary governmental grants. Herzl died in 1904, 44 years before the establishment of State of Israel, the Jewish state that he proposed, without having gained the political standing required to carry out his agenda.
Zionist leader Chaim Weizmann, later President of the World Zionist Organisation and first President of Israel, moved from Switzerland to the UK in 1904 and met Arthur Balfour – who had just launched his 1905–1906 election campaign after resigning as Prime Minister – in a session arranged by Charles Dreyfus, his Jewish constituency representative. Earlier that year, Balfour had successfully driven the Aliens Act through Parliament with impassioned speeches regarding the need to restrict the wave of immigration into Britain from Jews fleeing the Russian Empire. During this meeting, he asked what Weizmann's objections had been to the 1903 Uganda Scheme that Herzl had supported to provide a portion of British East Africa to the Jewish people as a homeland. The scheme, which had been proposed to Herzl by Joseph Chamberlain, Colonial Secretary in Balfour's Cabinet, following his trip to East Africa earlier in the year, had been subsequently voted down following Herzl's death by the Seventh Zionist Congress in 1905 after two years of heated debate in the Zionist Organization. Weizmann responded that he believed the English are to London as the Jews are to Jerusalem.
In January 1914 Weizmann first met Baron Edmond de Rothschild, a member of the French branch of the Rothschild family and a leading proponent of the Zionist movement, in relation to a project to build a Hebrew university in Jerusalem. The Baron was not part of the World Zionist Organization, but had funded the Jewish agricultural colonies of the First Aliyah and transferred them to the Jewish Colonization Association in 1899. This connection was to bear fruit later that year when the Baron's son, James deRothschild, requested a meeting with Weizmann on 25November 1914, to enlist him in influencing those deemed to be receptive within the British government to their agenda of a "Jewish State" in Palestine. Through James's wife Dorothy, Weizmann was to meet Rózsika Rothschild, who introduced him to the English branch of the familyin particular her husband Charles and his older brother Walter, a zoologist and former member of parliament (MP). Their father, Nathan Rothschild, 1st Baron Rothschild, head of the English branch of the family, had a guarded attitude towards Zionism, but he died in March 1915 and his title was inherited by Walter. 
Prior to the declaration, about 8,000 of Britain's 300,000 Jews belonged to a Zionist organisation. Globally, as of 1913 – the latest known date prior to the declaration – the equivalent figure was approximately 1%.
The year 1916 marked four centuries since Palestine had become part of the Ottoman Empire, also known as the Turkish Empire. For most of this period, the Jewish population represented a small minority, approximately 3% of the total, with Muslims representing the largest segment of the population, and Christians the second.
Ottoman government in Constantinople began to apply restrictions on Jewish immigration to Palestine in late 1882, in response to the start of the First Aliyah earlier that year. Although this immigration was creating a certain amount of tension with the local population, mainly among the merchant and notable classes, in 1901 the Sublime Porte (the Ottoman central government) gave Jews the same rights as Arabs to buy land in Palestine and the percentage of Jews in the population rose to 7% by 1914. At the same time, with growing distrust of the Young Turks – Turkish nationalists who had taken control of the Empire in 1908 – and the Second Aliyah, Arab nationalism and Palestinian nationalism was on the rise, and in Palestine anti-Zionism was a unifying characteristic. Historians do not know whether these strengthening forces would still have ultimately resulted in conflict in the absence of the Balfour Declaration.
In July 1914 war broke out in Europe between the Triple Entente (Britain, France, and the Russian Empire) and the Central Powers (Germany, Austria-Hungary, and, later that year, the Ottoman Empire).
The British Cabinet first discussed Palestine at a meeting on 9November 1914, four days after Britain's declaration of war on the Ottoman Empire, of which the Mutasarrifate of Jerusalemoften referred to as Palestinewas a component. At the meeting David Lloyd George, then Chancellor of the Exchequer, "referred to the ultimate destiny of Palestine". The Chancellor, whose law firm Lloyd George, Roberts and Co had been engaged a decade before by the Zionist Federation of Great Britain and Ireland to work on the Uganda Scheme, was to become Prime Minister by the time of the declaration, and was ultimately responsible for it.
Weizmann's political efforts picked up speed, and on 10December 1914 he met with Herbert Samuel, a British Cabinet member and a secular Jew who had studied Zionism; Samuel believed Weizmann's demands were too modest. Two days later, Weizmann met Balfour again, for the first time since their initial meeting in 1905; Balfour had been out of government ever since his electoral defeat in 1906, but remained a senior member of the Conservative Party in their role as Official Opposition.
A month later, Samuel circulated a memorandum entitled "The Future of Palestine" to his Cabinet colleagues. The memorandum stated: "I am assured that the solution of the problem of Palestine which would be much the most welcome to the leaders and supporters of the Zionist movement throughout the world would be the annexation of the country to the British Empire". Samuel discussed a copy of his memorandum with Nathan Rothschild in February 1915, a month before the latter's death. It was the first time in an official record that enlisting the support of Jews as a war measure had been proposed.
Many further discussions followed, including the initial meetings in 1915–16 between Lloyd George, who had been appointed Minister of Munitions in May 1915, and Weizmann, who was appointed as a scientific advisor to the ministry in September 1915. Seventeen years later, in his "War Memoirs", Lloyd George described these meetings as being the "fount and origin" of the declaration; historians have rejected this claim.
In late 1915 the British High Commissioner to Egypt, Henry McMahon, exchanged ten letters with Hussein bin Ali, Sharif of Mecca, in which he promised Hussein to recognize Arab independence "in the limits and boundaries proposed by the Sherif of Mecca" in return for Hussein launching a revolt against the Ottoman Empire. The pledge excluded "portions of Syria" lying to the west of "the districts of Damascus, Homs, Hama and Aleppo". In the decades after the war, the extent of this coastal exclusion was hotly disputed since Palestine lay to the southwest of Damascus and was not explicitly mentioned.
The Arab Revolt was launched on June5th, 1916, on the basis of the "quid pro quo" agreement in the correspondence. However, less than three weeks earlier the governments of the United Kingdom, France, and Russia secretly concluded the Sykes–Picot Agreement, which Balfour described later as a "wholly new method" for dividing the region, after the 1915 agreement "seems to have been forgotten".
This Anglo-French treaty was negotiated in late 1915 and early 1916 between Sir Mark Sykes and François Georges-Picot, with the primary arrangements being set out in draft form in a joint memorandum on 5 January 1916. Sykes was a British Conservative MP who had risen to a position of significant influence on Britain's Middle East policy, beginning with his seat on the 1915 De Bunsen Committee and his initiative to create the Arab Bureau. Picot was a French diplomat and former consul-general in Beirut. Their agreement defined the proposed spheres of influence and control in Western Asia should the Triple Entente succeed in defeating the Ottoman Empire during World WarI, dividing many Arab territories into British- and French-administered areas. In Palestine, internationalisation was proposed, with the form of administration to be confirmed after consultation with both Russia and Hussein; the January draft noted Christian and Muslim interests, and that "members of the Jewish community throughout the world have a conscientious and sentimental interest in the future of the country."
Prior to this point, no active negotiations with Zionists had taken place, but Sykes had been aware of Zionism, was in contact with Moses Gaster – a former President of the English Zionist Federation – and may have seen Samuel's 1915 memorandum. On 3 March, while Sykes and Picot were still in Petrograd, Lucien Wolf (secretary of the Foreign Conjoint Committee, set up by Jewish organizations to further the interests of foreign Jews) submitted to the Foreign Office, the draft of an assurance (formula) that could be issued by the allies in support of Jewish aspirations:
In the event of Palestine coming within the spheres of influence of Great Britain or France at the close of the war, the governments of those powers will not fail to take account of the historic interest that country possesses for the Jewish community. The Jewish population will be secured in the enjoyment of civil and religious liberty, equal political rights with the rest of the population, reasonable facilities for immigration and colonisation, and such municipal privileges in the towns and colonies inhabited by them as may be shown to be necessary.
On 11 March, telegrams were sent in Grey's name to Britain's Russian and French ambassadors for transmission to Russian and French authorities, including the formula, as well as :
The scheme might be made far more attractive to the majority of Jews if it held out to them the prospect that when in course of time the Jewish colonists in Palestine grow strong enough to cope with the Arab population they may be allowed to take the management of the internal affairs of Palestine (with the exception of Jerusalem and the holy places) into their own hands.
Sykes, having seen the telegram, had discussions with Picot and proposed (making reference to Samuel's memorandum ) the creation of an Arab Sultanate under French and British protection, some means of administering the holy places along with the establishment of a company to purchase land for Jewish colonists, who would then become citizens with equal rights to Arabs.
Shortly after returning from Petrograd, Sykes briefed Samuel, who then briefed a meeting of Gaster, Weizmann and Sokolow. Gaster recorded in his diary on 16 April 1916: "We are offered French-English condominium in Palest[ine]. Arab Prince to conciliate Arab sentiment and as part of the Constitution a Charter to Zionists for which England would stand guarantee and which would stand by us in every case of friction ... It practically comes to a complete realisation of our Zionist programme. However, we insisted on: national character of Charter, freedom of immigration and internal autonomy, and at the same time full rights of citizenship to [illegible] and Jews in Palestine." In Sykes' mind, the agreement which bore his name was outdated even before it was signed – in March 1916, he wrote in a private letter: "to my mind the Zionists are now the key of the situation". In the event, neither the French nor the Russians were enthusiastic about the proposed formulation and eventually on 4 July, Wolf was informed that "the present moment is inopportune for making any announcement." 
These wartime initiatives, inclusive of the declaration, are frequently considered together by historians because of the potential, real or imagined, for incompatibility between them, particularly in regard to the disposition of Palestine. In the words of Professor Albert Hourani, founder of the Middle East Centre at St Antony's College, Oxford: "The argument about the interpretation of these agreements is one which is impossible to end, because they were intended to bear more than one interpretation."
In terms of British politics, the declaration resulted from the coming into power of Lloyd George and his Cabinet, which had replaced the H. H. Asquith led-Cabinet in December 1916. Whilst both Prime Ministers were Liberals and both governments were wartime coalitions, Lloyd George and Balfour, appointed as his Foreign Secretary, favoured a post-war partition of the Ottoman Empire as a major British war aim, whereas Asquith and his Foreign Secretary, Sir Edward Grey, had favoured its reform.
Two days after taking office, Lloyd George told General Robertson, the Chief of the Imperial General Staff, that he wanted a major victory, preferably the capture of Jerusalem, to impress British public opinion, and immediately consulted his War Cabinet about a "further campaign into Palestine when El Arish had been secured." Subsequent pressure from Lloyd George, over the reservations of Robertson, resulted in the recapture of the Sinai for British-controlled Egypt, and, with the capture of El Arish in December 1916 and Rafah in January 1917, the arrival of British forces at the southern borders of the Ottoman Empire. Following two unsuccessful attempts to capture Gaza between 26 March and 19 April, a six-month stalemate in Southern Palestine began; the Sinai and Palestine Campaign would not make any progress into Palestine until 31October 1917.
Following the change in government, Sykes was promoted into the War Cabinet Secretariat with responsibility for Middle Eastern affairs. In January 1917, despite having previously built a relationship with Moses Gaster, he began looking to meet other Zionist leaders; by the end of the month he had been introduced to Weizmann and his associate Nahum Sokolow, a journalist and executive of the World Zionist Organization who had moved to Britain at the beginning of the war.
On 7February 1917, Sykes, claiming to be acting in a private capacity, entered into substantive discussions with the Zionist leadership. The previous British correspondence with "the Arabs" was discussed at the meeting; Sokolow's notes record Sykes' description that "The Arabs professed that language must be the measure [by which control of Palestine should be determined] and [by that measure] could claim all Syria and Palestine. Still the Arabs could be managed, particularly if they received Jewish support in other matters." At this point the Zionists were still unaware of the Sykes-Picot Agreement, although they had their suspicions. One of Sykes' goals was the mobilization of Zionism to the cause of British suzerainty in Palestine, so as to have arguments to put to France in support of that objective.
During the period of the British War Cabinet discussions leading up to the declaration, the war had reached a period of stalemate. On the Western Front the tide would first turn in favour of the Central Powers in spring 1918, before decisively turning in favour of the Allies from July 1918 onwards. Although the United States declared war on Germany in the spring of 1917, it did not suffer its first casualties until 2 November 1917, at which point President Woodrow Wilson still hoped to avoid dispatching large contingents of troops into the war. The Russian forces were known to be distracted by the ongoing Russian Revolution and the growing support for the Bolshevik faction, but Alexander Kerensky's Provisional Government had remained in the war; Russia only withdrew after the final stage of the revolution on 7November 1917.
Balfour met Weizmann at the Foreign Office on 22 March 1917; two days later, Weizmann described the meeting as being "the first time I had a real business talk with him". Weizmann explained at the meeting that the Zionists had a preference for a British protectorate over Palestine, as opposed to an American, French or international arrangement; Balfour agreed, but warned that "there may be difficulties with France and Italy".
The French position in regard to Palestine and the wider Syria region during the lead up to the Balfour Declaration was largely dictated by the terms of the Sykes-Picot Agreement, and was complicated from 23 November 1915 by increasing French awareness of the British discussions with the Sherif of Mecca. Prior to 1917, the British had led the fighting on the southern border of the Ottoman Empire alone, given their neighbouring Egyptian colony and the French preoccupation with the fighting on the Western Front that was taking place on their own soil. Italy's participation in the war, which began following the April 1915 Treaty of London, did not include involvement in the Middle Eastern sphere until the April 1917 Agreement of Saint-Jean-de-Maurienne; at this conference, Lloyd George had raised the question of a British protectorate of Palestine and the idea "had been very coldly received" by the French and the Italians. In May and June 1917, the French and Italians sent detachments to support the British as they built their reinforcements in preparation for a renewed attack on Palestine.
In early April, Sykes and Picot were appointed to act as the chief negotiators once more, this time on a month-long mission to the Middle East for further discussions with the Sherif of Mecca and other Arab leaders. On 3 April 1917, Sykes met with Lloyd George, Curzon and Hankey to receive his instructions in this regard, namely to keep the French onside while "not prejudicing the Zionist movement and the possibility of its development under British auspices, [and not] enter into any political pledges to the Arabs, and particularly none in regard to Palestine". Before travelling to the Middle East, Picot, via Sykes, invited Nahum Sokolow to Paris to educate the French government on Zionism. Sykes, who had prepared the way in correspondence with Picot, arrived a few days after Sokolow; in the meantime Sokolow had met Picot and other French officials, and convinced the French Foreign Office to accept for study a statement of Zionist aims "in regard to facilities of colonization, communal autonomy, rights of language and establishment of a Jewish chartered company." Sykes went on ahead to Italy and had meetings with the British ambassador and British Vatican representative to prepare the way for Sokolow once again.
Sokolow was granted an audience with Pope Benedict XV on 6 May 1917. Sokolow's notes of the meeting – the only meeting records known to historians – stated that the Pope expressed general sympathy and support for the Zionist project. On 21 May 1917 Angelo Sereni, president of the Committee of the Jewish Communities, presented Sokolow to Sidney Sonnino, the Italian Minister of Foreign Affairs. He was also received by Paolo Boselli, the Italian prime minister. Sonnino arranged for the secretary general of the ministry to send a letter to the effect that, although he could not express himself on the merits of a program which concerned all the allies, "generally speaking" he was not opposed to the legitimate claims of the Jews. On his return journey, Sokolow met with French leaders again and secured a letter dated 4 June 1917, giving assurances of sympathy towards the Zionist cause by Jules Cambon, head of the political section of the French foreign ministry. This letter was not published, but was deposited at the British Foreign Office.
Following the United States' entry into the war on 6 April, the British Foreign Secretary led the Balfour Mission to Washington D.C. and New York, where he spent a month between mid-April and mid-May. During the trip he spent significant time discussing Zionism with Louis Brandeis, a leading Zionist and a close ally of Wilson who had been appointed as a Supreme Court Justice a year previously.
By 13 June 1917, it was acknowledged by Ronald Graham, head of the Foreign Office's Middle Eastern affairs department, that the three most relevant politiciansthe Prime Minister, the Foreign Secretary, and the Parliamentary Under-Secretary of State for Foreign Affairs, Lord Robert Cecilwere all in favour of Britain supporting the Zionist movement; on the same day Weizmann had written to Graham to advocate for a public declaration.
Six days later, at a meeting on 19June, Balfour asked Lord Rothschild and Weizmann to submit a formula for a declaration. Over the next few weeks, a 143-word draft was prepared by the Zionist negotiating committee, but it was considered too specific on sensitive areas by Sykes, Graham and Rothschild. Separately, a very different draft had been prepared by the Foreign Office, described in 1961 by Harold Nicolson – who had been involved in preparing the draft – as proposing a "sanctuary for Jewish victims of persecution". The Foreign Office draft was strongly opposed by the Zionists, and was discarded; no copy of the draft has been found in the Foreign Office archives.
Following further discussion, a revised – and at just 46 words in length, much shorter – draft declaration was prepared and sent by Lord Rothschild to Balfour on 18 July. It was received by the Foreign Office, and the matter was brought to the Cabinet for formal consideration.
The decision to release the declaration was taken by the British War Cabinet on 31 October 1917. This followed discussion at four War Cabinet meetings (including the 31 October meeting) over the space of the previous two months. In order to aid the discussions, the War Cabinet Secretariat, led by Maurice Hankey and supported by his Assistant Secretaries – primarily Sykes and his fellow Conservative MP and pro-Zionist Leo Amery – solicited outside perspectives to put before the Cabinet. These included the views of government ministers, war allies – notably from President Woodrow Wilson – and in October, formal submissions from six Zionist leaders and four non-Zionist Jews.
British officials asked President Wilson for his consent on the matter on two occasions – first on 3 September, when he replied the time was not ripe, and later on 6 October, when he agreed with the release of the declaration.
Excerpts from the minutes of these four War Cabinet meetings provide a description of the primary factors that the ministers considered:
Declassification of British government archives has allowed scholars to piece together the choreography of the drafting of the declaration; in his widely cited 1961 book, Leonard Stein published four previous drafts of the declaration.
The drafting began with Weizmann's guidance to the Zionist drafting team on its objectives in a letter dated 20 June 1917, one day following his meeting with Rothschild and Balfour. He proposed that the declaration from the British government should state: "its conviction, its desire or its intention to support Zionist aims for the creation of a Jewish national home in Palestine; no reference must be made I think to the question of the Suzerain Power because that would land the British into difficulties with the French; it must be a Zionist declaration."
A month after the receipt of the much-reduced 12 July draft from Rothschild, Balfour proposed a number of mainly technical amendments. The two subsequent drafts included much more substantial amendments: the first in a late August draft by Lord Milner – one of the original five members of Lloyd George's War Cabinet as a minister without portfolio – which reduced the geographic scope from all of Palestine to "in Palestine", and the second from Milner and Amery in early October, which added the two "safeguard clauses".
Subsequent authors have debated who the "primary author" really was. In his posthumously published 1981 book "The Anglo-American Establishment", Georgetown University history professor Carroll Quigley explained his view that Lord Milner was the primary author of the declaration, and more recently, William D. Rubinstein, Professor of Modern History at Aberystwyth University, Wales, proposed Amery instead. Huneidi wrote that Ormsby-Gore, in a report he prepared for Shuckburgh, claimed authorship, together with Amery, of the final draft form.
The agreed version of the declaration, a single sentence of just 67 words, was sent on 2November 1917 in a short letter from Balfour to Walter Rothschild, for transmission to the Zionist Federation of Great Britain and Ireland. The declaration contained four clauses, of which the first two promised to support "the establishment in Palestine of a national home for the Jewish people", followed by two "safeguard clauses" with respect to "the civil and religious rights of existing non-Jewish communities in Palestine", and "the rights and political status enjoyed by Jews in any other country".
The term "national home" was intentionally ambiguous, having no legal value or precedent in international law, such that its meaning was unclear when compared to other terms such as "state". The term was intentionally used instead of "state" because of opposition to the Zionist program within the British Cabinet. According to historian Norman Rose, the chief architects of the declaration contemplated that a Jewish State would emerge in time while the Palestine Royal Commission concluded that the wording was "the outcome of a compromise between those Ministers who contemplated the ultimate establishment of a Jewish State and those who did not."
Interpretation of the wording has been sought in the correspondence leading to the final version of the declaration. An official report to the War Cabinet sent by Sykes on 22 September said that the Zionists did "not" want "to set up a Jewish Republic or any other form of state in Palestine or in any part of Palestine" but rather preferred some form of protectorate as provided in the Palestine Mandate. A month later, Curzon produced a memorandum circulated on 26 October 1917 where he addressed two questions, the first concerning the meaning of the phrase "a National Home for the Jewish race in Palestine"; he noted that there were different opinions ranging from a fully fledged state to a merely spiritual centre for the Jews.
Sections of the British press assumed that a Jewish state was intended even before the Declaration was finalized. In the United States the press began using the terms "Jewish National Home", "Jewish State", "Jewish republic" and "Jewish Commonwealth" interchangeably.
Treaty expert David Hunter Miller, who was at the conference and subsequently compiled a 22 volume compendium of documents, provides a report of the Intelligence Section of the American Delegation to the Paris Peace Conference of 1919 which recommended that "there be established a separate state in Palestine," and that "it will be the policy of the League of Nations to recognize Palestine as a Jewish state, as soon as it is a Jewish state in fact." The report further advised that an independent Palestinian state under a British League of Nations mandate be created. Jewish settlement would be allowed and encouraged in this state and this state's holy sites would be under the control of the League of Nations. Indeed, the Inquiry spoke positively about the possibility of a Jewish state eventually being created in Palestine if the necessary demographics for this were to exist.
Historian Matthew Jacobs later wrote that the US approach was hampered by the "general absence of specialist knowledge about the region" and that "like much of the Inquiry's work on the Middle East, the reports on Palestine were deeply flawed" and "presupposed a particular outcome of the conflict". He quotes Miller, writing about one report on the history and impact of Zionism, "absolutely inadequate from any standpoint and must be regarded as nothing more than material for a future report"
Lord Robert Cecil on 2 December 1917, assured an audience that the government fully intended that "Judea [was] for the Jews." Yair Auron opines that Cecil, then a deputy Foreign Secretary representing the British Government at a celebratory gathering of the English Zionist Federation, "possibly went beyond his official brief" in saying (he cites Stein) "Our wish is that Arabian countries shall be for the Arabs, Armenia for the Armenians and Judaea for the Jews".
The following October Neville Chamberlain, while chairing a Zionist meeting, discussed a "new Jewish State."At the time, Chamberlain was a Member of Parliament for Ladywood, Birmingham; recalling the event in 1939, just after Chamberlain had approved the 1939 White Paper, the Jewish Telegraph Agency noted that the Prime Minister had "experienced a pronounced change of mind in the 21 years intervening" A year later, on the Declaration's second anniversary, General Jan Smuts said that Britain "would redeem her pledge ... and a great Jewish state would ultimately rise." In similar vein, Churchill a few months later stated:
At the 22 June 1921 meeting of the Imperial Cabinet, Churchill was asked by Arthur Meighen, the Canadian Prime Minister, about the meaning of the national home. Churchill said "If in the course of many years they become a majority in the country, they naturally would take it over ... pro rata with the Arab. We made an equal pledge that we would not turn the Arab off his land or invade his political and social rights". 
Responding to Curzon in January 1919, Balfour wrote "Weizmann has never put forward a claim for the Jewish Government of Palestine. Such a claim in my opinion is clearly inadmissible and personally I do not think we should go further than the original declaration which I made to Lord Rothschild".
In February 1919, France issued a statement that it would not oppose putting Palestine under British trusteeship and the formation of a Jewish State. Friedman further notes that France's attitude went on to change; Yehuda Blum, while discussing France's "unfriendly attitude towards the Jewish national movement", notes the content of a report made by Robert Vansittart (a leading member of the British delegation to the Paris Peace Conference) to Curzon in November 1920 which said:
Greece's Foreign Minister told the editor of the Salonica Jewish organ Pro-Israel that "the establishment of a Jewish State meets in Greece with full and sincere sympathy ... A Jewish Palestine would become an ally of Greece." In Switzerland, a number of noted historians including professors Tobler, Forel-Yvorne, and Rogaz, supported the idea of establishing a Jewish state, with one referring to it as "a sacred right of the Jews." While in Germany, officials and most of the press took the Declaration to mean a British sponsored state for the Jews.
The British government, including Churchill, made it clear that the Declaration did not intend for the whole of Palestine to be converted into a Jewish National Home, "but that such a Home should be founded in Palestine." Emir Faisal, King of Syria and Iraq, made a formal written agreement with Zionist leader Chaim Weizmann, which was drafted by T.E. Lawrence, whereby they would try to establish a peaceful relationship between Arabs and Jews in Palestine. The 3 January 1919 Faisal–Weizmann Agreement was a short-lived agreement for Arab–Jewish cooperation on the development of a Jewish homeland in Palestine. Faisal did treat Palestine differently in his presentation to the Peace Conference on 6 February 1919 saying "Palestine, for its universal character, [should be] left on one side for the mutual consideration of all parties concerned". The agreement was never implemented. In a subsequent letter written in English by Lawrence for Faisal's signature, he explained:
When the letter was tabled at the Shaw Commission in 1929, Rustam Haidar spoke to Faisal in Baghdad and cabled that Faisal had "no recollection that he wrote anything of the sort". In January 1930, Haidar wrote to a newspaper in Baghdad that Faisal: "finds it exceedingly strange that such a matter is attributed to him as he at no time would consider allowing any foreign nation to share in an Arab country". Awni Abd al-Hadi, Faisal's secretary, wrote in his memoirs that he was not aware that a meeting between Frankfurter and Faisal took place and that: "I believe that this letter, assuming that it is authentic, was written by Lawrence, and that Lawrence signed it in English on behalf of Faisal. I believe this letter is part of the false claims made by Chaim Weizmann and Lawrence to lead astray public opinion." According to Allawi, the most likely explanation for the Frankfurter letter is that a meeting took place, a letter was drafted in English by Lawrence, but that its "contents were not entirely made clear to Faisal. He then may or may not have been induced to sign it", since it ran counter to Faisal's other public and private statements at the time. A 1 March interview by Le Matin quoted Faisal as saying:
This feeling of respect for other religions dictates my opinion about Palestine, our neighbor. That the unhappy Jews come to reside there and behave as good citizens of this country, our humanity rejoices given that they are placed under a Muslim or Christian government mandated by The League of Nations. If they want to constitute a state and claim sovereign rights in this region, I foresee very serious dangers. It is to be feared that there will be a conflict between them and the other races. 
Referring to his 1922 White Paper, Churchill later wrote that "there is nothing in it to prohibit the ultimate establishment of a Jewish State." And in private, many British officials agreed with the Zionists' interpretation that a state would be established when a Jewish majority was achieved.
When Chaim Weizmann met with Churchill, Lloyd George and Balfour at Balfour's home in London on 21 July 1921, Lloyd George and Balfour assured Weizmann "that by the Declaration they had always meant an eventual Jewish State," according to Weizmann minutes of that meeting. Lloyd George stated in 1937 that it was intended that Palestine would become a Jewish Commonwealth if and when Jews "had become a definite majority of the inhabitants", and Leo Amery echoed the same position in 1946. In the UNSCOP report of 1947, the issue of home versus state was subjected to scrutiny arriving at a similar conclusion to that of Lloyd George.
The statement that such a homeland would be found "in Palestine" rather than "of Palestine" was also deliberate. The proposed draft of the declaration contained in Rothschild's 12 July letter to Balfour referred to the principle "that Palestine should be reconstituted as the National Home of the Jewish people." In the final text, following Lord Milner's amendment, the word "reconstituted" was removed and the word "that" was replaced with "in".
This text thereby avoided committing the entirety of Palestine as the National Home of the Jewish people, resulting in controversy in future years over the intended scope, especially the Revisionist Zionism sector, which claimed entirety of Mandatory Palestine and Emirate of Transjordan as Jewish Homeland This was clarified by the 1922 Churchill White Paper, which wrote that "the terms of the declaration referred to do not contemplate that Palestine as a whole should be converted into a Jewish National Home, but that such a Home should be founded 'in Palestine.'"
The declaration did not include any geographical boundaries for Palestine. Following the end of the war, three documents – the declaration, the Hussein-McMahon Correspondence and the Sykes-Picot Agreement – became the basis for the negotiations to set the boundaries of Palestine.
The declaration's first safeguard clause referred to protecting the civil and religious rights of non-Jews in Palestine. The clause had been drafted together with the second safeguard by Leo Amery in consultation with Lord Milner, with the intention to "go a reasonable distance to meeting the objectors, both Jewish and pro-Arab, without impairing the substance of the proposed declaration".
The "non-Jews" constituted 90% of the population of Palestine; in the words of Ronald Storrs, Britain's Military Governor of Jerusalem between 1917 and 1920, the community observed that they had been "not so much as named, either as Arabs, Moslems or Christians, but were lumped together under the negative and humiliating definition of 'Non-Jewish Communities' and relegated to subordinate provisos". The community also noted that there was no reference to protecting their "political status" or political rights, as there was in the subsequent safeguard relating to Jews in other countries. This protection was frequently contrasted against the commitment to the Jewish community, and over the years a variety of terms were used to refer to these two obligations as a pair; a particularly heated question was whether these two obligations had "equal weight", and in 1930 this equal status was confirmed by the Permanent Mandates Commission and by the British government in the Passfield white paper.
Balfour stated in February 1919 that Palestine was considered an exceptional case in which, referring to the local population, "we deliberately and rightly decline to accept the principle of self-determination," although he considered that the policy provided self-determination to Jews. Avi Shlaim considers this the declaration's "greatest contradiction". This principle of self-determination had been declared on numerous occasions subsequent to the declarationPresident Wilson's January 1918 Fourteen Points, McMahon's Declaration to the Seven in June 1918, the November 1918 Anglo-French Declaration, and the June 1919 Covenant of the League of Nations that had established the mandate system. In an August 1919 memo Balfour acknowledged the inconsistency among these statements, and further explained that the British had no intention of consulting the existing population of Palestine. The results of the ongoing American King–Crane Commission of Enquiry consultation of the local population – from which the British had withdrawn – were suppressed for three years until the report was leaked in 1922. Subsequent British governments have acknowledged this deficiency, in particular the 1939 committee led by the Lord Chancellor, Frederic Maugham, which concluded that the government had not been "free to dispose of Palestine without regard for the wishes and interests of the inhabitants of Palestine", and the April 2017 statement by British Foreign Office minister of state Baroness Anelay that the government acknowledged that "the Declaration should have called for the protection of political rights of the non-Jewish communities in Palestine, particularly their right to self-determination."
The second safeguard clause was a commitment that nothing should be done which might prejudice the rights of the Jewish communities in other countries outside of Palestine. The original drafts of Rothschild, Balfour, and Milner did not include this safeguard, which was drafted together with the preceding safeguard in early October, in order to reflect opposition from influential members of the Anglo-Jewish community. Lord Rothschild took exception to the proviso on the basis that it presupposed the possibility of a danger to non-Zionists, which he denied.
The Conjoint Foreign Committee of the Board of Deputies of British Jews and the Anglo-Jewish Association had published a letter in "The Times" on 24 May 1917 entitled "Views of Anglo-Jewry", signed by the two organisations' presidents, David Lindo Alexander and Claude Montefiore, stating their view that: "the establishment of a Jewish nationality in Palestine, founded on this theory of homelessness, must have the effect throughout the world of stamping the Jews as strangers in their native lands, and of undermining their hard-won position as citizens and nationals of these lands." This was followed in late August by Edwin Montagu, an influential anti-Zionist Jew and Secretary of State for India, and the only Jewish member of the British Cabinet, who wrote in a Cabinet memorandum that: "The policy of His Majesty's Government is anti-Semitic in result and will prove a rallying ground for anti-Semites in every country of the world."
The text of the declaration was published in the press one week after it was signed, on 9November 1917. Other related events took place within a short timeframe, the two most relevant being the almost immediate British military capture of Palestine and the leaking of the previously secret Sykes-Picot Agreement. On the military side, both Gaza and Jaffa fell within several days, and Jerusalem was surrendered to the British on 9 December. The publication of the Sykes–Picot Agreement, following the Russian Revolution, in the Bolshevik "Izvestia" and "Pravda" on 23 November 1917 and in the British "Manchester Guardian" on 26 November 1917, represented a dramatic moment for the Allies' Eastern campaign: "the British were embarrassed, the Arabs dismayed and the Turks delighted." The Zionists had been aware of the outlines of the agreement since April and specifically the part relevant to Palestine, following a meeting between Weizmann and Cecil where Weizmann made very clear his objections to the proposed scheme.
The declaration represented the first public support for Zionism by a major political power – its publication galvanized Zionism, which finally had obtained an official charter. In addition to its publication in major newspapers, leaflets were circulated throughout Jewish communities. These leaflets were airdropped over Jewish communities in Germany and Austria, as well as the Pale of Settlement, which had been given to the Central Powers following the Russian withdrawal.
Weizmann had argued that the declaration would have three effects: it would swing Russia to maintain pressure on Germany's Eastern Front, since Jews had been prominent in the March Revolution of 1917; it would rally the large Jewish community in the United States to press for greater funding for the American war effort, underway since April of that year; and, lastly, that it would undermine German Jewish support for Kaiser Wilhelm II.
The declaration spurred an unintended and extraordinary increase in the number of adherents of American Zionism; in 1914 the 200 American Zionist societies comprised a total of 7,500 members, which grew to 30,000 members in 600 societies in 1918 and 149,000 members in 1919. Whilst the British had considered that the declaration reflected a previously established dominance of the Zionist position in Jewish thought, it was the declaration itself that was subsequently responsible for Zionism's legitimacy and leadership.
Exactly one month after the declaration was issued, a large-scale celebration took place at the Royal Opera House – speeches were given by leading Zionists as well as members of the British administration including Sykes and Cecil. From 1918 until the Second World War, Jews in Mandatory Palestine celebrated Balfour Day as an annual national holiday on 2November. The celebrations included ceremonies in schools and other public institutions and festive articles in the Hebrew press. In August 1919 Balfour approved Weizmann's request to name the first post-war settlement in Mandatory Palestine, "Balfouria", in his honour. It was intended to be a model settlement for future American Jewish activity in Palestine.
Herbert Samuel, the Zionist MP whose 1915 memorandum had framed the start of discussions in the British Cabinet, was asked by Lloyd George on 24April 1920 to act as the first civil governor of British Palestine, replacing the previous military administration that had ruled the area since the war. Shortly after beginning the role in July 1920, he was invited to read the "haftarah" from Isaiah 40 at the Hurva Synagogue in Jerusalem, which, according to his memoirs, led the congregation of older settlers to feel that the "fulfilment of ancient prophecy might at last be at hand".
The local Christian and Muslim community of Palestine, who constituted almost 90% of the population, strongly opposed the declaration. As described by the Palestinian-American philosopher Edward Said in 1979, it was perceived as being made: "(a)by a European power, (b)about a non-European territory, (c)in a flat disregard of both the presence and the wishes of the native majority resident in that territory, and (d)it took the form of a promise about this same territory to another foreign group."
According to the 1919 King–Crane Commission, "No British officer, consulted by the Commissioners, believed that the Zionist programme could be carried out except by force of arms." A delegation of the Muslim-Christian Association, headed by Musa al-Husayni, expressed public disapproval on 3November 1918, one day after the Zionist Commission parade marking the first anniversary of the Balfour Declaration. They handed a petition signed by more than 100 notables to Ronald Storrs, the British military governor:
The group also protested the carrying of new "white and blue banners with two inverted triangles in the middle", drawing the attention of the British authorities to the serious consequences of any political implications in raising the banners. Later that month, on the first anniversary of the occupation of Jaffa by the British, the Muslim-Christian Association sent a lengthy memorandum and petition to the military governor protesting once more any formation of a Jewish state.
In the broader Arab world, the declaration was seen as a betrayal of the British wartime understandings with the Arabs. The Sharif of Mecca and other Arab leaders considered the declaration a violation of a previous commitment made in the McMahon–Hussein correspondence in exchange for launching the Arab Revolt.
Following the publication of the declaration, the British dispatched Commander David George Hogarth to see Hussein in January 1918 bearing the message that the "political and economic freedom" of the Palestinian population was not in question. Hogarth reported that Hussein "would not accept an independent Jewish State in Palestine, nor was I instructed to warn him that such a state was contemplated by Great Britain". Hussein had also learned of the Sykes–Picot Agreement when it was leaked by the new Soviet government in December 1917, but was satisfied by two disingenuous messages from Sir Reginald Wingate, who had replaced McMahon as High Commissioner of Egypt, assuring him that the British commitments to the Arabs were still valid and that the Sykes–Picot Agreement was not a formal treaty.
Continuing Arab disquiet over Allied intentions also led during 1918 to the British Declaration to the Seven and the Anglo-French Declaration, the latter promising "the complete and final liberation of the peoples who have for so long been oppressed by the Turks, and the setting up of national governments and administrations deriving their authority from the free exercise of the initiative and choice of the indigenous populations".
In 1919, King Hussein refused to ratify the Treaty of Versailles. After February, 1920, the British ceased to pay subsidy to him. In August, 1920, five days after the signing of the Treaty of Sèvres, which formally recognized the Kingdom of Hejaz, Curzon asked Cairo to procure Hussein's signature to both treaties and agreed to make a payment of £30,000 conditional on signature. Hussein declined and in 1921, stated that he could not be expected to "affix his name to a document assigning Palestine to the Zionists and Syria to foreigners." Following the 1921 Cairo Conference, Lawrence was sent to try and obtain the King's signature to a treaty as well as to Versailles and Sèvres, a £60,000 annual subsidy being proposed; this attempt also failed. During 1923, the British made one further attempt to settle outstanding issues with Hussein and once again, the attempt foundered, Hussein continued in his refusal to recognize the Balfour Declaration or any of the Mandates that he perceived as being his domain. In March 1924, having briefly considered the possibility of removing the offending article from the treaty, the government suspended any further negotiations; within six months they withdrew their support in favour of their central Arabian ally Ibn Saud, who proceeded to conquer Hussein's kingdom.
The declaration was first endorsed by a foreign government on 27 December 1917, when Serbian Zionist leader and diplomat David Albala announced the support of Serbia's government in exile during a mission to the United States. The French and Italian governments offered their endorsements, on 14 February and 9 May 1918, respectively. At a private meeting in London on 1 December 1918, Lloyd George and French Prime Minister Georges Clemenceau agreed to certain modifications to the Sykes-Picot Agreement, including British control of Palestine.
On 25 April 1920, the San Remo conference – an outgrowth of the Paris Peace Conference attended by the prime ministers of Britain, France and Italy, the , and the United States Ambassador to Italy – established the basic terms for three League of Nations mandates: a French mandate for Syria, and British mandates for Mesopotamia and Palestine. With respect to Palestine, the resolution stated that the British were responsible for putting into effect the terms of the Balfour Declaration. The French and the Italians made clear their dislike of the "Zionist cast of the Palestinian mandate" and objected especially to language that did not safeguard the "political" rights of non-Jews, accepting Curzon's claim that "in the British language all ordinary rights were included in "civil rights"". At the request of France, it was agreed that an undertaking was to be inserted in the mandate's procès-verbal that this would not involve the surrender of the rights hitherto enjoyed by the non-Jewish communities in Palestine. The Italian endorsement of the Declaration had included the condition "... on the understanding that there is no prejudice against the legal and political status of the already existing religious communities ..." (in Italian "... che non ne venga nessun pregiudizio allo stato giuridico e politico delle gia esistenti communita religiose ..." The boundaries of Palestine were left unspecified, to "be determined by the Principal Allied Powers." Three months later, in July 1920, the French defeat of Faisal's Arab Kingdom of Syria precipitated the British need to know "what is the 'Syria' for which the French received a mandate at San Remo?" and "does it include Transjordania?" – it subsequently decided to pursue a policy of associating Transjordan with the mandated area of Palestine without adding it to the area of the Jewish National Home.
In 1922, Congress officially endorsed America's support for the Balfour Declaration through the passage of the Lodge-Fish Resolution, notwithstanding opposition from the State Department. Professor Lawrence Davidson, of West Chester University, whose research focuses on American relations with the Middle East, argues that President Wilson and Congress ignored democratic values in favour of "biblical romanticism" when they endorsed the declaration. He points to an organized pro-Zionist lobby in the United States, which was active at a time when the country's small Arab American community had little political power.
The publication of the Balfour Declaration was met with tactical responses from the Central Powers. Two weeks following the declaration, Ottokar Czernin, the Austrian Foreign Minister, gave an interview to Arthur Hantke, President of the Zionist Federation of Germany, promising that his government would influence the Turks once the war was over. On 12December, the Ottoman Grand Vizier, Talaat Pasha, gave an interview to the German newspaper "Vossische Zeitung" that was published on 31December and subsequently released in the German-Jewish periodical "" on 4January 1918, in which he referred to the declaration as "une blague" (a deception) and promised that under Ottoman rule "all justifiable wishes of the Jews in Palestine would be able to find their fulfilment" subject to the absorptive capacity of the country. This Turkish statement was endorsed by the German Foreign Office on 5January 1918. On 8January 1918, a German-Jewish Society, the Union of German Jewish Organizations for the Protection of the Rights of the Jews of the East (VJOD), was formed to advocate for further progress for Jews in Palestine.
Following the war, the Treaty of Sèvres was signed by the Ottoman Empire on 10August 1920. The treaty dissolved the Ottoman Empire, requiring Turkey to renounce sovereignty over much of the Middle East. Article95 of the treaty incorporated the terms of the Balfour Declaration with respect to "the administration of Palestine, within such boundaries as may be determined by the Principal Allied Powers". Since incorporation of the declaration into the Treaty of Sèvres did not affect the legal status of either the declaration or the Mandate, there was also no effect when Sèvres was superseded by the Treaty of Lausanne (1923), which did not include any reference to the declaration.
In 1922, German anti-Semitic theorist Alfred Rosenberg in his primary contribution to Nazi theory on Zionism, "Der Staatsfeindliche Zionismus" ("Zionism, the Enemy of the State"), accused German Zionists of working for a German defeat and supporting Britain and the implementation of the Balfour Declaration, in a version of the stab-in-the-back myth. Adolf Hitler took a similar approach in some of his speeches from 1920 onwards.
With the advent of the declaration and the British entry into Jerusalem on 9 December, the Vatican reversed its earlier sympathetic attitude to Zionism and adopted an oppositional stance that was to continue until the early 1990s.
The British policy as stated in the declaration was to face numerous challenges to its implementation in the following years. The first of these was the indirect peace negotiations which took place between Britain and the Ottomans in December 1917 and January 1918 during a pause in the hostilities for the rainy season; although these peace talks were unsuccessful, archival records suggest that key members of the War Cabinet may have been willing to permit leaving Palestine under nominal Turkish sovereignty as part of an overall deal.
In October 1919, almost a year after the end of the war, Lord Curzon succeeded Balfour as Foreign Secretary. Curzon had been a member of the 1917 Cabinet that had approved the declaration, and according to British historian Sir David Gilmour, Curzon had been "the only senior figure in the British government at the time who foresaw that its policy would lead to decades of Arab–Jewish hostility". He therefore determined to pursue a policy in line with its "narrower and more prudent rather than the wider interpretation". Following Bonar Law's appointment as Prime Minister in late 1922, Curzon wrote to Law that he regarded the declaration as "the worst" of Britain's Middle East commitments and "a striking contradiction of our publicly declared principles".
In August 1920 the report of the Palin Commission, the first in a long line of British Commissions of Inquiry on the question of Palestine during the Mandate period, noted that "The Balfour Declaration ... is undoubtedly the starting point of the whole trouble". The conclusion of the report, which was not published, mentioned the Balfour Declaration three times, stating that "the causes of the alienation and exasperation of the feelings of the population of Palestine" included:
British public and government opinion became increasingly unfavourable to state support for Zionism; even Sykes had begun to change his views in late 1918. In February 1922 Churchill telegraphed Samuel, who had begun his role as High Commissioner for Palestine 18 months earlier, asking for cuts in expenditure and noting:
Following the issuance of the Churchill White Paper in June 1922, the House of Lords rejected a Palestine Mandate that incorporated the Balfour Declaration by 60 votes to 25, following a motion issued by Lord Islington. The vote proved to be only symbolic as it was subsequently overruled by a vote in the House of Commons following a tactical pivot and variety of promises made by Churchill.
In February 1923, following the change in government, Cavendish, in a lengthy memorandum for the Cabinet, laid the foundation for a secret review of Palestine policy:
His covering note asked for a statement of policy to be made as soon as possible and that the cabinet ought to focus on three questions: (1) whether or not pledges to the Arabs conflict with the Balfour declaration; (2) if not, whether the new government should continue the policy set down by the old government in the 1922 White Paper; and (3) if not, what alternative policy should be adopted.
Stanley Baldwin, replacing Bonar Law, in June 1923 set up a cabinet subcommittee whose terms of reference were:
The Cabinet approved the report of this committee on 31 July 1923. Describing it as "nothing short of remarkable", Quigley noted that the government was admitting to itself that its support for Zionism had been prompted by considerations having nothing to do with the merits of Zionism or its consequences for Palestine. As Huneidi noted, "wise or unwise, it is well nigh impossible for any government to extricate itself without a substantial sacrifice of consistency and self-respect, if not honour."
The wording of the declaration was thus incorporated into the British Mandate for Palestine, a legal instrument that created Mandatory Palestine with an explicit purpose of putting the declaration into effect and was finally formalized in September,1923. Unlike the declaration itself, the Mandate was legally binding on the British government. In June, 1924, Britain made its report to the Permanent Mandates Commission for the period July 1920 to the end of 1923 containing nothing of the candor reflected in the internal documents; the documents relating to the 1923 reappraisal stayed secret until the early 1970s. 
Lloyd George and Balfour remained in government until the collapse of the coalition in October 1922. Under the new Conservative government, attempts were made to identify the background to and motivations for the declaration. A private Cabinet memorandum was produced in January 1923, providing a summary of the then-known Foreign Office and War Cabinet records leading up to the declaration. An accompanying Foreign Office note asserted that the primary authors of the declaration were Balfour, Sykes, Weizmann, and Sokolow, with "perhaps Lord Rothschild as a figure in the background", and that "negotiations seem to have been mainly oral and by means of private notes and memoranda of which only the scantiest records seem to be available."
Following the 1936 general strike that was to degenerate into the 1936–1939 Arab revolt in Palestine, the most significant outbreak of violence since the Mandate began, a British Royal Commission  – a high-profile public inquiry – was appointed to investigate the causes of the unrest. The Palestine Royal Commission, appointed with significantly broader terms of reference than the previous British inquiries into Palestine, completed its 404-page report after six months of work in June 1937, publishing it a month later. The report began by describing the history of the problem, including a detailed summary of the origins of the Balfour Declaration. Much of this summary relied on Lloyd-George's personal testimony; Balfour had died in 1930 and Sykes in 1919. He told the commission that the declaration was made "due to propagandist reasons ... In particular Jewish sympathy would confirm the support of American Jewry, and would make it more difficult for Germany to reduce her military commitments and improve her economic position on the eastern front". Two years later, in his "Memoirs of the Peace Conference", Lloyd George described a total of nine factors motivating his decision as Prime Minister to release the declaration, including the additional reasons that a Jewish presence in Palestine would strengthen Britain's position on the Suez Canal and reinforce the route to their imperial dominion in India.
These geopolitical calculations were debated and discussed in the following years. Historians agree that the British believed that expressing support would appeal to Jews in Germany and the United States, given two of Woodrow Wilson's closest advisors were known to be avid Zionists; they also hoped to encourage support from the large Jewish population in Russia. In addition, the British intended to pre-empt the expected French pressure for an international administration in Palestine.
Some historians argue that the British government's decision reflected what James Gelvin, Professor of Middle Eastern History at UCLA, calls 'patrician anti-Semitism' in the overestimation of Jewish power in both the United States and Russia. American Zionism was still in its infancy; in 1914 the Zionist Federation had a small budget of about $5,000 and only 12,000 members, despite an American Jewish population of three million. But the Zionist organizations had recently succeeded, following a show of force within the American Jewish community, in arranging a Jewish congress to debate the Jewish problem as a whole. This impacted British and French government estimates of the balance of power within the American Jewish public.
Avi Shlaim, Emeritus Professor of International Relations in the University of Oxford, asserts that two main schools of thought have been developed on the question of the primary driving force behind the declaration, one presented in 1961 by Leonard Stein, a lawyer and former political secretary to the World Zionist Organization, and the other in 1970 by Mayir Vereté, then Professor of Israeli History at the Hebrew University of Jerusalem. Shlaim states that Stein does not reach any clear cut conclusions, but that implicit in his narrative is that the declaration resulted primarily from the activity and skill of the Zionists, whereas according to Vereté, it was the work of hard-headed pragmatists motivated by British imperial interests in the Middle East. Much of modern scholarship on the decision to issue the declaration focuses on the Zionist movement and rivalries within it, with a key debate being whether the role of Weizmann was decisive or whether the British were likely to have issued a similar declaration in any event. Danny Gutwein, Professor of Jewish History at the University of Haifa, proposes a twist on an old idea, asserting that Sykes's February 1917 approach to the Zionists was the defining moment, and that it was consistent with the pursuit of the government's wider agenda to partition the Ottoman Empire. Historian J. C. Hurewitz has written that British support for a Jewish homeland in Palestine was part of an effort to secure a land bridge between Egypt and the Persian Gulf by annexing territory from the Ottoman Empire.
The declaration had two indirect consequences, the emergence of a Jewish state and a chronic state of conflict between Arabs and Jews throughout the Middle East. It has been described as the "original sin" with respect to both Britain's failure in Palestine and for wider events in Palestine. The statement also had a significant impact on the traditional anti-Zionism of religious Jews, some of whom saw it as divine providence; this contributed to the growth of religious Zionism amid the larger Zionist movement.
Starting in 1920, intercommunal conflict in Mandatory Palestine broke out, which widened into the regional Arab–Israeli conflict, often referred to as the world's "most intractable conflict". The "dual obligation" to the two communities quickly proved to be untenable; the British subsequently concluded that it was impossible for them to pacify the two communities in Palestine by using different messages for different audiences. The Palestine Royal Commission – in making the first official proposal for partition of the region – referred to the requirements as "contradictory obligations", and that the "disease is so deep-rooted that, in our firm conviction, the only hope of a cure lies in a surgical operation". Following the 1936–1939 Arab revolt in Palestine, and as worldwide tensions rose in the buildup to the Second World War, the British Parliament approved the White Paper of 1939 – their last formal statement of governing policy in Mandatory Palestine – declaring that Palestine should not become a Jewish State and placing restrictions on Jewish immigration. Whilst the British considered this consistent with the Balfour Declaration's commitment to protect the rights of non-Jews, many Zionists saw it as a repudiation of the declaration. Although this policy lasted until the British surrendered the Mandate in 1948, it served only to highlight the fundamental difficulty for Britain in carrying out the Mandate obligations.
Britain's involvement in this became one of the most controversial parts of its Empire's history, and damaged its reputation in the Middle East for generations. According to historian Elizabeth Monroe: "measured by British interests alone, [the declaration was] one of the greatest mistakes in [its] imperial history." The 2010 study by Jonathan Schneer, specialist in modern British history at Georgia Tech, concluded that because the build-up to the declaration was characterized by "contradictions, deceptions, misinterpretations, and wishful thinking", the declaration sowed dragon's teeth and "produced a murderous harvest, and we go on harvesting even today". The foundational stone for modern Israel had been laid, but the prediction that this would lay the groundwork for harmonious Arab-Jewish cooperation proved to be wishful thinking.
The document was presented to the British Museum in 1924 by Walter Rothschild; today it is held in the British Library, which separated from the British Museum in 1973, as Additional Manuscripts number 41178. From October 1987 to May 1988 it was lent outside the UK for display in Israel's Knesset. The Israeli government are currently in negotiations to arrange a second loan in 2018, with plans to display the document at Independence Hall in Tel Aviv.

</doc>
<doc id="4821" url="https://en.wikipedia.org/wiki?curid=4821" title="Black Hand (Serbia)">
Black Hand (Serbia)

Unification or Death (), popularly known as the Black Hand (Црна рука / Crna ruka), was a secret military society formed in 1901 by officers in the Army of the Kingdom of Serbia, best known for being allegedly involved in assassination of Archduke Franz Ferdinand in Sarajevo and the earlier conspiracy to assassinate the Serbian royal couple in 1903, under the aegis of Captain Dragutin Dimitrijević ( "Apis").
It was formed with the aim of uniting all of the territories with a South Slavic majority not ruled by either Serbia or Montenegro. Its inspiration was primarily the unification of Italy in 1859–70 but also that of Germany in 1871. Through its connections to the June 1914 assassination of Archduke Franz Ferdinand in Sarajevo, which was committed by the members of youth movement Young Bosnia, the Black Hand is often viewed as having contributed to the start of World War I by precipitating the July Crisis of 1914, which eventually led to Austria-Hungary's invasion of the Kingdom of Serbia.
In August 1901, a group of lower officers headed by captain Dragutin Dimitrijević "Apis" established a conspiracy group (called the Black Hand in literature), against the dynasty. The first meeting was held on 6 September 1901. In attendance were captains Radomir Aranđelović, Milan F. Petrović, and Dragutin Dimitrijević, as well as lieutenants Antonije Antić, Dragutin Dulić, Milan Marinković, and Nikodije Popović. They made a plan to kill the royal couple—King Alexander I Obrenović and Queen Draga. Captain Apis personally led the group of Army officers who killed the royal couple in the Old Palace at Belgrade on the night of 28/29 May 1903 (Old Style).
On 8 October 1908, just two days after Austria annexed Bosnia and Herzegovina, some Serbian ministers, officials, and generals held a meeting at the City Hall in Belgrade. They founded a semi-secret society, the "Narodna Odbrana" ("National Defense") which gave Pan-Serbism a focus and an organization. The purpose of the group was to liberate Serbs under the Austro-Hungarian occupation. They also undertook anti-Austrian propaganda and organized spies and saboteurs to operate within the occupied provinces. Satellite groups were formed in Slovenia, Bosnia, Herzegovina and Istria. The Bosnian group became deeply associated with local groups of pan-Serb activists such as "Mlada Bosna" ("Young Bosnia").
 
The Unification or Death was established in the beginning of May 1911, the original constitution of the organization being signed on 9 May. Ljuba Čupa, Bogdan Radenković and Vojislav Tankosić wrote the constitution of the organization. The constitution was modeled after similar German secret nationalist associations and the Italian Carbonari. The organization was mentioned in the Serbian parliament as the "Black Hand" in late 1911.
By 1911–12, Narodna Odbrana had established ties with the Black Hand, and the two became "parallel in action and overlapping in membership".
The organization used the magazine "Pijemont" (the Serbian name for Piedmont, the kingdom that led the unification of Italy, under the House of Savoy) for the dissemination of their ideas. The magazine was founded by Ljuba Čupa in August 1911.
By 1914, there were hundreds of members, many of whom were Serbian Army officers. The goal of uniting Serb-inhabited territories was implemented by training guerilla fighters and saboteurs. The Black Hand was organized at the grassroots level in cells of three to five members, supervised by district committees and by a Central Committee in Belgrade whose ten-member Executive Committee was led more or less by Colonel Dragutin Dimitrijević "Apis". To ensure secrecy, members rarely knew much more than the other members own cell and one superior above them. New members swore the oath:
The Black Hand took over the terrorist actions of "Narodna Odbrana" and worked deliberately at obscuring any distinctions between the two groups, trading on the prestige and network of the older organization. Black Hand members held important army and government positions. Crown Prince Alexander was an enthusiastic and financial supporter. The group held influence over government appointment and policy. The Serbian government was fairly well informed of Black Hand activities.
Friendly relations had fairly well cooled by 1914. The Black Hand was displeased with Prime Minister Nikola Pašić and thought that he did not act aggressively enough towards the Pan-Serb cause. The Black Hand engaged in a bitter power struggle over several issues, such as who would control territories that Serbia had annexed during the Balkan Wars. By then, disagreeing with the Black Hand was dangerous, as political murder was one of its tools.
It was also in 1914 that Apis allegedly decided that Archduke Franz Ferdinand, the heir-apparent of Austria, should be assassinated, as he was trying to pacify the Serbians, which would prevent a revolution if he was successful. Towards that end, three young Bosnian Serbs are claimed to have been recruited to kill the Archduke. They were certainly trained in bomb throwing and marksmanship by current and former members of the Serbian military. Gavrilo Princip, Nedeljko Čabrinović and Trifko Grabež were smuggled across the border back into Bosnia by a chain of contacts similar to the Underground Railroad.
The decision to kill the Archduke was apparently initiated by Apis and not sanctioned by the full Executive Committee if Apis was involved at all, a question that remains in dispute). Those involved probably realised that their plot would result in war between Austria and Serbia and had every reason to expect that Russia would side with Serbia. They likely did not, however, anticipate that the assassination would start a chain of events leading to World War I.
Others in the government and some of the Black Hand Executive Council were not as confident of Russian aid since Russia had recently let them down. When word of the plot allegedly percolated through Black Hand leadership and the Serbian government (Prime Minister Pašić was definitely informed of two armed men being smuggled across the border, but it is not clear if Pašić knew of the planned assassination), Apis was supposedly told not to proceed. He may have made a half-hearted attempt to intercept the young assassins at the border, but they had already crossed. Other sources say the attempted 'recall' began only after the assassins had reached Sarajevo. The 'recall' appears to make Apis look like a loose cannon and the young assassins as independent zealots. In fact, the 'recall' took place fully two weeks before the Archduke's visit. The assassins idled around in Sarajevo for a month. Nothing more was done to stop them.
The group encompassed a range of ideological outlooks, from conspiratorially-minded army officers to idealistic youths, sometimes tending towards republicanism, despite the acquisition of nationalistic royal circles in its activities (the movement's leader, Colonel Dragutin Dimitrijević or "Apis," had been instrumental in the June 1903 coup which had brought King Petar Karađorđević to the Serbian throne following 45 years of rule by the rival Obrenović dynasty). The group was denounced as nihilist by the Austro-Hungarian press and compared to the Russian People's Will and the Chinese Assassination Corps.
In 1938 a conspiracy group to overthrow the Yugoslav regency was founded by, among others, members of the Serbian Cultural Club (SKK). The organization was modeled after the Black Hand, including the recruitment process. Two members of the Black Hand, Antonije Antić, and Velimir Vemić were the organization's military advisors.

</doc>
<doc id="4822" url="https://en.wikipedia.org/wiki?curid=4822" title="Board of directors">
Board of directors

A board of directors is a group of people who jointly supervise the activities of an organization, which can be either a for-profit or a nonprofit organization such as a business, nonprofit organization, or a government agency. 
The powers, duties, and responsibilities of a board of directors are determined by government regulations (including the jurisdiction's corporate law) and the organization's own constitution and bylaws. These authorities may specify the number of members of the board, how they are to be chosen, and how often they are to meet.
In an organization with voting members, the board is accountable to, and may be subordinate to, the organization's full membership, which usually elect the members of the board. In a stock corporation, non-executive directors are elected by the shareholders, and the board has ultimate responsibility for the management of the corporation. In nations with codetermination (such as Germany and Sweden), the workers of a corporation elect a set fraction of the board's members. 
The board of directors appoints the chief executive officer of the corporation and sets out the overall strategic direction. In corporations with dispersed ownership, the identification and nomination of directors (that shareholders vote for or against) are often done by the board itself, leading to a high degree of self-perpetuation. In a non-stock corporation with no general voting membership, the board is the supreme governing body of the institution, and its members are sometimes chosen by the board itself.
Other names include board of directors and advisors, board of governors, board of managers, board of regents, board of trustees, or board of visitors. It may also be called "the executive board" and is often simply referred to as "the board".
Typical duties of boards of directors include:
The legal responsibilities of boards and board members vary with the nature of the organization, and between jurisdictions. For companies with publicly trading stock, these responsibilities are typically much more rigorous and complex than for those of other types.
Typically, the board chooses one of its members to be the "chairman" (often now called the "chair" or "chairperson"), who holds whatever title is specified in the by-laws or articles of association. However, in membership organizations, the members elect the president of the organization and the president becomes the board chair, unless the by-laws say otherwise.
The directors of an organization are the persons who are members of its board. Several specific terms categorize directors by the presence or absence of their other relationships to the organization.
An inside director is a director who is also an employee, officer, chief executive, major shareholder, or someone similarly connected to the organization. Inside directors represent the interests of the entity's stakeholders, and often have special knowledge of its inner workings, its financial or market position, and so on.
Typical inside directors are:
An inside director who is employed as a manager or executive of the organization is sometimes referred to as an executive director (not to be confused with the title executive director sometimes used for the CEO position in some organizations). Executive directors often have a specified area of responsibility in the organization, such as finance, marketing, human resources, or production.
An outside director is a member of the board who is not otherwise employed by or engaged with the organization, and does not represent any of its stakeholders. A typical example is a director who is president of a firm in a different industry. Outside directors are not employees of the company or affiliated with it in any other way.
Outside directors bring outside experience and perspectives to the board. For example, for a company that serves a domestic market only, the presence of CEOs from global multinational corporations as outside directors can help to provide insights on export and import opportunities and international trade options. One of the arguments for having outside directors is that they can keep a watchful eye on the inside directors and on the way the organization is run. Outside directors are unlikely to tolerate "insider dealing" between inside directors, as outside directors do not benefit from the company or organization. Outside directors are often useful in handling disputes between inside directors, or between shareholders and the board. They are thought to be advantageous because they can be objective and present little risk of conflict of interest. On the other hand, they might lack familiarity with the specific issues connected to the organization's governance, and they might not know about the industry or sector in which the organization is operating.
Individual directors often serve on more than one board. This practice results in an interlocking directorate, where a relatively small number of individuals have significant influence over many important entities. This situation can have important corporate, social, economic, and legal consequences, and has been the subject of significant research.
The process for running a board, sometimes called the board process, includes the selection of board members, the setting of clear board objectives, the dissemination of documents or board package to the board members, the collaborative creation of an agenda for the meeting, the creation and follow-up of assigned action items, and the assessment of the board process through standardized assessments of board members, owners, and CEOs. The science of this process has been slow to develop due to the secretive nature of the way most companies run their boards, however some standardization is beginning to develop. Some who are pushing for this standardization in the USA are the National Association of Corporate Directors, McKinsey and The Board Group.
A board of directors conducts its meetings according to the rules and procedures contained in its governing documents. These procedures may allow the board to conduct its business by conference call or other electronic means. They may also specify how a quorum is to be determined.
The responsibilities of a board of directors vary depending on the nature and type of business entity and the laws applying to the entity (see types of business entity). For example, the nature of the business entity may be one that is traded on a public market (public company), not traded on a public market (a private, limited or closely held company), owned by family members (a family business), or exempt from income taxes (a non-profit, not for profit, or tax-exempt entity). There are numerous types of business entities available throughout the world such as a corporation, limited liability company, cooperative, business trust, partnership, private limited company, and public limited company.
Much of what has been written about boards of directors relates to boards of directors of business entities actively traded on public markets. More recently, however, material is becoming available for boards of private and closely held businesses including family businesses.
A board-only organization is one whose board is self-appointed, rather than being accountable to a base of members through elections; or in which the powers of the membership are extremely limited.
In membership organizations, such as a society made up of members of a certain profession or one advocating a certain cause, a board of directors may have the responsibility of running the organization in between meetings of the membership, especially if the membership meets infrequently, such as only at an annual general meeting. The amount of powers and authority delegated to the board depend on the bylaws and rules of the particular organization. Some organizations place matters exclusively in the board's control while in others, the general membership retains full power and the board can only make recommendations.
The setup of a board of directors vary widely across organizations and may include provisions that are applicable to corporations, in which the "shareholders" are the members of the organization. A difference may be that the membership elects the officers of the organization, such as the president and the secretary, and the officers become members of the board in addition to the directors and retain those duties on the board. The directors may also be classified as officers in this situation. There may also be ex-officio members of the board, or persons who are members due to another position that they hold. These ex-officio members have all the same rights as the other board members.
Members of the board may be removed before their term is complete. Details on how they can be removed are usually provided in the bylaws. If the bylaws do not contain such details, the section on disciplinary procedures in "Robert's Rules of Order" may be used.
In a publicly held company, directors are elected to represent and are legally obligated as fiduciaries to represent owners of the company—the shareholders/stockholders. In this capacity they establish policies and make decisions on issues such as whether there is dividend and how much it is, stock options distributed to employees, and the hiring/firing and compensation of upper management.
Theoretically, the control of a company is divided between two bodies: the board of directors, and the shareholders in general meeting. In practice, the amount of power exercised by the board varies with the type of company. In small private companies, the directors and the shareholders are normally the same people, and thus there is no real division of power. In large public companies, the board tends to exercise more of a supervisory role, and individual responsibility and management tends to be delegated downward to individual professional executives (such as a finance director or a marketing director) who deal with particular areas of the company's affairs.
Another feature of boards of directors in large public companies is that the board tends to have more "de facto" power. Many shareholders grant proxies to the directors to vote their shares at general meetings and accept all recommendations of the board rather than try to get involved in management, since each shareholder's power, as well as interest and information is so small. Larger institutional investors also grant the board proxies. The large number of shareholders also makes it hard for them to organize. However, there have been moves recently to try to increase shareholder activism among both institutional investors and individuals with small shareholdings.
A contrasting view is that in large public companies it is upper management and not boards that wield practical power, because boards delegate nearly all of their power to the top executive employees, adopting their recommendations almost without fail. As a practical matter, executives even choose the directors, with shareholders normally following management recommendations and voting for them.
In most cases, serving on a board is not a career unto itself. For major corporations, the board members are usually professionals or leaders in their field. In the case of outside directors, they are often senior leaders of other organizations. Nevertheless, board members often receive remunerations amounting to hundreds of thousands of dollars per year since they often sit on the boards of several companies. Inside directors are usually not paid for sitting on a board, but the duty is instead considered part of their larger job description. Outside directors are usually paid for their services. These remunerations vary between corporations, but usually consist of a yearly or monthly salary, additional compensation for each meeting attended, stock options, and various other benefits. such as travel, hotel and meal expenses for the board meetings. Tiffany & Co., for example, pays directors an annual retainer of $46,500, an additional annual retainer of $2,500 if the director is also a chairperson of a committee, a per-meeting-attended fee of $2,000 for meetings attended in person, a $500 fee for each meeting attended via telephone, in addition to stock options and retirement benefits.
In some European and Asian countries, there are two separate boards, an executive board (or management board) for day-to-day business and a supervisory board (elected by the shareholders and employees) for supervising the executive board. In these countries, the chairman of the supervisory board is equivalent to the chairman of the board in a single-tier system, while the chairman of the management board is reckoned as the company's CEO or managing director. These two roles are always held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board and allows for clear lines of authority. The aim is to prevent a conflict of interest and too much power being concentrated in the hands of one person. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service. 
In the United States, the board of directors (elected by the shareholders) is often equivalent to the supervisory board, while the executive board may often be known as the executive committee (operating committee or executive council), composed of the CEO and their direct reports (other C-level officers, division/subsidiary heads).
The board of directors, in its modern sense, was one of the 17th-century Dutch pioneering institutional innovations. In other words, modern-day boards of directors are all the descendants of the VOC model in many respects.
The development of a separate board of directors to manage/govern/oversee a company has occurred incrementally and indefinitely over legal history. Until the end of the 19th century, it seems to have been generally assumed that the general meeting (of all shareholders) was the supreme organ of a company, and that the board of directors merely acted as an agent of the company subject to the control of the shareholders in general meeting.
However, by 1906, the English Court of Appeal had made it clear in the decision of "Automatic Self-Cleansing Filter Syndicate Co Ltd v Cuninghame" [1906] 2 Ch 34 that the division of powers between the board and the shareholders in general meaning depended on the construction of the articles of association and that, where the powers of management were vested in the board, the general meeting could not interfere with their lawful exercise. The articles were held to constitute a contract by which the members had agreed that "the directors and the directors alone shall manage."
The new approach did not secure immediate approval, but it was endorsed by the House of Lords in "Quin & Axtens v Salmon" [1909] AC 442 and has since received general acceptance. Under English law, successive versions of Table A have reinforced the norm that, unless the directors are acting contrary to the law or the provisions of the Articles, the powers of conducting the management and affairs of the company are vested in them.
The modern doctrine was expressed in "John Shaw & Sons (Salford) Ltd v Shaw" [1935] 2 KB 113 by Greer LJ as follows:
A company is an entity distinct alike from its shareholders and its directors. Some of its powers may, according to its articles, be exercised by directors, certain other powers may be reserved for the shareholders in general meeting. If powers of management are vested in the directors, they and they alone can exercise these powers. The only way in which the general body of shareholders can control the exercise of powers by the articles in the directors is by altering the articles, or, if opportunity arises under the articles, by refusing to re-elect the directors of whose actions they disapprove. They cannot themselves usurp the powers which by the articles are vested in the directors any more than the directors can usurp the powers vested by the articles in the general body of shareholders.
It has been remarked that this development in the law was somewhat surprising at the time, as the relevant provisions in Table A (as it was then) seemed to contradict this approach rather than to endorse it.
In most legal systems, the appointment and removal of directors is voted upon by the shareholders in general meeting or through a proxy statement. For publicly traded companies in the U.S., the directors which are available to vote on are largely selected by either the board as a whole or a nominating committee. Although in 2002 the New York Stock Exchange and the NASDAQ required that nominating committees consist of independent directors as a condition of listing, nomination committees have historically received input from management in their selections even when the CEO does not have a position on the board. Shareholder nominations can only occur at the general meeting itself or through the prohibitively expensive process of mailing out ballots separately; in May 2009 the SEC proposed a new rule allowing shareholders meeting certain criteria to add nominees to the proxy statement. In practice for publicly traded companies, the managers (inside directors) who are purportedly accountable to the board of directors have historically played a major role in selecting and nominating the directors who are voted on by the shareholders, in which case more "gray outsider directors" (independent directors with conflicts of interest) are nominated and elected.
Directors may also leave office by resignation or death. In some legal systems, directors may also be removed by a resolution of the remaining directors (in some countries they may only do so "with cause"; in others the power is unrestricted).
Some jurisdictions also permit the board of directors to appoint directors, either to fill a vacancy which arises on resignation or death, or as an addition to the existing directors.
In practice, it can be quite difficult to remove a director by a resolution in general meeting. In many legal systems, the director has a right to receive special notice of any resolution to remove them; the company must often supply a copy of the proposal to the director, who is usually entitled to be heard by the meeting. The director may require the company to circulate any representations that they wishe to make. Furthermore, the director's contract of service will usually entitle them to compensation if they are removed, and may often include a generous "golden parachute" which also acts as a deterrent to removal.
A recent study examines how corporate shareholders voted in director elections in the United States. It found that directors received fewer votes from shareholders when their companies performed poorly, had excess CEO compensation, or had poor shareholder protection. Also, directors received fewer votes when they did not regularly attend board meetings or received negative recommendations from a proxy advisory firm. The study also shows that companies often improve their corporate governance by removing poison pills or classified boards and by reducing excessive CEO pay after their directors receive low shareholder support.
Board accountability to shareholders is a recurring issue. In 2010, the "New York Times" noted that several directors who had overseen companies which had failed in the financial crisis of 2007–2010 had found new positions as directors. The SEC sometimes imposes a ban (a "D&O bar") on serving on a board as part of its fraud cases, and one of these was upheld in 2013.
The exercise by the board of directors of its powers usually occurs in board meetings. Most legal systems require sufficient notice to be given to all directors of these meetings, and that a quorum must be present before any business may be conducted. Usually, a meeting which is held without notice having been given is still valid if all of the directors attend, but it has been held that a failure to give notice may negate resolutions passed at a meeting, because the persuasive oratory of a minority of directors might have persuaded the majority to change their minds and vote otherwise.
In most common law countries, the powers of the board are vested in the board as a whole, and not in the individual directors. However, in instances an individual director may still bind the company by his acts by virtue of his ostensible authority (see also: the rule in "Turquand's Case").
Because directors exercise control and management over the organization, but organizations are (in theory) run for the benefit of the shareholders, the law imposes strict duties on directors in relation to the exercise of their duties. The duties imposed on directors are fiduciary duties, similar to those that the law imposes on those in similar positions of trust: agents and trustees.
The duties apply to each director separately, while the powers apply to the board jointly. Also, the duties are owed to the company itself, and not to any other entity. This does not mean that directors can never stand in a fiduciary relationship to the individual shareholders; they may well have such a duty in certain circumstances.
Directors must exercise their powers for a proper purpose. While in many instances an improper purpose is readily evident, such as a director looking to feather his or her own nest or divert an investment opportunity to a relative, such breaches usually involve a breach of the director's duty to act in good faith. Greater difficulties arise where the director, while acting in good faith, is serving a purpose that is not regarded by the law as proper.
The seminal authority in relation to what amounts to a proper purpose is the Supreme Court decision in Eclairs Group Ltd v JKX Oil & Gas plc (2015). The case concerned the powers of directors under the articles of association of the company to disenfranchise voting rights attached to shares for failure to properly comply with notice served on the shareholders. Prior to that case the leading authority was "Howard Smith Ltd v Ampol Ltd" [1974] AC 821. The case concerned the power of the directors to issue new shares. It was alleged that the directors had issued many new shares purely to deprive a particular shareholder of his voting majority. An argument that the power to issue shares could only be properly exercised to raise new capital was rejected as too narrow, and it was held that it would be a proper exercise of the director's powers to issue shares to a larger company to ensure the financial stability of the company, or as part of an agreement to exploit mineral rights owned by the company. If so, the mere fact that an incidental result (even if it was a desired consequence) was that a shareholder lost his majority, or a takeover bid was defeated, this would not itself make the share issue improper. But if the sole purpose was to destroy a voting majority, or block a takeover bid, that would be an improper purpose.
Not all jurisdictions recognised the "proper purpose" duty as separate from the "good faith" duty however.
Directors cannot, without the consent of the company, fetter their discretion in relation to the exercise of their powers, and cannot bind themselves to vote in a particular way at future board meetings. This is so even if there is no improper motive or purpose, and no personal advantage to the director.
This does not mean, however, that the board cannot agree to the company entering into a contract which binds the company to a certain course, even if certain actions in that course will require further board approval. The company remains bound, but the directors retain the discretion to vote against taking the future actions (although that may involve a breach by the company of the contract that the board previously approved).
As fiduciaries, the directors may not put themselves in a position where their interests and duties conflict with the duties that they owe to the company. The law takes the view that good faith must not only be done, but must be manifestly seen to be done, and zealously patrols the conduct of directors in this regard; and will not allow directors to escape liability by asserting that his decision was in fact well founded. Traditionally, the law has divided conflicts of duty and interest into three sub-categories.
By definition, where a director enters into a transaction with a company, there is a conflict between the director's interest (to do well for himself out of the transaction) and his duty to the company (to ensure that the company gets as much as it can out of the transaction). This rule is so strictly enforced that, even where the conflict of interest or conflict of duty is purely hypothetical, the directors can be forced to disgorge all personal gains arising from it. In "Aberdeen Ry v Blaikie" (1854) 1 Macq HL 461 Lord Cranworth stated in his judgment that:
However, in many jurisdictions the members of the company are permitted to ratify transactions which would otherwise fall foul of this principle. It is also largely accepted in most jurisdictions that this principle can be overridden in the company's constitution.
In many countries, there is also a statutory duty to declare interests in relation to any transactions, and the director can be fined for failing to make disclosure.
Directors must not, without the informed consent of the company, use for their own profit the company's assets, opportunities, or information. This prohibition is much less flexible than the prohibition against the transactions with the company, and attempts to circumvent it using provisions in the articles have met with limited success.
In "Regal (Hastings) Ltd v Gulliver" [1942] All ER 378 the House of Lords, in upholding what was regarded as a wholly unmeritorious claim by the shareholders, held that:
And accordingly, the directors were required to disgorge the profits that they made, and the shareholders received their windfall.
The decision has been followed in several subsequent cases, and is now regarded as settled law.
Directors cannot compete directly with the company without a conflict of interest arising. Similarly, they should not act as directors of competing companies, as their duties to each company would then conflict with each other.
Traditionally, the level of care and skill which has to be demonstrated by a director has been framed largely with reference to the non-executive director. In "Re City Equitable Fire Insurance Co" [1925] Ch 407, it was expressed in purely subjective terms, where the court held that:
However, this decision was based firmly in the older notions (see above) that prevailed at the time as to the mode of corporate decision making, and effective control residing in the shareholders; if they elected and put up with an incompetent decision maker, they should not have recourse to complain.
However, a more modern approach has since developed, and in "Dorchester Finance Co Ltd v Stebbing" [1989] BCLC 498 the court held that the rule in "Equitable Fire" related only to skill, and not to diligence. With respect to diligence, what was required was:
This was a dual subjective and objective test, and one deliberately pitched at a higher level.
More recently, it has been suggested that both the tests of skill and diligence should be assessed objectively and subjectively; in the United Kingdom, the statutory provisions relating to directors' duties in the new Companies Act 2006 have been codified on this basis.
In most jurisdictions, the law provides for a variety of remedies in the event of a breach by the directors of their duties:
Historically, directors' duties have been owed almost exclusively to the company and its members, and the board was expected to exercise its powers for the financial benefit of the company. However, more recently there have been attempts to "soften" the position, and provide for more scope for directors to act as good corporate citizens. For example, in the United Kingdom, the Companies Act 2006 requires directors of companies "to promote the success of the company for the benefit of its members as a whole" and sets out the following six factors regarding a director's duty to promote success:
This represents a considerable departure from the traditional notion that directors' duties are owed only to the company. Previously in the United Kingdom, under the Companies Act 1985, protections for non-member stakeholders were considerably more limited (see, for example, s.309 which permitted directors to take into account the interests of employees but which could only be enforced by the shareholders and not by the employees themselves). The changes have therefore been the subject of some criticism.
Board of Directors Technology
The adoption of technology that facilitates the meeting preparation and execution of directors continues to grow. Board directors are increasingly leveraging this technology to communicate and collaborate within a secure environment to access meeting materials, communicate with each other, and execute their governance responsibilities. This trend is particularly acute in the United States where a robust market of early adopters garnered acceptance of board software by organizations resulting in higher penetration of the board portal services in the region.
Most companies have weak mechanisms for bringing the voice of society into the board room. They rely on personalities who weren't appointed for their understanding of societal issues. Often they give limited focus (both through time and financial resource) to issues of corporate responsibility and sustainability. A Social Board has society designed into its structure. It elevates the voice of society through specialist appointments to the board and mechanisms that empower innovation from within the organisation. Social Boards align themselves with themes that are important to society. These may include measuring worker pay ratios, linking personal social and environmental objectives to remuneration, integrated reporting, fair tax and B-Corp Certification.
Social Boards recognise that they are part of society and that they require more than a licence to operate to succeed. They balance short-term shareholder pressure against long-term value creation, managing the business for a plurality of stakeholders including employees, shareholders, supply chains and civil society.
The Sarbanes–Oxley Act of 2002 has introduced new standards of accountability on boards of U.S. companies or companies listed on U.S. stock exchanges. Under the Act, directors risk large fines and prison sentences in the case of accounting crimes. Internal control is now the direct responsibility of directors. The vast majority of companies covered by the Act have hired internal auditors to ensure that the company adheres to required standards of internal control. The internal auditors are required by law to report directly to an audit board, consisting of directors more than half of whom are outside directors, one of whom is a "financial expert."
The law requires companies listed on the major stock exchanges (NYSE, NASDAQ) to have a majority of independent directors—directors who are not otherwise employed by the firm or in a business relationship with it.
According to the Corporate Library's study, the average size of publicly traded company's board is 9.2 members, and most boards range from 3 to 31 members. According to Investopedia, some analysts think the ideal size is seven. State law may specify a minimum number of directors, maximum number of directors, and qualifications for directors (e.g. whether board members must be individuals or may be business entities).
While a board may have several committees, two—the compensation committee and audit committee—are critical and must be made up of at least three independent directors and no inside directors. Other common committees in boards are nominating and governance.
Directors of Fortune 500 companies received median pay of $234,000 in 2011. Directorship is a part-time job. A recent National Association of Corporate Directors study found directors averaging just 4.3 hours a week on board work. Surveys indicate that about 20% of nonprofit foundations pay their board members, and 2% of American nonprofit organizations do. 80% of nonprofit organizations require board members to personally contribute to the organization, as BoardSource recommends. This percentage has increased in recent years.
According to John Gillespie, a former investment banker and co-author of a book critical of boards, "Far too much of their time has been for check-the-box and cover-your-behind activities rather than real monitoring of executives and providing strategic advice on behalf of shareholders". At the same time, scholars have found that individual directors have a large effect on major corporate initiatives such as mergers and acquisitions and cross-border investments.
The issue of gender representation on corporate boards of directors has been the subject of much criticism in recent years. Governments and corporations have responded with measures such as legislation mandating gender quotas and comply or explain systems to address the disproportionality of gender representation on corporate boards. A study of the French corporate elite has found that certain social classes are also disproportionately represented on boards, with those from the upper and, especially, upper-middle classes tending to dominate.

</doc>
<doc id="4823" url="https://en.wikipedia.org/wiki?curid=4823" title="Balkan Wars">
Balkan Wars

The Balkan Wars consisted of two conflicts that took place in the Balkan Peninsula in 1912 and 1913. Four Balkan states defeated the Ottoman Empire in the First Balkan War. In the Second Balkan War, Bulgaria fought against all four original combatants of the first war along with facing a surprise attack from Romania from the north. The conflicts ended catastrophically for the Ottoman Empire, which lost the bulk of its territory in Europe. Austria-Hungary, although not a combatant, became relatively weaker as a much enlarged Serbia pushed for union of the South Slavic peoples. The war set the stage for the Balkan crisis of 1914 and thus served as a "prelude to the First World War".
By the early 20th century, Bulgaria, Greece, Montenegro and Serbia had achieved independence from the Ottoman Empire, but large elements of their ethnic populations remained under Ottoman rule. In 1912, these countries formed the Balkan League. The First Balkan War began on 8 October 1912, when the League member states attacked the Ottoman Empire, and ended eight months later with the signing of the Treaty of London on 30 May 1913. The Second Balkan War began on 16 June 1913, when Bulgaria, dissatisfied with its loss of Macedonia, attacked its former Balkan League allies. The more numerous combined Serbian and Greek armies repelled the Bulgarian offensive and counter-attacked into Bulgaria from the west and the south. Romania, having taken no part in the conflict, had intact armies to strike with and invaded Bulgaria from the north in violation of a peace treaty between the two states. The Ottoman Empire also attacked Bulgaria and advanced in Thrace regaining Adrianople. In the resulting Treaty of Bucharest, Bulgaria conserved most of the territories it had gained in the First Balkan War in addition to being forced to cede the ex-Ottoman south part of Dobroudja province to Romania.
The background to the wars lies in the incomplete emergence of nation-states on the European territory of the Ottoman Empire during the second half of the 19th century. Serbia had gained substantial territory during the Russo-Turkish War, 1877–1878, while Greece acquired Thessaly in 1881 (although it lost a small area back to the Ottoman Empire in 1897) and Bulgaria (an autonomous principality since 1878) incorporated the formerly distinct province of Eastern Rumelia (1885). All three countries, as well as Montenegro, sought additional territories within the large Ottoman-ruled region known as Rumelia, comprising Eastern Rumelia, Albania, Macedonia, and Thrace.
The First Balkan War had some main causes briefly presented below:
Throughout the 19th century, the Great Powers shared different aims over the "Eastern Question" and the integrity of the Ottoman Empire. Russia wanted access to the "warm waters" of the Mediterranean from the Black Sea; it pursued a pan-Slavic foreign policy and therefore supported Bulgaria and Serbia. Britain wished to deny Russia access to the "warm waters" and supported the integrity of the Ottoman Empire, although it also supported a limited expansion of Greece as a backup plan in case integrity of the Empire was no longer possible. France wished to strengthen its position in the region, especially in the Levant (today's Lebanon, Syria, and Israel).
Habsburg-ruled Austria-Hungary wished for a continuation of the existence of the Ottoman Empire, since both were troubled multinational entities and thus the collapse of the one might weaken the other. The Habsburgs also saw a strong Ottoman presence in the area as a counterweight to the Serbian nationalistic call to their own Serb subjects in Bosnia, Vojvodina and other parts of the empire. Italy's primary aim at the time seems to have been the denial of access to the Adriatic Sea to another major sea power. The German Empire, in turn, under the "Drang nach Osten" policy, aspired to turn the Ottoman Empire into its own de facto colony, and thus supported its integrity. In the late 19th and early 20th century, Bulgaria and Greece contended for Ottoman Macedonia and Thrace. Ethnic Greeks sought the forced "Hellenization" of ethnic Bulgars, who sought "Bulgarization" of Greeks (Rise of nationalism). Both nations sent armed irregulars into Ottoman territory to protect and assist their ethnic kindred. From 1904, there was low intensity warfare in Macedonia between the Greek and Bulgarian bands and the Ottoman army (the Struggle for Macedonia). After the Young Turk revolution of July 1908, the situation changed drastically.
The 1908 Young Turk Revolution saw the reinstatement of constitutional monarchy in the Ottoman Empire and the start of the Second Constitutional Era. When the revolt broke out, it was supported by intellectuals, the army, and almost all the ethnic minorities of the Empire, and forced Sultan Abdul Hamid II to re-adopt the long defunct Ottoman constitution of 1876 and parliament. Hopes were raised among the Balkan ethnicities of reforms and autonomy, and elections were held to form a representative, multi-ethnic, Ottoman parliament. However, following the Sultan's attempted counter-coup, the liberal element of the Young Turks was sidelined and the nationalist element became dominant.
At the same time, in October 1908, Austria-Hungary seized the opportunity of the Ottoman political upheaval to annex the "de jure" Ottoman province of Bosnia and Herzegovina, which it had occupied since 1878 (see "Bosnian Crisis"). Bulgaria declared independence as it had done in 1878, but this time the independence was internationally recognised. The Greeks of the autonomous Cretan State proclaimed unification with Greece, though the opposition of the Great Powers prevented the latter action from taking practical effect. It has large influence in the consequent world order.
Serbia was frustrated in the north by Austria-Hungary's incorporation of Bosnia. In March 1909, Serbia was forced to accept the annexation and restrain anti-Habsburg agitation by Serbian nationalists. Instead, the Serbian government (PM: Nikola Pašić) looked to formerly Serb territories in the south, notably "Old Serbia" (the Sanjak of Novi Pazar and the province of Kosovo).
On 15 August 1909, the Military League, a group of Greek officers, took action against the government to reform their country's national government and reorganize the army. The Military League sought the creation of a new political system, thus summoned the Cretan politician Eleutherios Venizelos to Athens as its political advisor. Venizelos persuaded king George I to revise the constitution and asked the League to disband in favor of a National Assembly. In March 1910, the Military League dissolved itself.
Bulgaria, which had secured Ottoman recognition of her independence in April 1909 and enjoyed the friendship of Russia, also looked to annex districts of Ottoman Thrace and Macedonia. In August 1910, Montenegro followed Bulgaria's precedent by becoming a kingdom.
Following the Italian victory in the Italo-Turkish War of 1911–1912, the severity of the Ottomanizing policy of the Young Turkish regime and a series of three revolts in Ottoman held Albania, the Young Turks fell from power after a coup. The Christian Balkan countries were forced to take action and saw this as an opportunity to promote their national agenda by expanding in the territories of the falling empire and liberating their enslaved co-patriots. In order to achieve that, a wide net of treaties was constructed and an alliance was formed.
The negotiation among the Balkan States’ governments started in the latter part of 1911 and were all conducted in secret. The treaties and military conventions were published in French translations after the Balkan Wars, on 24-26 of November, in Le Matin, Paris, France 
In April 1911, Greek PM Eleutherios Venizelos’ attempt to reach an agreement with the Bulgarian PM and form a defensive alliance against the Ottoman Empire was fruitless, because of the doubts the Bulgarians held on the strength of the Greek Army. Later that year, in December 1911, Bulgaria and Serbia agreed to start negotiations in forming an alliance under the tight inspection of Russia. The treaty between Serbia and Bulgaria was signed on 29 of February/13 of March 1912. Serbia sought expansion to "Old Serbia" and as Milan Milovanovich noted in 1909 to the Bulgarian counterpart, "As long as we are not allied with you, our influence over the Croats and Slovens will be insignificant". On the other side, Bulgaria wanted the autonomy of Macedonia region under the influence of the two countries. The then Bulgarian Minister of Foreign Affairs General Stefan Paprikov stated in 1909 that, "It will be clear that if not today then tomorrow, the most important issue will again be the Macedonian Question. And this question, whatever happens, cannot be decided without more or less direct participation of the Balkan States". Last but not least, they noted down the divisions should be made of the Ottoman territories after a victorious outcome of the war. More specifically, Bulgaria would gain all the territories eastern of Rodopi Mountains and River Strimona, while Serbia would annex the territories northern and western of Mount Skardo.
The alliance pact between Greece and Bulgaria was finally signed on 16/29 of May 1912, without stipulating any specific division of Ottoman territories. In summer 1912, Greece proceeded on making "gentlemen’s’ agreements" with Serbia and Montenegro. Despite the fact that, a draft of the alliance pact with Serbia was submitted on 22 of October, a formal pact was never signed due to the outbreak of the war. As a result, Greece did not have any territorial or other commitments, other than the common cause to fight the Ottoman Empire.
In April 1912 Montenegro and Bulgaria reached an agreement including financial aid to Montenegro in case of war with the Ottoman Empire. A gentlemen's agreement with Greece was reached soon after, as mentioned before. By the end of September a political and military alliance between Montenegro and Serbia was achieved.
By the end of September 1912, Bulgaria had formal-written alliances with Serbia, Greece and Montenegro. A formal alliance was also signed between Serbia and Montenegro, while Greco-Montenegrin and Greco-Serbian agreements were basically oral "gentlemen’s agreements". All these completed the formation of the Balkan League.
At that time, the Balkan States had been able to maintain armies that were both numerous, in relation to each country's population, and eager to act, being aspired by the idea that they would free enslaved parts of their homeland. The Bulgarian Army was the leading army of the coalition. It was a well-trained and fully equipped army, capable of facing the Imperial Army. It was suggested that the bulk of the Bulgarian Army would be in the Thracian front, as it was expected that the front near the Ottoman Capital would be the most crucial one. The Serbian Army would act in the Macedonian front, while the Greek Army was thought powerless and was not taken under serious consideration. At that time, Greece was needed in the Balkan League only for its navy and its capability to dominate the Aegean Sea, cutting off the Ottoman Armies from reinforcements.
On 13/26 of September 1912, the Ottoman mobilization in Thrace forced Serbia and Bulgaria to act and order their own mobilization. On 17/30 of September Greece also order mobilization. On 25 of September/8 of October, Montenegro declared war on the Ottoman Empire, after negotiations failed regarding the border status. On 30 of September/13 of October the ambassadors of Serbia, Bulgaria and Greece delivered the common ultimatum to the Ottoman government, which was immediately rejected. The Empire withdrew its ambassadors from Sofia, Belgrade and Athens, while the Bulgarian, Serbian and Greek diplomats left the Ottoman capital delivering the war declaration on 4/17 of October 1912.
The three Slavic allies (Bulgaria, Serbia and Montenegro) had laid out extensive plans to coordinate their war efforts, in continuation of their secret prewar settlements and under close Russian supervision (Greece was not included). Serbia and Montenegro would attack in the theater of Sandjak, Bulgaria and Serbia in Macedonia and Thrace.
The Ottoman Empire's situation was difficult. Its population of about 26 million people provided a massive pool of manpower, but three-quarters of the population and nearly all of the Muslim component lived in the Asian part of the Empire. Reinforcements had to come from Asia mainly by sea, which depended on the result of battles between the Turkish and Greek navies in the Aegean.
With the outbreak of the war, the Ottoman Empire activated three Army HQs: the Thracian HQ in Constantinople, the Western HQ in Salonika, and the Vardar HQ in Skopje, against the Bulgarians, the Greeks and the Serbians respectively. Most of their available forces were allocated to these fronts. Smaller independent units were allocated elsewhere, mostly around heavily fortified cities.
Montenegro was the first that declared war on 8 October (25 September O.S.). Its main thrust was towards Shkodra, with secondary operations in the Novi Pazar area. The rest of the Allies, after giving a common ultimatum, declared war a week later. Bulgaria attacked towards Eastern Thrace, being stopped only at the outskirts of Constantinople at the Çatalca line and the isthmus of the Gallipoli peninsula, while secondary forces captured Western Thrace and Eastern Macedonia. Serbia attacked south towards Skopje and Monastir and then turned west to present-day Albania, reaching the Adriatic, while a second Army captured Kosovo and linked with the Montenegrin forces. Greece's main forces attacked from Thessaly into Macedonia through the Sarantaporo strait. On 7 November, in response to an Ottoman initiative, they entered into negotiations for the surrender of Thessaloniki. With the Greeks already there, and the Bulgarian 7th Rila Division moving swiftly from the north towards Thessaloniki, Hassan Tahsin Pasha considered his position to be hopeless. . The Greeks offered more attractive terms than the Bulgarians did. On 8 November, Tahsin Pasha agreed to terms and 26,000 Ottoman troops passed over into Greek captivity. Before the Greeks entered the city, a German warship whisked the former sultan Abdul Hamid II out of Thessaloniki to continue his exile, across the Bosporus from Constantinople. With their army in Thessaloniki, the Greeks took new positions to the east and northeast, including Nigrita. On 12 November (on 26 October 1912, O.S.) Greece expanded its occupied area and teamed up with the Serbian army to the northwest, while its main forces turned east towards Kavala, reaching the Bulgarians. Another Greek army attacked into Epirus towards Ioannina.
On the naval front, the Ottoman fleet twice exited the Dardanelles and was twice defeated by the Greek Navy, in the battles of Elli and Lemnos. Greek dominance on the Aegean Sea made it impossible for the Ottomans to transfer the planned troops from the Middle East to the Thracian (against the Bulgarian) and to the Macedonian (against the Greeks and Serbians) fronts. According to E.J. Erickson the Greek Navy also played a crucial, albeit indirect role, in the Thracian campaign by neutralizing no less than three Thracian Corps (see First Balkan War, the Bulgarian theater of operations), a significant portion of the Ottoman Army there, in the all-important opening round of the war. After the defeat of the Ottoman fleet, the Greek Navy was also free to liberate the islands of the Aegean. General Nikola Ivanov identified the activity of the Greek Navy as the chief factor in the general success of the allies.
In January, after a successful coup by young army officers, the Ottoman Empire decided to continue the war. After a failed Ottoman counter-attack in the Western-Thracian front, Bulgarian forces, with the help of the Serbian Army, managed to conquer Adrianople, while Greek forces managed to take Ioannina after defeating the Ottomans in the battle of Bizani. In the joint Serbian-Montenegrin theater of operation, the Montenegrin army besieged and captured the Shkodra, ending the Ottoman presence in Europe west of the Çatalca line after nearly 500 years. The war ended officially with the Treaty of London on 30(17) May 1913.
After pressure from the Great Powers towards Greece and Serbia, who had postponed signing in order to fortify their defensive positions, the signing of the Treaty of London took place on 30 May 1913. With this Treaty came the end of the war between the Balkan Allies and the Ottoman Empire. From now on, the Great Powers had the right of decision on the territorial adjustments that had to be made, which even led to the creation of an independent Albania. Every Aegean island belonging to the Ottoman Empire, with the exception of Imbros and Tenedos, was handed over to the Greeks, including the island of Crete.
Furthermore, all European territory of the Ottoman Empire west of the Enos-Midia (Enez-Midye) line, was ceded to the Balkan League, but the division of the territory among the League was not to be decided by the Treaty itself.
This event led to the formation of two ‘de facto’ military occupation zones on the Macedonian territory, as Greece and Serbia tried to create a common border. The Bulgarians were not satisfied with their share of spoils and as a result, the Second Balkan War broke out on the night of 29 June 1913, as Bulgaria confronted the Serbian and Greek lines in Macedonia. .
Though the Balkan allies had fought together against the common enemy, that was not enough to overcome their mutual rivalries. In the original document for the Balkans league, Serbia promised Bulgaria most of Macedonia. But before the first war come to an end, Serbia (in violation of the previous agreement) and Greece revealed their plan to keep possession of the territories that their forces had occupied. This act prompted the tsar of Bulgaria to invade his allies. The Second Balkan War broke out on 29 (16) June 1913, when Bulgaria attacked its erstwhile allies in the First Balkan War, Serbia and Greece, while Montenegro and the Ottoman Empire intervened later against Bulgaria, with Romania attacking Bulgaria from the north in violation of a peace treaty.
When the Greek army had entered Thessaloniki in the First Balkan War ahead of the Bulgarian 7th division by only a day, they were asked to allow a Bulgarian battalion to enter the city. Greece accepted in exchange for allowing a Greek unit to enter the city of Serres. The Bulgarian unit that entered Thessaloniki turned out to be an 18,000-strong division instead of the battalion, which caused concern among the Greeks, who viewed it as a Bulgarian attempt to establish a condominium over the city. In the event, due to the urgently needed reinforcements in the Thracian front, Bulgarian Headquarters was soon forced to remove its troops from the city (while the Greeks agreed by mutual treaty to remove their units based in Serres) and transport them to Dedeağaç (modern Alexandroupolis), but still it left behind a battalion that started fortifying its positions.
Greece had also allowed the Bulgarians to control the stretch of the Thessaloniki-Constantinople railroad that lay in Greek-occupied territory, since Bulgaria controlled the largest part of this railroad towards Thrace. After the end of the operations in Thrace, and confirming Greek concerns, Bulgaria was not satisfied with the territory it controlled in Macedonia and immediately asked Greece to relinquish its control over Thessaloniki and the land north of Pieria, effectively handing over all Aegean Macedonia. These unacceptable demands, with the Bulgarian refusal to demobilize its army after the Treaty of London had ended the common war against the Ottomans, alarmed Greece, which decided to also keep its army mobilized. A month after the Second Balkan War started, the Bulgarian community of Thessaloniki no longer existed, as hundreds of long-time Bulgarian locals were arrested. Thirteen hundred Bulgarian soldiers and about five hundred komitadjis were also arrested and transferred to Greek prisons. In November 1913, the Bulgarians were forced to admit their defeat, as the Greeks received international recognition on their claim of Thessaloniki .
Similarly, in North Macedonia, the tension between Serbia and Bulgaria due to the latter's aspirations over Vardar Macedonia generated many incidents between their respective armies, prompting Serbia to keep its army mobilized. Serbia and Greece proposed that each of the three countries reduce its army by one fourth, as a first step to facilitate a peaceful solution, but Bulgaria rejected it. Seeing the omens, Greece and Serbia started a series of negotiations and signed a treaty on 1 June(19 May) 1913. With this treaty, a mutual border was agreed between the two countries, together with an agreement for mutual military and diplomatic support in case of a Bulgarian or/and Austro-Hungarian attack. Tsar Nicholas II of Russia, being well informed, tried to stop the upcoming conflict on 8 June, by sending an identical personal message to the Kings of Bulgaria and Serbia, offering to act as arbitrator according to the provisions of the 1912 Serbo-Bulgarian treaty. But Bulgaria, by making the acceptance of Russian arbitration conditional, in effect denied any discussion, causing Russia to repudiate its alliance with Bulgaria (see Russo-Bulgarian military convention signed 31 May 1902).
The Serbs and the Greeks had a military advantage on the eve of the war because their armies confronted comparatively weak Ottoman forces in the First Balkan War and suffered relatively light casualties, while the Bulgarians were involved in heavy fighting in Thrace. The Serbs and Greeks had time to fortify their positions in Macedonia. The Bulgarians also held some advantages, controlling internal communication and supply lines.
On 29(16) June 1913, General Savov, under direct orders of Tsar Ferdinand I, issued attacking orders against both Greece and Serbia without consulting the Bulgarian government and without any official declaration of war. During the night of 30(17) June 1913, they attacked the Serbian army at Bregalnica river and then the Greek army in Nigrita. The Serbian army resisted the sudden night attack, while most of soldiers did not even know who they were fighting with, as Bulgarian camps were located next to Serbs and were considered allies. Montenegro's forces were just a few kilometers away and also rushed to the battle. The Bulgarian attack was halted.
The Greek army was also successful. It retreated according to plan for two days while Thessaloniki was cleared of the remaining Bulgarian regiment. Then, the Greek army counter-attacked and defeated the Bulgarians at Kilkis (Kukush), after which the mostly Bulgarian town was plundered and burnt and part of its mostly Bulgarian population massacred by the Greek army. Following the capture of Kilkis, the Greek army's pace was not quick enough to prevent the retaliatory destruction of Nigrita, Serres, and Doxato and massacres of non-combatant Greek inhabitants at Sidirokastro and Doxato by the Bulgarian army. The Greek army then divided its forces and advanced in two directions. Part proceeded east and occupied Western Thrace. The rest of the Greek army advanced up to the Struma River valley, defeating the Bulgarian army in the battles of Doiran and Mt. Beles, and continued its advance to the north towards Sofia. In the Kresna straits, the Greeks were ambushed by the Bulgarian 2nd and 1st Armies, newly arrived from the Serbian front, that had already taken defensive positions there following the Bulgarian victory at Kalimanci.
By 30 July, the Greek army was outnumbered by the counter-attacking Bulgarian army, which attempted to encircle the Greeks in a Cannae-type battle, by applying pressure on their flanks. The Greek army was exhausted and faced logistical difficulties. The battle was continued for 11 days, between 29 July and 9 August over 20 km of a maze of forests and mountains with no conclusion. The Greek King, seeing that the units he fought were from the Serbian front, tried to convince the Serbs to renew their attack, as the front ahead of them was now thinner, but the Serbs declined. By then, news came of the Romanian advance toward Sofia and its imminent fall. Facing the danger of encirclement, Constantine realized that his army could no longer continue hostilities. Thus, he agreed to Eleftherios Venizelos' proposal and accepted the Bulgarian request for armistice as had been communicated through Romania.
Romania had raised an army and declared war on Bulgaria on 10 July(27 June) as it had from 28(15) June officially warned Bulgaria that it would not remain neutral in a new Balkan war, due to Bulgaria's refusal to cede the fortress of Silistra as promised before the First Balkan war in exchange for Romanian neutrality. Its forces encountered little resistance and by the time the Greeks accepted the Bulgarian request for armistice they had reached Vrazhdebna, from the center of Sofia.
Seeing the military position of the Bulgarian army the Ottomans decided to intervene. They attacked, and, finding no opposition, managed to lose all of their land which was officialy ceded to Bulgaria as a part of the Sofia conference in 1914 Thrace with its fortified city of Adrianople, regaining an area in Europe which was only slightly larger than the present-day European territory of the Republic of Turkey.
The developments that led to the First Balkan War did not go unnoticed by the Great Powers. Although there was an official consensus between the European Powers over the territorial integrity of the Ottoman Empire, which led to a stern warning to the Balkan states, unofficially each of them took a different diplomatic approach due to their conflicting interests in the area. As a result, any possible preventive effect of the common official warning was cancelled by the mixed unofficial signals, and failed to prevent or to stop the war:
The Second Balkan war was a catastrophic blow to Russian policies in the Balkans, which for centuries had focused on access to the "warm seas". First, it marked the end of the Balkan League, a vital arm of the Russian system of defense against Austria-Hungary. Second, the clearly pro-Serbian position Russia had been forced to take in the conflict, mainly due to the disagreements over land partitioning between Serbia and Bulgaria, caused a permanent break-up between the two countries. Accordingly, Bulgaria reverted its policy to one closer to the Central Powers' understanding over an anti-Serbian front, due to its new national aspirations, now expressed mainly against Serbia. As a result, Serbia was isolated militarily against its rival Austria-Hungary, a development that eventually doomed Serbia in the coming war a year later. But, most damaging, the new situation effectively trapped Russian foreign policy: After 1913, Russia could not afford losing its last ally in this crucial area and thus had no alternatives but to unconditionally support Serbia when the crisis between Serbia and Austria broke out in 1914. This was a position that inevitably drew her, although unwillingly, into a World War with devastating results for her, since she was less prepared (both militarily and socially) for that event than any other Great Power.
Austria-Hungary took alarm at the great increase in Serbia's territory at the expense of its national aspirations in the region, as well as Serbia's rising status, especially to Austria-Hungary's Slavic populations. This concern was shared by Germany, which saw Serbia as a satellite of Russia. This contributed significantly to the two Central Powers' willingness to go to war as soon as possible.
Finally, when a Serbian backed organization assassinated the heir of the Austro-Hungarian throne, causing the 1914 July Crisis, no-one could stop the conflict and the First World War broke out.
The epilogue to this nine-month pan-Balkan war was drawn mostly by the treaty of Bucharest, August 10, 1913. Delegates of Greece, Serbia, Montenegro and Bulgaria, hosted by the deputy of Romania arrived in Bucharest to settle negotiations. Ottoman's request to participate was rejected, on the basis that the talks were to deal with matters strictly among the Balkan allies. The Great Powers maintained a very influential presence, but they did not dominate the proceedings. The Treaty partitioned Macedonia, made changes to the Balkan borders and established the independent state of Albania. 
Serbia, gained the territory of north-east Macedonia, settled the eastern borders with Bulgaria and gained the eastern half of the Sanjak of Novi-Bazar, doubling its size. Montenegro gained the western half of the Sanjak of Novi-Bazar and secured the borders with Serbia. Greece over-doubled its size by gaining southern Epirus, southern Macedonia (the biggest part), including the city-port of Kavala in its eastern border. What is more, the Aegean Islands were annexed by the Greek Kingdom, apart from Dodecanese, and Cretan unification was completed and established. Romania annexed the southern part of Dobrudja province. Bulgaria, finally, even though defeated, managed to hold some territorial gains from the First Balkan War. Bulgaria embraced a portion of Macedonia, including the town of Strumnitza, and western Thrace with a 70-mile Aegean littoral including the port-town of Alexandroupolis.
The need to deal with the Ottoman counter-attack brought the Bulgarian delegates in Constantinople to negotiate with the Ottomans. Basic purpose and hope of the Bulgarians was to regain the territories in Eastern Thrace where the bulk of the Bulgarian forces had struggled to conquer and many soldiers died there. This hope soon dashed, as the Turks insisted on retaining the lands that had been regained after the counter-attack. Thus, a straight line of Ainos-Midia became the eastern Border, which was never implemented as the regions of Lozengrad, Lyule Burgas-Buni Hisar, and Adrianople reverted to the Ottomans. Right after the Treaty of Constantinople, 30 September 1913, Bulgaria sought an alliance pact with the Ottoman Empire, as they claimed Macedonia as their national target on a future war with Greece and Serbia.
The Treaty of Constantinople was followed by the Treaty of Athens, 14 November 1913, between the Turks and the Greeks. This treaty concluded the conflict between the two states. However, the status of the Aegean Islands, which were under the Greek control, was left in a question, especially the islands of Imvros and Tenedos that were in a strategic position against the Dardanels Straights. Despite the fact that a treaty was signed, the relations between the two countries remained very bad, and war almost broke out in spring 1914.
Finally, a second Treaty in Constantinople re-established the relations between Serbia and the Ottoman Empire, concluding officially the Balkan Wars. Montenegro never signed a pact with the Turks.
The Balkan Wars brought to an end the Ottoman rule of the Balkan Peninsula, except for eastern Thrace and Constantinople. The Young Turk regime was unable to reverse the decline of the Empire. It remained in power, though, and in June 1913, establish a dictatorship. A large influx of Turks started to flee into the Ottoman heartland from the lost lands. By 1914, the remaining core region of the Ottoman Empire had experienced a population increase of around 2.5 million because of the flood of immigration from the Balkans.
Soviet demographer Boris Urlanis estimated in "Voini I Narodo-Nacelenie Europi" (1960) that in the first and second Balkan wars there were 122,000 killed in action, 20,000 dead of wounds, and 82,000 dead of disease.
Another major issue was the partitioning of these Ottoman territories. This large area that hosted Greeks, Bulgarians, Aromanians, Serbs, Jews, Turks, Albanians and other nations after the 19th century rise of nationalism the Ottoman empire. What is more, another state-nation emerged. Albania was establish on lands that were occupied by Greeks and Serbs. Both armies were asked to leave after the establishment of the new country. Greece never gained North Epirus, and Serbia lost a wide littoral to the Adriatic Sea. The purpose behind this arrangement was the denial of Italy and Austro-Hungary to a greater and more powerful Serbia.
Finally, during and after the wars, the Greek fleet emerged as the only considerable naval power in the Aegean Sea, blocking the Turkish fleet inside the Dardanels Straight. The Hellenic Navy managed to liberate the Greek islanders and boost the moral of the Greeks. However, the Greek populations in Asia Minor and Pontus faced the rage of the Young Turks' regime, who answered to the defeat with embargoes, exiles, persecutions and, finally, genocide.
Citizens of Turkey regard the Balkan Wars as a major disaster ("Balkan harbi faciası") in the nation's history. The Ottoman Empire lost all its European territories to the west of the River Maritsa as a result of the two Balkan Wars, which thus delineated present-day Turkey's western border. The unexpected fall and sudden relinquishing of Turkish-dominated European territories created a psycho-traumatic event amongst many Turks that triggered the ultimate collapse of the empire itself within five years. Nazım Pasha, Chief of Staff of the Ottoman Army, was held responsible for the failure and was assassinated on 23 January 1913 during the 1913 Ottoman coup d'état.
Most Greeks regard the Balkan Wars as a period of epic achievements. They managed to liberate and gain by conquest territories that had been inhabited by Greeks since ancient times and doubled the size of the Greek Kingdom. The Greek Army, very little in numbers and ill-equipped in comparison to the superior Ottoman but also Bulgarian and Serbian armies, won very important battles that made it accountable to the Great Powers' chessplay. Two great personalities rose in the Greek political arena, Prime Minister Eleftherios Venizelos, the leading mind behind the Greek foreign policy, and Crown Prince, and later King, Konstantinos I, the major General of the Greek Army.

</doc>
<doc id="4824" url="https://en.wikipedia.org/wiki?curid=4824" title="Buffalo">
Buffalo

Buffalo most commonly refers to:
Buffalo or buffaloes may also refer to:

</doc>
<doc id="4825" url="https://en.wikipedia.org/wiki?curid=4825" title="BeBox">
BeBox

The BeBox is a dual CPU personal computer, briefly sold by Be Inc. to run the company's own operating system, BeOS. Notable aspects of the system include its CPU configuration, I/O board with "GeekPort", and "Blinkenlights" on the front bezel.
The BeBox made its debut in October 1995 (BeBox Dual603-66). The processors were upgraded to 133 MHz in August 1996 (BeBox Dual603e-133). Production was halted in January 1997, following the port of BeOS to the Macintosh, in order for the company to concentrate on software. Be sold around a thousand 66 MHz BeBoxes and 800 133 MHz BeBoxes.
BeBox creator Jean-Louis Gassée did not see the BeBox as a general consumer device, warning that "Before we let you use the BeBox, we believe you must have some aptitude toward programming the standard language is C++."
Initial prototypes are equipped with two AT&T Hobbit processors and three AT&T 9308S DSPs.
Production models use two PowerPC 603 processors running at 66 or 133 MHz to power the BeBox. Prototypes having dual 200 MHz CPUs or four CPUs exist, but these were never publicly available.
Two yellow/green vertical LED arrays, dubbed the "blinkenlights", are built into the front bezel to illustrate the CPU load. The bottommost LED on the right side indicates hard disk activity.

</doc>
<doc id="4827" url="https://en.wikipedia.org/wiki?curid=4827" title="Biomedical engineering">
Biomedical engineering

Biomedical engineering (BME) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). BME is also traditionally known as "bioengineering", but this term has come to also refer to biological engineering. This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy. Also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. This involves making equipment recommendations, procurement, routine testing and preventive maintenance, a role also known as a Biomedical Equipment Technician (BMET) or as clinical engineering.
Biomedical engineering has recently emerged as its own study, as compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EKG/ECGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.
Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.
Bioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.
Biomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles, using the methods of mechanics.
A biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.
Biomedical optics refers to the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.
Tissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with BME.
One of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones and tracheas from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.
Genetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology ("not a medical application", but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.
Neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.
Pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment.
This is an "extremely broad category"—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.
A medical device is intended for use in:
Some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.
Stereolithography is a practical example of "medical modeling" being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases.
Medical devices are regulated and classified (in the US) as follows (see also "Regulation"):
Medical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly "view" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.
Imaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.
An implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.
Artificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other tools.
In recent years biomedical sensors based in microwave technology have gained more attention. Different sensors can be manufactured for specific uses in both diagnosing and monitoring disease conditions, for example microwave sensors can be used as a complementary technique to X-ray to monitor lower extremity trauma. The sensor monitor the dielectric properties and can thus notice change in tissue (bone, muscle, fat etc.) under the skin so when measuring at different times during the healing process the response from the sensor will change as the trauma heals.
Clinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.
Their inherent focus on "practical" implementation of technology has tended to keep them oriented more towards "incremental"-level redesigns and re configurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a "bridge" between the primary designers and the end-users, by combining the perspectives of being both close to the point-of-use, while also trained in product and process engineering. Clinical engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems. Clinical engineering department is constructed with a manager, supervisor, engineer and technician. One engineer per eighty beds in the hospital is the ratio. Clinical engineers is also authorized audit pharmaceutical and associated stores to monitor FDA recalls of invasive items.
Rehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.
While some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility. Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.
The rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote inclusion of their users into the mainstream of society, commerce, and recreation.
Regulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to "a situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death"
Regardless of the country-specific legislation, the main regulatory objectives coincide worldwide. For example, in the medical device regulations, a product must be: 1) safe "and" 2) effective and 3) for all the manufactured devices
A product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, ...) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.
A product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.
The previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecycle.
The medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical "devices, drugs, biologics, and combination" products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for "consumer" use, such as physical therapy devices (which are also "medical" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K "clearance" (typically for Class 2 devices) or pre-market "approval" (typically for drugs and class 3 devices).
In the European context, safety effectiveness and quality is ensured through the "Conformity Assessment" that is defined as "the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.
In the European Union, there are certifying entities named "Notified Bodies", accredited by the European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.
The different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the "optimal extent" of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.
Directive 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation "Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices" (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.
RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.
The scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.
The new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.
The mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.
AS/ANS 3551:2012 is the Australian and New Zealand standards for the management of medical devices. The standard specifies the procedures required to maintain a wide range of medical assets in a clinical setting (e.g. Hospital). The standards are based on the IEC 606101 standards.
The standard covers a wide range of medical equipment management elements including, procurement, acceptance testing, maintenance (electrical safety and preventive maintenance testing) and decommissioning.
Biomedical engineers require considerable knowledge of both engineering and biology, and typically have a Bachelor's (B.Sc., B.S., B.Eng. or B.S.E.) or Master's (M.S., M.Sc., M.S.E., or M.Eng.) or a doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Sc., B.S., B.Eng. or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as "its own discipline" rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a "pre-med" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.
In the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET.
In Canada and Australia, accredited graduate programs in biomedical engineering are common. For example, McMaster University offers an M.A.Sc, an MD/PhD, and a PhD in Biomedical engineering. The first Canadian undergraduate BME program was offered at Ryerson University as a four-year B.Eng. program. The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering as is Flinders University.
As with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees is also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.
Graduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them. Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.
Graduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or "another engineering" discipline (plus certain life science coursework), or "life science" (plus certain engineering coursework).
Education in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards. Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education. Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.
As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but, in US, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of American engineers). The US model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.
Biomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.
In the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division. The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.
The Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.
Beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.
In 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 27% (much faster than average) from 2012 to 2022. Biomedical engineering has the highest percentage of female engineers compared to other common engineering professions.

</doc>
<doc id="4829" url="https://en.wikipedia.org/wiki?curid=4829" title="Balkans">
Balkans

The Balkans ( ), also known as the Balkan Peninsula, are a geographic area in southeastern Europe with various definitions and meanings, including geopolitical and historical. The region takes its name from the Balkan Mountains that stretch throughout the whole of Bulgaria from the Serbian–Bulgarian border to the Black Sea coast. The Balkan Peninsula is bordered by the Adriatic Sea in the northwest, the Ionian Sea in the southwest, the Aegean Sea in the south, the Turkish Straits in the east, and the Black Sea in the northeast. The northern border of the peninsula is variously defined. The highest point of the Balkans is Mount Musala, , in the Rila mountain range, Bulgaria.
The concept of the Balkan Peninsula was created by the German geographer August Zeune in 1808, who mistakenly considered the Balkan Mountains the dominant mountain system of Southeast Europe spanning from the Adriatic Sea to the Black Sea. The term of Balkan Peninsula was a synonym for Rumelia (European Turkey) in the 19th century, the former provinces of the Ottoman Empire in Southeast Europe. It had a geopolitical rather than a geographical definition, further promoted during the creation of the Kingdom of Yugoslavia in the early 20th century. The definition of the Balkan Peninsula's natural borders do not coincide with the technical definition of a peninsula and hence modern geographers reject the idea of a Balkan peninsula, while scholars usually discuss the Balkans as a region. The term has acquired a stigmatized and pejorative meaning related to the process of Balkanization, and hence the preferred alternative term used for the region is Southeast Europe.
The origin of the word "Balkan" is obscure; it may be related to Persian "bālk" 'mud', and the Turkish suffix "an" 'swampy forest' or Persian "balā-khāna" 'big high house'. Related words are also found in other Turkic languages. The term was brought in Europe with Ottoman Turkish influence, where "" means 'chain of wooded mountains' in Turkic languages.
From classical antiquity through the Middle Ages, the Balkan Mountains were called by the local Thracian name "Haemus". According to Greek mythology, the Thracian king Haemus was turned into a mountain by Zeus as a punishment and the mountain has remained with his name. A reverse name scheme has also been suggested. D. Dechev considers that Haemus (Αἷμος) is derived from a Thracian word "*saimon", 'mountain ridge'. A third possibility is that "Haemus" () derives from the Greek word "haima" () meaning 'blood'. The myth relates to a fight between Zeus and the monster/titan Typhon. Zeus injured Typhon with a thunder bolt and Typhon's blood fell on the mountains, from which they got their name.
The earliest mention of the name appears in an early 14th-century Arab map, in which the Haemus mountains are referred to as "Balkan". The first attested time the name "Balkan" was used in the West for the mountain range in Bulgaria was in a letter sent in 1490 to Pope Innocent VIII by Buonaccorsi Callimaco, an Italian humanist, writer and diplomat. The Ottomans first mention it in a document dated from 1565. There has been no other documented usage of the word to refer to the region before that, although other Turkic tribes had already settled in or were passing through the region. There is also a claim about an earlier Bulgar Turkic origin of the word popular in Bulgaria, however it is only an unscholarly assertion. The word was used by the Ottomans in Rumelia in its general meaning of mountain, as in "Kod̲j̲a-Balkan", "Čatal-Balkan", and "Ungurus-Balkani̊", but especially it was applied to the Haemus mountain. The name is still preserved in Central Asia with the Balkan Daglary (Balkan Mountains) and the Balkan Province of Turkmenistan. English traveler John Morritt introduced this term into the English literature at the end of the 18th-century, and other authors started applying the name to the wider area between the Adriatic and the Black Sea. The concept of the "Balkans" was created by the German geographer August Zeune in 1808, who mistakenly considered it as the dominant central mountain system of Southeast Europe spanning from the Adriatic Sea to the Black Sea. During the 1820s, "Balkan became the preferred although not yet exclusive term alongside Haemus among British travelers... Among Russian travelers not so burdened by classical toponymy, Balkan was the preferred term".
The term was not commonly used in geographical literature until the mid-19th century because already then scientists like Carl Ritter warned that only the part South of the Balkan Mountains can be considered as a peninsula and considered it to be renamed as "Greek peninsula". Other prominent geographers who didn't agree with Zeune were Hermann Wagner, Theobald Fischer, Marion Newbigin, Albrecht Penck, while Austrian diplomat Johann Georg von Hahn in 1869 for the same territory used the term "Südostereuropäische Halbinsel" ("Southeasterneuropean peninsula"). Another reason it was not commonly accepted as the definition of then European Turkey had a similar land extent. However, after the Congress of Berlin (1878) there was a political need for a new term and gradually "the Balkans" was revitalized, but in the maps, the northern border was in Serbia and Montenegro without Greece (it only depicted the Ottoman occupied parts of Europe), while Yugoslavian maps also included Croatia and Bosnia. The term Balkan Peninsula was a synonym for European Turkey, the political borders of former Ottoman Empire provinces.
The usage of the term changed in the very end of the 19th and beginning of the 20th century when was embraced by Serbian geographers, most prominently by Jovan Cvijić. It was done with political reasoning as affirmation for Serbian nationalism on the whole territory of the South Slavs, and also included anthropological and ethnological studies of the South Slavs through which were claimed various nationalistic and racialist theories. Through such policies and Yugoslavian maps the term was elevated to the modern status of a geographical region. The term acquired political nationalistic connotations far from its initial geographic meaning, arising from political changes from the late 19th century to the creation of post–World War I Yugoslavia (initially the Kingdom of Serbs, Croats and Slovenes in 1918). After the dissolution of Yugoslavia beginning in June 1991, the term "Balkans" acquired a negative political meaning, especially in Croatia and Slovenia, as well in worldwide casual usage for war conflicts and fragmentation of territory (see Balkanization).
In part due to the historical and political connotations of the term "Balkans", especially since the military conflicts of the 1990s in Yugoslavia in the western half of the region, the term "Southeast Europe" is becoming increasingly popular. A European Union initiative of 1999 is called the "Stability Pact for South Eastern Europe", and the online newspaper "Balkan Times" renamed itself "Southeast European Times" in 2003.
In other languages of the region, the region is known as:
The Balkan Peninsula is bounded by the Adriatic Sea to the west, the Mediterranean Sea (including the Ionian and Aegean seas) and the Marmara Sea to the south and the Black Sea to the east. Its northern boundary is often given as the Danube, Sava and Kupa Rivers. The Balkan Peninsula has a combined area of about (slightly smaller than Spain). It is more or less identical to the region known as Southeast Europe.
From 1920 until World War II, Italy included Istria and some Dalmatian areas (like "Zara", today's Zadar) that are within the general definition of the Balkan Peninsula. The current territory of Italy includes only the small area around Trieste inside the Balkan Peninsula. However, the regions of Trieste and Istria are not usually considered part of the Balkans by Italian geographers, due to their definition of the Balkans that limits its western border to the Kupa River.
Share of total area in brackets within the Balkan Peninsula by country, by the Danube–Sava definition, with Bulgaria and Greece occupying almost the half of the territory of the Balkan Peninsula, with around 23% of the total area each:
Entirely within the Balkan Peninsula:
Mostly or partially within the Balkan Peninsula:
The term "the Balkans" is used more generally for the region; it includes states in the region, which may extend beyond the peninsula, and is not defined by the geography of the peninsula itself.
Historians state the Balkans comprise Albania, Bosnia and Herzegovina, Bulgaria, Croatia, Greece, Kosovo, Montenegro, North Macedonia, Romania, Serbia, and Slovenia. Its total area is usually given as and the population as 59,297,000 (est. 2002). Italy, although having a small part of its territory in the Balkan Peninsula, is not included in the term "the Balkans".
The term Southeast Europe is also used for the region, with various definitions. Individual Balkan states can also be considered part of other regions, including Southern Europe, Eastern Europe and Central Europe. Turkey, often including its European territory, is also included in Western or Southwestern Asia.
"Western Balkans" is a political neologism coined to refer to Albania and the territory of the former Yugoslavia, except Slovenia, since the early 1990s. The region of the "Western Balkans", a coinage exclusively used in Pan-European parlance, roughly corresponds to the Dinaric Alps territory.
The institutions of the European Union have generally used the term "Western Balkans" to mean the Balkan area that includes countries that are not members of the European Union, while others refer to the geographical aspects.
Each of these countries aims to be part of the future enlargement of the European Union and reach democracy and transmission scores but, until then, they will be strongly connected with the pre-EU waiting program CEFTA. Croatia, considered part of the Western Balkans, joined the EU in July 2013.
The term is criticized for having a geopolitical, rather than a geographical meaning and definition, as a multiethnic and political area in the southeastern part of Europe. The geographical term of a peninsula defines that the water border must be longer than land, with the land side being the shortest in the triangle, but that is not the case with the Balkan Peninsula. Both Eastern and Western water cathetus from Odessa to Cape Matapan (ca. 1230–1350 km) and from Trieste to Cape Matapan (ca. 1270–1285 km) are shorter than land cathetus from Trieste to Odessa (ca. 1330–1365 km). The land has a too wide line connected to the continent to be technically proclaimed as a peninsula - Szczecin (920 km) and Rostock (950 km) at the Baltic Sea are closer to Trieste than Odessa yet it is not considered as another European peninsula. Since the late 19th and early 20th-century literature is not known where is exactly the northern border between the peninsula and the continent, with an issue, whether the rivers are suitable for its definition. In the studies the Balkans natural borders, especially the northern border, are often avoided to be addressed, considered as a "fastidious problem" by André Blanc in "Geography of the Balkans" (1965), while John Lampe and Marvin Jackman in "Balkan Economic History" (1971) noted that "modern geographers seem agreed in rejecting the old idea of a Balkan Peninsula". Another issue is the name because the Balkan Mountains which are mostly located in Northern Bulgaria are not dominating the region by length and area like the Dinaric Alps. An eventual Balkan peninsula can be considered a territory South of the Balkan Mountains, with a possible name "Greek-Albanian Peninsula." The term influenced the meaning of Southeast Europe which again is not properly defined by geographical factors yet historical borders of the Balkans.
Croatian geographers and academics are highly critical of inclusion of Croatia within the broad geographical, social-political and historical context of the Balkans, while the neologism Western Balkans is perceived as a humiliation of Croatia by the European political powers. According to M. S. Altić, the term has two different meanings, "geographical, ultimately undefined, and cultural, extremely negative, and recently strongly motivated by the contemporary political context". In 2018, President of Croatia Kolinda Grabar-Kitarović stated that the use of the term "Western Balkans" should be avoided because it does not imply only a geographic area, but also negative connotations, and instead must be perceived as and called Southeast Europe because it is part of Europe.
As the Slovenian philosopher Slavoj Žižek put it,
Most of the area is covered by mountain ranges running from the northwest to southeast. The main ranges are the Balkan mountains (Stara Planina in Bulgarian language), running from the Black Sea coast in Bulgaria to the border with Serbia, the Rilo-Rhodope massif in southern Bulgaria, northern Greece and southeastern North Macedonia, the Dinaric Alps in Bosnia and Herzegovina, Croatia and Montenegro, the Šar massif which spreads from Albania to North Macedonia, and the Pindus range, spanning from southern Albania into central Greece and the Albanian Alps. The highest mountain of the region is Rila in Bulgaria, with Musala at 2925 m, Mount Olympus in Greece, being second at 2917 m and Vihren in Bulgaria being the third at 2914 m. The karst field or polje is a common feature of the landscape.
On the Adriatic and Aegean coasts the climate is Mediterranean, on the Black Sea coast the climate is humid subtropical and oceanic, and inland it is humid continental. In the northern part of the peninsula and on the mountains, winters are frosty and snowy, while summers are hot and dry. In the southern part winters are milder. The humid continental climate is predominant in Bosnia and Herzegovina, northern Croatia, Bulgaria, Kosovo, northern Montenegro, the Republic of North Macedonia, the interior of Albania and Serbia, while the other, less common climates, the humid subtropical and oceanic climates, are seen on the Black Sea coast of Bulgaria and Balkan Turkey (European Turkey); and the Mediterranean climate is seen on the coast of Albania, the coast of Croatia, Greece, southern Montenegro and the Aegean coast of Balkan Turkey (European Turkey).
Over the centuries forests have been cut down and replaced with bush. In the southern part and on the coast there is evergreen vegetation. Inland there are woods typical of Central Europe (oak and beech, and in the mountains, spruce, fir and pine). The tree line in the mountains lies at the height of 1800–2300 m. The land provides habitats for numerous endemic species, including extraordinarily abundant insects and reptiles that serve as food for a variety of birds of prey and rare vultures.
The soils are generally poor, except on the plains, where areas with natural grass, fertile soils and warm summers provide an opportunity for tillage. Elsewhere, land cultivation is mostly unsuccessful because of the mountains, hot summers and poor soils, although certain cultures such as olive and grape flourish.
Resources of energy are scarce, except in Kosovo, where considerable coal, lead, zinc, chromium and silver deposits are located. Other deposits of coal, especially in Bulgaria, Serbia and Bosnia, also exist. Lignite deposits are widespread in Greece. Petroleum scarce reserves exist in Greece, Serbia and Albania. Natural gas deposits are scarce. Hydropower is in wide use, from over 1,000 dams. The often relentless bora wind is also being harnessed for power generation.
Metal ores are more usual than other raw materials. Iron ore is rare, but in some countries there is a considerable amount of copper, zinc, tin, chromite, manganese, magnesite and bauxite. Some metals are exported.
The Balkan region was the first area in Europe to experience the arrival of farming cultures in the Neolithic era. The Balkans have been inhabited since the Paleolithic and are the route by which farming from the Middle East spread to Europe during the Neolithic (7th millennium BC). The practices of growing grain and raising livestock arrived in the Balkans from the Fertile Crescent by way of Anatolia and spread west and north into Central Europe, particularly through Pannonia. Two early culture-complexes have developed in the region, Starčevo culture and Vinča culture. The Balkans are also the location of the first advanced civilizations. Vinča culture developed a form of proto-writing before the Sumerians and Minoans, known as the Old European script, while the bulk of the symbols had been created in the period between 4500 and 4000 BC, with the ones on the Tărtăria clay tablets even dating back to around 5300 BC.
The identity of the Balkans is dominated by its geographical position; historically the area was known as a crossroads of cultures. It has been a juncture between the Latin and Greek bodies of the Roman Empire, the destination of a massive influx of pagan Bulgars and Slavs, an area where Orthodox and Catholic Christianity met, as well as the meeting point between Islam and Christianity.
In pre-classical and classical antiquity, this region was home to Greeks, Illyrians, Paeonians, Thracians, Dacians, and other ancient groups. The Achaemenid Persian Empire incorporated parts of the Balkans comprising Macedonia, Thrace, Bulgaria, and the Black Sea coastal region of Romania between the late 6th and the first half of the 5th-century BC into its territories. Later the Roman Empire conquered most of the region and spread Roman culture and the Latin language, but significant parts still remained under classical Greek influence. The Romans considered the Rhodope Mountains to be the northern limit of the Peninsula of Haemus and the same limit applied approximately to the border between Greek and Latin use in the region (later called the Jireček Line). However large spaces south of Jireček Line were and are inhabited by Vlachs (Aromanians), the Romance-speaking heirs of Roman Empire. The Bulgars and Slavs arrived in the 6th-century and began assimilating and displacing already-assimilated (through Romanization and Hellenization) older inhabitants of the northern and central Balkans, forming the Bulgarian Empire. During the Middle Ages, the Balkans became the stage for a series of wars between the Byzantine Roman and the Bulgarian Empires.
By the end of the 16th-century, the Ottoman Empire had become the controlling force in the region after expanding from Anatolia through Thrace to the Balkans. Many people in the Balkans place their greatest folk heroes in the era of either the onslaught or the retreat of the Ottoman Empire. As examples, for Greeks, Constantine XI Palaiologos and Kolokotronis; and for Serbs, Miloš Obilić and Tzar Lazar; for Montenegrins, Đurađ I Balšić and Ivan Crnojević; for Albanians, George Kastrioti Skanderbeg; for ethnic Macedonians, Nikola Karev and Goce Delčev; for Bulgarians, Vasil Levski, Georgi Sava Rakovski and Hristo Botev and for Croats, Nikola Šubić Zrinjski.
In the past several centuries, because of the frequent Ottoman wars in Europe fought in and around the Balkans and the comparative Ottoman isolation from the mainstream of economic advance (reflecting the shift of Europe's commercial and political centre of gravity towards the Atlantic), the Balkans have been the least developed part of Europe. According to Halil İnalcık, "The population of the Balkans, according to one estimate, fell from a high of 8 million in the late 16th-century to only 3 million by the mid-eighteenth. This estimate is based on Ottoman documentary evidence."
Most of the Balkan nation-states emerged during the 19th and early 20th centuries as they gained independence from the Ottoman Empire or the Austro-Hungarian empire: Greece in 1821, Serbia, and Montenegro in 1878, Romania in 1881, Bulgaria in 1908 and Albania in 1912.
In 1912–1913 the First Balkan War broke out when the nation-states of Bulgaria, Serbia, Greece and Montenegro united in an alliance against the Ottoman Empire. As a result of the war, almost all remaining European territories of the Ottoman Empire were captured and partitioned among the allies. Ensuing events also led to the creation of an independent Albanian state. Bulgaria insisted on its status quo territorial integrity, divided and shared by the Great Powers next to the Russo-Turkish War (1877–78) in other boundaries and on the pre-war Bulgarian-Serbian agreement. Bulgaria was provoked by the backstage deals between its former allies, Serbia and Greece, on the allocation of the spoils at the end of the First Balkan War. At the time, Bulgaria was fighting at the main Thracian Front. Bulgaria marks the beginning of Second Balkan War when it attacked them. The Serbs and the Greeks repulsed single attacks, but when the Greek army invaded Bulgaria together with an unprovoked Romanian intervention in the back, Bulgaria collapsed. The Ottoman Empire used the opportunity to recapture Eastern Thrace, establishing its new western borders that still stand today as part of modern Turkey.
The First World War was sparked in the Balkans in 1914 when members of Young Bosnia, a revolutionary organization with predominantly Serb and pro-Yugoslav members, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand of Austria in Bosnia and Herzegovina's capital, Sarajevo. That caused a war between Austria-Hungary and Serbia, which—through the existing chains of alliances—led to the First World War. The Ottoman Empire soon joined the Central Powers becoming one of the three empires participating in that alliance. The next year Bulgaria joined the Central Powers attacking Serbia, which was successfully fighting Austro-Hungary to the north for a year. That led to Serbia's defeat and the intervention of the Entente in the Balkans which sent an expeditionary force to establish a new front, the third one of that war, which soon also became static. The participation of Greece in the war three years later, in 1918, on the part of the Entente finally altered the balance between the opponents leading to the collapse of the common German-Bulgarian front there, which caused the exit of Bulgaria from the war, and in turn, the collapse of the Austro-Hungarian Empire, ending the First World War.
With the start of the Second World War, all Balkan countries, with the exception of Greece, were allies of Nazi Germany, having bilateral military agreements or being part of the Axis Pact. Fascist Italy expanded the war in the Balkans by using its protectorate Albania to invade Greece. After repelling the attack, the Greeks counterattacked, invading Italy-held Albania and causing Nazi Germany's intervention in the Balkans to help its ally. Days before the German invasion, a successful coup d'état in Belgrade by neutral military personnel seized power.
Although the new government reaffirmed Serbia's intentions to fulfil its obligations as a member of the Axis, Germany, with Bulgaria, invaded both Greece and Yugoslavia. Yugoslavia immediately disintegrated when those loyal to the Serbian King and the Croatian units mutinied. Greece resisted, but, after two months of fighting, collapsed and was occupied. The two countries were partitioned between the three Axis allies, Bulgaria, Germany and Italy, and the Independent State of Croatia, a puppet state of Italy and Germany.
During the occupation, the population suffered considerable hardship due to repression and starvation, to which the population reacted by creating a mass resistance movement. Together with the early and extremely heavy winter of that year (which caused hundreds of thousands of deaths among the poorly fed population), the German invasion had disastrous effects in the timetable of the planned invasion in Russia causing a significant delay, which had major consequences during the course of the war.
Finally, at the end of 1944, the Soviets entered Romania and Bulgaria forcing the Germans out of the Balkans. They left behind a region largely ruined as a result of wartime exploitation.
During the Cold War, most of the countries on the Balkans were governed by communist governments. Greece became the first battleground of the emerging Cold War. The Truman Doctrine was the US response to the civil war, which raged from 1944 to 1949. This civil war, unleashed by the Communist Party of Greece, backed by communist volunteers from neighboring countries (Albania, Bulgaria and Yugoslavia), led to massive American assistance for the non-communist Greek government. With this backing, Greece managed to defeat the partisans and, ultimately, remained the only non-communist country in the region.
However, despite being under communist governments, Yugoslavia (1948) and Albania (1961) fell out with the Soviet Union. Yugoslavia, led by Marshal Josip Broz Tito (1892–1980), first propped up then rejected the idea of merging with Bulgaria and instead sought closer relations with the West, later even spearheaded, together with India and Egypt the Non-Aligned Movement. Albania on the other hand gravitated toward Communist China, later adopting an isolationist position.
As the only non-communist countries, Greece and Turkey were (and still are) part of NATO composing the southeastern wing of the alliance.
In the 1990s, the transition of the regions' ex-Eastern bloc countries towards democratic free-market societies went peacefully. While in the non-aligned Yugoslavia, Wars between the former Yugoslav republics broke out after Slovenia and Croatia held free elections and their people voted for independence on their respective countries' referenda. Serbia, in turn, declared the dissolution of the union as unconstitutional and the Yugoslavian army unsuccessfully tried to maintain the status quo. Slovenia and Croatia declared independence on 25 June 1991, followed by the Ten-Day War in Slovenia. Till October 1991, the Army withdrew from Slovenia, and in Croatia, the Croatian War of Independence would continue until 1995. In the ensuing 10 years armed confrontation, gradually all the other Republics declared independence, with Bosnia being the most affected by the fighting. The long-lasting wars resulted in a United Nations intervention and NATO ground and air forces took action against Serb forces in Bosnia and Herzegovina and Serbia.
From the dissolution of Yugoslavia six republics achieved international recognition as sovereign republics, but these are traditionally included in Balkans: Slovenia, Croatia, Bosnia and Herzegovina, North Macedonia, Montenegro and Serbia. In 2008, while under UN administration, Kosovo declared independence (according to the official Serbian policy, Kosovo is still an internal autonomous region). In July 2010, the International Court of Justice, ruled that the declaration of independence was legal. Most UN member states recognise Kosovo. After the end of the wars a revolution broke in Serbia and Slobodan Milošević, the Serbian communist leader (elected president between 1989 and 2000), was overthrown and handed for a trial to the International Criminal Tribunal for crimes against the International Humanitarian Law during the Yugoslav wars. Milošević died of a heart attack in 2006 before a verdict could have been released. Ιn 2001 an Albanian uprising in North Macedonia forced the country to give local autonomy to the ethnic Albanians in the areas where they predominate.
With the dissolution of Yugoslavia an issue emerged over the name under which the former (federated) republic of Macedonia would internationally be recognized, between the new country and Greece. Being the Macedonian part of Yugoslavia (see Vardar Macedonia), the federated Republic under the Yugoslav identity had the name Republic of Macedonia on which it declared its sovereignty in 1991. Greece, having a large region (see Macedonia) also under the same name opposed to the usage of this name as an indication of a nationality. The issue was resolved under UN mediation and the Prespa agreement was reached, which saw the country's renaming into North Macedonia.
Balkan countries control the direct land routes between Western Europe and South-West Asia (Asia Minor and the Middle East). Since 2000, all Balkan countries are friendly towards the EU and the US.
Greece has been a member of the European Union since 1981 while Slovenia is a member since 2004, Bulgaria and Romania are members since 2007, and Croatia is a member since 2013. In 2005, the European Union decided to start accession negotiations with candidate countries; Turkey, and North Macedonia were accepted as candidates for EU membership. In 2012, Montenegro started accession negotiations with the EU. In 2014, Albania is an official candidate for accession to the EU. In 2015, Serbia was expected to start accession negotiations with the EU, however this process has been stalled over the recognition of Kosovo as an independent state by existing EU member states.
Greece and Turkey have been NATO members since 1952. In March 2004, Bulgaria, Romania and Slovenia have become members of NATO. As of April 2009, Albania and Croatia are members of NATO. Montenegro joined in June 2017.
All other countries have expressed a desire to join the EU or NATO at some point in the future.
Currently, all of the states are republics, but until World War II all countries were monarchies. Most of the republics are parliamentary, excluding Romania and Bosnia which are semi-presidential. All the states have open market economies, most of which are in the upper-middle-income range ($4,000–12,000 p.c.), except Croatia, Romania, Greece and Slovenia that have high income economies (over $12,000 p.c.), and are classified with very high HDI, along with Bulgaria, in contrast to the remaining states, which are classified with high HDI. The states from the former Eastern Bloc that formerly had planned economy system and Turkey mark gradual economic growth each year, only the economy of Greece drops for 2012 and meanwhile, it was expected to grow in 2013. The Gross domestic product (Purchasing power parity) per capita is highest in Slovenia (over $36,000), followed by Greece (over $30,000), Croatia, Bulgaria and Romania (over $23,000), Turkey, Montenegro, Serbia, North Macedonia ($10,000–15,000) and Bosnia, Albania and Kosovo (below $10,000). The Gini coefficient, which indicates the level of difference by monetary welfare of the layers, is on the second level at the highest monetary equality in Albania, Bulgaria and Serbia, on the third level in Greece, Montenegro and Romania, on the fourth level in North Macedonia, on the fifth level in Turkey, and the most unequal by Gini coefficient is Bosnia at the eighth level which is the penultimate level and one of the highest in the world. The unemployment is lowest in Romania (below 5%), followed by Bulgaria, Serbia (5-10%), Albania, Turkey (10–15%), Greece, Bosnia, Montenegro (15–20%), North Macedonia (over 20%) and Kosovo (over 25%).
See also the Black Sea regional organizations
The region is inhabited by Albanians, Aromanians, Bulgarians, Bosniaks, Croats, Gorani, Greeks, Macedonians, Montenegrins, Serbs, Slovenes, Romanians, Turks, and other ethnic groups which present minorities in certain countries like the Romani and Ashkali.
The region is a meeting point of Orthodox Christianity, Islam and Roman Catholic Christianity. Eastern Orthodoxy is the majority religion in both the Balkan Peninsula and the Balkan region, The Eastern Orthodox Church has played a prominent role in the history and culture of Eastern and Southeastern Europe. A variety of different traditions of each faith are practiced, with each of the Eastern Orthodox countries having its own national church. A part of the population in the Balkans defines itself as irreligious.
The Jewish communities of the Balkans were some of the oldest in Europe and date back to ancient times. These communities were Sephardi Jews, except in Transylvania, Croatia and Slovenia, where the Jewish communities were mainly Ashkenazi Jews. In Bosnia and Herzegovina, the small and close-knit Jewish community is 90% Sephardic, and Ladino is still spoken among the elderly. The Sephardi Jewish cemetery in Sarajevo has tombstones of a unique shape and inscribed in ancient Ladino. Sephardi Jews used to have a large presence in the city of Thessaloniki, and by 1900, some 80,000, or more than half of the population, were Jews. The Jewish communities in the Balkans suffered immensely during World War II, and the vast majority were killed during the Holocaust. An exception was the Bulgarian Jews, most of whom were saved by Boris III of Bulgaria, who resisted Adolf Hitler, opposing their deportation to Nazi concentration camps. Almost all of the few survivors have emigrated to the (then) newly founded state of Israel and elsewhere. Almost no Balkan country today has a significant Jewish minority.
The Balkan region today is a very diverse ethnolinguistic region, being home to multiple Slavic and Romance languages, as well as Albanian, Greek, Turkish, and others. Romani is spoken by a large portion of the Romanis living throughout the Balkan countries. Throughout history, many other ethnic groups with their own languages lived in the area, among them Thracians, Illyrians, Romans, Celts and various Germanic tribes. All of the aforementioned languages from the present and from the past belong to the wider Indo-European language family, with the exception of the Turkic languages (e.g., Turkish and Gagauz).
Most of the states in the Balkans are predominantly urbanized, with the lowest number of urban population as % of the total population found in Kosovo at under 40%, Bosnia and Herzegovina at 40% and Slovenia at 50%.
A list of largest cities:
The time zones in the Balkans are defined as the following:

</doc>
<doc id="4831" url="https://en.wikipedia.org/wiki?curid=4831" title="Bohr model">
Bohr model

In atomic physics, the Bohr model or Rutherford–Bohr model, presented by Niels Bohr and Ernest Rutherford in 1913, is a system consisting of a small, dense nucleus surrounded by orbiting electrons—similar to the structure of the Solar System, but with attraction provided by electrostatic forces in place of gravity. After the cubic model (1902), the plum-pudding model (1904), the Saturnian model (1904), and the Rutherford model (1911) came the "Rutherford–Bohr model" or just "Bohr model" for short (1913). The improvement to the Rutherford model is mostly a quantum physical interpretation of it. 
The model's key success lay in explaining the Rydberg formula for the spectral emission lines of atomic hydrogen. While the Rydberg formula had been known experimentally, it did not gain a theoretical underpinning until the Bohr model was introduced. Not only did the Bohr model explain the reasons for the structure of the Rydberg formula, it also provided a justification for the fundamental physical constants that make up the formula's empirical results.
The Bohr model is a relatively primitive model of the hydrogen atom, compared to the "valence shell atom" model. As a theory, it can be derived as a first-order approximation of the hydrogen atom using the broader and much more accurate quantum mechanics and thus may be considered to be an obsolete scientific theory. However, because of its simplicity, and its correct results for selected systems (see below for application), the Bohr model is still commonly taught to introduce students to quantum mechanics or energy level diagrams before moving on to the more accurate, but more complex, valence shell atom. A related model was originally proposed by Arthur Erich Haas in 1910 but was rejected. The quantum theory of the period between Planck's discovery of the quantum (1900) and the advent of a mature quantum mechanics (1925) is often referred to as the old quantum theory.
In the early 20th century, experiments by Ernest Rutherford established that atoms consisted of a diffuse cloud of negatively charged electrons surrounding a small, dense, positively charged nucleus. Given this experimental data, Rutherford naturally considered a planetary model of the atom, the Rutherford model of 1911 – electrons orbiting a solar nucleus – however, the said planetary model of the atom has a technical difficulty: the laws of classical mechanics (i.e. the Larmor formula) predict that the electron will release electromagnetic radiation while orbiting a nucleus. Because the electron would lose energy, it would rapidly spiral inwards, collapsing into the nucleus on a timescale of around 16 picoseconds. This atom model is disastrous, because it predicts that all atoms are unstable.
Also, as the electron spirals inward, the emission would rapidly increase in frequency as the orbit got smaller and faster. This would produce a continuous smear, in frequency, of electromagnetic radiation. However, late 19th century experiments with electric discharges have shown that atoms will only emit light (that is, electromagnetic radiation) at certain discrete frequencies.
To overcome this hard difficulty, Niels Bohr proposed, in 1913, what is now called the "Bohr model of the atom". He put forward these three postulates that sum up most of the model:
Other points are:
Bohr's condition, that the angular momentum is an integer multiple of "ħ" was later reinterpreted in 1924 by de Broglie as a standing wave condition: the electron is described by a wave and a whole number of wavelengths must fit along the circumference of the electron's orbit:
According to de Broglie hypothesis, matter particles such as the electron behaves as waves. So, de Broglie wavelength of electron is:
which implies that,
or
where formula_7 is the angular momentum formula_8 of the orbiting electron.
which is the Bohr's second postulate.
Bohr described angular momentum of the electron orbit as 1/2h while de Broglie's wavelength of described h divided by the electron momentum. In 1913, however, Bohr justified his rule by appealing to the correspondence principle, without providing any sort of wave interpretation. In 1913, the wave behavior of matter particles such as the electron (i.e., matter waves) was not suspected.
In 1925, a new kind of mechanics was proposed, quantum mechanics, in which Bohr's model of electrons traveling in quantized orbits was extended into a more accurate model of electron motion. The new theory was proposed by Werner Heisenberg. Another form of the same theory, wave mechanics, was discovered by the Austrian physicist Erwin Schrödinger independently, and by different reasoning. Schrödinger employed de Broglie's matter waves, but sought wave solutions of a three-dimensional wave equation describing electrons that were constrained to move about the nucleus of a hydrogen-like atom, by being trapped by the potential of the positive nuclear charge.
The Bohr model gives almost exact results only for a system where two charged points orbit each other at speeds much less than that of light. This not only involves one-electron systems such as the hydrogen atom, singly ionized helium, and doubly ionized lithium, but it includes positronium and Rydberg states of any atom where one electron is far away from everything else. It can be used for K-line X-ray transition calculations if other assumptions are added (see Moseley's law below). In high energy physics, it can be used to calculate the masses of heavy quark mesons.
Calculation of the orbits requires two assumptions.
If an electron in an atom is moving on an orbit with period "T", classically the electromagnetic radiation will repeat itself every orbital period. If the coupling to the electromagnetic field is weak, so that the orbit doesn't decay very much in one cycle, the radiation will be emitted in a pattern which repeats every period, so that the Fourier transform will have frequencies which are only multiples of 1/"T". This is the classical radiation law: the frequencies emitted are integer multiples of 1/"T".
In quantum mechanics, this emission must be in quanta of light, of frequencies consisting of integer multiples of 1/"T", so that classical mechanics is an approximate description at large quantum numbers. This means that the energy level corresponding to a classical orbit of period 1/"T" must have nearby energy levels which differ in energy by "h"/"T", and they should be equally spaced near that level,
Bohr worried whether the energy spacing 1/"T" should be best calculated with the period of the energy state formula_15, or formula_16, or some average—in hindsight, this model is only the leading semiclassical approximation.
Bohr considered circular orbits. Classically, these orbits must decay to smaller circles when photons are emitted. The level spacing between circular orbits can be calculated with the correspondence formula. For a Hydrogen atom, the classical orbits have a period "T" determined by Kepler's third law to scale as "r". The energy scales as 1/"r", so the level spacing formula amounts to
It is possible to determine the energy levels by recursively stepping down orbit by orbit, but there is a shortcut.
The angular momentum "L" of the circular orbit scales as . The energy in terms of the angular momentum is then
Assuming, with Bohr, that quantized values of "L" are equally spaced, the spacing between neighboring energies is
This is as desired for equally spaced angular momenta. If one kept track of the constants, the spacing would be "ħ", so the angular momentum should be an integer multiple of "ħ",
This is how Bohr arrived at his model.
An electron in the lowest energy level of hydrogen () therefore has about 13.6 eV less energy than a motionless electron infinitely far from the nucleus. The next energy level () is −3.4 eV. The third (3) is −1.51 eV, and so on. For larger values of "n", these are also the binding energies of a highly excited atom with one electron in a large circular orbit around the rest of the atom. The hydrogen formula also coincides with the Wallis product.
The combination of natural constants in the energy formula is called the Rydberg energy ("R"):
This expression is clarified by interpreting it in combinations that form more natural units:
Since this derivation is with the assumption that the nucleus is orbited by one electron, we can generalize this result by letting the nucleus have a charge , where "Z" is the atomic number. This will now give us energy levels for hydrogenic ("hydrogen-like") atoms, which can serve as a rough order-of-magnitude approximation of the actual energy levels. So for nuclei with "Z" protons, the energy levels are (to a rough approximation):
The actual energy levels cannot be solved analytically for more than one electron (see "n"-body problem) because the electrons are not only affected by the nucleus but also interact with each other via the Coulomb Force.
When "Z" = 1/"α" (), the motion becomes highly relativistic, and "Z" cancels the "α" in "R"; the orbit energy begins to be comparable to rest energy. Sufficiently large nuclei, if they were stable, would reduce their charge by creating a bound electron from the vacuum, ejecting the positron to infinity. This is the theoretical phenomenon of electromagnetic charge screening which predicts a maximum nuclear charge. Emission of such positrons has been observed in the collisions of heavy ions to create temporary super-heavy nuclei.
The Bohr formula properly uses the reduced mass of electron and proton in all situations, instead of the mass of the electron,
However, these numbers are very nearly the same, due to the much larger mass of the proton, about 1836.1 times the mass of the electron, so that the reduced mass in the system is the mass of the electron multiplied by the constant 1836.1/(1+1836.1) = 0.99946. This fact was historically important in convincing Rutherford of the importance of Bohr's model, for it explained the fact that the frequencies of lines in the spectra for singly ionized helium do not differ from those of hydrogen by a factor of exactly 4, but rather by 4 times the ratio of the reduced mass for the hydrogen vs. the helium systems, which was much closer to the experimental ratio than exactly 4.
For positronium, the formula uses the reduced mass also, but in this case, it is exactly the electron mass divided by 2. For any value of the radius, the electron and the positron are each moving at half the speed around their common center of mass, and each has only one fourth the kinetic energy. The total kinetic energy is half what it would be for a single electron moving around a heavy nucleus.
The Rydberg formula, which was known empirically before Bohr's formula, is seen in Bohr's theory as describing the energies of transitions or quantum jumps between orbital energy levels. Bohr's formula gives the numerical value of the already-known and measured the Rydberg constant, but in terms of more fundamental constants of nature, including the electron's charge and the Planck constant.
When the electron gets moved from its original energy level to a higher one, it then jumps back each level until it comes to the original position, which results in a photon being emitted. Using the derived formula for the different energy levels of hydrogen one may determine the wavelengths of light that a hydrogen atom can emit.
The energy of a photon emitted by a hydrogen atom is given by the difference of two hydrogen energy levels:
where is the final energy level, and is the initial energy level.
Since the energy of a photon is
the wavelength of the photon given off is given by
This is known as the Rydberg formula, and the Rydberg constant is , or in natural units. This formula was known in the nineteenth century to scientists studying spectroscopy, but there was no theoretical explanation for this form or a theoretical prediction for the value of , until Bohr. In fact, Bohr's derivation of the Rydberg constant, as well as the concomitant agreement of Bohr's formula with experimentally observed spectral lines of the Lyman ( =1), Balmer ( =2), and Paschen ( =3) series, and successful theoretical prediction of other lines not yet observed, was one reason that his model was immediately accepted.
To apply to atoms with more than one electron, the Rydberg formula can be modified by replacing with or with where is constant representing a screening effect due to the inner-shell and other electrons (see Electron shell and the later discussion of the "Shell Model of the Atom" below). This was established empirically before Bohr presented his model.
Bohr extended the model of hydrogen to give an approximate model for heavier atoms. This gave a physical picture that reproduced many known atomic properties for the first time.
Heavier atoms have more protons in the nucleus, and more electrons to cancel the charge. Bohr's idea was that each discrete orbit could only hold a certain number of electrons. After that orbit is full, the next level would have to be used. This gives the atom a shell structure, in which each shell corresponds to a Bohr orbit.
This model is even more approximate than the model of hydrogen, because it treats the electrons in each shell as non-interacting. But the repulsions of electrons are taken into account somewhat by the phenomenon of screening. The electrons in outer orbits do not only orbit the nucleus, but they also move around the inner electrons, so the effective charge Z that they feel is reduced by the number of the electrons in the inner orbit.
For example, the lithium atom has two electrons in the lowest 1s orbit, and these orbit at "Z" = 2. Each one sees the nuclear charge of "Z" = 3 minus the screening effect of the other, which crudely reduces the nuclear charge by 1 unit. This means that the innermost electrons orbit at approximately 1/2 the Bohr radius. The outermost electron in lithium orbits at roughly the Bohr radius, since the two inner electrons reduce the nuclear charge by 2. This outer electron should be at nearly one Bohr radius from the nucleus. Because the electrons strongly repel each other, the effective charge description is very approximate; the effective charge "Z" doesn't usually come out to be an integer. But Moseley's law experimentally probes the innermost pair of electrons, and shows that they do see a nuclear charge of approximately "Z" − 1, while the outermost electron in an atom or ion with only one electron in the outermost shell orbits a core with effective charge "Z" − "k" where "k" is the total number of electrons in the inner shells.
The shell model was able to qualitatively explain many of the mysterious properties of atoms which became codified in the late 19th century in the periodic table of the elements. One property was the size of atoms, which could be determined approximately by measuring the viscosity of gases and density of pure crystalline solids. Atoms tend to get smaller toward the right in the periodic table, and become much larger at the next line of the table. Atoms to the right of the table tend to gain electrons, while atoms to the left tend to lose them. Every element on the last column of the table is chemically inert (noble gas).
In the shell model, this phenomenon is explained by shell-filling. Successive atoms become smaller because they are filling orbits of the same size, until the orbit is full, at which point the next atom in the table has a loosely bound outer electron, causing it to expand. The first Bohr orbit is filled when it has two electrons, which explains why helium is inert. The second orbit allows eight electrons, and when it is full the atom is neon, again inert. The third orbital contains eight again, except that in the more correct Sommerfeld treatment (reproduced in modern quantum mechanics) there are extra "d" electrons. The third orbit may hold an extra 10 d electrons, but these positions are not filled until a few more orbitals from the next level are filled (filling the n=3 d orbitals produces the 10 transition elements). The irregular filling pattern is an effect of interactions between electrons, which are not taken into account in either the Bohr or Sommerfeld models and which are difficult to calculate even in the modern treatment.
Niels Bohr said in 1962, "You see actually the Rutherford work was not taken seriously. We cannot understand today, but it was not taken seriously at all. There was no mention of it any place. The great change came from Moseley."
In 1913 Henry Moseley found an empirical relationship between the strongest X-ray line emitted by atoms under electron bombardment (then known as the K-alpha line), and their atomic number . Moseley's empiric formula was found to be derivable from Rydberg and Bohr's formula (Moseley actually mentions only Ernest Rutherford and Antonius Van den Broek in terms of models). The two additional assumptions that [1] this X-ray line came from a transition between energy levels with quantum numbers 1 and 2, and [2], that the atomic number when used in the formula for atoms heavier than hydrogen, should be diminished by 1, to .
Moseley wrote to Bohr, puzzled about his results, but Bohr was not able to help. At that time, he thought that the postulated innermost "K" shell of electrons should have at least four electrons, not the two which would have neatly explained the result. So Moseley published his results without a theoretical explanation.
Later, people realized that the effect was caused by charge screening, with an inner shell containing only 2 electrons. In the experiment, one of the innermost electrons in the atom is knocked out, leaving a vacancy in the lowest Bohr orbit, which contains a single remaining electron. This vacancy is then filled by an electron from the next orbit, which has n=2. But the n=2 electrons see an effective charge of "Z" − 1, which is the value appropriate for the charge of the nucleus, when a single electron remains in the lowest Bohr orbit to screen the nuclear charge +"Z", and lower it by −1 (due to the electron's negative charge screening the nuclear positive charge). The energy gained by an electron dropping from the second shell to the first gives Moseley's law for K-alpha lines,
or
Here, R" = R"/"h" is the Rydberg constant, in terms of frequency equal to 3.28 x 10 Hz. For values of Z between 11 and 31 this latter relationship had been empirically derived by Moseley, in a simple (linear) plot of the square root of X-ray frequency against atomic number (however, for silver, Z = 47, the experimentally obtained screening term should be replaced by 0.4). Notwithstanding its restricted validity, Moseley's law not only established the objective meaning of atomic number (see Henry Moseley for detail) but, as Bohr noted, it also did more than the Rydberg derivation to establish the validity of the Rutherford/Van den Broek/Bohr nuclear model of the atom, with atomic number (place on the periodic table) standing for whole units of nuclear charge.
The K-alpha line of Moseley's time is now known to be a pair of close lines, written as (Kα and Kα) in Siegbahn notation.
The Bohr model gives an incorrect value for the ground state orbital angular momentum: The angular momentum in the true ground state is known to be zero from experiment. Although mental pictures fail somewhat at these levels of scale, an electron in the lowest modern "orbital" with no orbital momentum, may be thought of as not to rotate "around" the nucleus at all, but merely to go tightly around it in an ellipse with zero area (this may be pictured as "back and forth", without striking or interacting with the nucleus). This is only reproduced in a more sophisticated semiclassical treatment like Sommerfeld's. Still, even the most sophisticated semiclassical model fails to explain the fact that the lowest energy state is spherically symmetric – it doesn't point in any particular direction.
Nevertheless, in the modern "fully quantum treatment in phase space", the proper deformation (careful full extension) of the semi-classical result adjusts the angular momentum value to the correct effective one. As a consequence, the physical ground state expression is obtained through a shift of the vanishing quantum angular momentum expression, which corresponds to spherical symmetry.
In modern quantum mechanics, the electron in hydrogen is a spherical cloud of probability that grows denser near the nucleus. The rate-constant of probability-decay in hydrogen is equal to the inverse of the Bohr radius, but since Bohr worked with circular orbits, not zero area ellipses, the fact that these two numbers exactly agree is considered a "coincidence". (However, many such coincidental agreements are found between the semiclassical vs. full quantum mechanical treatment of the atom; these include identical energy levels in the hydrogen atom and the derivation of a fine structure constant, which arises from the relativistic Bohr–Sommerfeld model (see below) and which happens to be equal to an entirely different concept, in full modern quantum mechanics).
The Bohr model also has difficulty with, or else fails to explain:
Several enhancements to the Bohr model were proposed, most notably the Sommerfeld model or Bohr–Sommerfeld model, which suggested that electrons travel in elliptical orbits around a nucleus instead of the Bohr model's circular orbits. This model supplemented the quantized angular momentum condition of the Bohr model with an additional radial quantization condition, the Wilson–Sommerfeld quantization condition
where "p" is the radial momentum canonically conjugate to the coordinate "q" which is the radial position and "T" is one full orbital period. The integral is the action of action-angle coordinates. This condition, suggested by the correspondence principle, is the only one possible, since the quantum numbers are adiabatic invariants.
The Bohr–Sommerfeld model was fundamentally inconsistent and led to many paradoxes. The magnetic quantum number measured the tilt of the orbital plane relative to the "xy"-plane, and it could only take a few discrete values. This contradicted the obvious fact that an atom could be turned this way and that relative to the coordinates without restriction. The Sommerfeld quantization can be performed in different canonical coordinates and sometimes gives different answers. The incorporation of radiation corrections was difficult, because it required finding action-angle coordinates for a combined radiation/atom system, which is difficult when the radiation is allowed to escape. The whole theory did not extend to non-integrable motions, which meant that many systems could not be treated even in principle. In the end, the model was replaced by the modern quantum mechanical treatment of the hydrogen atom, which was first given by Wolfgang Pauli in 1925, using Heisenberg's matrix mechanics. The current picture of the hydrogen atom is based on the atomic orbitals of wave mechanics which Erwin Schrödinger developed in 1926.
However, this is not to say that the Bohr-Sommerfeld model was without its successes. Calculations based on the Bohr–Sommerfeld model were able to accurately explain a number of more complex atomic spectral effects. For example, up to first-order perturbations, the Bohr model and quantum mechanics make the same predictions for the spectral line splitting in the Stark effect. At higher-order perturbations, however, the Bohr model and quantum mechanics differ, and measurements of the Stark effect under high field strengths helped confirm the correctness of quantum mechanics over the Bohr model. The prevailing theory behind this difference lies in the shapes of the orbitals of the electrons, which vary according to the energy state of the electron.
The Bohr–Sommerfeld quantization conditions lead to questions in modern mathematics. Consistent semiclassical quantization condition requires a certain type of structure on the phase space, which places topological limitations on the types of symplectic manifolds which can be quantized. In particular, the symplectic form should be the curvature form of a connection of a Hermitian line bundle, which is called a prequantization.
Niels Bohr proposed a model of the atom and a model of the chemical bond. According to his model for a diatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. The dynamic equilibrium of the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account the Coulomb repulsion – the electrons in the ring are at the maximum distance from each other.

</doc>
<doc id="4832" url="https://en.wikipedia.org/wiki?curid=4832" title="Bombay Sapphire">
Bombay Sapphire

Bombay Sapphire is a brand of gin that was first launched in 1986 by English wine-merchant IDV. In 1997 Diageo sold the brand to Bacardi. Its name originates from the popularity of gin in India during the British Raj and "Sapphire" refers to the violet-blue Star of Bombay which was mined from Sri Lanka and is now on display at the Smithsonian Institution. Bombay Sapphire is marketed in a flat-sided, sapphire-coloured bottle that bears a picture of Queen Victoria on the label.
The flavouring of the drink comes from a recipe of ten ingredients: almond, lemon peel, liquorice, juniper berries, orris root, angelica, coriander, cassia, cubeb, and grains of paradise. Alcohol brought in from another supplier is evaporated three times using a carterhead still, and the alcohol vapours are passed through a mesh/basket containing the ten botanicals, in order to gain flavour and aroma. This is felt to give the gin a lighter, more floral taste compared to those gins that are created using a copper pot still. Water from Lake Vyrnwy is added to bring the strength of Bombay Sapphire down to 40.0% (UK, the Nordics, several continental European markets, Canada and Australia).
The 47.0% version is the standard for sale at duty-free stores in all markets.
In 2011, plans were announced to move the manufacturing process to a new facility at Laverstoke Mill in Whitchurch, Hampshire, including the restoration of the former Portal's paper mill at the proposed site, and the construction of a visitor centre. 
Planning permission was granted in February 2012, and the centre opened to the public in the autumn of 2014. The visitor centre included a new construction by Thomas Heatherwick of two glasshouses for plants used as botanicals in the production of Bombay Sapphire gin.
Production and bottling of the drink is contracted out by Bacardi to G&J Greenall.
Bacardi also markets Bombay Original London Dry Gin (or Bombay Original Dry). Eight botanical ingredients are used in the production of the Original Dry variety, as opposed to the ten in Bombay Sapphire. "Wine Enthusiast" preferred it to Bombay Sapphire.
In September 2011, Bombay Sapphire East was launched in test markets in New York and Las Vegas. This variety has another two botanicals, lemongrass and black peppercorns, in addition to the original ten. It is bottled at 42% and was designed to counteract the sweetness of American tonic water.
A special edition of Bombay gin called Star of Bombay was produced in 2015 for the UK market. It is bottled at 47.5% and is distilled from grain. It features bergamot and ambrette seeds in harmony with Bombay's signature botanicals. This version has later been extended to several other markets.
In summer 2019, Bacardi launched a limited edition gin called Bombay Sapphire English Estate, which features three additional English sourced botanicals: Pennyroyal Mint, rosehip and hazelnut. It is bottled at 41%.
The brand started a series of design collaborations. Their first step into the design world was a series of advertisements featuring work from currently popular designers. Their works, varying from martini glasses to tiles and cloth patterns, are labelled as “Inspired by Bombay Sapphire”. The campaign featured designers such as Marcel Wanders, Yves Behar, Karim Rashid, Ulla Darni, and Dror Benshetrit and performance artist Jurgen Hahn.
From the success of this campaign, the company began a series of events and sponsored locations. The best known is the Bombay Sapphire Designer Glass Competition, held each year, where design students from all over the world can participate by designing their own “inspired” martini cocktail glass. The finalists (one from each participating country) are then invited to the yearly Salone del Mobile, an international design fair in Milano, where the winner is chosen.
Bombay Sapphire also endorses glass artists and designers with the Bombay Sapphire Prize, which is awarded every year to an outstanding design which features glass. Bombay Sapphire also showcases the designers' work in the Bombay Sapphire endorsed blue room, which is a design exhibition touring the world each year.
From 2008 the Bombay Sapphire Designer Glass Competition final will be held at 100% Design in London, UK and the Bombay Sapphire Prize will take place in Milan at the Salone Del Mobile.
Bombay Sapphire has been reviewed by several outside spirit ratings organizations to various degrees of success. Recently, it was awarded a score of 92 (on a 100-point scale) from the Beverage Testing Institute. Ratings aggregator Proof66.com categorizes the Sapphire as a Tier 2 spirit, indicating highly favourable "expert" reviews.

</doc>
<doc id="4833" url="https://en.wikipedia.org/wiki?curid=4833" title="Bob Wills">
Bob Wills

James Robert Wills (March 6, 1905 – May 13, 1975) was an American Western swing musician, songwriter, and bandleader. Considered by music authorities as the co-founder of Western swing, he was known widely as the King of Western Swing (although Spade Cooley self-promoted the moniker "King of Western Swing" from 1942 to 1969).
Wills formed several bands and played radio stations around the South and West until he formed the Texas Playboys in 1934 with Wills on fiddle, Tommy Duncan on piano and vocals, rhythm guitarist June Whalin, tenor banjoist Johnnie Lee Wills, and Kermit Whalin, who played steel guitar and bass. The band played regularly on Tulsa, Oklahoma radio station KVOO and added Leon McAuliffe on steel guitar, pianist Al Stricklin, drummer Smokey Dacus, and a horn section that expanded the band's sound. Wills favored jazz-like arrangements and the band found national popularity into the 1940s with such hits as "Steel Guitar Rag", "New San Antonio Rose", "Smoke On The Water", "Stars And Stripes On Iwo Jima", and "New Spanish Two Step".
Wills and the Texas Playboys recorded with several publishers and companies, including Vocalion, Okeh, Columbia, and MGM, frequently moving. In 1950, he had two top 10 hits, "Ida Red Likes the Boogie" and "Faded Love", which were his last hits for a decade. Throughout the 1950s, he struggled with poor health and tenuous finances, but continued to perform frequently despite the decline in popularity of his earlier music as rock and roll took over. Wills had a heart attack in 1962 and a second one the next year, which forced him to disband the Playboys, although Wills continued to perform solo.
The Country Music Hall of Fame inducted Wills in 1968 and the Texas State Legislature honored him for his contribution to American music.
In 1972, Wills accepted a citation from the American Society of Composers, Authors and Publishers in Nashville. He was recording an album with fan Merle Haggard in 1973 when a stroke left him comatose until his death in 1975. The Rock and Roll Hall of Fame inducted Wills and the Texas Playboys in 1999.
He was born on a farm in Kosse, Limestone County, Texas to Emma Lee Foley and John Tompkins Wills. His parents were both of primarily English ancestry but they had distant Irish ancestry as well. His father was a statewide champion fiddle player, and either the Wills family was playing music or someone was "always wanting us to play for them", in addition to raising cotton on their farm.
In addition to picking cotton, the young Jim Bob learned to play the fiddle and the mandolin. Several of his sisters and brothers played musical instruments. The Wills family frequently held country dances in their home, and there was dancing in all four rooms. While living in Hall County, Texas, they also played at 'ranch dances', which were popular throughout west Texas.
Wills not only learned traditional music from his family, he learned some blues songs directly from African Americans in the cotton fields near Lakeview, Texas, and said that he did not play with many white children other than his siblings, until he was seven or eight years old. African Americans were his playmates, and his father enjoyed watching him jig dance with the black children.
The family moved to Hall County in the Texas Panhandle in 1913, and in 1919 they bought a farm between the towns of Lakeview, Texas and Turkey, Texas.
At the age of 16, Wills left the family and hopped a freight train. Jim Rob, as he became known, drifted for several years, traveling from town to town trying to earn a living, at one point almost losing his life when he nearly fell from a moving train, and later being chased by railroad police.
In his 20s, he attended barber school, married his first wife Edna, and moved first to Roy, New Mexico, then returned to Turkey in Hall County (now considered his home town) to work as a barber at Hamm's Barber Shop. He alternated barbering and fiddling even when he moved to Fort Worth, Texas after leaving Hall County in 1929. There he played in minstrel and medicine shows, and, as with other Texas musicians such as Ocie Stockard, continued to earn money as a barber. He wore blackface makeup to appear in comedy routines, something that was common at the time. "He was playing his violin and singing." There were two guitars and a banjo player with him. "Bob was in blackface and was the comic; he cracked jokes, sang, and did an amazing jig dance."
Since there was already a Jim on the show, the manager began calling him Bob. However, it was as Jim Rob Wills, paired with Herman Arnspiger, that he made his first commercial (though unissued) recordings in November 1929 for Brunswick/Vocalion.
Wills was known for his hollering and wisecracking. One source for this was when, as a very young boy, he heard his father, grandfather, and cowboys give out loud cries when the music moved them.
When asked if his wisecracking and talking on the bandstand came from his medicine show experience, he said it did not. Rather, he said that it came directly from playing and living close to Negroes, and that he never did it necessarily as show, but more as a way to express his feelings.
While in Fort Worth, Wills added the "rowdy city blues" of Bessie Smith and Emmett Miller to a repertoire of mainly waltzes and breakdowns he had learned from his father, and patterned his vocal style after that of Miller and other performers such as Al Bernard.
Wills acknowledged that he idolized Miller. Furthermore, his 1935 version of "St. Louis Blues" is nearly a word-for-word copy of Al Bernard's patter on his 1928 recording of the same song.
That Wills made his professional debut in blackface was commented on by Wills' daughter, Rosetta: "He had a lot of respect for the musicians and music of his black friends," Rosetta is quoted as saying on the Bob Wills and the Texas Playboys website. She remembers that her father was such a fan of Bessie Smith that "[h]e once rode fifty miles on horseback just to see her perform live." (Wills is quoted as saying, "I rode horseback from the place between the rivers to Childress to see Bessie Smith ... She was about the greatest thing I had ever heard. In fact, there was no doubt about it. She was the greatest thing I ever heard.")
In Fort Worth, Wills met Herman Arnspiger and formed The Wills Fiddle Band. In 1930 Milton Brown joined the group as lead vocalist and brought a sense of innovation and experimentation to the band, which became known as the Aladdin Laddies and then soon renamed itself the Light Crust Doughboys because of radio sponsorship by the makers of Light Crust Flour. Brown left the band in 1932 to form the Musical Brownies, the first true Western swing band. Brown added twin fiddles, tenor banjo and slap bass, pointing the music in the direction of swing, which they played on local radio and at dancehalls.
Wills recalled the early days of what became known as Western swing music in a 1949 interview. "Here's the way I figure it. We sure not tryin' to take credit for swingin' it." Speaking of Milt Brown and himself working with songs done by Jimmie Davis, the Skillet Lickers, "One Star Rag", "Rat Cheese Under the Hill", "Take Me Back to Tulsa", "Basin Street Blues", "Steel Guitar Rag", and "Trouble in Mind" were some of the songs in Wills' extensive repertory.
After forming a new band, The Playboys, and relocating to Waco, Texas, Wills found enough popularity there to decide on a bigger market. They left Waco in January 1934 for Oklahoma City. Wills soon settled the renamed Texas Playboys in Tulsa, Oklahoma, and began broadcasting noon shows over the 50,000-watt KVOO radio station. Their 12:30–1:15 pm, Monday–Friday broadcasts became a veritable institution in the region. Nearly all of the daily (except Sunday) shows originated from the stage of Cain's Ballroom. In addition, they played dances in the evenings, including regular ones at the ballroom on Thursdays and Saturdays.
Wills added a trumpet to the band inadvertently when he hired Everet Stover as an announcer, not knowing that he had played with the New Orleans symphony and had directed the governor's band in Austin. Stover, thinking he had been hired as a trumpeter, began playing with the band with no comment from Wills. Young sax player Zeb McNally was allowed to play with the band, although Wills initially discouraged it. With two horns in the band, Wills realized he would have to add a drummer to balance things and create a fuller sound. He hired the young, "modern-style musician" Smoky Dacus.
By 1935, Wills had added horn and reed players as well as drums to the Playboys. The addition of steel guitar whiz Leon McAuliffe in March 1935 added not only a formidable instrumentalist, but also a second engaging vocalist. Wills largely sang blues and sentimental ballads. Wills and the Texas Playboys did their first recordings on September 23–25, 1935, in Dallas. Session rosters from 1938 show both lead guitar and electric guitar in addition to guitar and steel guitar in the Texas Playboys recordings. Wills' 1938 recording of "Ida Red" served as a model for Chuck Berry's decades later version of the same song "Maybellene".
About this time, Wills purchased and performed with an old Guadagnini violin that had once fetched $7,600 for $1,600, the equivalent of about $24,000 in 2009.
In 1940, "New San Antonio Rose" sold a million records and became the signature song of The Texas Playboys. The "front line" of Wills' orchestra consisted of either fiddles or guitars after 1944.
In 1940, Wills, along with the Texas Playboys, co-starred with Tex Ritter in "Take Me Back to Oklahoma". Other films followed. In December 1942, after several band members had left the group, and as World War II raged, Wills joined the army at the age of 37, but he received a medical discharge in 1943.
Wills also appeared in "The Lone Prairie" (1942), "Riders Of The Northwest Mounted" (1943), "Saddles and Sagebrush" (1943), "The Vigilantes Ride" (1943), "The Last Horseman" (1944), "Rhythm Round-Up" (1945), "Blazing the Western Trail" (1945), and "Lawless Empire" (1945). According to one source, he appeared in a total of 19 films.
After leaving the Army in 1943, Wills moved to Hollywood, moving into a rented house in September, and began to reorganize the Texas Playboys. He became an enormous draw in Los Angeles, where many of his Texas, Oklahoma and regional fans had relocated during the Great Depression and World War II in search of jobs. Monday through Friday, the band broadcast from 12:01 to 1:00 pm PT over KMTR-AM (now KLAC) in Los Angeles. They also played regularly every Friday, Saturday, and Sunday night at the Mission Beach Ballroom in San Diego.
He commanded enormous fees playing dances there, and began to make more creative use of electric guitars to replace the big horn sections the Tulsa band had boasted. For a very brief period in 1944, the Wills band included 23 members, and around mid-year he toured Northern California and the Pacific Northwest with 21 pieces in the orchestra. "Billboard" reported that Wills out-grossed Harry James, Benny Goodman, "both Dorseys, et al." at Civic Auditorium in Oakland, California in January 1944.
Wills and His Texas Playboys began their first cross-country tour in November 1944, and appeared at the Grand Ole Opry on December 30, 1944. According to Opry policy, drums and horns were considered pop instruments, inappropriate to country music. The Opry had two western swing bands on its roster, led by Pee Wee King and Paul Howard. Neither were allowed to use their drummers at the Opry. Wills' band at the time consisted of two fiddlers, two bass fiddles, two electric guitars, electric steel guitar, and a trumpet. Wills's then-drummer was Monte Mountjoy, who played in the Dixieland style. Wills battled Opry officials and refused to perform without his drummer. An attempt to compromise by keeping Mountjoy behind a curtain collapsed when Wills had his drums placed front and center onstage at the last minute.
In 1945, Wills' dances were outdrawing those of Tommy Dorsey and Benny Goodman, and he moved to Fresno, California. Then in 1947, he opened the Wills Point nightclub in Sacramento, California and continued touring the Southwest and Pacific Northwest from Texas to Washington State. While based in Sacramento, his radio broadcasts over 50,000-watt KFBK were heard all over the West.
Famous swing orchestras in California realized that many of their followers were leaving to dance to Bob Will's Western swing. Because he was in such demand, some places booked Wills any time he had an opening, regardless of how undesirable the date. The manager of a popular auditorium in the LA Basin town of Wilmington, California: "Although Monday night dancing is frankly an experiment it was the only night of the week on which this outstanding band could be secured."
During the postwar period, KGO radio in San Francisco syndicated a Bob Wills and His Texas Playboys show recorded at the Fairmont Hotel. Many of these recordings survive today as the Tiffany Transcriptions and are available on CD. They show off the band's strengths significantly, in part because the group was not confined to the three-minute limits of 78 RPM discs. On April 3, 1948, Wills and the Texas Playboys appeared for the inaugural broadcast of the "Louisiana Hayride" on KWKH, broadcasting from the Municipal Auditorium in Shreveport, Louisiana.
Wills and the Texas Playboys played dances throughout the West to more than 10,000 people every week. They held dance attendance records at Jantzen Beach in Portland, Oregon; in Santa Monica, California, and at the Oakland (California) Auditorium, where they drew 19,000 people in two nights. Wills also broke an attendance record of 2,100 previously held by Jan Garber at the Armory in Klamath Falls, Oregon, by attracting 2,514 dancers. Wills and the Playboys also played small towns on the West Coast. Actor Clint Eastwood recalled seeing Wills when he was 18 or 19 (1948 or 1949) and working at a pulp mill in Springfield, Oregon.
Appearances at the Bostonia Ballroom in San Diego continued throughout the 1950s.
Still a binge drinker, Wills became increasingly unreliable in the late 1940s, causing a rift with Tommy Duncan (who bore the brunt of audience anger when Wills's binges prevented him from appearing). It ended when he fired Duncan in the fall of 1948.
Having lived a lavish lifestyle in California, Wills moved back to Oklahoma City in 1949, then went back on the road to maintain his payroll and Wills Point. He opened a second club, the Bob Wills Ranch House in Dallas, Texas. Turning the club over to managers, later revealed to be dishonest, left Wills in desperate financial straits with heavy debts to the IRS for back taxes. This caused him to sell many assets, including, mistakenly, the rights to "New San Antonio Rose". It wrecked him financially.
In 1950, Wills had two top 10 hits, "Ida Red Likes the Boogie" and "Faded Love". After 1950, radio stations began to increasingly specialize in one form or another of commercially popular music. Wills did not fit into the popular Nashville country and western stations, although he was usually labeled "country and western". Neither did he fit into the pop or middle of the road stations, although he played a good deal of pop music, and was not accepted in the pop music world.
He continued to tour and record through the 1950s into the early 1960s despite the fact that Western Swing's popularity, even in the Southwest, had greatly diminished. Bob could draw "a thousand people on Monday night between 1950 and 1952, but he could not do that by 1956. Entertainment habits had changed."
On Wills' return to Tulsa late in 1957, Jim Downing of the "Tulsa Tribune" wrote an article headlined "Wills Brothers Together Again: Bob Back with Heavy Beat". The article quotes Wills as saying "Rock and roll? Why, man, that's the same kind of music we've been playin' since 1928! ... We didn't call it rock and roll back when we introduced it as our style back in 1928, and we don't call it rock and roll the way we play it now. But it's just basic rhythm and has gone by a lot of different names in my time. It's the same, whether you just follow a drum beat like in Africa or surround it with a lot of instruments. The rhythm's what's important." The use of amplified guitars accentuates Wills's claim; some Bob Wills recordings from the 1930s and 1940s sound similar to rock and roll records of the 1950s.
Even a 1958 return to KVOO, where his younger brother Johnnie Lee Wills had maintained the family's presence, did not produce the success he hoped. He appeared twice on ABC-TV's "Jubilee USA" and kept the band on the road into the 1960s. After two heart attacks, in 1965 he dissolved the Texas Playboys (who briefly continued as an independent unit) to perform solo with house bands. While he did well in Las Vegas and other areas, and made records for the Kapp Records label, he was largely a forgotten figure—even though inducted into the Country Music Hall of Fame in 1968. A 1969 stroke left his right side paralyzed, ending his active career. He did, however, recover sufficiently to appear in a wheelchair at various Wills tributes held in the early 1970s. A revival of interest in his music, spurred by Merle Haggard's 1970 album "A Tribute to the Best Damn Fiddle Player in the World", led to a 1973 reunion album, teaming Wills, who spoke with difficulty, with key members of the early band, as well as Haggard.
May 26, 1975 issue of "Time" (Milestones section) read: "Died. Bob Wills, 70, "Western Swing" bandleader-composer; of pneumonia; in Fort Worth. Wills turned out dance tunes that are now called country rock, introducing with his Texas Playboys such C & W classics as Take Me Back to Tulsa and New San Antonio Rose".
Bob Wills was married six times and divorced five times. He was twice married to, and divorced from Mary Helen Brown, the widow of Wills' ex-band member Milton Brown.
Wills' style influenced performers Buck Owens, Merle Haggard, and The Strangers and helped to spawn a style of music now known as the Bakersfield Sound. (Bakersfield, California was one of Wills' regular stops in his heyday). A 1970 tribute album by Haggard, "A Tribute to the Best Damn Fiddle Player in the World (or, My Salute to Bob Wills)" directed a wider audience to Wills' music, as did the appearance of younger "revival" bands like Asleep at the Wheel and Commander Cody and His Lost Planet Airmen plus the growing popularity of longtime Wills disciple and fan Willie Nelson. By 1971, Wills recovered sufficiently to travel occasionally and appear at tribute concerts. In 1973, he participated in a final reunion session with members of some the Texas Playboys from the 1930s to the 1960s. Merle Haggard was invited to play at this reunion. The session, scheduled for two days, took place in December 1973, with the album to be titled "For the Last Time". Wills, speaking or attempting to holler, appeared on a couple tracks from the first day's session but suffered a stroke overnight. He had a more severe one a few days later. The musicians completed the album without him. Wills by then was comatose. He lingered until his death on May 13, 1975.
Reviewing "For the Last Time" in "" (1981), Robert Christgau wrote: "This double-LP doesn't represent the band at its peak. But though earlier recordings of most of these classic tunes are at least marginally sharper, it certainly captures the relaxed, playful, eclectic Western swing groove that Wills invited in the '30s."
In addition to being inducted into the Country Music Hall of Fame in 1968, Wills was inducted into the Nashville Songwriters Hall of Fame in 1970, the Rock and Roll Hall of Fame in the Early Influence category along with the Texas Playboys in 1999, and received the Grammy Lifetime Achievement Award in 2007.
From 1974 until his 2002 death, Waylon Jennings performed a song he had written called "Bob Wills Is Still the King". Released as the B-side of a single that was a double-sided hit, it went to number one on the country charts. The song has become a staple of classic country radio station formats. In addition, The Rolling Stones performed this song live in Austin, Texas at Zilker Park on their A Bigger Bang Tour, a shout-out to Wills. This performance was included on their subsequent DVD "The Biggest Bang". In a 1968 issue of "Guitar Player", rock guitarist Jimi Hendrix said of Wills and the Playboys: "I dig them. The Grand Ole Opry used to come on, and I used to watch that. They used to have some pretty heavy cats, some heavy guitar players." In fact, Bob Wills and His Texas Playboys only performed on the Opry twice: in 1944 and 1948. Hendrix almost surely referred to Nashville guitarists.
Wills ranked #27 in "CMT's 40 Greatest Men in Country Music" in 2003.
Wills' upbeat 1938 song Ida Red was Chuck Berry's primary inspiration for creating his first rock-and-roll hit "Maybellene".
Fats Domino once remarked that he patterned his 1960 rhythm section after that of Bob Wills.
During the 49th Grammy Awards in 2007, Carrie Underwood performed his song "San Antonio Rose". Today, George Strait performs Wills' music on concert tours and records songs influenced by Wills and his Texas-style swing.
The Austin-based Western swing band Asleep at the Wheel have honored Wills' music since the band's inception, mostly notably with their continuing performances of the musical drama "A Ride with Bob", which debuted in Austin in March 2005 to coincide with celebrations of Wills' 100th birthday.
The Bob Wills Birthday Celebration is held every year in March at the Cain's Ballroom in Tulsa, Oklahoma with a Western swing concert and dance.
In 2004, a documentary film about his life and music, titled "Fiddlin' Man: The Life and Music of Bob Wills", was released by VIEW Inc.
In 2011, Proper Records released an album by Hot Club of Cowtown titled "What Makes Bob Holler: A Tribute to Bob Wills and His Texas Playboys".
In 2011, the Texas Legislature adopted a resolution designating western swing as the official State Music of Texas.
The Greenville Chamber of Commerce hosts an annual Bob Wills Fiddle Festival and Contest in downtown Greenville, Texas in November.
Bob Wills was honored in Episode 2 of Ken Burn's 2019 series on PBS called Country Music.

</doc>
<doc id="4834" url="https://en.wikipedia.org/wiki?curid=4834" title="Badtrans">
Badtrans

BadTrans is a malicious Microsoft Windows computer worm distributed by e-mail. Because of a known vulnerability in older versions of Internet Explorer, some e-mail programs, such as Microsoft's Outlook Express and Microsoft Outlook programs, may install and execute the worm as soon as the e-mail message is viewed.
Once executed, the worm replicates by sending copies of itself to other e-mail addresses found on the host's machine, and installs a keystroke logger, which then captures everything typed on the affected computer. Badtrans then transmits the data to one of several e-mail addresses. 
Among the e-mail addresses that received the keyloggers were free addresses at Excite, Yahoo, and IJustGotFired.com.
The target address at IJustGotFired began receiving e-mails at 3:23pm on November 24, 2001. Once the account exceeded its quotas, it was automatically disabled, but the messages were still saved as they arrived. The address received over 100,000 keylogs in the first day alone.
In mid-December, the FBI contacted Rudy Rucker, Jr., owner of MonkeyBrains, and requested a copy of the keylogged data. All of that data was stolen from the victims of the worm; it includes no information about the creator of Badtrans.
Instead of complying with the FBI request, MonkeyBrains published a database website https://web.archive.org/web/20070621140432/https://badtrans.monkeybrains.net/ for the public to determine if a given address has been compromised. The database does not reveal the actual passwords or keylogged data.

</doc>
<doc id="4836" url="https://en.wikipedia.org/wiki?curid=4836" title="Barış Manço">
Barış Manço

Mehmet Barış Manço (born Tosun Yusuf Mehmet Barış Manço; 2 January 1943 – 1 February 1999), better known by his stage name Barış Manço, was a Turkish rock musician, singer, songwriter, composer, actor, television producer and show host. Beginning his musical career while attending Galatasaray High School, he was a pioneer of rock music in Turkey and one of the founders of the Anatolian rock genre. Manço composed around 200 songs and is among the best-selling and most awarded Turkish artists to date. Many of his songs were translated into a variety of languages including English, French, Japanese, Greek, Italian, Bulgarian, Romanian, Persian, Hebrew, Urdu, Arabic, and German, among others. Through his TV program, "7'den 77'ye" ("From 7 to 77"), Manço traveled the world and visited most countries on the globe. He remains one of the most popular public figures of Turkey.
Barış Manço was born in Üsküdar, Istanbul, Turkey on 2 January 1943. His mother, Rikkat Uyanık, was a famous singer in the early 1940s. His older brother, who was born during World War II, was named Savaş ("war" in Turkish) while he was named Barış ("peace" in Turkish) by his parents to celebrate the end of the war. At birth, he was additionally named Tosun Yusuf after his deceased uncle Yusuf called Tosun (literally: Joseph the Sturdy). However, this name was erased just before he attended the primary school.
In primary school his head was shaven to prevent head lice, a serious threat back then, which he cited among reasons for his later signature long hair.
During his highschool days in Galatasaray High School (and later in Şişli Terakki High School) he formed his first band, Kafadarlar ("The Buddies"), allegedly upon seeing Erkin Koray's band performing, all students of Deutsche Schule Istanbul ("İstanbul Alman Lisesi"), a nearby highschool. Prof. Dr. Asaf Savaş Akat, a famous economist in Turkey, played saxophone, and guitarist Ender Enön made his own guitar because it was difficult to find a real one on the market in those years.
In 1962 and 1963, with his next band, Harmoniler ("The Harmonies"), he recorded cover versions of some of popular American twist songs and rearrangements of Turkish folk songs in rock and roll form, marking the beginning of the Anatolian rock movement, a synthesis of Turkish folk music and rock. In this period, his key visual and musical influence was Elvis Presley.
After graduating from high school in 1963, he moved to Europe, travelling around Paris and Liège, where he formed bands with local musicians and recorded some singles mainly in English and in French but also in Turkish. Then, in 1964, Barış Manço continued his studies at the Royal Academy of Fine Arts in Liège, Belgium. He toured with his band Les Mistigris (not related to Mistigris) in Germany, Belgium, France and Turkey until 1967.
In 1967, he suffered a serious car accident, after which he started to grow his signature mustache to disguise his scar.
Frustrated by the difficulties of working with musicians from different nationalities, he formed Kaygısızlar (The Carefrees), featuring Mazhar Alanson and Fuat Güner, future members of the band MFÖ. He recorded several singles and toured with the band, both domestically and internationally, until the band members revealed that they did not want to live abroad.
In 1970, he formed Barış Manço Ve... ("Barış Manço and...") again with foreign musicians, to record his first hit single, both in Turkey and in Belgium, "Dağlar Dağlar" (Mountains, Mountains!), selling over 700,000 copies. Today, the song remains one of his most popular works.
After the success of "Dağlar Dağlar", Manço recorded a couple of singles with Moğollar (The Mongols), another influential Turkish Anatolian rock band. He then decided to return to Turkey where he recorded with the reformed Kaygısızlar for a short period. In 1971, his early works were compiled under his first full-length album "Dünden Bugüne", today commonly referred as "Dağlar Dağlar".
In 1972, he formed Kurtalan Ekspres, a legend by itself, the band that would accompany him until his death. In 1975 until when he continued to release singles, he released his first non-compilation LP "2023", a concept album that includes many instrumental songs.
As a last attempt to reach international success, he released the LP titled "Baris Mancho" (1976), a strange transcription of his name, mostly with George Hayes Orchestra under CBS Records label, in Europe and South Africa. Although the album did not bring the fame he was expecting, it did reach the top of the charts in Romania and Morocco. The following year, the album was released in Turkey under the title "Nick the Chopper". In 1975 he starred in the movie Baba Bizi Eversene (Father make us marry) which is the only movie he ever starred during his career. The music of the movie consists of a compilation of tracks composed by Barış Manço and Kurtalan Ekspres.
From 1977 to 1980, he released three more albums in Turkey, partly consisting of compilations of older singles, namely "Sakla Samanı Gelir Zamanı" (1977), "Yeni Bir Gün" (1979) and "20. Sanat Yılı Disko Manço" (1980), all following a similar sound with "2023". All these albums are now rarity items, but most of the material from the era are available in later compilations "Ben Bilirim" and "Sarı Çizmeli Mehmet Ağa".
In 1981, Manço released "Sözüm Meclisten Dışarı" with Kurtalan Ekspres, containing many hit songs including "Alla Beni Pulla Beni", "Arkadaşım Eşek", "Gülpembe", "Halhal" and "Dönence" among others. The album remains as one of their most popular works and launched a boost of popularity for Barış Manço during the 1980s.
"Arkadaşım Eşek" ("My Friend Donkey"), quickly grew very popular among children (the song is about rural nostalgia and was not initially intended as a children's song). Throughout his career, he went on to write many other songs primarily for children to achieve an iconic acceptance among Turkish children of the 1980s and 1990s.
On the other hand, "Gülpembe", composed by Kurtalan Ekspres bassist Ahmet Güvenç, a requiem for Manço's grandmother, caught older audiences and probably is the artist's most popular song, competing perhaps only with "Dağlar Dağlar".
In 1983, "Estağfurullah, Ne Haddimize" was released. It contained hit songs "Halil İbrahim Sofrası" and "Kol Düğmeleri", a new version of the artist's first song. "Halil İbrahim Sofrası" exemplified Manço's signature moral themed lyrics, a rare feature in Turkish pop music.
In 1985, "24 Ayar Manço" which included "Gibi Gibi" and a long conceptual song "Lahburger" was released. It also marked the beginning of the shift in Manço's sound characterized with the heavy use of synthesizers and drum machine in contrast with his older works consisting of a group oriented rock based sound. In subsequent years, Manço released "Değmesin Yağlı Boya" (1986), "Sahibinden İhtiyaçtan" (1988) and "Darısı Başınıza" (1989), all containing a couple of hit songs and demonstrating his new sound.
In 1988, "7'den 77'ye" ("From 7 to 77"), a TV show directed and presented by Manço, began to run on TRT 1, the Turkish state television channel. It was a combined music, talk show, and documentary program which was a major hit during the eight years it stayed on air. Manço traveled to almost 150 countries for the show. "Adam Olacak Çocuk", a section of his show dedicated to children, strengthened Manço's popularity among the young generations.
Although his fame continued in the 1990s thanks to the wide audience of his TV show, which was followed by all age groups, his musical works in this period were not as successful as those in the previous decades. The albums "Mega Manço" (1992) and "Müsadenizle Çocuklar" (1995) were considered the weakest efforts of his career, despite the limited success of the 1992 children hit "Ayı" (The Bear). On the other hand, in 1995 he toured in Japan with Kurtalan Ekspres, leading to "Live In Japan" (1996), his only live album. He released two albums in that country with some recognition as "the man who writes songs about vegetables", referring to "Domates, Biber, Patlıcan" ("Tomato, Pepper, Aubergine") and "Nane, Limon Kabuğu" (Mint, Lemon Rind), two of his hit songs from the 1980s.
On 1 February 1999, Barış Manço died of a sudden heart attack before the release of his just finished last work "Mançoloji" ("Mançology" or "Manchology") (1999), a double album containing the new recordings of his hit songs along with an unfinished instrumental song "40. Yıl" ("The 40th Anniversary"), celebrating his 40th year in music. His sudden death caused an almost unanimous shock in Turkey with millions of people mourning and tens of thousands of people attending his funeral.
He was interred at Kanlıca Cemetery in Istanbul.
Barış Manço was one of the most influential Turkish musicians. In his early career he and his bands contributed to the Turkish rock movement by combining traditional Turkish music with rock influences, which is still one of the main trends of Turkish popular music.
His visual image, characterized by his long hair, mustache and big rings, softened the reaction of the otherwise conservative Turkish public opinion.
Manço pioneered the progressive rock-influenced Anatolian rock movement in the 1970s. His experimentation with electronic instruments in the late 1980s contributed to the 1990s sound of Turkish popular music.
His lyrics with diverse themes, mostly following a somewhat modernized version of the "aşık" (wandering folk poets) tradition were heavily marginal in the popular music scene of the 1980s which was mostly dominated by love-themed lyrics.
In 2002, a tribute album was released under the name "Yüreğimdeki Barış Şarkıları" ("Songs of Barış (Peace) In My Heart"), featuring 15 popular Turkish artists of such diverse genres like arabesque, pop and rock (both Anatolian and western style) demonstrating his wide range of influence.
With Harmoniler
With Jacques Denjean Orchestra
With Les Mistigris
With Kaygısızlar
With Barış Manço Ve
With Moğollar
With Moğollar / Kaygısızlar
With Kaygısızlar / Les Mistigris
With Kurtalan Ekspres
With George Hayes Orchestra / Kurtalan Ekspres
With Kurtalan Ekspres

</doc>
<doc id="4840" url="https://en.wikipedia.org/wiki?curid=4840" title="Blitz BASIC">
Blitz BASIC

Blitz BASIC is the programming language dialect of the first Blitz compilers, devised by New Zealand-based developer Mark Sibly. Being derived from BASIC, Blitz syntax was designed to be easy to pick up for beginners first learning to program. The languages are game-programming oriented but are often found general-purpose enough to be used for most types of application. The Blitz language evolved as new products were released, with recent incarnations offering support for more advanced programming techniques such as object-orientation and multithreading. This led to the languages losing their BASIC moniker in later years.
The first iteration of the Blitz language was created for the Amiga platform and published by the Australian firm Memory and Storage Technology. Returning to New Zealand, Blitz BASIC 2 was published several years later (around 1993 according this press release ) by Acid Software (a local Amiga game publisher). Since then, Blitz compilers have been released on several platforms. Following the demise of the Amiga as a commercially viable platform, the Blitz BASIC 2 source code was released to the Amiga community. Development continues to this day under the name AmiBlitz.
Idigicon published BlitzBasic for Microsoft Windows in October 2000. The language included a built-in API for performing basic 2D graphics and audio operations. Following the release of Blitz3D, BlitzBasic is often synonymously referred to as Blitz2D.
Recognition of BlitzBasic increased when a limited range of "free" versions were distributed in popular UK computer magazines such as "PC Format". This resulted in a legal dispute between the developer and publisher which was eventually resolved amicably.
In February 2003, Blitz Research Ltd. released BlitzPlus also for Microsoft Windows. It lacked the 3D engine of Blitz3D, but did bring new features to the 2D side of the language by implementing limited Microsoft Windows control support for creating native GUIs. Backwards compatibility of the 2D engine was also extended, allowing compiled BlitzPlus games and applications to run on systems that might only have DirectX 1.
The first BlitzMax compiler was released in December 2004 for Mac OS X. This made it the first Blitz dialect that could be compiled on *nix platforms. Compilers for Microsoft Windows and Linux were subsequently released in May 2005. BlitzMax brought the largest change of language structure to the modern range of Blitz products by extending the type system to include object-oriented concepts and modifying the graphics API to better suit OpenGL. BlitzMax was also the first of the Blitz languages to represent strings internally using UCS-2, allowing native-support for string literals composed of non-ASCII characters.
BlitzMax's platform-agnostic command-set allows developers to compile and run source code on multiple platforms. However the official compiler and build chain will only generate binaries for the platform that it is executing on. Unofficially, users have been able to get Linux and Mac OS X to cross-compile to the Windows platform.
BlitzMax is also the first modular version of the Blitz languages, improving the extensibility of the command-set. In addition, all of the standard modules shipped with the compiler are open-source and so can be tweaked and recompiled by the programmer if necessary. The official BlitzMax cross-platform GUI module (known as MaxGUI) allows developers to write GUI interfaces for their applications on Linux (FLTK), Mac (Cocoa) and Windows. Various user-contributed modules extend the use of the language by wrapping such libraries as wxWidgets, Cairo, and Fontconfig as well as a selection of database modules. There are also a selection of third-party 3D modules available namely MiniB3D - an open-source OpenGL engine which can be compiled and used on all three of BlitzMax's supported platforms.
In October 2007, BlitzMax 1.26 was released which included the addition of a reflection module. BlitzMax 1.32 shipped new threading and Lua scripting modules and most of the standard library functions have been updated so that they are unicode friendly.
Blitz3D SDK is a 3D graphics engine based on the engine in Blitz3D. It was marketed for use with C++, C#, BlitzMax, and PureBasic, however it could also be used with other languages that follow compatible calling conventions.
In 2008, the source code to Max3D - a C++-based cross-platform 3D engine - was released under a BSD license. This engine focused on OpenGL but had an abstract backend for other graphics drivers (such as DirectX) and made use of several open-source libraries, namely Assimp, Boost, and ODE.
Despite the excitement in the Blitz community of Max3D being the eagerly awaited successor to Blitz3D, interest and support died off soon after the source code was released and eventually development came to a halt. There is no indication that Blitz Research will pick up the project again.
BlitzPlus was released as open-source on 28 April 2014 under the zlib license on GitHub. Blitz3D followed soon after and was released as Open Source on 3 August 2014. BlitzMax was later released as Open Source on 21 September 2015.
Hello World program that prints to the screen, waits until a key is pressed, and then terminates: 
Print "Hello World" ; Prints to the screen.
WaitKey() ; Pauses execution until a key is pressed.
End ; Ends Program.
Program that demonstrates the declaration of variables using the three main data types (Strings, Integers and Floats) and printing them onto the screen:
name$ = "John" ; Create a string variable ($) 
age = 36 ; Create an integer variable (No Suffix)
temperature# = 27.3 ; Create a float variable (#)
print "My name is " + name$ + " and I am " + age + " years old."
print "Today, the temperature is " + temperature# + " degrees."
Waitkey() ; Pauses execution until a key is pressed.
End ; Ends program.
Program that creates a windowed application that shows the current time in binary and decimal format. See below for the BlitzMax and BlitzBasic versions:
In 2011, BRL released a new cross-platform programming language called Monkey and its first official module called Mojo. Monkey has a similar syntax to BlitzMax, but instead of compiling direct to assembly code, it translates Monkey source files directly into source code for a chosen language, framework or platform e.g. Windows, Mac OS X, iOS, Android, HTML5, and Adobe Flash.
Development of Monkey X has been halted in favor of Monkey 2, an updated version of the language by Mark Sibly.

</doc>
<doc id="4842" url="https://en.wikipedia.org/wiki?curid=4842" title="Bliss bibliographic classification">
Bliss bibliographic classification

The Bliss bibliographic classification (BC) is a library classification system that was created by Henry E. Bliss (1870–1955) and published in four volumes between 1940 and 1953. Although originally devised in the United States, it was more commonly adopted by British libraries. A second edition of the system (BC2) has been in ongoing development in Britain since 1977.
Henry E. Bliss began working on the Bliss Classification system while working at the City College of New York Library as Assistant Librarian. He was a critic of Melvil Dewey's work with the Dewey Decimal System and believed that organization of titles needed to be done with an intellectual mind frame. Being overly pragmatic or simply alphabetical, would be inadequate. In fact, Bliss is the only theorist who created an organizational scheme based on societal needs. Bliss wanted a classification system that would provide distinct rules yet still be adaptable to whatever kind of collection a library might have, as different libraries have different needs. His solution was the concept of "alternative location," in which a particular subject could be put in more than one place, as long as the library made a specific choice and used it consistently.
Bliss discusses his theories and basis of organization for the Bliss Classification for the first time in his 1910 article, "A Modern Classification for Libraries, with Simple Notation, Mnemonics, and Alternatives". This publication followed his 1908 reclassification of the City College collection. His work, "Organization of Knowledge and the System of the Sciences" was published in four volumes between 1940 and 1953.
The four broad underlying policies of the BC system are: 
Bliss deliberately avoided the use of the decimal point because of his objection to Dewey's system. Instead he used capital and lower-case letters, numerals, and every typographical symbol available on his extensive and somewhat eccentric typewriter.
Single letter codes refer to broad subject areas and further letters are added to refer to increasingly specific subdisciplines. For example, at Lancaster University:
In 1967 the Bliss Classification Association was formed. Its first publication was the Abridged Bliss Classification (ABC), intended for school libraries. In 1977 it began to publish and maintain a revised version of Bliss's system, the Bliss Bibliographic Classification (Second Edition) or BC2. This retains only the broad outlines of Bliss's scheme, replacing most of the detailed notation with a new scheme based on the principles of faceted classification. 15 of approximately 28 volumes of schedules have so far been published. A revision of this nature has been considered by some to be a completely new system.
The City College library in New York continued to use Bliss's system until 1967, when it switched to the Library of Congress system. It had become too expensive to train new staff members to use BC, and too expensive to maintain in general. Much of the Bliss stacks remain, however, as no-one has re-cataloged the books.
The case was different, however, in Britain. BC proved more popular there and also spread to other English-speaking countries. Part of the reason for its success was that libraries in teachers’ colleges liked the way Bliss had organized the subject areas on teaching and education. By the mid-1950s the system was being used in at least sixty British libraries and in a hundred by the 1970s The Bliss Classification system has been found to be successful in academic, specialty, government, and law libraries. It has also found success in libraries outside of the United States of America, as many of these libraries do not have a history of using either the Dewey Decimal, or the Library of Congress classification system.
The general organizational pattern for classifying titles in the BC2 method are:
The Class Schedule is:

</doc>
<doc id="4845" url="https://en.wikipedia.org/wiki?curid=4845" title="Blood alcohol content">
Blood alcohol content

Blood alcohol content/concentration (BAC) is a measurement of alcohol intoxication used for legal or medical purposes. A BAC of 0.10 (0.10% or one tenth of one percent) means that there are 0.10 g of alcohol for every 100 ml of blood which is the same as 21.7 mmol/l. A BAC of 0.0 is sober, while in the United States 0.08 is legally intoxicated, and above that is very impaired. BAC levels above 0.40 are potentially fatal.
To calculate estimated peak blood alcohol concentration (EBAC), a variation, including drinking period in hours, of the Widmark formula was used. The formula is:
where:
Regarding metabolism (MR) in the formula; females demonstrated a higher average rate of elimination (mean, 0.017; range, 0.014–0.021 g/210 L) than males (mean, 0.015; range, 0.013–0.017 g/210 L). Female subjects on average had a higher percentage of body fat (mean, 26.0; range, 16.7–36.8%) than males (mean, 18.0; range, 10.2–25.3%). Additionally, men are, on average, heavier than women but it is not strictly accurate to say that the water content of a person alone is responsible for the dissolution of alcohol within the body, because alcohol does dissolve in fatty tissue as well. When it does, a certain amount of alcohol is temporarily taken out of the blood and briefly stored in the fat. For this reason, most calculations of alcohol to body mass simply use the weight of the individual, and not specifically his/her water content. Finally, it is speculated that the bubbles in sparkling wine may speed up alcohol intoxication by helping the alcohol to reach the bloodstream faster.
Examples:
Note: This chart defines a drink as 14 g of ethanol, while the formula defines a drink as 10 g of ethanol.
Standard drink sizes (Australia)
The National Institute on Alcohol Abuse and Alcoholism (NIAAA) define the term "binge drinking" as a pattern of drinking that brings a person's blood alcohol concentration (BAC) to 0.08 grams percent or above. This typically happens when men consume 5 or more drinks, and when women consume 4 or more drinks, in about 2 hours.
There are several different units in use around the world for defining blood alcohol concentration. Each is defined as either a mass of alcohol per volume of blood or a mass of alcohol per mass of blood (never a volume per volume). 1 milliliter of blood has a mass of approximately 1.06 grams. Because of this, units by volume are similar but not identical to units by mass. In the U.S. the concentration unit 1% w/v (percent mass/volume, equivalent to 10 g/l or 1 g per 100 ml) is in use. 
This is not to be confused with the amount of alcohol measured on the breath, as with a breathalyzer. The amount of alcohol measured on the breath is generally accepted as proportional to the amount of alcohol present in the blood at a rate of 1:2100. Therefore, a breathalyzer measurement of 0.10 mg/L of breath alcohol converts to 0.0001×2100 g/10dL, or 0.21 g/dL of blood alcohol (the units of the BAC in the United States, this is equivalent to 2.1 permille). While a variety of units (or sometimes lack thereof) is used throughout the world, many countries use the g/L unit, which does not create confusion as percentages do. Usual units are highlighted in the table below.
For purposes of law enforcement, blood alcohol content is used to define intoxication and provides a rough measure of impairment. Although the degree of impairment may vary among individuals with the same blood alcohol content, it can be measured objectively and is therefore legally useful and difficult to contest in court. Most countries disallow operation of motor vehicles and heavy machinery above prescribed levels of blood alcohol content. Operation of boats and aircraft is also regulated.
The alcohol level at which a person is considered legally impaired varies by country. The list below gives limits by country. These are typically blood alcohol content limits for the operation of a vehicle.
It is illegal to have any measurable alcohol in the blood while driving in these countries. Most jurisdictions have a tolerance slightly higher than zero to account for false positives and naturally occurring alcohol in the body. Some of the following jurisdictions have a general prohibition of alcohol.
In certain countries, alcohol limits are determined by the breath alcohol content (BrAC), not to be confused with blood alcohol content (BAC).
Blood alcohol tests assume the individual being tested is average in various ways. For example, on average the ratio of blood alcohol content to breath alcohol content (the "partition ratio") is 2100 to 1. In other words, there are 2100 parts of alcohol in the blood for every part in the breath. However, the actual ratio in any given individual can vary from 1300:1 to 3100:1, or even more widely. This ratio varies not only from person to person, but within one person from moment to moment. Thus a person with a true blood alcohol level of .08% but a partition ratio of 1700:1 at the time of testing would have a .10 reading on a Breathalyzer calibrated for the average 2100:1 ratio.
Retrograde extrapolation is the mathematical process by which someone's blood alcohol concentration at the time of driving is estimated by projecting backwards from a later chemical test. This involves estimating the absorption and elimination of alcohol in the interim between driving and testing. The rate of elimination in the average person is commonly estimated at .015 to .020 grams per deciliter per hour (g/dl/h), although again this can vary from person to person and in a given person from one moment to another. Metabolism can be affected by numerous factors, including such things as body temperature, the type of alcoholic beverage consumed, and the amount and type of food consumed.
In an increasing number of states, laws have been enacted to facilitate this speculative task: the blood alcohol content at the time of driving is legally presumed to be the same as when later tested. There are usually time limits put on this presumption, commonly two or three hours, and the defendant is permitted to offer evidence to rebut this presumption.
Forward extrapolation can also be attempted. If the amount of alcohol consumed is known, along with such variables as the weight and sex of the subject and period and rate of consumption, the blood alcohol level can be estimated by extrapolating forward. Although subject to the same infirmities as retrograde extrapolation—guessing based upon averages and unknown variables—this can be relevant in estimating BAC when driving and/or corroborating or contradicting the results of a later chemical test.
Alcohol is absorbed throughout the gastrointestinal tract, but more slowly in the stomach than in the small or large intestine. For this reason, alcohol consumed with food is absorbed more slowly, because it spends a longer time in the stomach. Furthermore, alcohol dehydrogenase is present in the stomach lining. After absorption, the alcohol passes to the liver through the hepatic portal vein, where it undergoes a first pass of metabolism before entering the general bloodstream.
Alcohol is removed from the bloodstream by a combination of metabolism, excretion, and evaporation.
Alcohol is metabolized mainly by the group of six enzymes collectively called alcohol dehydrogenase. These convert the ethanol into acetaldehyde (an intermediate more toxic than ethanol). The enzyme acetaldehyde dehydrogenase then converts the acetaldehyde into non-toxic acetic acid.
Many physiologically active materials are removed from the bloodstream (whether by metabolism or excretion) at a rate proportional to the current concentration, so that they exhibit exponential decay with a characteristic halflife (see pharmacokinetics). This is not true for alcohol, however. Typical doses of alcohol actually saturate the enzymes' capacity, so that alcohol is removed from the bloodstream at an approximately constant rate. This rate varies considerably between individuals. Another sex based difference is in the elimination of alcohol. People under 25, women or with liver disease may process alcohol more slowly. False High (BAC) readings are related to patients with proteinuria and hematuria, due to kidney-liver metabolism and failure (for example, Hematuria 1+ protenuria 1+ )
Such persons have impaired acetaldehyde dehydrogenase, which causes acetaldehyde levels to peak higher, producing more severe hangovers and other effects such as flushing and tachycardia. Conversely, members of certain ethnicities that traditionally did not use alcoholic beverages have lower levels of alcohol dehydrogenases and thus "sober up" very slowly, but reach lower aldehyde concentrations and have milder hangovers. Rate of detoxification of alcohol can also be slowed by certain drugs which interfere with the action of alcohol dehydrogenases, notably aspirin, furfural (which may be found in fusel alcohol), fumes of certain solvents, many heavy metals, and some pyrazole compounds. Also suspected of having this effect are cimetidine, ranitidine, and acetaminophen (paracetamol).
Currently, the only known substance that can increase the rate of metabolism of alcohol is fructose. The effect can vary significantly from person to person, but a 100 g dose of fructose has been shown to increase alcohol metabolism by an average of 80%. Fructose also increases false positives of high BAC ratio readings in anyone with proteinuria and hematuria, due to kidney-liver metabolism.
Alcohol absorption can be slowed by ingesting alcohol on a full stomach. The belief that the food absorbs the alcohol is a common misconception. Alcohol absorption is slowed because the stomach sphincter closes in order to break down the food. The alcohol cannot be absorbed through the stomach, thus cannot be absorbed until the sphincter is opened and the consumed alcohol can flow to the small intestine.
There have been reported cases of blood alcohol content higher than 1%:

</doc>
<doc id="4848" url="https://en.wikipedia.org/wiki?curid=4848" title="Barrister">
Barrister

A barrister is a type of lawyer in common law jurisdictions. Barristers mostly specialise in courtroom advocacy and litigation. Their tasks include taking cases in superior courts and tribunals, drafting legal pleadings, researching the philosophy, hypothesis and history of law, and giving expert legal opinions.
Barristers are distinguished from solicitors, who have more direct access to clients, and may do transactional-type legal work. It is mainly barristers who are appointed as judges, and they are rarely hired by clients directly. In some legal systems, including those of Scotland, South Africa, Scandinavia, Pakistan, India, Bangladesh, and the British Crown dependencies of Jersey, Guernsey and the Isle of Man, the word "barrister" is also regarded as an honorific title.
In a few jurisdictions, barristers are usually forbidden from "conducting" litigation, and can only act on the instructions of a solicitor, who performs tasks such as corresponding with parties and the court, and drafting court documents. In England and Wales, barristers may seek authorisation from the Bar Standards Board to conduct litigation. This allows a barrister to practise in a "dual capacity", fulfilling the role of both barrister and solicitor.
In some countries with common law legal systems, such as New Zealand and some regions of Australia, lawyers are entitled to practise both as barristers and solicitors, but it remains a separate system of qualification to practise exclusively as a barrister. Barristers can argue a case in both higher and lower courts.
A barrister, who can be considered a jurist, is a lawyer who represents a litigant as advocate before a court of appropriate jurisdiction. A barrister speaks in court and presents the case before a judge or jury. In some jurisdictions, a barrister receives additional training in evidence law, ethics, and court practice and procedure. In contrast, a solicitor generally meets with clients, does preparatory and administrative work and provides legal advice. In this role, he or she may draft and review legal documents, interact with the client as necessary, prepare evidence, and generally manage the day-to-day administration of a lawsuit. A solicitor can provide a crucial support role to a barrister when in court, such as managing large volumes of documents in the case or even negotiating a settlement outside the courtroom while the trial continues inside.
There are other essential differences. A barrister will usually have rights of audience in the higher courts, whereas other legal professionals will often have more limited access, or will need to acquire additional qualifications to have such access. As in common law countries in which there is a split between the roles of barrister and solicitor, the barrister in civil law jurisdictions is responsible for appearing in trials or pleading cases before the courts.
Barristers usually have particular knowledge of case law, precedent, and the skills to "build" a case. When a solicitor in general practice is confronted with an unusual point of law, they may seek the "opinion of counsel" on the issue.
In most countries, barristers operate as sole practitioners and are prohibited from forming partnerships or from working as a barrister as part of a corporation. (In 2009, the Clementi Report recommended the abolition of this restriction in England and Wales.) However, barristers normally band together into "chambers" to share clerks (administrators) and operating expenses. Some chambers grow to be large and sophisticated and have a distinctly corporate feel. In some jurisdictions, they may be employed by firms of solicitors, banks, or corporations as in-house legal advisers.
In contrast, solicitors and attorneys work directly with the clients and are responsible for engaging a barrister with the appropriate expertise for the case. Barristers generally have little or no direct contact with their "lay clients", particularly without the presence or involvement of the solicitor. All correspondence, inquiries, invoices, and so on, will be addressed to the solicitor, who is primarily responsible for the barrister's fees.
In court, barristers are often visibly distinguished from solicitors by their apparel. For example, in Ireland, England, and Wales, a barrister usually wears a horsehair wig, stiff collar, bands, and a gown. Since January 2008, solicitor advocates have also been entitled to wear wigs, but wear different gowns.
In many countries the traditional divisions between barristers and solicitors are breaking down. Barristers once enjoyed a monopoly on appearances before the higher courts, but in Great Britain this has now been abolished, and solicitor advocates can generally appear for clients at trial. Increasingly, firms of solicitors are keeping even the most advanced advisory and litigation work in-house for economic and client relationship reasons. Similarly, the prohibition on barristers taking instructions directly from the public has also been widely abolished. But, in practice, direct instruction is still a rarity in most jurisdictions, partly because barristers with narrow specializations, or who are only really trained for advocacy, are not prepared to provide general advice to members of the public.
Historically, barristers have had a major role in trial preparation, including drafting pleadings and reviewing evidence. In some areas of law, that is still the case. In other areas, it is relatively common for the barrister to receive the brief from the instructing solicitor to represent a client at trial only a day or two before the proceeding. Part of the reason for this is cost. A barrister is entitled to a "brief fee" when a brief is delivered, and this represents the bulk of her/his fee in relation to any trial. They are then usually entitled to a "refresher" for each day of the trial after the first, but if a case is settled before the trial, the barrister is not needed and the brief fee would be wasted. Some solicitors avoid this by delaying delivery of the brief until it is certain the case will go to trial.
Some benefits of maintaining the split include:
Some disadvantages of the split include:
A detailed examination of the justifications for a split legal profession and of the arguments in favor of a fused profession can be found in English solicitor Peter Reeve's 1986 book, "Are Two Legal Professions Necessary?"
Barristers are regulated by the Bar for the jurisdiction where they practise, and in some countries, by the Inn of Court to which they belong. In some countries, there is external regulation.
Inns of Court, where they exist, regulate admission to the profession. Inns of Court are independent societies that are titularly responsible for the training, admission (calling), and discipline of barristers. Where they exist, a person may only be called to the Bar by an Inn, of which they must first be a member. In fact, historically, call to and success at the Bar, to a large degree, depended upon social connections made early in life.
A Bar collectively describes all members of the profession of barrister within a given jurisdiction. While as a minimum the Bar is an association embracing all its members, it is usually the case, either "de facto" or "de jure", that the Bar is invested with regulatory powers over the manner in which barristers practise.
In the common law tradition, the respective roles of a lawyer – that is as legal adviser and advocate – were formally split into two separate, regulated sub-professions, the other being the office of solicitor. Historically, the distinction was absolute, but in the modern legal age, some countries that had a split legal profession now have a fused profession – anyone entitled to practise as a barrister may also practise as a solicitor, and vice versa. In practice, the distinction may be non-existent, minor, or marked, depending on the jurisdiction. In some jurisdictions, such as Australia, Scotland and Ireland, there is little overlap.
In the Australian states of New South Wales, Victoria and Queensland, there is a split profession. Nevertheless, subject to conditions, barristers can accept direct access work from clients. Each state Bar Association regulates the profession and essentially has the functions of the English Inns of Court. In the states of South Australia and Western Australia, as well as the Australian Capital Territory, the professions of barrister and solicitor are fused, but an independent bar nonetheless exists, regulated by the Legal Practice Board of the state or territory. In Tasmania and the Northern Territory, the profession is fused, although a very small number of practitioners operate as an independent bar.
Generally, counsel dress in the traditional English manner (wig, gown, bar jacket and jabot) before superior courts, although this is not usually done for interlocutory applications. Wigs and robes are still worn in the Supreme Court and the District Court in civil matters and are dependent on the judicial officer's attire. Robes and wigs are worn in all criminal cases. In Western Australia, wigs are no longer worn in any court.
Each year, the Bar Association appoints certain barristers of seniority and eminence to the rank of "Senior Counsel" (in most States and Territories) or "Queen's Counsel" (in the Northern Territory, Queensland, and Victoria). Such barristers carry the title "SC" or "QC" after their name. The appointments are made after a process of consultation with members of the profession and the judiciary. Senior Counsel appear in particularly complex or difficult cases. They make up about 14 per cent of the bar in New South Wales.
In Bangladesh, the law relating to the Barristers is the Bangladesh Legal Practitioners and Bar Council Order, 1972 (President Order No. 46) as amended which is administered and enforced by the Bangladesh Bar Council. Bangladesh Bar Council is the supreme statutory body to regulate the legal professions in Bangladesh and ensure educational standard and regulatory compliance by the Advocates on roll of the Bar Council. The Bar Council, with the help of government, prescribes rules to regulate the profession. All law graduates educating from home or abroad have to write and pass the Bar Council Examination to be enrolled and admitted as professional Advocates to practise law both as Barristers & Solicitors. The newly enrolled advocates are permitted to start practice in the lower (District) courts after admitting as members of the local (District) Bar Associations. After two years of Practice in lower court, the Advocates are eligible to be enrolled in the High Court Division of the Supreme Court of Bangladesh. By passing the Bar Council Examination, the advocates are issued with certificates of enrollment and permission in prescribed form to practise in the High Court Division of the Supreme Court also. Only those advocates who became Barristers in U.K. maintain their honorific title of barristers. In Bangladesh there is an association called Barristers' Association of Bangladesh that represents the such U.K. bases barristers.[10]
In Canada (except Quebec), the professions of barrister and solicitor are fused, and many lawyers refer to themselves with both names, even if they do not practise in both areas. In colloquial parlance within the Canadian legal profession, lawyers often term themselves as "litigators" (or "barristers"), or as "solicitors", depending on the nature of their law practice though some may in effect practise as both litigators and solicitors. However, "litigators" would generally perform all litigation functions traditionally performed by barristers and solicitors; in contrast, those terming themselves "solicitors" would generally limit themselves to legal work not involving practice before the courts (not even in a preparatory manner as performed by solicitors in England), though some might practise before chambers judges. As is the practice in many other Commonwealth jurisdictions such as Australia, Canadian litigators are "gowned", but without a wig, when appearing before courts of "superior jurisdiction". All law graduates from Canadian law schools, and holders of NCA certificates of Qualification (Internationally trained lawyers or graduates from other law schools in common-law jurisdictions outside Canada) from the Federation of Law Societies of Canada after can apply to the relevant Provincial regulating body (law society) for admission (note here that the Canadian Provinces are technically each considered different legal jurisdictions). Prerequisites to admission as a member to a law society involve the completion of a Canadian law degree (or completion of exams to recognize a foreign common law degree), a year of articling as a student supervised by a qualified lawyer, and passing the bar exams mandated by the province the student has applied for a licence in. Once these requirements are complete then the articling student may be "called to the bar" after the review if their application and consideration of any "good character" issues at which they are presented to the Court in a call ceremony. The applicant then becomes a member of the law society as a "barrister and solicitor".
The situation is somewhat different in Quebec as a result of its civil law tradition. The profession of solicitor, or "avoué", never took hold in colonial Quebec, so attorneys ("avocats") have traditionally been a fused profession, arguing and preparing cases in contentious matters, whereas Quebec's other type of lawyer, civil-law notaries ("notaires"), handle out-of-court non-contentious matters. However, a number of areas of non-contentious private law are not monopolized by notaries so that attorneys often specialise in handling either trials, cases, advising, or non-trial matters. The only disadvantage is that attorneys cannot draw up public instruments that have the same force of law as notarial acts. Most large law firms in Quebec offer the full range of legal services of law firms in common-law provinces. Intending Quebec attorneys must earn a bachelor's degree in civil law, pass the provincial bar examination, and successfully complete a legal internship to be admitted to practice. Attorneys are regulated by the Quebec Law Society ("Barreau du Québec").
In France, "avocats", or attorneys, were, until the 20th century, the equivalent of barristers. The profession included several grades ranked by seniority: "avocat-stagiaire" (trainee, who was already qualified but needed to complete two years (or more, depending on the period) of training alongside seasoned lawyers), "avocat", and "avocat honoraire" (senior barrister). Since the 14th century and during the course of the 19th and 20th in particular, French barristers competed in territorial battles over respective areas of legal practice against the "conseil juridique" (legal advisor, transactional solicitor) and "avoué" (procedural solicitor), and expanded to become the generalist legal practitioner, with the notable exception of "notaires" (notaries), who are ministry appointed lawyers (with a separate qualification) and who retain exclusivity over conveyancing and probate. After the 1971 and 1990 legal reforms, the "avocat" was fused with the "avoué" and the "conseil juridique", making the "avocat" (or, if female, "avocate") an all-purpose lawyer for matters of contentious jurisdiction, analogous to an American attorney. French attorneys usually do not (although it they are entitled to) act both as litigators (trial lawyers) and legal consultants (advising lawyers), known respectively as "avocat plaidant" and "avocat-conseil". This distinction is however purely informal and does not correspond to any difference in qualification or admission to the roll. All intending attorneys must pass an examination to be able to enrol in one of the "Centre régional de formation à la profession d'avocat (CRFPA)" (Regional centre for the training of lawyers). The "CRFPA" course has a duration of two years and is a mix between classroom teachings and internships. Its culmination is the "stage final" (final training), where the intending attorney spends 6 months in a law firm (generally in his/her favoured field of practice and in a firm in which he/she hopes to be recruited afterwards). The intending attorney then needs to pass the "Certificat d'Aptitude à la Profession d'Avocat (CAPA)", which is the last professional examination allowing him/her to join a court's bar ("barreau"). It is generally recognised that the first examination is much more difficult than the CAPA and is dreaded by most law students. Each bar is regulated by a Bar Council ("Ordre du barreau").
A separate body of barristers exists called the "avocats au Conseil d'Etat et à la Cour de Cassation". Although their legal background, training and status is the same as the all-purpose avocats, these have a monopoly over litigation taken to the supreme courts, in civil, criminal or administrative matters.
In Germany, no distinction between barristers and solicitors is made. Lawyers may plead at all courts except the civil branch of the Federal Court of Justice ("Bundesgerichtshof"), to which fewer than fifty lawyers are admitted. Those lawyers, who deal almost exclusively with litigation, may not plead at other courts and are usually instructed by a lawyer who represented the client in the lower courts. However, these restrictions do not apply to criminal cases, nor to pleadings at courts of the other court systems, including labour, administrative, taxation, and social courts and the European Union court system.
The legal profession in Hong Kong is also divided into two branches: barristers and solicitors.
In the High Court and the Court of Final Appeal, as a general rule, only barristers and solicitor-advocates are allowed to speak on behalf of any party in open court. This means that solicitors are restricted from doing so. In these two courts, barristers dress in the traditional English manner, as do the judges and other lawyers.
In Hong Kong, the rank of Queen's Counsel was granted prior to the transfer of the sovereignty of Hong Kong from the United Kingdom to China in 1997. After the handover, the rank has been replaced by Senior Counsel post-nominal letters: SC. Senior Counsel may still, however, style themselves as silks, like their British counterparts.
In India, the law relating to the Barrister is the Advocates Act, 1961 introduced and thought up by Ashoke Kumar Sen, the then law minister of India, which is a law passed by the Parliament and is administered and enforced by the Bar Council of India. Under the act, the Bar Council of India is the supreme regulatory body to regulate the legal profession in India and also to ensure the compliance of the laws and maintenance of professional standards by the legal profession in the country. For this purpose, the Bar Council of India is authorized to pass regulations and make orders in individual cases and also generally.
Each State has a Bar Council of its own whose function is to enroll the Barristers willing to practise predominately within the territorial confines of that State and to perform the functions of the Bar Council of India within the territory assigned to them. Therefore, each law degree holder must be enrolled with a (single) State Bar Council to practise in India. However, enrollment with any State Bar Council does not restrict the Barrister from appearing before any court in India, even though it is beyond the territorial jurisdiction of the State Bar Council which he is enrolled in.
The advantage with having the State Bar Councils is that the workload of the Bar Council of India can be divided into these various State Bar Councils and also that matters can be dealt with locally and in an expedited manner. However, for all practical and legal purposes, the Bar Council of India retains with it, the final power to take decisions in any and all matters related to the legal profession on the whole or with respect to any 
The process for being entitled to practise in India is twofold. First, the applicant must be a holder of a law degree from a recognized institution in India (or from one of the four recognised Universities in the United Kingdom) and second, must pass the enrollment qualifications of the Bar Council of the state where he/she seeks to be enrolled. For this purpose, the Bar Council of India has an internal Committee whose function is to supervise and examine the various institutions conferring law degrees and to grant recognition to these institutions once they meet the required standards. In this manner the Bar Council of India also ensures the standard of education required for practising in India is met with. As regards the qualification for enrollment with the State Bar Council, while the actual formalities may vary from one State to another, yet predominately they ensure that the application has not been a bankrupt /criminal and is generally fit to practise before courts of India.
Enrollment with a Bar Council also means that the law degree holder is recognized as a Barrister and is required to maintain a standards of conduct and professional demeanor at all times, both on and off the profession. The Bar Council of India also prescribes "Rules of Conduct" to be observed by the Barristers in the courts, while interacting with clients and even otherwise.
In the Republic of Ireland, admission to the Bar by the Chief Justice of Ireland is restricted to those on whom a Barrister-at-Law degree (BL) has first been conferred. The Honorable Society of King's Inns is the only educational establishment which runs vocational courses for barristers in the Republic and degrees of Barrister-at-Law can only be conferred by King's Inns. King's Inns are also the only body with the capacity to call individuals to the bar and to disbar them.
Most Irish barristers choose to be governed thereafter by the Bar Council of Ireland, a quasi-private entity. Senior members of the profession may be selected for elevation to the Inner Bar, when they may describe themselves as Senior Counsel ("SC"). All barristers who have not been called to the Inner Bar are known as Junior Counsel (and are identified by the postnominal initials "BL"), regardless of age or experience. Admission to the Inner Bar is made by declaration before the Supreme Court, patents of precedence having been granted by the Government. Irish barristers are sole practitioners and may not form chambers or partnerships if they wish to remain members of the Bar Council's Law Library.
To practise under the Bar Council of Ireland's rules, a newly qualified barrister is apprenticed to an experienced barrister of at least seven years' experience. This apprenticeship is known as pupillage or devilling. Devilling is compulsory for those barristers who wish to be members of the Law Library and lasts for one legal year. It is common to devil for a second year in a less formal arrangement but this is not compulsory. Devils are not generally paid for their work in their devilling year.
In Israel, there is no distinction between barristers and solicitors, even though the judicial system is based mostly on English common law, as a continuation of the British Mandate in Palestine.
Japan adopts a unified system. However, there are certain classes of qualified professionals who are allowed to practise in certain limited areas of law, such as scriveners ("shiho shoshi", qualified to handle title registration, deposit, and certain petite court proceedings with additional certification), tax accountants ("zeirishi", qualified to prepare tax returns, provide advice on tax computation and represent a client in administrative tax appeals) and patent agents ("benrishi", qualified to practise patent registration and represent a client in administrative patent appeals). Only the lawyers ("bengoshi") can appear before court and are qualified to practise in any areas of law, including, but not limited to, areas that those qualified law-related professionals above are allowed to practise. Most attorneys still focus primarily on court practice and still a very small number of attorneys give sophisticated and expert legal advice on a day-to-day basis to large corporations.
The Netherlands used to have a semi-separated legal profession comprising the lawyer and the "procureur", the latter resembling, to some extent, the profession of barrister. Under that system, lawyers were entitled to represent their clients in law, but were only able to file cases before the court at which they were registered. Cases falling under the jurisdiction of another court had to be filed by a "procureur" registered at that court, in practice often another lawyer exercising both functions. Questions were raised on the necessity of the separation, given the fact that its main purpose – the preservation of the quality of the legal profession and observance of local court rules and customs – had become obsolete. For that reason, the "procureur" as a separate profession was abolished and its functions merged with the legal profession in 2008. Currently, lawyers can file cases before any court, regardless of where they are registered. The only notable exception concerns civil cases brought before the Supreme Court, which have to be handled by lawyers registered at the Supreme Court, thus gaining from it the title "lawyer at the Supreme Court".
In New Zealand, the professions are not formally fused but practitioners are enrolled in the High Court as "Barristers and Solicitors". They may choose, however, to practise as barristers sole. About 15% practise solely as barristers, mainly in the larger cities and usually in "chambers" (following the British terminology). They receive "instructions" from other practitioners, at least nominally. They usually conduct the proceedings in their entirety.
Any lawyer may apply to become a Queen's Counsel (QC) to recognize long-standing contribution to the legal profession but this status is only conferred on those practising as solicitors in exceptional circumstances. This step, referred to as "being called to the inner bar" or "taking silk", is considered highly prestigious and has been a step in the career of many New Zealand judges.
Unlike other jurisdictions, the term "junior barrister" is popularly used to refer to a lawyer who holds a practising certificate as a barrister, but is employed by another, more senior barrister. Generally, junior barristers are within their first five years of practise and are not yet qualified to practise as barristers sole. Barristers sole (i.e. barristers who are not employed by another barrister) who are not Queen's Counsel are never referred to as junior barristers.
In Nigeria, there is no formal distinction between barristers and solicitors. All students who pass the bar examinations – offered exclusively by the Nigerian Law School – are called to the Nigerian bar, by the Body of Benchers. Lawyers may argue in any Federal trial or appellate court as well as any of the courts in Nigeria's 36 states and the Federal Capital Territory. The Legal Practitioner's Act, refers to Nigerian lawyers as Legal Practitioners, and following their call to the Bar, Nigerian lawyers enter their names in the register or Roll of Legal Practitioners kept at the Supreme Court. Perhaps for this reason, a Nigerian lawyer is also often referred to as a Barrister and Solicitor of the Supreme Court of Nigeria, and many Nigerian lawyers term themselves Barrister-at-Law complete with the postnominal initials "B.L.".
The vast majority of Nigerian lawyers combine contentious and non-contentious work, although there is a growing tendency for practitioners in the bigger practices to specialise in one or the other. In colloquial parlance within the Nigerian legal profession, lawyers may, for this reason, be referred to as "litigators" or as "solicitors".
Consistent with the practice in England and elsewhere in the Commonwealth, senior members of the profession may be selected for elevation to the Inner Bar by conferment of the rank of Senior Advocate of Nigeria (SAN).
The profession in Pakistan is fused; an advocate works both as a barrister and a solicitor, with higher rights of audience being provided. To practice as a barrister in Pakistan, a law graduate must complete three steps: pass the Bar Practice and Training Course (BPTC), be called to the Bar by an Inn of Court, and attain a licence to practice as an advocate in the [courts of Pakistan from the relevant Bar Council, provincial or federal.
In Poland, there are two main types of legal professions: advocate and legal counsel. Both are regulated and these professions are restricted only for people who graduated five-year law studies, have at least three years of experience and passed five difficult national exams (civil law, criminal law, company law, administrative law and ethic) or have a doctor of law degree. Before 2015, the only difference was that advocates have a right to represent clients before the court in all cases and the legal advisors could not represent clients before the court in criminal cases. Presently, the legal advisors can also represent clients in criminal cases so currently, the differences between this professions are only historical significance.
In South Africa the employment and practise of advocates (as barristers are known in South Africa) is consistent with the rest of the Commonwealth. Advocates carry the rank of Junior or Senior Counsel (SC), and are mostly briefed and paid by solicitors (known as attorneys). They are usually employed in the higher courts, particularly in the Appeal Courts where they often appear as specialist counsel. South African solicitors (attorneys) follow a practice of referring cases to Counsel for an opinion before proceeding with a case, when Counsel in question practises as a specialist in the case law at stake. Aspiring advocates currently spend one year in pupillage (formerly only six months) before being admitted to the bar in their respective provincial or judicial jurisdictions. The term "Advocate" is sometimes used in South Africa as a title, e. g. "Advocate John Doe, SC" ("Advokaat" in Afrikaans) in the same fashion as "Dr. John Doe" for a medical doctor.
In South Korea, there is no distinction between the judiciary and lawyers. Previously, a person who has passed the national bar exam after two years of national education is able to become a judge, prosecutor, or "lawyer" in accordance to their grades upon graduation. As a result of changes from implementing an accommodated law school system, there are two standard means of becoming a lawyer. Under the current legal system, to be a judge or a prosecutor, lawyers need to practise their legal knowledge. A "lawyer" does not have any limitation of practice.
Spain has a division but it does not correspond to the division in Britain between barristers/advocates and solicitors. "Procuradores" represent the litigant procedurally in court, generally under the authority of a power of attorney executed by a civil law notary, while "abogados" represent the substantive claims of the litigant through trial advocacy. "Abogados" perform both transactional work and advise in connection with court proceedings, and they have full right of audience in front of the court. The court proceeding is carried out with "abogados", not with procuradores. In a nutshell, procuradores are court agents that operate under the instructions of an "abogado". Their practice is confined to the locality of the court to which they are admitted.
Under EU law, barristers, along with advocates and solicitors, are recognised as lawyers.
Although with somewhat different laws, England and Wales are considered within the United Kingdom a single united and unified legal jurisdiction for the purposes of both civil and criminal law, alongside Scotland and Northern Ireland, the other two legal jurisdictions within the United Kingdom. England and Wales are covered by a common bar (an organisation of barristers) and a single law society (an organisation of solicitors).
The profession of barrister in England and Wales is a separate profession from that of solicitor. It is, however, possible to hold the qualification of both barrister and solicitor at the same time. It is not necessary to leave the bar to qualify as a solicitor.
Barristers are regulated by the Bar Standards Board, a division of the General Council of the Bar. A barrister must be a member of one of the Inns of Court, which traditionally educated and regulated barristers. There are four Inns of Court: The Honourable Society of Lincoln's Inn, The Honourable Society of Gray's Inn, The Honourable Society of the Middle Temple, and The Honourable Society of the Inner Temple. All are situated in central London, near the Royal Courts of Justice. They perform scholastic and social roles, and in all cases, provide financial aid to student barristers (subject to merit) through scholarships. It is the Inns that actually "call" the student to the Bar at a ceremony similar to a graduation. Social functions include dining with other members and guests and hosting other events.
Law graduates wishing to work and be known as barristers must take the Bar Professional Training Course (BPTC – previously Bar Vocational Course or BVC) at one of the institutions authorised by the Bar Council to offer the BPTC. On successful completion of the BPTC student barristers are "called" to the bar by their respective inns and are elevated to the degree of "Barrister". However, before they can practise independently they must first undertake 12 months of pupillage. The first six months of this period is spent shadowing more senior practitioners, after which pupil barristers may begin to undertake some court work of their own. Following successful completion of this stage, most barristers then join a set of Chambers, a group of counsel who share the costs of premises and support staff whilst remaining individually self-employed.
In December 2014 there were just over 15,500 barristers in independent practice, of whom about ten percent are Queen's Counsel and the remainder are junior barristers. Many barristers (about 2,800) are employed in companies as "in-house" counsel, or by local or national government or in academic institutions.
Certain barristers in England and Wales are now instructed directly by members of the public. Members of the public may engage the services of the barrister directly within the framework of the Public Access Scheme; a solicitor is not involved at any stage. Barristers undertaking public access work can provide legal advice and representation in court in almost all areas of law (see the Public Access Information on the Bar Council website) and are entitled to represent clients in any court or tribunal in England and Wales. Once instructions from a client are accepted, it is the barrister (rather than the solicitor) who advises and guides the client through the relevant legal procedure or litigation.
Before a barrister can undertake Public Access work, they must have completed a special course. At present, about one in 20 barristers has so qualified. There is also a separate scheme called "Licensed Access", available to certain nominated classes of professional client; it is not open to the general public. Public access work is experiencing a huge surge at the bar, with barristers taking advantage of the new opportunity for the bar to make profit in the face of legal aid cuts elsewhere in the profession.
The ability of barristers to accept such instructions is a recent development; it results from a change in the rules set down by the General Council of the Bar in July 2004. The Public Access Scheme has been introduced as part of the drive to open up the legal system to the public and to make it easier and cheaper to obtain access to legal advice. It further reduces the distinction between solicitors and barristers. The distinction remains however because there are certain aspects of a solicitor's role that a barrister is not able to undertake.
Some honorific suffixes to signify notable barristers may be "Esquire". Even though the term "barrister-at-law" is sometimes seen, and was once very common, it has never been formally correct in England and Wales. Barrister is the only correct nomenclature.
In April 2003 there were 554 barristers in independent practice in Northern Ireland. 66 were Queen's Counsel (QCs), barristers who have earned a high reputation and are appointed by the Queen on the recommendation of the Lord Chancellor as senior advocates and advisers.
Those barristers who are not QCs are called Junior Counsel and are styled "BL" or "Barrister-at-Law". The term "junior" is often misleading since many members of the Junior Bar are experienced barristers with considerable expertise.
Benchers are, and have been for centuries, the governing bodies of the four Inns of Court in London and King's Inns, Dublin. The Benchers of the Inn of Court of Northern Ireland governed the Inn until the enactment of the Constitution of the Inn in 1983, which provides that the government of the Inn is shared between the Benchers, the Executive Council of the Inn and members of the Inn assembled in General Meeting.
The Executive Council (through its Education Committee) is responsible for considering Memorials submitted by applicants for admission as students of the Inn and by Bar students of the Inn for admission to the degree of Barrister-at-Law and making recommendations to the Benchers. The final decisions on these Memorials are taken by the Benchers. The Benchers also have the exclusive power of expelling or suspending a Bar student and of disbarring a barrister or suspending a barrister from practice.
The Executive Council is also involved with: education; fees of students; calling counsel to the Bar, although call to the Bar is performed by the Lord Chief Justice of Northern Ireland on the invitation of the Benchers; administration of the Bar Library (to which all practising members of the Bar belong); and liaising with corresponding bodies in other countries.
The Bar Council is responsible for the maintenance of the standards, honour and independence of the Bar and, through its Professional Conduct Committee, receives and investigates complaints against members of the Bar in their professional capacity.
In Scotland, an advocate is, in all respects except name, a barrister, but there are significant differences in professional practice.
In Scotland, admission to and the conduct of the profession is regulated by the Faculty of Advocates (as opposed to an Inn).
In the Bailiwick of Jersey, there are solicitors (called "ecrivains") and advocates (French "avocat"). In the Bailiwicks of Jersey and Guernsey and on the Isle of Man, Advocates perform the combined functions of both solicitors and barristers.
Gibraltar is a British Overseas Territory boasting a legal profession based on the common law. The legal profession includes both barristers and solicitors with most barristers also acting as solicitors. Admission and Disciplinary matters in Gibraltar are dealt with by the Bar Council of Gibraltar and the Supreme Court of Gibraltar. In order for barristers or solicitors to be admitted as practising lawyers in Gibraltar they must comply with the Supreme Court Act 1930 as amended by the Supreme Court Amendment Act 2015 which requires, amongst other things, for all newly admitted lawyers as of 1 July 2015 to undertake a year's course in Gibraltar law at the University of Gibraltar. Solicitors also have right of audience in Gibraltar's courts.
The United States does not draw a distinction between lawyers as pleaders (barristers) and lawyers as agents (or solicitors). All lawyers who have passed a bar examination and have been admitted to practice may prosecute or defend in the courts of the state where they are admitted. Historically, a distinction was made, and a separate label for barristers (called "counselors", hence the expression "attorney "and" counselor at law") existed in certain states, though both professions have long since been fused into the all-purpose attorney. Attorneys specializing in court procedure, combining advocacy and case preparation, are called "trial attorneys" or "litigators".
Additionally, some state appellate courts require attorneys to obtain a separate certificate of admission to plead and practise in the appellate court. Federal courts require specific admission to that court's bar to practise before it. At the state appellate level and in Federal courts, there is generally no separate examination process, although some U.S. district courts require an examination on practices and procedures in their specific courts. Unless an examination is required, admission is usually granted as a matter of course to any licensed attorney in the state where the court is located. Some federal courts will grant admission to any attorney licensed in any U.S. jurisdiction.

</doc>
<doc id="4849" url="https://en.wikipedia.org/wiki?curid=4849" title="Battle of Gettysburg">
Battle of Gettysburg

The Battle of Gettysburg () was fought July 1–3, 1863, in and around the town of Gettysburg, Pennsylvania, by Union and Confederate forces during the American Civil War. The battle involved the largest number of casualties of the entire war and is often described as the war's turning point. Union Maj. Gen. George Meade's Army of the Potomac defeated attacks by Confederate Gen. Robert E. Lee's Army of Northern Virginia, halting Lee's invasion of the North.
After his success at Chancellorsville in Virginia in May 1863, Lee led his army through the Shenandoah Valley to begin his second invasion of the North—the Gettysburg Campaign. With his army in high spirits, Lee intended to shift the focus of the summer campaign from war-ravaged northern Virginia and hoped to influence Northern politicians to give up their prosecution of the war by penetrating as far as Harrisburg, Pennsylvania, or even Philadelphia. Prodded by President Abraham Lincoln, Maj. Gen. Joseph Hooker moved his army in pursuit, but was relieved of command just three days before the battle and replaced by Meade.
Elements of the two armies initially collided at Gettysburg on July 1, 1863, as Lee urgently concentrated his forces there, his objective being to engage the Union army and destroy it. Low ridges to the northwest of town were defended initially by a Union cavalry division under Brig. Gen. John Buford, and soon reinforced with two corps of Union infantry. However, two large Confederate corps assaulted them from the northwest and north, collapsing the hastily developed Union lines, sending the defenders retreating through the streets of the town to the hills just to the south.
On the second day of battle, most of both armies had assembled. The Union line was laid out in a defensive formation resembling a fishhook. In the late afternoon of July 2, Lee launched a heavy assault on the Union left flank, and fierce fighting raged at Little Round Top, the Wheatfield, Devil's Den, and the Peach Orchard. On the Union right, Confederate demonstrations escalated into full-scale assaults on Culp's Hill and Cemetery Hill. All across the battlefield, despite significant losses, the Union defenders held their lines.
On the third day of battle, fighting resumed on Culp's Hill, and cavalry battles raged to the east and south, but the main event was a dramatic infantry assault by 12,500 Confederates against the center of the Union line on Cemetery Ridge, known as Pickett's Charge. The charge was repulsed by Union rifle and artillery fire, at great loss to the Confederate army.
Lee led his army on a torturous retreat back to Virginia. Between 46,000 and 51,000 soldiers from both armies were casualties in the three-day battle, the most costly in US history.
On November 19, President Lincoln used the dedication ceremony for the Gettysburg National Cemetery to honor the fallen Union soldiers and redefine the purpose of the war in his historic Gettysburg Address.
Shortly after the Army of Northern Virginia won a major victory over the Army of the Potomac at the Battle of Chancellorsville (April 30 – May 6, 1863), Robert E. Lee decided upon a second invasion of the North (the first was the unsuccessful Maryland campaign of September 1862, which ended in the bloody Battle of Antietam). Such a move would upset the Union's plans for the summer campaigning season and possibly reduce the pressure on the besieged Confederate garrison at Vicksburg. The invasion would allow the Confederates to live off the bounty of the rich Northern farms while giving war-ravaged Virginia a much-needed rest. In addition, Lee's 72,000-man army could threaten Philadelphia, Baltimore, and Washington, and possibly strengthen the growing peace movement in the North.
Thus, on June 3, Lee's army began to shift northward from Fredericksburg, Virginia. Following the death of Thomas J. "Stonewall" Jackson, Lee reorganized his two large corps into three new corps, commanded by Lt. Gen. James Longstreet (First Corps), Lt. Gen. Richard S. Ewell (Second), and Lt. Gen. A.P. Hill (Third); both Ewell and Hill, who had formerly reported to Jackson as division commanders, were new to this level of responsibility. The Cavalry Division remained under the command of Maj. Gen. J.E.B. Stuart.
The Union Army of the Potomac, under Maj. Gen. Joseph Hooker, consisted of seven infantry corps, a cavalry corps, and an Artillery Reserve, for a combined strength of more than 100,000 men.
The first major action of the campaign took place on June 9 between cavalry forces at Brandy Station, near Culpeper, Virginia. The 9,500 Confederate cavalrymen under Stuart were surprised by Maj. Gen. Alfred Pleasonton's combined arms force of two cavalry divisions (8,000 troopers) and 3,000 infantry, but Stuart eventually repulsed the Union attack. The inconclusive battle, the largest predominantly cavalry engagement of the war, proved for the first time that the Union horse soldier was equal to his Southern counterpart.
By mid-June, the Army of Northern Virginia was poised to cross the Potomac River and enter Maryland. After defeating the Union garrisons at Winchester and Martinsburg, Ewell's Second Corps began crossing the river on June 15. Hill's and Longstreet's corps followed on June 24 and 25. Hooker's army pursued, keeping between Washington, D.C. and Lee's army. The Union army crossed the Potomac from June 25 to 27.
Lee gave strict orders for his army to minimize any negative impacts on the civilian population. Food, horses, and other supplies were generally not seized outright, although quartermasters reimbursing Northern farmers and merchants with Confederate money were not well received. Various towns, most notably York, Pennsylvania, were required to pay indemnities in lieu of supplies, under threat of destruction. During the invasion, the Confederates seized some 1,000 northern African Americans. A few of them were escaped fugitive slaves, but most were freemen; all were sent south into slavery under guard.
On June 26, elements of Maj. Gen. Jubal Early's division of Ewell's Corps occupied the town of Gettysburg after chasing off newly raised Pennsylvania militia in a series of minor skirmishes. Early laid the borough under tribute, but did not collect any significant supplies. Soldiers burned several railroad cars and a covered bridge, and destroyed nearby rails and telegraph lines. The following morning, Early departed for adjacent York County.
Meanwhile, in a controversial move, Lee allowed J.E.B. Stuart to take a portion of the army's cavalry and ride around the east flank of the Union army. Lee's orders gave Stuart much latitude, and both generals share the blame for the long absence of Stuart's cavalry, as well as for the failure to assign a more active role to the cavalry left with the army. Stuart and his three best brigades were absent from the army during the crucial phase of the approach to Gettysburg and the first two days of battle. By June 29, Lee's army was strung out in an arc from Chambersburg ( northwest of Gettysburg) to Carlisle ( north of Gettysburg) to near Harrisburg and Wrightsville on the Susquehanna River.
In a dispute over the use of the forces defending the Harpers Ferry garrison, Hooker offered his resignation, and Abraham Lincoln and General-in-Chief Henry W. Halleck, who were looking for an excuse to rid themselves of him, immediately accepted. They replaced Hooker early on the morning of June 28 with Maj. Gen. George Gordon Meade, then commander of the V Corps.
On June 29, when Lee learned that the Army of the Potomac had crossed the Potomac River, he ordered a concentration of his forces around Cashtown, located at the eastern base of South Mountain and west of Gettysburg. On June 30, while part of Hill's Corps was in Cashtown, one of Hill's brigades, North Carolinians under Brig. Gen. J. Johnston Pettigrew, ventured toward Gettysburg. In his memoirs, Maj. Gen. Henry Heth, Pettigrew's division commander, claimed that he sent Pettigrew to search for supplies in town—especially shoes.
When Pettigrew's troops approached Gettysburg on June 30, they noticed Union cavalry under Brig. Gen. John Buford arriving south of town, and Pettigrew returned to Cashtown without engaging them. When Pettigrew told Hill and Heth what he had seen, neither general believed that there was a substantial Union force in or near the town, suspecting that it had been only Pennsylvania militia. Despite General Lee's order to avoid a general engagement until his entire army was concentrated, Hill decided to mount a significant reconnaissance in force the following morning to determine the size and strength of the enemy force in his front. Around 5 a.m. on Wednesday, July 1, two brigades of Heth's division advanced to Gettysburg.
The Army of the Potomac, initially under Maj. Gen. Joseph Hooker (Maj. Gen. George Meade replaced Hooker in command on June 28), consisted of more than 100,000 men in the following organization:
During the advance on Gettysburg, Maj. Gen. Reynolds was in operational command of the left, or advanced, wing of the Army, consisting of the I, III, and XI Corps. Note that many other Union units (not part of the Army of the Potomac) were actively involved in the Gettysburg Campaign, but not directly involved in the Battle of Gettysburg. These included portions of the Union IV Corps, the militia and state troops of the Department of the Susquehanna, and various garrisons, including that at Harpers Ferry.
In reaction to the death of Lt. Gen. Thomas J. "Stonewall" Jackson after Chancellorsville, Lee reorganized his Army of Northern Virginia (75,000 men) from two infantry corps into three.
Anticipating that the Confederates would march on Gettysburg from the west on the morning of July 1, Buford laid out his defenses on three ridges west of the town: Herr Ridge, McPherson Ridge and Seminary Ridge. These were appropriate terrain for a delaying action by his small cavalry division against superior Confederate infantry forces, meant to buy time awaiting the arrival of Union infantrymen who could occupy the strong defensive positions south of town at Cemetery Hill, Cemetery Ridge, and Culp's Hill. Buford understood that if the Confederates could gain control of these heights, Meade's army would have difficulty dislodging them.
Confederate General Henry Heth's division advanced with two brigades forward, commanded by Brig. Gens. James J. Archer and Joseph R. Davis. They proceeded easterly in columns along the Chambersburg Pike. west of town, about 7:30 a.m. on July 1, the two brigades met light resistance from vedettes of Union cavalry, and deployed into line. According to lore, the Union soldier to fire the first shot of the battle was Lt. Marcellus Jones. Lt. Jones later returned to Gettysburg, in 1886 erecting a monument marking the spot where he fired the first shot. Eventually Heth's men encountered dismounted troopers of Col. William Gamble's cavalry brigade. The dismounted troopers resisted stoutly, delaying the Confederate advance by firing their breechloading carbine from behind fences and trees. Still, by 10:20 a.m., the Confederates had pushed the Union cavalrymen east to McPherson Ridge, when the vanguard of the I Corps (Maj. Gen. John F. Reynolds) finally arrived.
North of the pike, Davis gained a temporary success against Brig. Gen. Lysander Cutler's brigade but was repulsed with heavy losses in an action around an unfinished railroad bed cut in the ridge. South of the pike, Archer's brigade assaulted through Herbst (also known as McPherson's) Woods. The Union Iron Brigade under Brig. Gen. Solomon Meredith enjoyed initial success against Archer, capturing several hundred men, including Archer himself.
General Reynolds was shot and killed early in the fighting while directing troop and artillery placements just to the east of the woods. Shelby Foote wrote that the Union cause lost a man considered by many to be "the best general in the army." Maj. Gen. Abner Doubleday assumed command. Fighting in the Chambersburg Pike area lasted until about 12:30 p.m. It resumed around 2:30 p.m., when Heth's entire division engaged, adding the brigades of Pettigrew and Col. John M. Brockenbrough.
As Pettigrew's North Carolina Brigade came on line, they flanked the 19th Indiana and drove the Iron Brigade back. The 26th North Carolina (the largest regiment in the army with 839 men) lost heavily, leaving the first day's fight with around 212 men. By the end of the three-day battle, they had about 152 men standing, the highest casualty percentage for one battle of any regiment, North or South. Slowly the Iron Brigade was pushed out of the woods toward Seminary Ridge. Hill added Maj. Gen. William Dorsey Pender's division to the assault, and the I Corps was driven back through the grounds of the Lutheran Seminary and Gettysburg streets.
As the fighting to the west proceeded, two divisions of Ewell's Second Corps, marching west toward Cashtown in accordance with Lee's order for the army to concentrate in that vicinity, turned south on the Carlisle and Harrisburg roads toward Gettysburg, while the Union XI Corps (Maj. Gen. Oliver O. Howard) raced north on the Baltimore Pike and Taneytown Road. By early afternoon, the Union line ran in a semicircle west, north, and northeast of Gettysburg.
However, the Union did not have enough troops; Cutler, whose brigade was deployed north of the Chambersburg Pike, had his right flank in the air. The leftmost division of the XI Corps was unable to deploy in time to strengthen the line, so Doubleday was forced to throw in reserve brigades to salvage his line.
Around 2 p.m., the Confederate Second Corps divisions of Maj. Gens. Robert E. Rodes and Jubal Early assaulted and out-flanked the Union I and XI Corps positions north and northwest of town. The Confederate brigades of Col. Edward A. O'Neal and Brig. Gen. Alfred Iverson suffered severe losses assaulting the I Corps division of Brig. Gen. John C. Robinson south of Oak Hill. Early's division profited from a blunder by Brig. Gen. Francis C. Barlow, when he advanced his XI Corps division to Blocher's Knoll (directly north of town and now known as Barlow's Knoll); this represented a salient in the corps line, susceptible to attack from multiple sides, and Early's troops overran Barlow's division, which constituted the right flank of the Union Army's position. Barlow was wounded and captured in the attack.
As Union positions collapsed both north and west of town, Gen. Howard ordered a retreat to the high ground south of town at Cemetery Hill, where he had left the division of Brig. Gen. Adolph von Steinwehr in reserve. Maj. Gen. Winfield S. Hancock assumed command of the battlefield, sent by Meade when he heard that Reynolds had been killed. Hancock, commander of the II Corps and Meade's most trusted subordinate, was ordered to take command of the field and to determine whether Gettysburg was an appropriate place for a major battle. Hancock told Howard, "I think this the strongest position by nature upon which to fight a battle that I ever saw." When Howard agreed, Hancock concluded the discussion: "Very well, sir, I select this as the battle-field." Hancock's determination had a morale-boosting effect on the retreating Union soldiers, but he played no direct tactical role on the first day.
General Lee understood the defensive potential to the Union if they held this high ground. He sent orders to Ewell that Cemetery Hill be taken "if practicable." Ewell, who had previously served under Stonewall Jackson, a general well known for issuing peremptory orders, determined such an assault was not practicable and, thus, did not attempt it; this decision is considered by historians to be a great missed opportunity.
The first day at Gettysburg, more significant than simply a prelude to the bloody second and third days, ranks as the 23rd biggest battle of the war by number of troops engaged. About one quarter of Meade's army (22,000 men) and one third of Lee's army (27,000) were engaged.
Throughout the evening of July 1 and morning of July 2, most of the remaining infantry of both armies arrived on the field, including the Union II, III, V, VI, and XII Corps. Two of Longstreet's divisions were on the road: Brig. Gen. George Pickett, had begun the 22 mile (35 km) march from Chambersburg, while Brig. Gen. E. M. Law had begun the march from Guilford. Both arrived late in the morning. Law completed his 28-mile (45 km) march in eleven hours.
The Union line ran from Culp's Hill southeast of the town, northwest to Cemetery Hill just south of town, then south for nearly along Cemetery Ridge, terminating just north of Little Round Top. Most of the XII Corps was on Culp's Hill; the remnants of I and XI Corps defended Cemetery Hill; II Corps covered most of the northern half of Cemetery Ridge; and III Corps was ordered to take up a position to its flank. The shape of the Union line is popularly described as a "fishhook" formation.
The Confederate line paralleled the Union line about a mile (1,600 m) to the west on Seminary Ridge, ran east through the town, then curved southeast to a point opposite Culp's Hill. Thus, the Union army had interior lines, while the Confederate line was nearly long.
Lee's battle plan for July 2 called for a general assault of Meade's positions. On the right, Longstreet's First Corps was to position itself to attack the Union left flank, facing northeast astraddle the Emmitsburg Road, and to roll up the Union line. The attack sequence was to begin with Maj. Gens. John Bell Hood's and Lafayette McLaws's divisions, followed by Maj. Gen. Richard H. Anderson's division of Hill's Third Corps.
On the left, Lee instructed Ewell to position his Second Corps to attack Culp's Hill and Cemetery Hill when he heard the gunfire from Longstreet's assault, preventing Meade from shifting troops to bolster his left. Though it does not appear in either his or Lee's Official Report, Ewell claimed years later that Lee had changed the order to simultaneously attack, calling for only a "diversion", to be turned into a full-scale attack if a favorable opportunity presented itself.
Lee's plan, however, was based on faulty intelligence, exacerbated by Stuart's continued absence from the battlefield. Though Lee personally reconnoitered his left during the morning, he did not visit Longstreet's position on the Confederate right. Even so, Lee rejected suggestions that Longstreet move beyond Meade's left and attack the Union flank, capturing the supply trains and effectively blocking Meade's escape route.
Lee did not issue orders for the attack until 11:00 a.m. About noon, General Anderson's advancing troops were discovered by General Sickles' outpost guard and the Third Corps–upon which Longstreet's First Corps was to form–did not get into position until 1:00 p.m.
Hood and McLaws, after their long march, were not yet in position and did not launch their attacks until just after 4 p.m. and 5 p.m., respectively.
As Longstreet's left division, under Maj. Gen. Lafayette McLaws, advanced, they unexpectedly found Maj. Gen. Daniel Sickles's III Corps directly in their path. Sickles had been dissatisfied with the position assigned him on the southern end of Cemetery Ridge. Seeing ground better suited for artillery positions a half mile (800 m) to the west— centered at the Sherfy farm's Peach Orchard—he violated orders and advanced his corp to the slightly higher ground along the Emmitsburg Road, moving away from Cemetery Ridge. The new line ran from Devil's Den, northwest to the Peach Orchard, then northeast along the Emmitsburg Road to south of the Codori farm. This created an untenable salient at the Peach Orchard; Brig. Gen. Andrew A. Humphreys's division (in position along the Emmitsburg Road) and Maj. Gen. David B. Birney's division (to the south) were subject to attacks from two sides and were spread out over a longer front than their small corps could defend effectively. The Confederate artillery was ordered to open fire at 3:00 p.m. After failing to attend a meeting at this time of Meade's corps commanders, Meade rode to Sickles' position and demanded an explanation of the situation. Knowing a Confederate attack was imminent and a retreat would be endangered, Meade refused Sickles' offer to withdraw.
Meade was forced to send 20,000 reinforcements: the entire V Corps, Brig. Gen. John C. Caldwell's division of the II Corps, most of the XII Corps, and portions of the newly arrived VI Corps. Hood's division moved more to the east than intended, losing its alignment with the Emmitsburg Road, attacking Devil's Den and Little Round Top. McLaws, coming in on Hood's left, drove multiple attacks into the thinly stretched III Corps in the Wheatfield and overwhelmed them in Sherfy's Peach Orchard. McLaws's attack eventually reached Plum Run Valley (the "Valley of Death") before being beaten back by the Pennsylvania Reserves division of the V Corps, moving down from Little Round Top. The III Corps was virtually destroyed as a combat unit in this battle, and Sickles's leg was amputated after it was shattered by a cannonball. Caldwell's division was destroyed piecemeal in the Wheatfield. Anderson's division, coming from McLaws's left and starting forward around 6 p.m., reached the crest of Cemetery Ridge, but could not hold the position in the face of counterattacks from the II Corps, including an almost suicidal bayonet charge by the 1st Minnesota regiment against a Confederate brigade, ordered in desperation by Hancock to buy time for reinforcements to arrive.
As fighting raged in the Wheatfield and Devil's Den, Col. Strong Vincent of V Corps had a precarious hold on Little Round Top, an important hill at the extreme left of the Union line. His brigade of four relatively small regiments was able to resist repeated assaults by Brig. Gen. Evander M. Law's brigade of Hood's division. Meade's chief engineer, Brig. Gen. Gouverneur K. Warren, had realized the importance of this position, and dispatched Vincent's brigade, an artillery battery, and the 140th New York to occupy Little Round Top mere minutes before Hood's troops arrived. The defense of Little Round Top with a bayonet charge by the 20th Maine, ordered by Col. Joshua L. Chamberlain but possibly led by Lt. Holman S. Melcher, was one of the most fabled episodes in the Civil War and propelled Col. Chamberlain into prominence after the war.
Ewell interpreted his orders as calling only for a cannonade. His 32 guns, along with A. P. Hill's 55 guns, engaged in a two-hour artillery barrage at extreme range that had little effect. Finally, about six o'clock, Ewell sent orders to each of his division commanders to attack the Union lines in his front.
Maj. Gen. Edward "Allegheny" Johnson's Division "had not been pushed close to [Culp's Hill] in preparation for an assault, although one had been contemplated all day. It now had a full mile (1,600 m) to advance and Rock Creek had to be crossed. This could only be done at few places and involved much delay. Only three of Johnson's four brigades moved to the attack." Most of the hill's defenders, the Union XII Corps, had been sent to the left to defend against Longstreet's attacks, leaving only a brigade of New Yorkers under Brig. Gen. George S. Greene behind strong, newly constructed defensive works. With reinforcements from the I and XI Corps, Greene's men held off the Confederate attackers, though giving up some of the lower earthworks on the lower part of Culp's Hill.
Early was similarly unprepared when he ordered Harry T. Hays' and Isaac E. Avery's Brigades to attack the Union XI Corps positions on East Cemetery Hill. Once started, fighting was fierce: Col. Andrew L. Harris of the Union 2nd Brigade, 1st Division, came under a withering attack, losing half his men. Avery was wounded early on, but the Confederates reached the crest of the hill and entered the Union breastworks, capturing one or two batteries. Seeing he was not supported on his right, Hays withdrew. His right was to be supported by Robert E. Rodes' Division, but Rodes—like Early and Johnson—had not been ordered up in preparation for the attack. He had twice as far to travel as Early; by the time he came in contact with the Union skirmish line, Early's troops had already begun to withdraw.
Jeb Stuart and his three cavalry brigades arrived in Gettysburg around noon but had no role in the second day's battle. Brig. Gen. Wade Hampton's brigade fought a minor engagement with newly promoted 23-year-old Brig. Gen. George Armstrong Custer's Michigan cavalry near Hunterstown to the northeast of Gettysburg.
General Lee wished to renew the attack on Friday, July 3, using the same basic plan as the previous day: Longstreet would attack the Union left, while Ewell attacked Culp's Hill. However, before Longstreet was ready, Union XII Corps troops started a dawn artillery bombardment against the Confederates on Culp's Hill in an effort to regain a portion of their lost works. The Confederates attacked, and the second fight for Culp's Hill ended around 11 a.m. Harry Pfanz judged that, after some seven hours of bitter combat, "the Union line was intact and held more strongly than before."
Lee was forced to change his plans. Longstreet would command Pickett's Virginia division of his own First Corps, plus six brigades from Hill's Corps, in an attack on the Union II Corps position at the right center of the Union line on Cemetery Ridge. Prior to the attack, all the artillery the Confederacy could bring to bear on the Union positions would bombard and weaken the enemy's line.
Much has been made over the years of General Longstreet's objections to General Lee's plan. In his memoirs, Longstreet described their discussion as follows:
Around 1 p.m., from 150 to 170 Confederate guns began an artillery bombardment that was probably the largest of the war. In order to save valuable ammunition for the infantry attack that they knew would follow, the Army of the Potomac's artillery, under the command of Brig. Gen. Henry Jackson Hunt, at first did not return the enemy's fire. After waiting about 15 minutes, about 80 Union cannons added to the din. The Army of Northern Virginia was critically low on artillery ammunition, and the cannonade did not significantly affect the Union position.
Around 3 p.m., the cannon fire subsided, and 12,500 Southern soldiers stepped from the ridgeline and advanced the three-quarters of a mile (1,200 m) to Cemetery Ridge in what is known to history as "Pickett's Charge". As the Confederates approached, there was fierce flanking artillery fire from Union positions on Cemetery Hill and north of Little Round Top, and musket and canister fire from Hancock's II Corps. In the Union center, the commander of artillery had held fire during the Confederate bombardment (in order to save it for the infantry assault, which Meade had correctly predicted the day before), leading Southern commanders to believe the Northern cannon batteries had been knocked out. However, they opened fire on the Confederate infantry during their approach with devastating results. Nearly one half of the attackers did not return to their own lines.
Although the Union line wavered and broke temporarily at a jog called the "Angle" in a low stone fence, just north of a patch of vegetation called the Copse of Trees, reinforcements rushed into the breach, and the Confederate attack was repulsed. The farthest advance of Brig. Gen. Lewis A. Armistead's brigade of Maj. Gen. George Pickett's division at the Angle is referred to as the "High-water mark of the Confederacy". Union and Confederate soldiers locked in hand-to-hand combat, attacking with their rifles, bayonets, rocks and even their bare hands. Armistead ordered his Confederates to turn two captured cannons against Union troops, but discovered that there was no ammunition left, the last double canister shots having been used against the charging Confederates. Armistead was wounded shortly afterward three times.
There were two significant cavalry engagements on July 3. Stuart was sent to guard the Confederate left flank and was to be prepared to exploit any success the infantry might achieve on Cemetery Hill by flanking the Union right and hitting their trains and lines of communications. east of Gettysburg, in what is now called "East Cavalry Field" (not shown on the accompanying map, but between the York and Hanover Roads), Stuart's forces collided with Union cavalry: Brig. Gen. David McMurtrie Gregg's division and Brig. Gen. Custer's brigade. A lengthy mounted battle, including hand-to-hand sabre combat, ensued. Custer's charge, leading the 1st Michigan Cavalry, blunted the attack by Wade Hampton's brigade, blocking Stuart from achieving his objectives in the Union rear.
Meanwhile, after hearing news of the day's victory, Brig. Gen. Judson Kilpatrick launched a cavalry attack against the infantry positions of Longstreet's Corps southwest of Big Round Top. Brig. Gen. Elon J. Farnsworth protested against the futility of such a move, but obeyed orders. Farnsworth was killed in the attack, and his brigade suffered significant losses.
The two armies suffered between 46,000 and 51,000 casualties, nearly one third of all total troops engaged, 28% of the Army of the Potomac and 37% of the Army of Northern Virginia. Union casualties were 23,055 (3,155 killed, 14,531 wounded, 5,369 captured or missing), while Confederate casualties are more difficult to estimate. Many authors have referred to as many as 28,000 Confederate casualties, and Busey and Martin's more recent 2005 work, "Regimental Strengths and Losses at Gettysburg", documents 23,231 (4,708 killed, 12,693 wounded, 5,830 captured or missing). Nearly a third of Lee's general officers were killed, wounded, or captured. The casualties for both sides during the entire campaign were 57,225.
In addition to being the deadliest battle of the war, Gettysburg also had the highest number of generals killed in action. The Confederacy lost generals Paul Jones Semmes, William Barksdale, William Dorsey Pender, Richard Garnett, and Lewis Armistead, as well as J. Johnston Pettigrew during the retreat after the battle. The Union lost Generals John Reynolds, Samuel K. Zook, Stephen H. Weed, and Elon J. Farnsworth, as well as Strong Vincent, who after being mortally wounded was given a deathbed promotion to brigadier general. Additional senior officer casualties included the wounding of Union Generals Dan Sickles (lost a leg), Francis C. Barlow, Daniel Butterfield, and Winfield Scott Hancock. For the Confederacy, Major General John Bell Hood lost the use of his left arm, while Major General Henry Heth received a shot to the head on the first day of battle (though incapacitated for the rest of the battle, he remarkably survived without long-term injuries, credited in part due to his hat stuffed full of paper dispatches). Confederate Generals James L. Kemper and Isaac R. Trimble were severely wounded during Pickett's charge and captured during the Confederate retreat. General James J. Archer, in command of a brigade that most likely was responsible for killing Reynolds, was taken prisoner shortly after Reynolds' death.
The following tables summarize casualties by corps for the Union and Confederate forces during the three-day battle.
Bruce Catton wrote, "The town of Gettysburg looked as if some universal moving day had been interrupted by catastrophe." But there was only one documented civilian death during the battle: Ginnie Wade (also widely known as Jennie), 20 years old, was hit by a stray bullet that passed through her kitchen in town while she was making bread. Another notable civilian casualty was John L. Burns, a 69-year old veteran of the War of 1812 who walked to the front lines on the first day of battle and participated in heavy combat as a volunteer, receiving numerous wounds in the process. Despite his age and injuries, Burns survived the battle and lived until 1872. Nearly 8,000 had been killed outright; these bodies, lying in the hot summer sun, needed to be buried quickly. Over 3,000 horse carcasses were burned in a series of piles south of town; townsfolk became violently ill from the stench. Meanwhile, the town of Gettysburg, with its population of just 2,400, found itself tasked with taking care of 14,000 wounded Union troops and an additional 8,000 Confederate prisoners.
The armies stared at one another in a heavy rain across the bloody fields on July 4, the same day that, some 900 miles (1,500 km) away, the Vicksburg garrison surrendered to Maj. Gen. Ulysses S. Grant. Lee had reformed his lines into a defensive position on Seminary Ridge the night of July 3, evacuating the town of Gettysburg. The Confederates remained on the battlefield, hoping that Meade would attack, but the cautious Union commander decided against the risk, a decision for which he would later be criticized. Both armies began to collect their remaining wounded and bury some of the dead. A proposal by Lee for a prisoner exchange was rejected by Meade.
Lee started his Army of Northern Virginia in motion late the evening of July 4 towards Fairfield and Chambersburg. Cavalry under Brig. Gen. John D. Imboden was entrusted to escort the miles-long wagon train of supplies and wounded men that Lee wanted to take back to Virginia with him, using the route through Cashtown and Hagerstown to Williamsport, Maryland. Meade's army followed, although the pursuit was half-spirited. The recently rain-swollen Potomac trapped Lee's army on the north bank of the river for a time, but when the Union troops finally caught up, the Confederates had forded the river. The rear-guard action at Falling Waters on July 14 added some more names to the long casualty lists, including General Pettigrew, who was mortally wounded. General James L. Kemper, severely wounded during Pickett's charge, was captured during Lee's retreat.
In a brief letter to Maj. Gen. Henry W. Halleck written on July 7, Lincoln remarked on the two major Union victories at Gettysburg and Vicksburg. He continued:
Halleck then relayed the contents of Lincoln's letter to Meade in a telegram. However, the Army of the Potomac was exhausted by days of fighting and heavy losses. Furthermore, Meade was forced to detach 4,000 troops North to suppress the New York City Draft Riots, further reducing the effectiveness of his pursuit. Despite repeated pleas from Lincoln and Halleck, which continued over the next week, Meade did not pursue Lee's army aggressively enough to destroy it before it crossed back over the Potomac River to safety in the South. The campaign continued into Virginia with light engagements until July 23, in the minor Battle of Manassas Gap, after which Meade abandoned any attempts at pursuit and the two armies took up positions across from each other on the Rappahannock River.
The news of the Union victory electrified the North. A headline in "The Philadelphia Inquirer" proclaimed "VICTORY! WATERLOO ECLIPSED!" New York diarist George Templeton Strong wrote:
However, the Union enthusiasm soon dissipated as the public realized that Lee's army had escaped destruction and the war would continue. Lincoln complained to Secretary of the Navy Gideon Welles that "Our army held the war in the hollow of their hand and they would not close it!" Brig. Gen. Alexander S. Webb wrote to his father on July 17, stating that such Washington politicians as "Chase, Seward and others," disgusted with Meade, "write to me that Lee really won that Battle!"
In fact, the Confederates had lost militarily and also politically. During the final hours of the battle, Confederate Vice President Alexander Stephens was approaching the Union lines at Norfolk, Virginia, under a flag of truce. Although his formal instructions from Confederate President Jefferson Davis had limited his powers to negotiate on prisoner exchanges and other procedural matters, historian James M. McPherson speculates that he had informal goals of presenting peace overtures. Davis had hoped that Stephens would reach Washington from the south while Lee's victorious army was marching toward it from the north. President Lincoln, upon hearing of the Gettysburg results, refused Stephens's request to pass through the lines. Furthermore, when the news reached London, any lingering hopes of European recognition of the Confederacy were finally abandoned. Henry Adams, whose father was serving as the U.S ambassador to the United Kingdom at the time, wrote, "The disasters of the rebels are unredeemed by even any hope of success. It is now conceded that all idea of intervention is at an end."
Compounding the effects of the defeat would be the end of the Siege of Vicksburg, which surrendered to Grant's Federal armies in the West on July 4, the day after the Gettysburg battle.
The immediate reaction of the Southern military and public sectors was that Gettysburg was a setback, not a disaster. The sentiment was that Lee had been successful on July 1 and had fought a valiant battle on July 2–3, but could not dislodge the Union Army from the strong defensive position to which it fled. The Confederates successfully stood their ground on July 4 and withdrew only after they realized Meade would not attack them. The withdrawal to the Potomac that could have been a disaster was handled masterfully. Furthermore, the Army of the Potomac had been kept away from Virginia farmlands for the summer and all predicted that Meade would be too timid to threaten them for the rest of the year. Lee himself had a positive view of the campaign, writing to his wife that the army had returned "rather sooner than I had originally contemplated, but having accomplished what I proposed on leaving the Rappahannock, viz., relieving the Valley of the presence of the enemy and drawing his Army north of the Potomac." He was quoted as saying to Maj. John Seddon, brother of the Confederate secretary of war, "Sir, we did whip them at Gettysburg, and it will be seen for the next six months that "that army" will be as quiet as a sucking dove." Some Southern publications, such as the "Charleston Mercury", were critical of Lee's actions. On August 8, Lee offered his resignation to President Davis, who quickly rejected it.
Gettysburg became a postbellum focus of the "Lost Cause", a movement by writers such as Edward A. Pollard and Jubal Early to explain the reasons for the Confederate defeat in the war. A fundamental premise of their argument was that the South was doomed because of the overwhelming advantage in manpower and industrial might possessed by the North. They also contend that Robert E. Lee, who up until this time had been almost invincible, was betrayed by the failures of some of his key subordinates at Gettysburg: Ewell, for failing to seize Cemetery Hill on July 1; Stuart, for depriving the army of cavalry intelligence for a key part of the campaign; and especially Longstreet, for failing to attack on July 2 as early and as forcefully as Lee had originally intended. In this view, Gettysburg was seen as a great lost opportunity, in which a decisive victory by Lee could have meant the end of the war in the Confederacy's favor.
After the war, General Pickett was asked why Confederates lost at Gettysburg. He was reported to have said, "I always thought the Yankees had something to do with it."
The ravages of war were still evident in Gettysburg more than four months later when, on November 19, the Soldiers' National Cemetery was dedicated. During this ceremony, President Abraham Lincoln honored the fallen and redefined the purpose of the war in his historic Gettysburg Address.
There were 72 Medals of Honor awarded for the Gettysburg Campaign. 64 of the awards were for actions taken during the battle itself, with the first recipient being awarded in December 1864. The last Medal of Honor was posthumously awarded to Lieutenant Alonzo Cushing in 2014.
The nature of the result of the Battle of Gettysburg has been the subject of controversy. Although not seen as overwhelmingly significant at the time, particularly since the war continued for almost two years, in retrospect it has often been cited as the "turning point", usually in combination with the fall of Vicksburg the following day. This is based on the observation that, after Gettysburg, Lee's army conducted no more strategic offensives—his army merely reacted to the initiative of Ulysses S. Grant in 1864 and 1865—and by the speculative viewpoint of the Lost Cause writers that a Confederate victory at Gettysburg might have resulted in the end of the war.
It is currently a widely held view that Gettysburg was a decisive victory for the Union, but the term is considered imprecise. It is inarguable that Lee's offensive on July 3 was turned back decisively and his campaign in Pennsylvania was terminated prematurely (although the Confederates at the time argued that this was a temporary setback and that the goals of the campaign were largely met). However, when the more common definition of "decisive victory" is intended—an indisputable military victory of a battle that determines or significantly influences the ultimate result of a conflict—historians are divided. For example, David J. Eicher called Gettysburg a "strategic loss for the Confederacy" and James M. McPherson wrote that "Lee and his men would go on to earn further laurels. But they never again possessed the power and reputation they carried into Pennsylvania those palmy summer days of 1863."
However, Herman Hattaway and Archer Jones wrote that the "strategic impact of the Battle of Gettysburg was ... fairly limited." Steven E. Woodworth wrote that "Gettysburg proved only the near impossibility of decisive action in the Eastern theater." Edwin Coddington pointed out the heavy toll on the Army of the Potomac and that "after the battle Meade no longer possessed a truly effective instrument for the accomplishments of his task. The army needed a thorough reorganization with new commanders and fresh troops, but these changes were not made until Grant appeared on the scene in March 1864." Joseph T. Glatthaar wrote that "Lost opportunities and near successes plagued the Army of Northern Virginia during its Northern invasion," yet after Gettysburg, "without the distractions of duty as an invading force, without the breakdown of discipline, the Army of Northern Virginia [remained] an extremely formidable force." Ed Bearss wrote, "Lee's invasion of the North had been a costly failure. Nevertheless, at best the Army of the Potomac had simply preserved the strategic stalemate in the Eastern Theater ..." Furthermore, the Confederacy soon proved it was still capable of winning significant victories over the Northern forces in both the East (Battle of Cold Harbor) and West (Battle of Chickamauga).
Peter Carmichael refers to the military context for the armies, the "horrendous losses at Chancellorsville and Gettysburg, which effectively destroyed Lee's offensive capacity," implying that these cumulative losses were not the result of a single battle. Thomas Goss, writing in the U.S. Army's "Military Review" journal on the definition of "decisive" and the application of that description to Gettysburg, concludes: "For all that was decided and accomplished, the Battle of Gettysburg fails to earn the label 'decisive battle'." The military historian John Keegan agrees. Gettysburg was a landmark battle, the largest of the war and it would not be surpassed. The Union had restored to it the belief in certain victory, and the loss dispirited the Confederacy. If "not exactly a decisive battle", Gettysburg was the end of Confederate use of Northern Virginia as a military buffer zone, the setting for Grant's Overland Campaign.
Prior to Gettysburg, Robert E. Lee had established a reputation as an almost invincible general, achieving stunning victories against superior numbers—although usually at the cost of high casualties to his army—during the Seven Days, the Northern Virginia Campaign (including the Second Battle of Bull Run), Fredericksburg, and Chancellorsville. Only the Maryland Campaign, with its tactically inconclusive Battle of Antietam, had been less than successful. Therefore, historians have attempted to explain how Lee's winning streak was interrupted so dramatically at Gettysburg. Although the issue is tainted by attempts to portray history and Lee's reputation in a manner supporting different partisan goals, the major factors in Lee's loss arguably can be attributed to: (1) his overconfidence in the invincibility of his men; (2) the performance of his subordinates, and his management thereof; (3) his failing health, and (4) the performance of his opponent, George G. Meade, and the Army of the Potomac.
Throughout the campaign, Lee was influenced by the belief that his men were invincible; most of Lee's experiences with the Army of Northern Virginia had convinced him of this, including the great victory at Chancellorsville in early May and the rout of the Union troops at Gettysburg on July 1. Since morale plays an important role in military victory when other factors are equal, Lee did not want to dampen his army's desire to fight and resisted suggestions, principally by Longstreet, to withdraw from the recently captured Gettysburg to select a ground more favorable to his army. War correspondent Peter W. Alexander wrote that Lee "acted, probably, under the impression that his troops were able to carry any position however formidable. If such was the case, he committed an error, such however as the ablest commanders will sometimes fall into." Lee himself concurred with this judgment, writing to President Davis, "No blame can be attached to the army for its failure to accomplish what was projected by me, nor should it be censured for the unreasonable expectations of the public—I am alone to blame, in perhaps expecting too much of its prowess and valor."
The most controversial assessments of the battle involve the performance of Lee's subordinates. The dominant theme of the Lost Cause writers and many other historians is that Lee's senior generals failed him in crucial ways, directly causing the loss of the battle; the alternative viewpoint is that Lee did not manage his subordinates adequately, and did not thereby compensate for their shortcomings. Two of his corps commanders—Richard S. Ewell and A.P. Hill—had only recently been promoted and were not fully accustomed to Lee's style of command, in which he provided only general objectives and guidance to their former commander, Stonewall Jackson; Jackson translated these into detailed, specific orders to his division commanders. All four of Lee's principal commanders received criticism during the campaign and battle:
In addition to Hill's illness, Lee's performance was affected by heart troubles, which would eventually lead to his death in 1870; he had been diagnosed with pericarditis by his staff physicians in March 1863, though modern doctors believe he had in fact suffered a heart attack. He wrote to Jefferson Davis that his physical condition prevented him from offering full supervision in the field, and said, "I am so dull that in making use of the eyes of others I am frequently misled."
As a final factor, Lee faced a new and formidable opponent in George G. Meade, and the Army of the Potomac fought well on its home territory. Although new to his army command, Meade deployed his forces relatively effectively; relied on strong subordinates such as Winfield S. Hancock to make decisions where and when they were needed; took great advantage of defensive positions; nimbly shifted defensive resources on interior lines to parry strong threats; and, unlike some of his predecessors, stood his ground throughout the battle in the face of fierce Confederate attacks.
Lee was quoted before the battle as saying Meade "would commit no blunders on my front and if I make one ... will make haste to take advantage of it." That prediction proved to be correct at Gettysburg. Stephen Sears wrote, "The fact of the matter is that George G. Meade, unexpectedly and against all odds, thoroughly outgeneraled Robert E. Lee at Gettysburg." Edwin B. Coddington wrote that the soldiers of the Army of the Potomac received a "sense of triumph which grew into an imperishable faith in [themselves]. The men knew what they could do under an extremely competent general; one of lesser ability and courage could well have lost the battle."
Meade had his own detractors as well. Similar to the situation with Lee, Meade suffered partisan attacks about his performance at Gettysburg, but he had the misfortune of experiencing them in person. Supporters of his predecessor, Maj. Gen. Joseph Hooker, lambasted Meade before the U.S. Congress's Joint Committee on the Conduct of the War, where Radical Republicans suspected that Meade was a Copperhead and tried in vain to relieve him from command. Daniel E. Sickles and Daniel Butterfield accused Meade of planning to retreat from Gettysburg during the battle. Most politicians, including Lincoln, criticized Meade for what they considered to be his half-hearted pursuit of Lee after the battle. A number of Meade's most competent subordinates—Winfield S. Hancock, John Gibbon, Gouverneur K. Warren, and Henry J. Hunt, all heroes of the battle—defended Meade in print, but Meade was embittered by the overall experience.
Today, the Gettysburg National Cemetery and Gettysburg National Military Park are maintained by the U.S. National Park Service as two of the nation's most revered historical landmarks. Although Gettysburg is one of the best known of all Civil War battlefields, it too faces threats to its preservation and interpretation. Many historically significant locations on the battlefield lie outside the boundaries of Gettysburg National Military Park and are vulnerable to residential or commercial development.
On July 20, 2009, a Comfort Inn and Suites opened on Cemetery Hill, adjacent to Evergreen Cemetery, just one of many modern edifices infringing on the historic field. The Baltimore Pike corridor attracts development that concerns preservationists.
Some preservation successes have emerged in recent years. Two proposals to open a casino at Gettysburg were defeated in 2006 and most recently in 2011, when public pressure forced the Pennsylvania Gaming Control Board to reject the proposed gambling hub at the intersection of Routes 15 and 30, near East Cavalry Field. The Civil War Trust also successfully purchased and transferred 95 acres at the former site of the Gettysburg Country Club to the control of the U.S. Department of the Interior in 2011.
Less than half of the over 11,500 acres on the old Gettysburg Battlefield have been preserved for posterity thus far. The Civil War Trust (a division of the American Battlefield Trust) and its partners have acquired and preserved of the battlefield in more than 30 separate transactions since 1997. Some of these acres are now among the 4,998 acres of the Gettysburg National Military Park. In 2015, the Trust made one of its most important and expensive acquisitions, paying $6 million for a four-acre parcel that included the stone house that Confederate Gen. Robert E. Lee used as his headquarters during the battle. The Trust razed a motel, restaurant and other buildings within the parcel to restore Lee's Headquarters and the site to their wartime appearance, adding interpretive signs. It opened the site to the public in October 2016.
During the Civil War Centennial, the U.S. Post Office issued five postage stamps commemorating the 100th anniversaries of famous battles, as they occurred over a four-year period, beginning with the Battle of Fort Sumter Centennial issue of 1961. The Battle of Shiloh commemorative stamp was issued in 1962, the Battle of Gettysburg in 1963, the Battle of the Wilderness in 1964, and the Appomattox Centennial commemorative stamp in 1965.
A commemorative half dollar for the battle was produced in 1936. As was typical for the period, mintage for the coin was very low, just 26,928. On January 24, 2011, the America the Beautiful quarters released a 25-cent coin commemorating Gettysburg National Military Park and the Battle of Gettysburg. The reverse side of the coin depicts the monument on Cemetery Ridge to the 72nd Pennsylvania Infantry.
Film records survive of two Gettysburg reunions, held on the battlefield. At the 50th anniversary (1913), veterans re-enacted Pickett's Charge in a spirit of reconciliation, a meeting that carried great emotional force for both sides. At the 75th anniversary (1938), 2500 veterans attended, and there was a ceremonial mass hand-shake across a stone wall. This was recorded on sound film, and some Confederates can be heard giving the Rebel Yell.
The Battle of Gettysburg was depicted in the 1993 film "Gettysburg", based on Michael Shaara's 1974 novel "The Killer Angels". The film and novel focused primarily on the actions of Joshua Lawrence Chamberlain, John Buford, Robert E. Lee, and James Longstreet during the battle. The first day focused on Buford's cavalry defense, the second day on Chamberlain's defense at Little Round Top, and the third day on Pickett's Charge.
The south winning the Battle of Gettysburg is a popular premise for a point of divergence in American Civil War alternate histories. Here are some examples which either depict or make significant reference to an alternate Battle of Gettysburg (sometimes simply inserting fantasy or sci-fi elements in an account of the battle):

</doc>
<doc id="4851" url="https://en.wikipedia.org/wiki?curid=4851" title="Budweiser">
Budweiser

Budweiser () is an American-style pale lager produced by Anheuser-Busch, currently part of the transnational corporation AB InBev and produced in various breweries around the world. There is an unrelated lager also called Budweiser, originating centuries ago in České Budějovice, Czechia (historically known as "Budweis"); the existence of the two separate beers has given rise to a trademark dispute preventing Anheuser-Busch from using the "Budweiser" name in some regions, leading to the exclusive use of "Bud" in those markets.
Introduced in 1876 by Carl Conrad & Co. of St. Louis, Missouri, Budweiser has become one of the largest-selling beers in the United States. The lager is available in over 80 countries, though not under the Budweiser name where Anheuser-Busch does not own the trademark. Budweiser is a filtered beer, available on draft and in bottles and cans, made (unlike the Czech lager) with up to 30% rice in addition to the hops and barley malt used by all lagers.
The name "Budweiser" is a German derivative adjective, meaning "of Budweis".
Beer has been brewed in "Budweis" (now České Budějovice, Czech Republic) since it was founded in 1245. In 1876, German-born Adolphus Busch and his friend Carl Conrad developed a "Bohemian-style" lager in the United States, inspired after a trip to Bohemia and produced it in their brewery in St. Louis, Missouri.
Anheuser–Busch has been involved in a trademark dispute with the Budweiser Budvar Brewery of České Budějovice over the trademark rights to the name "Budweiser".
In the European Union, excluding the Republic of Ireland, Sweden, Finland and Spain, the American beer is marketed as "Bud", as the Budweiser trademark name is owned solely by the Czech beer maker, Budweiser Budvar. In some countries both the Budvar and Anheuser–Busch lagers are available under the Budweiser name.
In 2008, Anheuser-Busch had a market share in the United States of 50.9% for all beers sold. Budweiser brands account for about half of Anheuser-Busch's sales volume, a figure which has been steadily declining at –2% per year.
Anheuser-Busch advertises the Budweiser brand heavily, expending $449 million in 2012 in the United States. This made it the most advertised drink brand in America, and accounted for a third of the company's US marketing budget.
The Budweiser from Budějovice has been called "The Beer of Kings" since the 16th century. Adolphus Busch adapted this slogan to "The King of Beers." This history notwithstanding, Anheuser Busch owns the trademark to these slogans in the United States.
From 1987 to 1989, Bud Light ran an advertising campaign centered around a canine mascot, Spuds MacKenzie.
In 2010, the Bud Light brand paid $1 billion for a six-year licensing agreement with the NFL. Budweiser pays $20 million annually for MLB licensing rights.
Budweiser has produced a number of TV advertisements, such as the Budweiser Frogs, lizards impersonating the Budweiser frogs, a campaign built around the phrase "Whassup?", and a team of Clydesdale horses commonly known as the Budweiser Clydesdales.
Budweiser also advertises in motorsports, from Bernie Little's Miss Budweiser hydroplane boat to sponsorship of the Budweiser King Top Fuel Dragster driven by Brandon Bernstein. Anheuser-Busch has sponsored the CART championship. It is the "Official Beer of NHRA" and it was the "Official Beer of NASCAR" from 1998 to 2007. It has sponsored motorsport events such as the Daytona Speedweeks, Budweiser Shootout, Budweiser Duel, Budweiser Pole Award, Budweiser 500, Budweiser 400, Budweiser 300, Budweiser 250, Budweiser 200, and Carolina Pride / Budweiser 200. However, starting in 2016, the focus of A-B's NASCAR sponsorship became its Busch brand.
Budweiser has been sponsor of NASCAR teams such as Junior Johnson, Hendrick Motorsports, DEI, and Stewart-Haas Racing. Sponsored drivers include Dale Earnhardt, Jr. (1999–2007), Kasey Kahne (2008–2010), and Kevin Harvick (2011–2015). In IndyCar, Budweiser sponsored Mario Andretti (1983–1984), Bobby Rahal (1985–1988), Scott Pruett (1989–1992), Roberto Guerrero (1993), Scott Goodyear (1994), Paul Tracy (1995), Christian Fittipaldi (1996–1997), and Richie Hearn (1998–1999).
Between 2003 and 2006, Budweiser was a sponsor of the BMW Williams Formula One team.
Anheuser-Busch has placed Budweiser as an official partner and sponsor of Major League Soccer and Los Angeles Galaxy and was the headline sponsor of the British Basketball League in the 1990s. Anheuser-Busch has also placed Budweiser as an official sponsor of the Premier League and the presenting sponsor of the FA Cup.
In the early 20th century, the company commissioned a play-on-words song called "Under the Anheuser Bush," which was recorded by several early phonograph companies.
In 2009, Anheuser-Busch partnered with popular Chinese video-sharing site, Tudou.com for a user-generated online video contest. The contest encourages users to suggest ideas that include ants for a Bud TV spot set to run in February 2010 during the Chinese New Year.
In 2010, Budweiser produced an online reality TV series, called "Bud House," centered around the 2010 FIFA World Cup in South Africa, following the lives of 32 international football fans (one representing each nation in the World Cup) living together in a house in South Africa.
On November 5, 2012, Anheuser-Busch asked Paramount Pictures to obscure or remove the Budweiser logo from the film "Flight" (2012), directed by Robert Zemeckis and starring Denzel Washington.
In an advertisement titled "Brewed the Hard Way" that aired during Super Bowl XLIX, Budweiser touted itself as "Proudly A Macro Beer", distinguishing it from smaller production craft beers.
In 2016, Beer Park by Budweiser opened on the Las Vegas Strip.
In October 7, 2016, the Budweiser Clydesdales made a special appearance on the Danforth Campus at Washington University in St. Louis ahead of the presidential debate. A special batch beer named Lilly's Lager was exclusively brewed for the occasion.
Over the years, Budweiser has been distributed in many sizes and containers. Until the early 1950s Budweiser was primarily distributed in three packages: kegs, bottles and bottles. Cans were first introduced in 1936, which helped sales to climb. In 1955 August Busch Jr. made a strategic move to expand Budweiser's national brand and distributor presence. Along with this expansion came advances in bottling automation, new bottling materials and more efficient distribution methods. These advances brought to market many new containers and package designs. Budweiser is distributed in four large container volumes: half-barrel kegs (), quarter-barrel kegs (), 1/6 barrel kegs () and "beer balls". Budweiser produces a variety of cans and bottles ranging from . On August 3, 2011, Anheuser-Busch announced its twelfth can design since 1936, one which emphasizes the bowtie.
Packages are sometimes tailored to local customs and traditions. In St. Mary's County, Maryland, ten ounce cans are the preferred package.
The Budweiser bottle has remained relatively unchanged since its introduction in 1876. A small label is affixed to the neck of the bottle with the Budweiser "bow-tie" logo. The main label is red with a white box in the center, overlaid with a Budweiser logo resembling a coat of arms, with the word "Budweiser" below it.
In attempt to re-stimulate interest in their beer after the repeal of Prohibition, Budweiser began canning their beer in 1936. This new packaging led to an increase in sales which lasted until the start of World War II in 1939.
Over the years, Budweiser cans have undergone various design changes in response to market conditions and consumer tastes. Since 1936, 12 major can design changes have occurred, not including the temporary special edition designs.
Budweiser cans have traditionally displayed patriotic American symbols, such as eagles and the colors red, white, and blue. In 2011, there was a branding redesign that eliminated some of the traditional imagery. The new design was largely in response to the huge decline in sales threatening Budweiser's status as America's best-selling beer. In order to regain the domestic market share that Budweiser has lost, the company tried to update its appearance by giving the can a more contemporary look. The company hopes that the new design will offset the effects that unemployment had on its sales. Although the more modern design is intended for young male Americans, the new design was also part of an attempt to focus on the international market. Budweiser began selling its beer in Russia in 2010, and is currently expanding its operations in China.
Budweiser is produced using barley malt, rice, water, hops and yeast. The brewing happens in 7 steps: milling, mashing, straining, brew kettle, primary fermentation, beechwood lagering and finishing. It is lagered with beechwood chips in the aging vessel. While beechwood chips are used in the maturation tank, there is little to no flavor contribution from the wood, mainly because they are boiled in sodium bicarbonate (baking soda) for seven hours for the very purpose of removing any flavor from the wood. The maturation tanks that Anheuser-Busch uses are horizontal and, as such, flocculation of the yeast occurs much more quickly. Anheuser-Busch refers to this process as a secondary fermentation, with the idea being that the chips give the yeast more surface area to rest on. This is also combined with a krausening procedure that re-introduces wort into the chip tank, therefore reactivating the fermentation process. Placing the beechwood chips at the bottom of the tank keeps the yeast in suspension longer, giving it more time to reabsorb and process green beer flavors, such as acetaldehyde and diacetyl, that Anheuser-Busch believes are off-flavors which detract from overall drinkability.
Budweiser and "Bud Light" are sometimes advertised as vegan beers, in that their ingredients and conditioning do not use animal by-products. Some may object to the inclusion of genetically engineered rice and animal products used in the brewing process. In July 2006, Anheuser-Busch brewed a version of Budweiser with organic rice, for sale in Mexico. It has yet to extend this practice to any other countries.
In addition to the regular Budweiser, Anheuser-Busch brews several different beers under the Budweiser brand, including Bud Light, Bud Ice, and Bud Light lime.
In July 2010, Anheuser-Busch launched Budweiser 66 in the United Kingdom. Budweiser Brew No.66 has 4% alcohol by volume, and is brewed and distributed in the UK by Inbev UK Limited.
On May 10, 2016, "Advertising Age" reported that the Alcohol and Tobacco Tax and Trade Bureau had approved new Budweiser labels to be used on 12-ounce cans and bottles from May 23 until the November elections. The name "Budweiser" was changed to "America". Much of the text on the packaging was replaced with patriotic American slogans, such as E pluribus unum and "Liberty & Justice For All".
Budweiser is licensed, produced and distributed in Canada by Labatt Brewing Company. Of the 15 Anheuser-Busch breweries outside of the United States, 14 of them are positioned in China. Budweiser is the fourth leading brand in the Chinese beer market.

</doc>
<doc id="4854" url="https://en.wikipedia.org/wiki?curid=4854" title="Bermuda Triangle">
Bermuda Triangle

The Bermuda Triangle, also known as the Devil's Triangle or Hurricane Alley, is a loosely defined region in the western part of the North Atlantic Ocean where a number of aircraft and ships are said to have disappeared under mysterious circumstances. Most reputable sources dismiss the idea that there is any mystery.
The vicinity of the Bermuda Triangle is amongst the most heavily traveled shipping lanes in the world, with ships frequently crossing through it for ports in the Americas, Europe and the Caribbean islands. Cruise ships and pleasure craft regularly sail through the region, and commercial and private aircraft routinely fly over it.
Popular culture has attributed various disappearances to the paranormal or activity by extraterrestrial beings. Documented evidence indicates that a significant percentage of the incidents were spurious, inaccurately reported, or embellished by later authors.
The earliest suggestion of unusual disappearances in the Bermuda area appeared in a September 17, 1950, article published in "The Miami Herald" (Associated Press) by Edward Van Winkle Jones. Two years later, "Fate" magazine published "Sea Mystery at Our Back Door", a short article by George Sand covering the loss of several planes and ships, including the loss of Flight 19, a group of five US Navy Grumman TBM Avenger torpedo bombers on a training mission. Sand's article was the first to lay out the now-familiar triangular area where the losses took place, as well as the first to suggest a supernatural element to the Flight 19 incident. Flight 19 alone would be covered again in the April 1962 issue of "American Legion" magazine. In it, author Allan W. Eckert wrote that the flight leader had been heard saying, "We are entering white water, nothing seems right. We don't know where we are, the water is green, no white." He also wrote that officials at the Navy board of inquiry stated that the planes "flew off to Mars."
In February 1964, Vincent Gaddis wrote an article called "The Deadly Bermuda Triangle" in the pulp magazine "Argosy" saying Flight 19 and other disappearances were part of a pattern of strange events in the region. The next year, Gaddis expanded this article into a book, "Invisible Horizons".
Other writers elaborated on Gaddis' ideas: John Wallace Spencer ("Limbo of the Lost", 1969, repr. 1973); Charles Berlitz ("The Bermuda Triangle", 1974); Richard Winer ("The Devil's Triangle", 1974), and many others, all keeping to some of the same supernatural elements outlined by Eckert.
The Gaddis "Argosy" article delineated the boundaries of the triangle, giving its vertices as Miami; San Juan, Puerto Rico; and Bermuda. Subsequent writers did not necessarily follow this definition. Some writers gave different boundaries and vertices to the triangle, with the total area varying from . "Indeed, some writers even stretch it as far as the Irish coast." Consequently, the determination of which accidents occurred inside the triangle depends on which writer reported them.
Larry Kusche, author of "The Bermuda Triangle Mystery: Solved" (1975) argued that many claims of Gaddis and subsequent writers were exaggerated, dubious or unverifiable. Kusche's research revealed a number of inaccuracies and inconsistencies between Berlitz's accounts and statements from eyewitnesses, participants, and others involved in the initial incidents. Kusche noted cases where pertinent information went unreported, such as the disappearance of round-the-world yachtsman Donald Crowhurst, which Berlitz had presented as a mystery, despite clear evidence to the contrary. Another example was the ore-carrier recounted by Berlitz as lost without trace three days out of an "Atlantic" port when it had been lost three days out of a port with the same name in the "Pacific" Ocean. Kusche also argued that a large percentage of the incidents that sparked allegations of the Triangle's mysterious influence actually occurred well outside it. Often his research was simple: he would review period newspapers of the dates of reported incidents and find reports on possibly relevant events like unusual weather, that were never mentioned in the disappearance stories.
Kusche concluded that:
In a 2013 study, the World Wide Fund for Nature identified the world's 10 most dangerous waters for shipping, but the Bermuda Triangle was not among them.
When the UK Channel 4 television program "The Bermuda Triangle" (1992) was being produced by John Simmons of Geofilms for the "Equinox" series, the marine insurance market Lloyd's of London was asked if an unusually large number of ships had sunk in the Bermuda Triangle area. Lloyd's determined that large numbers of ships had not sunk there. Lloyd's does not charge higher rates for passing through this area. United States Coast Guard records confirm their conclusion. In fact, the number of supposed disappearances is relatively insignificant considering the number of ships and aircraft that pass through on a regular basis.
The Coast Guard is also officially skeptical of the Triangle, noting that they collect and publish, through their inquiries, much documentation contradicting many of the incidents written about by the Triangle authors. In one such incident involving the 1972 explosion and sinking of the tanker , the Coast Guard photographed the wreck and recovered several bodies, in contrast with one Triangle author's claim that all the bodies had vanished, with the exception of the captain, who was found sitting in his cabin at his desk, clutching a coffee cup. In addition, "V. A. Fogg" sank off the coast of Texas, nowhere near the commonly accepted boundaries of the Triangle.
The Nova/Horizon episode "The Case of the Bermuda Triangle", aired on June 27, 1976, was highly critical, stating that "When we've gone back to the original sources or the people involved, the mystery evaporates. Science does not have to answer questions about the Triangle because those questions are not valid in the first place ... Ships and planes behave in the Triangle the same way they behave everywhere else in the world."
Skeptical researchers, such as Ernest Taves and Barry Singer, have noted how mysteries and the paranormal are very popular and profitable. This has led to the production of vast amounts of material on topics such as the Bermuda Triangle. They were able to show that some of the pro-paranormal material is often misleading or inaccurate, but its producers continue to market it. Accordingly, they have claimed that the market is biased in favor of books, TV specials, and other media that support the Triangle mystery, and against well-researched material if it espouses a skeptical viewpoint.
Benjamin Radford, an author and scientific paranormal investigator, noted in an interview on the Bermuda Triangle that it could be very difficult locating an aircraft lost at sea due to the vast search area, and although the disappearance might be mysterious, that did not make it paranormal or unexplainable. Radford further noted the importance of double-checking information as the mystery surrounding the Bermuda Triangle had been created by people who had neglected to do so.
Persons accepting the Bermuda Triangle as a real phenomenon have offered a number of explanatory approaches.
Triangle writers have used a number of supernatural concepts to explain the events. One explanation pins the blame on leftover technology from the mythical lost continent of Atlantis. Sometimes connected to the Atlantis story is the submerged rock formation known as the Bimini Road off the island of Bimini in the Bahamas, which is in the Triangle by some definitions. Followers of the purported psychic Edgar Cayce take his prediction that evidence of Atlantis would be found in 1968, as referring to the discovery of the Bimini Road. Believers describe the formation as a road, wall, or other structure, but the Bimini Road is of natural origin.
Other writers attribute the events to UFOs. Charles Berlitz, author of various books on anomalous phenomena, lists several theories attributing the losses in the Triangle to anomalous or unexplained forces.
Compass problems are one of the cited phrases in many Triangle incidents. While some have theorized that unusual local magnetic anomalies may exist in the area, such anomalies have not been found. Compasses have natural magnetic variations in relation to the magnetic poles, a fact which navigators have known for centuries. Magnetic (compass) north and geographic (true) north are exactly the same only for a small number of places – for example, , in the United States, only those places on a line running from Wisconsin to the Gulf of Mexico. But the public may not be as informed, and think there is something mysterious about a compass "changing" across an area as large as the Triangle, which it naturally will.
The Gulf Stream is a major surface current, primarily driven by thermohaline circulation that originates in the Gulf of Mexico and then flows through the Straits of Florida into the North Atlantic. In essence, it is a river within an ocean, and, like a river, it can and does carry floating objects. It has a maximum surface velocity of about . A small plane making a water landing or a boat having engine trouble can be carried away from its reported position by the current.
One of the most cited explanations in official inquiries as to the loss of any aircraft or vessel is human error. Human stubbornness may have caused businessman Harvey Conover to lose his sailing yacht, "Revonoc", as he sailed into the teeth of a storm south of Florida on January 1, 1958.
Hurricanes are powerful storms that form in tropical waters and have historically cost thousands of lives and caused billions of dollars in damage. The sinking of Francisco de Bobadilla's Spanish fleet in 1502 was the first recorded instance of a destructive hurricane. These storms have in the past caused a number of incidents related to the Triangle.
A powerful downdraft of cold air was suspected to be a cause in the sinking of "Pride of Baltimore" on May 14, 1986. The crew of the sunken vessel noted the wind suddenly shifted and increased velocity from to . A National Hurricane Center satellite specialist, James Lushine, stated "during very unstable weather conditions the downburst of cold air from aloft can hit the surface like a bomb, exploding outward like a giant squall line of wind and water." A similar event occurred to "Concordia" in 2010, off the coast of Brazil. Scientists are currently investigating whether "hexagonal" clouds may be the source of these up-to- "air bombs".
An explanation for some of the disappearances has focused on the presence of large fields of methane hydrates (a form of natural gas) on the continental shelves. Laboratory experiments carried out in Australia have proven that bubbles can, indeed, sink a scale model ship by decreasing the density of the water; any wreckage consequently rising to the surface would be rapidly dispersed by the Gulf Stream. It has been hypothesized that periodic methane eruptions (sometimes called "mud volcanoes") may produce regions of frothy water that are no longer capable of providing adequate buoyancy for ships. If this were the case, such an area forming around a ship could cause it to sink very rapidly and without warning.
Publications by the USGS describe large stores of undersea hydrates worldwide, including the Blake Ridge area, off the coast of the southeastern United States. However, according to the USGS, no large releases of gas hydrates are believed to have occurred in the Bermuda Triangle for the past 15,000 years.
The incident resulting in the single largest loss of life in the history of the US Navy not related to combat occurred when the collier "Cyclops", carrying a full load of manganese ore and with one engine out of action, went missing without a trace with a crew of 309 sometime after March 4, 1918, after departing the island of Barbados. Although there is no strong evidence for any single theory, many independent theories exist, some blaming storms, some capsizing, and some suggesting that wartime enemy activity was to blame for the loss. In addition, two of "Cyclops"s sister ships, and were subsequently lost in the North Atlantic during World War II. Both ships were transporting heavy loads of metallic ore similar to that which was loaded on "Cyclops" during her fatal voyage. In all three cases structural failure due to overloading with a much denser cargo than designed is considered the most likely cause of sinking.
A five-masted schooner built in 1919, "Carroll A. Deering" was found hard aground and abandoned at Diamond Shoals, near Cape Hatteras, North Carolina, on January 31, 1921. Rumors and more at the time indicated "Deering" was a victim of piracy, possibly connected with the illegal rum-running trade during Prohibition, and possibly involving another ship, , which disappeared at roughly the same time. Just hours later, an unknown steamer sailed near the lightship along the track of "Deering", and ignored all signals from the lightship. It is speculated that "Hewitt" may have been this mystery ship, and possibly involved in "Deering"s crew disappearance.
Flight 19 was a training flight of five TBM Avenger torpedo bombers that disappeared on December 5, 1945, while over the Atlantic. The squadron's flight plan was scheduled to take them due east from Fort Lauderdale for , north for , and then back over a final leg to complete the exercise. The flight never returned to base. The disappearance was attributed by Navy investigators to navigational error leading to the aircraft running out of fuel.
One of the search and rescue aircraft deployed to look for them, a PBM Mariner with a 13-man crew, also disappeared. A tanker off the coast of Florida reported seeing an explosion and observing a widespread oil slick when fruitlessly searching for survivors. The weather was becoming stormy by the end of the incident. According to contemporaneous sources the Mariner had a history of explosions due to vapour leaks when heavily loaded with fuel, as it might have been for a potentially long search-and-rescue operation.
G-AHNP "Star Tiger" disappeared on January 30, 1948, on a flight from the Azores to Bermuda; G-AGRE "Star Ariel" disappeared on January 17, 1949, on a flight from Bermuda to Kingston, Jamaica. Both were Avro Tudor IV passenger aircraft operated by British South American Airways. Both planes were operating at the very limits of their range and the slightest error or fault in the equipment could keep them from reaching the small island.
On December 28, 1948, a Douglas DC-3 aircraft, number NC16002, disappeared while on a flight from San Juan, Puerto Rico, to Miami. No trace of the aircraft, or the 32 people on board, was ever found. A Civil Aeronautics Board investigation found there was insufficient information available on which to determine probable cause of the disappearance.
A pleasure yacht was found adrift in the Atlantic south of Bermuda on September 26, 1955; it is usually stated in the stories (Berlitz, Winer) that the crew vanished while the yacht survived being at sea during three hurricanes. The 1955 Atlantic hurricane season shows Hurricane Ione passing nearby between 14 and 18 September, with Bermuda being affected by winds of almost gale force. In his second book on the Bermuda Triangle, Winer quoted from a letter he had received from Mr J.E. Challenor of Barbados:
On August 28, 1963, a pair of US Air Force KC-135 Stratotanker aircraft collided and crashed into the Atlantic 300 miles west of Bermuda. Some writers say that while the two aircraft did collide there were two distinct crash sites, separated by over of water. However, Kusche's research showed that the unclassified version of the Air Force investigation report revealed that the debris field defining the second "crash site" was examined by a search and rescue ship, and found to be a mass of seaweed and driftwood tangled in an old buoy.
Notes
Bibliography<br>
The incidents cited above, apart from the official documentation, come from the following works. Some incidents mentioned as having taken place within the Triangle are found "only" in these sources:
Further reading
ProQuest has newspaper source material for many incidents, archived in Portable Document Format (PDF). The newspapers include "The New York Times", "The Washington Post", and "The Atlanta Constitution". To access this website, registration is required, usually through a library connected to a college or university.
Flight 19
SS "Cotopaxi"
USS "Cyclops" (AC-4)
Carroll A. Deering
Wreckers
S.S. "Suduffco"
Star Tiger" and "Star Ariel
DC-3 Airliner NC16002 disappearance
Harvey Conover and "Revonoc"
KC-135 Stratotankers
B-52 Bomber ("Pogo 22")
Charter vessel "Sno'Boy"
SS "Marine Sulphur Queen"
SS "Sylvia L. Ossa"
The following websites have either online material that supports the popular version of the Bermuda Triangle, or documents published from official sources as part of hearings or inquiries, such as those conducted by the United States Navy or United States Coast Guard. Copies of some inquiries are not online and may have to be ordered; for example, the losses of Flight 19 or USS Cyclops can be ordered direct from the United States Naval Historical Center.
Most of the works listed here are largely out of print. Copies may be obtained at your local library, or purchased used at bookstores, or through eBay or Amazon.com. These books are often the "only" source material for some of the incidents that have taken place within the Triangle.

</doc>
<doc id="4856" url="https://en.wikipedia.org/wiki?curid=4856" title="Borough">
Borough

A borough is an administrative division in various English-speaking countries. In principle, the term "borough" designates a self-governing walled town, although in practice, official use of the term varies widely.
In the Middle Ages, boroughs were settlements in England that were granted some self-government; burghs were the Scottish equivalent. In medieval England, boroughs were also entitled to elect members of parliament. The use of the word "borough" probably derives from the burghal system of Alfred the Great. Alfred set up a system of defensive strong points (Burhs); in order to maintain these particular settlements, he granted them a degree of autonomy. After the Norman Conquest, when certain towns were granted self-governance, the concept of the burh/borough seems to have been reused to mean a self-governing settlement.
The concept of the borough has been used repeatedly (and often differently) throughout the world. Often, a borough is a single town with its own local government. However, in some cities it is a subdivision of the city (for example, New York City, London, and Montreal). In such cases, the borough will normally have either limited powers delegated to it by the city's local government, or no powers at all. In other places, such as the U.S. state of Alaska, "borough" designates a whole region; Alaska's largest borough, the North Slope Borough, is comparable in area to the entire United Kingdom, although its population is less than that of Swanage on England's south coast with around 9,600 inhabitants. In Australia, a "borough" was once a self-governing small town, but this designation has all but vanished, except for the only remaining borough in the country, which is the Borough of Queenscliffe.
Boroughs as administrative units are to be found in Ireland and the United Kingdom, more specifically in England and Northern Ireland. Boroughs also exist in the Canadian province of Quebec and formerly in Ontario, in some states of the United States, in Israel, formerly in New Zealand and only one left in Australia.
The word "borough" derives from the Old English word "burg, burh", meaning a fortified settlement; the word appears as modern English "bury", "-brough", Scots "burgh", "borg" in Scandinavian languages.
A number of other European languages have cognate words that were borrowed from the Germanic languages during the Middle Ages, including "brog" in Irish, "bwr" or "bwrc", meaning "wall, rampart" in Welsh, "bourg" in French, "burg" in Catalan (in Catalonia there is a town named "Burg"), "borgo" in Italian, "burgo" in Portuguese and Castilian (hence the place-name Burgos), the "-bork" of Lębork and Malbork in Polish and the "-bor" of Maribor in Slovenian.
The 'burg' element, which means "castle" or "fortress", is often confused with 'berg' meaning "hill" or "mountain" (c.f. iceberg, inselberg). Hence the 'berg' element in Bergen or Heidelberg relates to a hill, rather than a fort. In some cases, the 'berg' element in place names has converged towards burg/borough; for instance Farnborough, from "fernaberga" (fern-hill).
In many parts of England, "borough" is pronounced as an independent word, and as when a suffix of a place-name. As a suffix, it is sometimes spelled "-brough".
In the United States, "borough" is pronounced . When appearing as the suffix "-burg(h)" in place-names, it is pronounced .
In Australia, the term "borough" is an occasionally used term for a local government area. Currently there is only one borough in Australia, the Borough of Queenscliffe in Victoria, although there have been more in the past. However, in some cases it can be integrated into the council's name instead of used as an official title, such as the Municipality of Kingborough in Tasmania.
In Quebec, the term borough is generally used as the English translation of arrondissement, referring to an administrative division of a municipality, or a district. It was previously used in Metropolitan Toronto, Ontario, to denote suburban municipalities including Scarborough, York, North York, Etobicoke prior to their conversion into cities. The Borough of East York was the last Toronto municipality to hold this status, relinquishing it upon becoming part of the City of Toronto government on January 1, 1998. Only eight municipalities in Quebec are divided into boroughs. See List of boroughs in Quebec.
The Colombian Municipalities are subdivided into boroughs (English translation of the Spanish term "localidades") with a local executive and an administrative board for local government.
These Boroughs are divided in neighborhoods.
Also, the principal cities had "localidades" with the same features as the European or American cities, including Soacha in Bogotá, Bello, La Estrella, Sabaneta, Envigado and Itagüí on Medellín.
There are four borough districts designated by the Local Government Reform Act 2014: Clonmel, Drogheda, Sligo, and Wexford. A local boundary review reporting in 2018 proposed granting borough status to any district containing a census town with a population over 30,000; this would have included the towns of Dundalk, Bray, and Navan. This requires an amendment to the 2014 act, promised for 2019 by minister John Paul Phelan.
Historically, there were 117 parliamentary boroughs in the Irish House of Commons, of which 80 were disfranchised by the Acts of Union 1800 and all but 11 abolished under the Municipal Corporations (Ireland) Act 1840. Under the Local Government (Ireland) Act 1898, six of these became county boroughs: Dublin, Belfast, Cork, Derry, Limerick and Waterford. From 1921, Belfast and Derry were part of Northern Ireland and stayed within the United Kingdom on the establishment of the Irish Free State in 1922. Galway was a borough from 1937 until upgraded to a county borough in 1985. The county boroughs in the Republic of Ireland were redesignated as "cities" under the Local Government Act 2001.
Dún Laoghaire was a borough from 1930 until merged into Dún Laoghaire–Rathdown county in 1994.
There were five borough councils in place at the time of the Local Government Reform Act 2014 which abolished all second-tier local government units of borough and town councils. Each local government authority outside of Dublin, Cork City and Galway City was divided into areas termed municipal districts. In four of the areas which had previously been contained borough councils, as listed above, these were instead termed Borough Districts. Kilkenny had previously had a borough council, but its district was to be called the Municipal District of Kilkenny City, in recognition of its historic city status.
Under Israeli law, inherited from British Mandate municipal law, the possibility of creating a municipal borough exists. However, no borough was actually created under law until 2005–2006, when Neve Monosson and Maccabim-Re'ut, both communal settlements (Heb: yishuv kehilati) founded in 1953 and 1984, respectively, were declared to be autonomous municipal boroughs (Heb: vaad rova ironi), within their mergers with the towns of Yehud and Modi'in. Similar structures have been created under different types of legal status over the years in Israel, notably Kiryat Haim in Haifa, Jaffa in Tel Aviv-Yafo and Ramot and Gilo in Jerusalem. However, Neve Monosson is the first example of a full municipal borough actually declared under law by the Minister of the Interior, under a model subsequently adopted in Maccabim-Re'ut as well.
It is the declared intention of the Interior Ministry to use the borough mechanism in order to facilitate municipal mergers in Israel, after a 2003 wide-reaching merger plan, which, in general, ignored the sensitivities of the communal settlements, and largely failed.
In Mexico as translations from English to Spanish applied to Mexico City, the word "borough" has resulted in a delegación (delegation), referring to the 16 administrative areas within the Mexico City, now call Alcaldías. (see: Boroughs of Mexico and Municipalities of Mexico City)
In the Netherlands, the municipalities of Rotterdam and Amsterdam were divided into administrative boroughs, or deelgemeenten, which had their own borough council and a borough mayor. Other large cities are usually divided into districts, or stadsdelen, for census purposes. The deelgemeenten were abolished in 2014.
New Zealand formerly used the term borough to designate self-governing towns of more than 1,000 people, although 19th century census records show many boroughs with populations as low as 200. A borough of more than 20,000 people could become a city by proclamation. Boroughs and cities were collectively known as municipalities, and were enclaves separate from their surrounding counties. Boroughs proliferated in the suburban areas of the larger cities: By the 1980s there were 19 boroughs and three cities in the area that is now the City of Auckland.
In the 1980s, some boroughs and cities began to be merged with their surrounding counties to form districts with a mixed urban and rural population. A nationwide reform of local government in 1989 completed the process. Counties and boroughs were abolished and all boundaries were redrawn. Under the new system, most territorial authorities cover both urban and rural land. The more populated councils are classified as cities, and the more rural councils are classified as districts. Only Kawerau District, an enclave within Whakatane District, continues to follow the tradition of a small town council that does not include surrounding rural area.
In Trinidad and Tobago, a Borough is a unit of Local Government. There are 3 boroughs in The Republic of Trinidad and Tobago:
During the medieval period many towns were granted self-governance by the Crown, at which point they became referred to as boroughs. The formal status of borough came to be conferred by Royal Charter. These boroughs were generally governed by a self-selecting corporation (i.e., when a member died or resigned his replacement would be by co-option). Sometimes boroughs were governed by bailiffs or headboroughs.
Debates on the Reform Bill (eventually the Reform Act 1832) had highlighted the variations in systems of governance of towns, and a Royal Commission was set up to investigate the issue. This resulted in a regularisation of municipal government (Municipal Corporations Act 1835). 178 of the ancient boroughs were reformed as "municipal boroughs", with all municipal corporations to be elected according to a standard franchise based on property ownership. The unreformed boroughs either lapsed in borough status, or were reformed (or abolished) at a later time. Several new municipal boroughs were formed in the new industrial cities after the bill enacted, according to the provisions of the bill.
As part of a large-scale reform of local government in England and Wales in 1974, municipal boroughs were finally abolished (having become increasingly irrelevant). However, the civic traditions of many boroughs were continued by the grant of a charter to their successor district councils. In smaller boroughs, a town council was formed for the area of the abolished borough, while charter trustees were formed in other former boroughs. In each case, the new body was allowed to use the regalia of the old corporation, and appoint ceremonial office holders such as sword and mace bearers as provided in their original charters. The council or trustees may apply for an Order in Council or Royal Licence to use the former borough coat of arms.
From 1265, two burgesses from each borough were summoned to the Parliament of England, alongside two knights from each county. Thus parliamentary constituencies were derived from the ancient boroughs. Representation in the House of Commons was decided by the House itself, which resulted in boroughs being established in some small settlements for the purposes of parliamentary representation, despite their possessing no actual corporation.
After the Reform Act, which disenfranchised many of the rotten boroughs (boroughs that had declined in importance, had only a small population, and had only a handful of eligible voters), parliamentary constituencies began to diverge from the ancient boroughs. While many ancient boroughs remained as municipal boroughs, they were disenfranchised by the Reform Act.
The Local Government Act 1888 established a new sort of borough – the county borough. These were designed to be 'counties-to-themselves'; administrative divisions to sit alongside the new administrative counties. They allowed urban areas to be administered separately from the more rural areas. They, therefore, often contained pre-existing municipal boroughs, which thereafter became part of the second tier of local government, below the administrative counties and county boroughs.
The county boroughs were, like the municipal boroughs, abolished in 1974, being reabsorbed into their parent counties for administrative purposes.
In 1899, as part of a reform of local government in the County of London, the various parishes in London were reorganised as new entities, the 'metropolitan boroughs'. These were reorganised further when Greater London was formed out of Middlesex, parts of Surrey, Kent, Essex, Hertfordshire and the County of London in 1965. These council areas are now referred to as "London boroughs" rather than "metropolitan boroughs".
When the new metropolitan counties (Greater Manchester, Merseyside, South Yorkshire, Tyne and Wear, West Midlands, and West Yorkshire) were created in 1974, their sub-divisions also became metropolitan boroughs in many, but not all, cases; in many cases these metropolitan boroughs recapitulated abolished county boroughs (for example, Stockport). The metropolitan boroughs possessed slightly more autonomy from the metropolitan county councils than the shire county districts did from their county councils.
With the abolition of the metropolitan county councils in 1986, these metropolitan boroughs became independent, and continue to be so at present.
Elsewhere in England a number of districts and unitary authority areas are called "borough". Until 1974, this was a status that denoted towns with a certain type of local government (a municipal corporation, or a self-governing body). Since 1974, it has been a purely ceremonial style granted by royal charter to districts which may consist of a single town or may include a number of towns or rural areas. Borough status entitles the council chairman to bear the title of mayor. Districts may apply to the British Crown for the grant of borough status upon advice of the Privy Council of the United Kingdom.
In Northern Ireland, local government was reorganised in 1973. Under the legislation that created the 26 districts of Northern Ireland, a district council whose area included an existing municipal borough could resolve to adopt the charter of the old municipality and thus continue to enjoy borough status. Districts that do not contain a former borough can apply for a charter in a similar manner to English districts.
In the United States, a borough is a unit of local government below the level of the state. The term is currently used in seven states.
The following states use, or have used, the word with the following meanings:
Certain names of places, such as Hillsboro, Oregon; Greensboro, North Carolina; Tyngsborough, Massachusetts; and Maynesborough, New Hampshire reflect the historical use of "borough" as a geographical unit in the United States.

</doc>
<doc id="4858" url="https://en.wikipedia.org/wiki?curid=4858" title="Bodmin">
Bodmin

Bodmin () is a civil parish and historic town in Cornwall, England, United Kingdom. It is situated south-west of Bodmin Moor.
The extent of the civil parish corresponds fairly closely to that of the town so is mostly urban in character. It is bordered to the east by Cardinham parish, to the southeast by Lanhydrock parish, to the southwest and west by Lanivet parish, and to the north by Helland parish.
Bodmin had a population of 14,736 as of the 2011 Census. It was formerly the county town of Cornwall until the Crown Courts moved to Truro which is also the administrative centre (before 1835 the county town was Launceston). Bodmin was in the administrative North Cornwall District until local government reorganisation in 2009 abolished the District ("see also Cornwall Council"). The town is part of the North Cornwall parliamentary constituency, which is represented by Scott Mann MP.
Bodmin Town Council is made up of sixteen councillors who are elected to serve a term of four years. Each year, the Council elects one of its number as Mayor to serve as the town's civic leader and to chair council meetings.
Bodmin lies in the east of Cornwall, south-west of Bodmin Moor. It has been suggested that the town's name comes from an archaic word in the Cornish language "bod" (meaning a dwelling; the later word is "bos") and a contraction of "menegh" (monks). The "monks' dwelling" may refer to an early monastic settlement instituted by St. Guron, which St. Petroc took as his site. Guron is said to have departed to St Goran on the arrival of Petroc.
The hamlets of Cooksland, Dunmere and Turfdown are in the parish.
St. Petroc founded a monastery in Bodmin in the 6th century and gave the town its alternative name of "Petrockstow". The monastery was deprived of some of its lands at the Norman conquest but at the time of Domesday still held eighteen manors, including Bodmin, Padstow and Rialton. Bodmin is one of the oldest towns in Cornwall, and the only large Cornish settlement recorded in the Domesday Book in 1086. In the 15th century the Norman church of St Petroc was largely rebuilt and stands as one of the largest churches in Cornwall (the largest after the cathedral at Truro). Also built at that time was an abbey of canons regular, now mostly ruined. For most of Bodmin's history, the tin industry was a mainstay of the economy.
The name of the town probably derives from the Cornish "Bod-meneghy", meaning "dwelling of or by the sanctuary of monks". Variant spellings recorded include "Botmenei" in 1100, "Bodmen" in 1253, "Bodman" in 1377 and "Bodmyn" in 1522. The "Bodman" spelling also appears in sources and maps from the 16th and 17th centuries, most notably in the celebrated map of Cornwall produced by John Speed but actually engraved by the Dutch cartographer Jodocus Hondius the Elder (1563–1612) in Amsterdam in 1610 (published in London by Sudbury and Humble in 1626). It is unclear whether the Bodman spelling signifies any historical or monastic connection with the equally ancient settlement of Bodman at the western end of the Bodensee in the German province of Baden.
An inscription on a stone built into the wall of a summer house in Lancarffe furnishes proof of a settlement in Bodmin in the early Middle Ages. It is a memorial to one "Duno[.]atus son of Me[.]cagnus" and has been dated from the 6th to 8th centuries.
Arthur Langdon (1896) records three Cornish crosses at Bodmin; one was near the Berry Tower, one was outside Bodmin Gaol and another was in a field near Castle Street Hill. There is also Carminow Cross at a road junction southeast of the town.
The Black Death killed half of Bodmin's population in the mid 14th century (1,500 people).
Bodmin was the centre of three Cornish uprisings. The first was the Cornish Rebellion of 1497 when a Cornish army, led by Michael An Gof, a blacksmith from St. Keverne and Thomas Flamank, a lawyer from Bodmin, marched to Blackheath in London where they were eventually defeated by 10,000 men of the King's army under Baron Daubeny. Then, in the autumn of 1497, Perkin Warbeck tried to usurp the throne from Henry VII. Warbeck was proclaimed King Richard IV in Bodmin but Henry had little difficulty crushing the uprising. In 1549, Cornishmen, allied with other rebels in neighbouring Devon, rose once again in rebellion when the staunchly Protestant Edward VI tried to impose a new Prayer Book. The lower classes of Cornwall and Devon were still strongly attached to the Roman Catholic religion and again a Cornish army was formed in Bodmin which marched across the border into Devon to lay siege to Exeter. This became known as the Prayer Book Rebellion. Proposals to translate the Prayer Book into Cornish were suppressed and in total 4,000 people were killed in the rebellion.
The Borough of Bodmin was one of the 178 municipal boroughs which under the auspices of the Municipal Corporations Act 1835 was mandated to create an electable council and a Police Watch Committee responsible for overseeing a police force in the town. The new system directly replaced the Parish Constables that had policed the borough since time immemorial and brought paid, uniformed and accountable law enforcement for the first time. Bodmin Borough Police was the municipal police force for the Borough of Bodmin from 1836 to 1866. The creation of the Cornwall Constabulary in 1857 put pressure on smaller municipal police forces to merge with the county. The two-man force of Bodmin came under threat almost immediately, but it would take until 1866 for the Mayor of Bodmin and the Chairman of the Police Watch Committee to agree on the terms of amalgamation. After a public enquiry, the force was disbanded in January 1866 and policing of the borough was deferred to the county from thereon.
The song "Bodmin Town" was collected from the Cornishman William Nichols at Whitchurch, Devon, in 1891 by Sabine Baring-Gould who published a version in his "A Garland of Country Song" (1924).
The existing church building is dated 1469–72 and was until the building of Truro Cathedral the largest church in Cornwall. The tower which remains from the original Norman church and stands on the north side of the church (the upper part is 15th century) was, until the loss of its spire in 1699, 150 ft high. The building underwent two Victorian restorations and another in 1930. It is now listed Grade I. There are a number of interesting monuments, most notably that of Prior Vivian which was formerly in the Priory Church (Thomas Vivian's effigy lying on a chest: black Catacleuse stone and grey marble). The font of a type common in Cornwall is of the 12th century: large and finely carved.
The Chapel of St Thomas Becket is a ruin of a 14th-century building in Bodmin churchyard. The holy well of St Guron is a small stone building at the churchyard gate. The Berry Tower is all that remains of the former church of the Holy Rood and there are even fewer remains from the substantial Franciscan Friary established ca. 1240: a gateway in Fore Street and two pillars elsewhere in the town. The Roman Catholic Abbey of St Mary and St Petroc, formerly belonging to the Canons Regular of the Lateran was built in 1965 next to the already existing seminary. The Roman Catholic parish of Bodmin includes a large area of North Cornwall and there are churches also at Wadebridge, Padstow and Tintagel. In 1881 the Roman Catholic mass was celebrated in Bodmin for the first time since 1539. A church was planned in the 1930s but delayed by the Second World War: the Church of St Mary and St Petroc was eventually consecrated in 1965: it was built next to the already existing seminary. There are also five other churches in Bodmin, including a Methodist church.
Bodmin Jail, operational for over 150 years but now a semi-ruin, was built in the late 18th century, and was the first British prison to hold prisoners in separate cells (though often up to ten at a time) rather than communally. Over fifty prisoners condemned at the Bodmin Assize Court were hanged at the prison. It was also used for temporarily holding prisoners sentenced to transportation, awaiting transfer to the prison hulks lying in the highest navigable reaches of the River Fowey. Also, during the First World War the prison held some of Britain's priceless national treasures including the Domesday Book, the ring and the Crown Jewels of the United Kingdom.
Other buildings of interest include the former Shire Hall, now a tourist information centre, and Victoria Barracks, formerly depot of the now defunct Duke of Cornwall's Light Infantry and now the site of the regimental museum. It includes the history of the regiment from 1702, plus a military library. The original barracks house the regimental museum which was founded in 1925. There is a fine collection of small arms and machine guns, plus maps, uniforms and paintings on display. The Honey Street drill hall was the mobilisation point for reservists being deployed to serve on the Western Front.
Bodmin County Lunatic Asylum was designed by John Foulston and afterwards George Wightwick. William Robert Hicks the humorist was domestic superintendent in the mid-19th century.
Walker Lines, named after Harold Bridgwood Walker, was a Second World War camp built as an extension to the DCLI Barracks. It was used to harbour men evacuated from Dunkirk and later to house troops for the D-Day landings. In the 1950s it was the site of the JSSL. The site is now an industrial estate but still known as 'Walker Lines'.
Bosvenna House, an Edwardian manor house, was formerly the home of the Royal British Legion Club, but has become a private residence.
There is a sizable single storey Masonic Hall in St Nicholas Street, which is home to no less than eight Masonic bodies.
The Masonic Hall opens to the public on every Heritage and Riding Day as well as on other Special occasions.
Bodmin Beacon Local Nature Reserve is the hill overlooking the town. The reserve has of public land and at its highest point it reaches with the distinctive landmark at the summit. The tall monument to Sir Walter Raleigh Gilbert was built in 1857 by the townspeople of Bodmin to honour the soldier's life and work in India.
In 1966, the "Finn VC Estate" was named in honour of Victoria Cross winner James Henry Finn who once lived in the town. An ornate granite drinking bowl which serves the needs of thirsty dogs at the entrance to Bodmin's Priory car park was donated by Prince Chula Chakrabongse of Thailand who lived at Tredethy.
There are no independent schools in the area.
Beacon ACE Academy opened as a primary school for pupils aged between 3–11 in September 2017 following the merger of Beacon Infant and Nursery School and Robartes Junior School. Beacon ACE Academy is part of Kernow Learning Multi Academy Trust]and is rated Good by Ofsted. The school offers places for 420 pupils as well as 30 places within its Nursery and 10 places within its Area Resource Base for pupils with Special Educational Needs.
St Petroc's Voluntary Aided Church of England Primary School, Athelstan Park, Bodmin, was given this title in September 1990 after the amalgamation of St. Petroc's Infant School and St. Petroc's Junior School. St. Petroc's is a large school with some 440 pupils between the ages of four and 11. Eight of its fourteen governors are nominated by the Diocese of Truro or the Parochial Church Council of St. Petroc's, Bodmin.
There are a further two primary schools within Bodmin; Berrycoombe School in the northwest corner of the town, and St. Mary's Catholic Primary School.
Bodmin College is a large state comprehensive school for ages 11–18 on the outskirts of the town and on the edge of Bodmin Moor. Its headmaster is Mr Brett Elliott. The college is home to the nationally acclaimed "Bodmin College Jazz Orchestra", founded and run by the previous Director of Music, Adrian Evans, until 2007 and more recently, by the current Director, Ben Vincent. In 1997, Systems & Control students at Bodmin College constructed Roadblock, a robot which entered and won the first series of Robot Wars and was succeeded by "The Beast of Bodmin". The school also has one of the largest sixth forms in the county.
Callywith College is a Further Education college in Bodmin, Cornwall, due to open in September 2017, with applications being accepted from September 2016. A new-build college on a site close to the Bodmin Asda supermarket, it will eventually cater for 1,280 students, with 197 staff employed. A total of 660 places will be available in its first year. It is being created with the assistance of the Ofsted Outstanding Truro and Penwith College to serve students aged 16–19 from Bodmin, North Cornwall and East Cornwall. It received the go-ahead in February 2016, funded as a Free School. Its aim is to "provide the outstanding Truro and Penwith College experience for up to 1280 young people in Bodmin and North and East Cornwall."
Aspirant National Service Sergeant Instructors of the Royal Army Education Corps underwent training at the Army School of Education, situated at the end of the Second World War at Buchanan Castle, Drymen in Scotland, and later, from 1948, at the Walker Lines, Bodmin, until it moved to Wilton Park, Beaconsfield.
Bodmin Parkway railway station – once known as Bodmin Road – is a principal calling point on the Cornish Main Line about 3½ miles (5½ km) south-east of the town centre. Buses to central Bodmin, Wadebridge, Padstow, Rock, Polzeath, Port Isaac and Camelford depart from outside the station entrance. It is connected to Bodmin town by a branch line that is home to the local steam railway.
Bodmin is just off the A30 providing a connection to the M5 motorway at Exeter 62 miles (99 km) northeast.
Bus and coach services connect Bodmin with some other districts of Cornwall and Devon.
Bodmin has a non-league football club Bodmin Town playing in the South West Peninsula League; a level 10 league in the English football league system. Their home ground is at Priory Park. Bodmin Rugby Club play rugby union at Clifden Parc and compete in the Tribute Cornwall/Devon league; a level 8 league in the English rugby union system.
The Royal Cornwall Golf Club (now defunct) was located on Bodmin Moor. It was founded in 1889. The club disbanded following WW2.
There is an active running club: Bodmin RoadRunners.
The "Cornish Guardian" is a weekly newspaper published every Wednesday in seven separate editions, including the Bodmin edition.
Bodmin is the home of NCB Radio, an Internet radio station which aims to bring a dedicated station to North Cornwall.
See also 
Bodmin is twinned with Bederkesa in Germany; Grass Valley, in California, United States; and Le Relecq-Kerhuon (Ar Releg-Kerhuon in Brittany), France.
W. H. Pascoe's 1979 "A Cornish Armory" gives the arms of the priory and the monastery and the seal of the borough.
On Halgaver Moor (Goats' Moor) near Bodmin there was once an annual carnival in July which was on one occasion attended by King Charles II. Halgaver is in the parish of Lanhydrock.
Bodmin Riding, a horseback procession through the town, is a traditional annual ceremony.
In 1865–66 William Robert Hicks was mayor of Bodmin, when he revived the custom of Beating the bounds of the town. He was – according to the Dictionary of National Biography – a very good man of business. This still takes place more or less every five years and concludes with a game of Cornish hurling. Hurling survives as a traditional part of beating the bounds at Bodmin, commencing at the close of the 'Beat'. The game is organised by the Rotary club of Bodmin and was last played in 2015. The game is started by the Mayor of Bodmin by throwing a silver ball into a body of water known as the "Salting Pool". There are no teams and the hurl follows a set route. The aim is to carry the ball from the "Salting Pool" via the old A30, along Callywith Road, then through Castle Street, Church Square and Honey Street to finish at the Turret Clock in Fore Street. The participant carrying the ball when it reaches the turret clock will receive a £10 reward from the mayor. 
In 2015, beating of the bounds and Cornish hurling took place at Bodmin 8 April organised by the Rotary club of Bodmin.

</doc>
<doc id="4859" url="https://en.wikipedia.org/wiki?curid=4859" title="Bodmin Moor">
Bodmin Moor

Bodmin Moor () is a granite moorland in northeastern Cornwall, England. It is in size, and dates from the Carboniferous period of geological history. It includes Brown Willy, the highest point in Cornwall, and Rough Tor, a slightly lower peak. Many of Cornwall's rivers have their sources here. It has been inhabited since at least the Neolithic era, when primitive farmers started clearing trees and farming the land. They left their megalithic monuments, hut circles and cairns, and the Bronze Age culture that followed left further cairns, and more stone circles and stone rows. By medieval and modern times, nearly all the forest was gone and livestock rearing predominated.
The name Bodmin Moor is relatively recent. An early mention is in the "Royal Cornwall Gazette" of 28 November 1812. The upland area was formerly known as Fowey Moor after the River Fowey, which rises within it.
Bodmin Moor is one of five granite plutons in Cornwall that make up part of the Cornubian batholith. The intrusion dates from the Cisuralian epoch, the earliest part of the Permian period, and outcrops across about 190 square km. Around the pluton's margins where it intruded into slates, the country rock has been hornfelsed. Numerous peat deposits occur across the moor whilst large areas are characterised by blockfields of granite boulders; both deposits are of Holocene age (see also Geology of Cornwall).
Dramatic granite tors rise from the rolling moorland: the best known are Brown Willy, the highest point in Cornwall at , and Rough Tor at . To the south-east Kilmar Tor and Caradon Hill are the most prominent hills. Considerable areas of the moor are poorly drained and form marshes (in hot summers these can dry out). The rest of the moor is mostly rough pasture or covered with heather and other low vegetation.
The moor contains about 500 holdings with around 10,000 beef cows, 55,000 breeding ewes and 1,000 horses and ponies. Most of the moor is a Site of Special Scientific Interest (SSSI), "Bodmin Moor, North", and has been designated an Area of Outstanding Natural Beauty (AONB), as part of Cornwall AONB. The moor has been identified by BirdLife International as an Important Bird Area (IBA) because it supports about 260 breeding pairs of European stonechats as well as a wintering population of 10,000 Eurasian golden plovers. The moor has also been recognised as a separate natural region and designated as national character area 153 by Natural England.
Bodmin Moor is the source of several of Cornwall's rivers: they are mentioned here anti-clockwise from the south.
The River Fowey rises at a height of and flows through Lostwithiel and into the Fowey estuary.
The River Tiddy rises near Pensilva and flows southeast to its confluence with the River Lynher (the Lynher flows generally south-east until it joins the Hamoaze near Plymouth). The River Inny rises near Davidstow and flows southeast to its confluence with the River Tamar.
The River Camel rises on Hendraburnick Down and flows for approximately before joining the sea at Padstow. The River Camel and its tributary the De Lank River are an important habitat for the otter, and both have been proposed as Special Areas of Conservation (SAC) The De Lank River rises near Roughtor and flows along an irregular course before joining the Camel south of Wenford.
The River Warleggan rises near Temple and flows south to join the Fowey.
On the southern slopes of the moor lies Dozmary Pool. It is Cornwall's only natural inland lake and is glacial in origin. In the 20th century three reservoirs have been constructed on the moor; these are Colliford Lake, Siblyback Lake and Crowdy reservoirs, which supply water for a large part of the county's population. Various species of waterfowl are resident around these waters.
The parishes on the moor are as follows:
10,000 years ago, in the Mesolithic period, hunter-gatherers wandered the area when it was wooded. There are several documented cases of flint scatters being discovered by archaeologists, indicating that these hunter-gatherers practised flint knapping in the region.
During the Neolithic era, from about 4,500 to 2,300 BC, people began clearing trees and farming the land. It was also in this era that the production of various megalithic monuments began, predominantly long cairns (three of which have currently been identified, at Louden, Catshole and Bearah) and stone circles (sixteen of which have been identified). It was also likely that the naturally forming tors were also viewed in a similar manner to the manmade ceremonial sites.
In the following Bronze Age, the creation of monuments increased dramatically, with the production of over 300 further cairns, and more stone circles and stone rows. More than 200 Bronze Age settlements with enclosures and field patterns have been recorded. and many prehistoric stone barrows and circles lie scattered across the moor. In a programme shown in 2007 Channel 4's "Time Team "investigated a 500-metre cairn and the site of a Bronze Age village on the slopes of Rough Tor.
King Arthur's Hall, thought to be a late Neolithic or early Bronze Age ceremonial site, can be found to the east of St Breward on the moor.
Where practicable, areas of the moor were used for pasture by herdsmen from the parishes surrounding the moor. Granite boulders were also taken from the moor and used for stone posts and to a certain extent for building (such material is known as moorstone). Granite quarrying only became reasonably productive when gunpowder became available.
The moor gave its name (Foweymore) to one of the medieval districts called stannaries which administered tin mining: the boundaries of these were never defined precisely. Until the establishment of a turnpike road through the moor (the present A30) in the 1770s the size of the moorland area made travel within Cornwall very difficult.
Its Cornish name, Goen Bren, is first recorded in the 12th century.
English Heritage monographs "Bodmin Moor: An Archaeological Survey" Volume 1 and Volume 2 covering the post-medieval and modern landscape are publicly available through the Archaeology Data Service.
Jamaica Inn is a traditional inn on the Moor. Built as a coaching inn in 1750 and having an association with smuggling, it was used as a staging post for changing horses.
Roughtor was the site of a medieval chapel of St Michael and is now designated as a memorial to the 43rd Wessex Division of the British Army. In 1844 on Bodmin Moor the body of 18-year-old Charlotte Dymond was discovered. Local labourer Matthew Weeks was accused of the murder, and at noon on 12 August 1844 he was led from Bodmin Gaol and hanged. The murder site now has a monument erected from public money, and the grave is at Davidstow churchyard.
Dozmary Pool is identified by some people with the lake in which, according to Arthurian legend, Sir Bedivere threw Excalibur to The Lady of the Lake. Another legend relating to the pool concerns Jan Tregeagle.
The Beast of Bodmin has been reported many times but never identified with certainty.
"Cornish Cowboy", a 2014 short documentary film screened at the 2015 Cannes Film Festival, was shot on Bodmin Moor. The film features the work of St Neot horse trainer, Dan Wilson.

</doc>
<doc id="4860" url="https://en.wikipedia.org/wiki?curid=4860" title="Berkeley, California">
Berkeley, California

Berkeley ( ) is a city on the east shore of San Francisco Bay in northern Alameda County, California. It is named after the 18th-century Irish bishop and philosopher George Berkeley. It borders the cities of Oakland and Emeryville to the south and the city of Albany and the unincorporated community of Kensington to the north. Its eastern border with Contra Costa County generally follows the ridge of the Berkeley Hills. The 2010 census recorded a population of 112,580.
Berkeley is home to the oldest campus in the University of California system, the University of California, Berkeley, and the Lawrence Berkeley National Laboratory, which is managed and operated by the University. It also has the Graduate Theological Union, one of the largest religious studies institutions in the world. Berkeley is considered one of the most socially liberal cities in the United States.
The site of today's City of Berkeley was the territory of the Chochenyo/Huchiun band of the Ohlone people when the first Europeans arrived. Evidence of their existence in the area include pits in rock formations, which they used to grind acorns, and a shellmound, now mostly leveled and covered up, along the shoreline of San Francisco Bay at the mouth of Strawberry Creek. Other artifacts were discovered in the 1950s in the downtown area during remodeling of a commercial building, near the upper course of the creek.
The first people of European descent (most of whom were of mixed race and born in America) arrived with the De Anza Expedition in 1776. The De Anza Expedition led to establishment of the Spanish Presidio of San Francisco at the entrance to San Francisco Bay (the "Golden Gate)". Luis Peralta was among the soldiers at the Presidio. For his services to the King of Spain, he was granted a vast stretch of land on the east shore of San Francisco Bay (the "contra costa", "opposite shore") for a ranch, including that portion that now comprises the City of Berkeley.
Luis Peralta named his holding "Rancho San Antonio". The primary activity of the ranch was raising cattle for meat and hides, but hunting and farming were also pursued. Eventually, Peralta gave portions of the ranch to each of his four sons. What is now Berkeley lies mostly in the portion that went to Peralta's son Domingo, with a little in the portion that went to another son, Vicente. No artifact survives of the Domingo or Vicente ranches, but their names survive in Berkeley street names (Vicente, Domingo, and Peralta). However, legal title to all land in the City of Berkeley remains based on the original Peralta land grant.
The Peraltas' Rancho San Antonio continued after Alta California passed from Spanish to Mexican sovereignty after the Mexican War of Independence. However, the advent of U.S. sovereignty after the Mexican–American War, and especially, the Gold Rush, saw the Peraltas' lands quickly encroached on by squatters and diminished by dubious legal proceedings. The lands of the brothers Domingo and Vicente were quickly reduced to reservations close to their respective ranch homes. The rest of the land was surveyed and parceled out to various American claimants ("See" Kellersberger's Map).
Politically, the area that became Berkeley was initially part of a vast Contra Costa County. On March 25, 1853, Alameda County was created from a division of Contra Costa County, as well as from a small portion of Santa Clara County. The area that became Berkeley was then the northern part of the "Oakland Township" subdivision of Alameda County. During this period, "Berkeley" was mostly a mix of open land, farms, and ranches, with a small, though busy, wharf by the bay.
In 1866, Oakland's private College of California looked for a new site. It settled on a location north of Oakland along the foot of the Contra Costa Range (later called the Berkeley Hills) astride Strawberry Creek, at an elevation about above the bay, commanding a view of the Bay Area and the Pacific Ocean through the Golden Gate.
According to the "Centennial Record of the University of California", "In 1866…at Founders' Rock, a group of College of California men watched two ships standing out to sea through the Golden Gate. One of them, Frederick Billings, thought of the lines of the Anglo-Irish Anglican Bishop George Berkeley, 'westward the course of empire takes its way,' and suggested that the town and college site be named for the eighteenth-century Anglo-Irish philosopher." The philosopher's name is pronounced "BARK-lee", but the city's name, to accommodate American English, is pronounced "BERK-lee".
The College of California's "College Homestead Association" planned to raise funds for the new campus by selling off adjacent parcels of land. To this end, they laid out a plat and street grid that became the basis of Berkeley's modern street plan. Their plans fell far short of their desires, and they began a collaboration with the State of California that culminated in 1868 with the creation of the public University of California.
As construction began on the new site, more residences were constructed in the vicinity of the new campus. At the same time, a settlement of residences, saloons, and various industries grew around the wharf area called "Ocean View". A horsecar ran from Temescal in Oakland to the university campus along what is now Telegraph Avenue. The first post office opened in 1872.
By the 1870s, the Transcontinental Railroad reached its terminus in Oakland. In 1876, a branch line of the Central Pacific Railroad, the Berkeley Branch Railroad, was laid from a junction with the mainline called Shellmound (now a part of Emeryville) into what is now downtown Berkeley. That same year, the mainline of the transcontinental railroad into Oakland was re-routed, putting the right-of-way along the bay shore through Ocean View.
There was a strong prohibition movement in Berkeley at this time. In 1876, the state enacted the "mile limit law", which forbade sale or public consumption of alcohol within one mile (1.6 km) of the new University of California. Then, in 1899 Berkeley residents voted to make their city an alcohol-free zone. Scientists, scholars and religious leaders spoke vehemently of the dangers of alcohol.
On April 1, 1878, the people of Ocean View and the area around the university campus, together with local farmers, were granted incorporation by the State of California as the Town of Berkeley. The first elected trustees of the town were the slate of Denis Kearney's Workingman's Party, who were particularly favored in the working class area of the former Ocean View, now called "West Berkeley". The area near the university became known for a time as "East Berkeley".
Due to the influence of the university, the modern age came quickly to Berkeley. Electric lights and the telephone were in use by 1888. Electric streetcars soon replaced the horsecar. A silent film of one of these early streetcars in Berkeley can be seen at the Library of Congress website.
Berkeley's slow growth ended abruptly with the Great San Francisco earthquake of 1906. The town and other parts of the East Bay escaped serious damage, and thousands of refugees flowed across the Bay. Among them were most of San Francisco's painters and sculptors, who between 1907 and 1911 created one of the largest art colonies west of Chicago. Artist and critic Jennie V. Cannon described the founding of the Berkeley Art Association and the rivalries of competing studios and art clubs.
In 1904, the first hospitals in Berkeley were created: the Alta Bates Sanatorium for women and children, founded by nurse Alta Bates on Walnut Street, and the Roosevelt (later, Herrick) Hospital, by Dr. LeRoy Francis Herrick, on the corner of Dwight Way and Milvia Street.
In 1908, a statewide referendum that proposed moving the California state capital to Berkeley was defeated by a margin of about 33,000 votes. The city named streets around the proposed capitol grounds for California counties. They bear those names today, a legacy of the failed referendum.
On March 4, 1909, following public referendums, the citizens of Berkeley were granted a new charter by the State of California, and the Town of Berkeley became the City of Berkeley. Rapid growth continued up to the Crash of 1929. The Great Depression hit Berkeley hard, but not as hard as many other places in the U.S., thanks in part to the university.
On September 17, 1923, a major fire swept down the hills toward the university campus and the downtown section. Around 640 structures burned before a late afternoon sea breeze stopped its progress, allowing firefighters to put it out.
The next big growth occurred with the advent of World War II, when large numbers of people moved to the Bay Area to work in the many war industries, such as the immense Kaiser Shipyards in nearby Richmond. One who moved out, but played a big role in the outcome of the War was U.C. Professor and Berkeley resident J. Robert Oppenheimer. During the war, an Army base, Camp Ashby, was temporarily sited in Berkeley.
The element berkelium was synthesized utilizing the 60-inch cyclotron at UC Berkeley, and named in 1949, recognizing the university, thus also placing the city's name in the list of elements.
During the 1940s, many African Americans migrated to Berkeley. In 1950, the Census Bureau reported Berkeley's population as 11.7% black and 84.6% white.
The postwar years brought moderate growth to the city, as events on the U.C. campus began to build up to the recognizable activism of the sixties. In the 1950s, McCarthyism induced the university to demand a loyalty oath from its professors, many of whom refused to sign the oath on the principle of freedom of thought. In 1960, a U.S. House committee (HUAC) came to San Francisco to investigate the influence of communists in the Bay Area. Their presence was met by protesters, including many from the university. Meanwhile, a number of U.C. students became active in the civil rights movement. Finally, in 1964, the university provoked a massive student protest by banning distribution of political literature on campus. This protest became the Free Speech Movement. As the Vietnam War rapidly escalated in the ensuing years, so did student activism at the university, particularly that organized by the Vietnam Day Committee.
Berkeley is strongly identified with the rapid social changes, civic unrest, and political upheaval that characterized the late 1960s. In that period, Berkeley—especially Telegraph Avenue—became a focal point for the hippie movement, which spilled over the Bay from San Francisco. Many hippies were apolitical drop-outs, rather than students, but in the heady atmosphere of Berkeley in 1967–1969 there was considerable overlap between the hippie movement and the radical left. An iconic event in the Berkeley Sixties scene was a conflict over a parcel of university property south of the contiguous campus site that came to be called "People's Park."
The battle over the disposition of People's Park resulted in a month-long occupation of Berkeley by the National Guard on orders of then-Governor Ronald Reagan. In the end, the park remained undeveloped, and remains so today. A spin-off, "People's Park Annex", was established at the same time by activist citizens of Berkeley on a strip of land above the Bay Area Rapid Transit subway construction along Hearst Avenue northwest of the U.C. campus. The land had also been intended for development, but was turned over to the city by BART and is now Ohlone Park.
The era of large public protest in Berkeley waned considerably with the end of the Vietnam War in 1975. While the 1960s were the heyday of liberal activism in Berkeley, it remains one of the most overwhelmingly Democratic cities in the United States.
The Berkeley population declined in the 1970s, partly due to an exodus to the suburbs. Some moved because of the rising cost of living throughout the Bay Area, and others because of the decline and disappearance of many industries in West Berkeley.
Increasing enrollment at the university led to replacement of older buildings by large apartment buildings, especially in older parts of the city near the University and downtown. Increasing enrollment also led the university to wanting to redevelop certain places of Berkeley, especially Southside, but more specifically People's Park. Preservationists passed the Neighborhood Protection Ordinance in 1973 by ballot measure and the Landmarks Preservation Ordinance in 1974 by City Council. Together, these ordinances brought most new construction to a halt. Facing rising housing costs, residents voted to enact rent control and vacancy control in 1980. Though more far-reaching in their effect than those of some of the other jurisdictions in California that chose to use rent-control where they could, these policies were limited by the Costa-Hawkins Act, a statewide ban on rent control that came into effect in 1995 and limited rent control to multi-family units that were built (or technically buildings that were issued their original certificate of occupation) before the state law came into effect in 1995. For cities such as Berkeley, where rent-control was already in place, the law limited the use of rent-control to units built before the local rent-control law was enacted, i.e. 1980.
During the 1970s and 1980s, activists increased their power in local government. This era also saw major developments in Berkeley's environmental and food culture. Berkeley's last Republican mayor, Wallace J.S. Johnson, left office in 1971. Alice Waters opened Chez Panisse in 1971. The first curbside recycling program in the U.S. was started by the Ecology Center in 1973. Styrofoam was banned in 1988.
As the city leaned more and more Democratic, local politics became divided between "Progressives" and "Moderates". 1984 saw the Progressives take the majority for the first time. Nancy Skinner became the first UC Berkeley student elected to City Council. In 1986, in reaction to the 1984 election, a ballot measure switched Berkeley from at-large to district-based elections for city council.
In 1983, Berkeley's Domestic Partner Task Force was established, which in 1984 made policy recommendation to the school board, which passed domestic partner legislation. The legislation became a model for similar measures nationwide.
In 1995, California's Costa-Hawkins Rental Housing Act ended vacancy control, allowing rents to increase when a tenant moved out. Despite a slow down in 2005–2007, median home prices and rents remain dramatically higher than the rest of the nation, fueled by spillover from the San Francisco housing shortage and population growth.
South and West Berkeley underwent gentrification, with some historically Black neighborhoods such as the Adeline Corridor seeing a 50% decline in Black / African American population from 1990 to 2010. In the 1990s, Public Television's Frontline documentary series featured race relations at Berkeley's only public high school, Berkeley High School.
With an economy dominated by the University of California and a high-demand housing market, Berkeley was relatively unaffected by the Great Recession. State budget cuts caused the University to increase the number of out-of-state and international students, with international enrollment, mostly from Asia, rising from 2,785 in 2007 to 5,951 in 2016. Since then, more international restaurants have opened downtown and on Telegraph Avenue, including East Asian chains such as Ippudo and Daiso.
A wave of downtown apartment construction began in 1998.
In 2006, the Berkeley Oak Grove Protest began protesting construction of a new sports center annex to Memorial Stadium at the expense of a grove of oak trees on the UC campus. The protest ended in September 2008 after a lengthy court process.
In 2007–2008, Berkeley received media attention due to demonstrations against a Marine Corps recruiting office in downtown Berkeley and a series of controversial motions by Berkeley's city council regarding opposition to Marine recruiting. ("See" Berkeley Marine Corps Recruiting Center controversy.)
During the fall of 2010, the Berkeley Student Food Collective opened after many protests on the UC Berkeley campus due to the proposed opening of the fast food chain Panda Express. Students and community members worked together to open a collectively run grocery store right off of the UC Berkeley campus, where the community can buy local, seasonal, humane, and organic foods. The Berkeley Student Food Collective still operates at 2440 Bancroft Way.
On September 18, 2012, Berkeley became what may be the first city in the U.S. to officially proclaim a day recognizing bisexuals September 23, which is known as Celebrate Bisexuality Day.
On September 2, 2014, the city council approved a measure to provide free medical marijuana to low-income patients.
The Measure D soda tax was approved by Berkeley voters on November 4, 2014, the first such tax in the United States.
In the Fall of 2011, the nationwide Occupy Wall Street movement came to two Berkeley locations: on the campus of the University of California and as an encampment in Civic Center Park.
During a Black Lives Matter protest on December 6, 2014, police use of tear gas and batons to clear protesters from Telegraph Avenue led to a riot and five consecutive days and nights of protests, marches, and freeway occupations in Berkeley and Oakland. Afterwards, changes were implemented by the Police Department to avoid escalation of violence and to protect bystanders during protests.
During a protest against bigotry and President Trump in August 2017, anti-fascist protesters grew violent against Trump supporters in attendance. Police intervened, arresting 14 people. Sometimes called "antifa", these anti-fascist activists were clad in all black, while some carried shields and others had masks or bandanas hiding their faces. These protests spanned February to September 2017 (See more at 2017 Berkeley Protests).
In 2019, protesters took up residence in People's Park against tree-chopping and were arrested by police in riot gear. Many activists saw this as the university preparing to develop the park.
The city of Berkeley has historically been a central location for homeless communities in the Bay Area. Since the 1930s, the city of Berkeley has fostered a tradition of political activism. However, though the city has been perceived as a hub for liberal thought and action, it has passed ordinances to oust homeless individuals from Berkeley on multiple occasions. Despite efforts to remove unhoused individuals from the streets and projects to improve social service provision for this demographic, homelessness has continued to be a significant problem in Berkeley.
A culture of anti-establishment and sociopolitical activism marked the 1960s. The San Francisco Bay Area became a hotspot for hippie counterculture, and Berkeley became a haven for nonconformists and anarchists from all over the United States. Most public discourse around homelessness in Berkeley at this time was centered around the idea of street-living as an expression of counterculture.
During the Free Speech Movement in the Fall of 1964, Berkeley became a hub of civil unrest, with demonstrators and UC Berkeley students sympathizing with the statewide protests for free speech and assembly, as well as revolting against university restrictions against student political activities and organizations established by UC President Clark Kerr in 1959. Many non-student youth and adolescents sought alternative lifestyles and opted for voluntary homelessness during this time.
In 1969, People's Park was created and eventually became a haven for “small-time drug dealers, street people, and the homeless”. Although the City of Berkeley has moved unhoused individuals from its streets, sometimes even relocating them to an unused landfill, People's Park has remained a safe space for them since its inception. The park has become one of the few relatively safe spaces for homeless individuals to congregate in Berkeley and the greater Bay Area.
Stereotypes of homeless people as deviant individuals who chose to live vagrant lifestyles continued to color the discourse around street-dwellers in American cities. However, this time period was also characterized by a subtle shift in the perception of unhoused individuals. The public began to realize that homelessness affected not only single men, but also women, children, and entire families. This recognition set the stage for the City of Berkeley's attitude towards homelessness in the next decade.
Federal policy changes led to increased rates of homelessness in California, and the deinstitutionalization of those with mental conditions led to greater visibility of the homeless. Although homelessness increased substantially during the 1980s, the deinstitutionalization of the mentally ill has been occurring steadily since the mid-1950s. Large-scale deinstitutionalization of the mentally ill in the last quarter of the 20th century coincided with growth in the number of public shelters and increased visibility of the homeless. Organizations such as Building Opportunities for Self Sufficiency (BOSS) were established in 1971 in response to the needs of mentally ill individuals being released to the streets by state hospital closures.
In the 1990s, the City of Berkeley faced a substantial increase in the need for emergency housing shelters and saw a rise in the average amount of time individuals spent without stable housing. As housing became a more widespread problem, the general public, Berkeley City Council, and the University of California became increasingly anti-homeless in their opinions. In 1994, Berkeley City Council considered the implementation of a set of anti-homeless laws that the "San Francisco Chronicle" described as being "among the strictest in the country". These laws prohibited sitting, sleeping and begging in public spaces, and outlawed panhandling from people in a variety of contexts, such as sitting on public benches, buying a newspaper from a rack, or waiting in line for a movie. In February 1995, the American Civil Liberties Union (ACLU) sued the City for infringing free speech rights through its proposed anti-panhandling law. In May of that same year, a federal judge ruled that the anti-panhandling law did violate the First Amendment, but left the anti-sitting and sleeping laws untouched.
Following the implementation of these anti-sitting and sleeping ordinances in 1998, Berkeley increased its policing of homeless adults and youth, particularly in the shopping district surrounding Telegraph Avenue. The mayor at that time, Shirley Dean, proposed a plan to increase both social support services for homeless youth and enforcement of anti-encampment laws. Unhoused youth countered this plan with a request for the establishment of the city's first youth shelter, more trash cans, and more frequent cleaning of public bathrooms.
The City of Berkeley's 2017 annual homeless report and point-in-time count (PIT) estimate that on a given night, 972 people are homeless. Sixty-eight percent (664 people) of these individuals are also unsheltered, living in places not considered suitable for human habitation, such as cars or streets. Long-term homelessness in Berkeley is double the national average, with 27% of the city's homeless population facing chronic homelessness. Chronic homelessness has been on the rise since 2015, and has been largely a consequence of the constrained local housing market. In 2015, rent in Alameda County increased by 25%, while the average household income only grew by 5%. This disparity not only contributes to the growing homeless population in Berkeley, but also presents an increased need for more affordable housing in the greater East Bay. According to the 2017 report, only 3% of unhoused Berkeley individuals reported that they would be "uninterested in permanent, affordable housing if it were available", demonstrating that homelessness in Berkeley is largely an issue of economic insecurity rather than individual choice, as it once was. The over-representation of unsheltered minorities, primarily African-Americans, in Berkeley is a consequence of systemic and institutionalized racism and economic disadvantages, preventing homeless people of color from competing for rising rent, thus limiting access to housing for minority groups in Berkeley. The City of Berkeley's 2017 report also estimated the number of unaccompanied youth in Berkeley at 189 individuals, 19% of the total homeless population in the city. Homeless youth display greater risk of mental health issues, behavioral problems, and substance abuse, than any other homeless age group. Furthermore, homeless youth identifying as LGBTQ+ are exposed to greater rates of physical and sexual abuse, and higher risk for sexually-transmitted diseases, predominantly HIV.
The City of Berkeley has seen a consistent rise in the number of chronically homeless individuals over the past 30 years, and has implemented a number of different projects to reduce the number of people living on the streets. In 2008, the City focused its efforts on addressing chronic homelessness. This led to a 48% decline in the number of chronically homeless individuals reported in the 2009 Berkeley PIT. However, the number of "hidden homeless" individuals (those coping with housing insecurity by staying at a friend or relative's residence), increased significantly, likely in response to rising housing costs and costs of living. In 2012, the City considered measures that banned sitting in commercial areas throughout Berkeley. The measure was met with strong public opposition and did not pass. However, the City saw a strong need for it to implement rules addressing encampments and public usage of space as well as assessing the resources needed to assist the unhoused population. In response to these needs the City of Berkeley established the Homeless Task Force, headed by then-Councilmember Jesse Arreguín. Since its formation, the Task Force has proposed a number of different recommendations, from expanding the City Homeless Outreach and Mobile Crisis Teams, to building a short-term transitional shelter for unhoused individuals.
With the political activism of the UC, Berkeley has historically been vocal about the housing crisis that affects students and locals alike. An example of these efforts, to create and maintain space for those who cannot fight for themselves, lies in the movement to preserve People's Park as a place for the homeless population to call its own, instead of destroying it to make room for more student housing in the area. The efforts made by the community to create and maintain space for the homeless population in Berkeley did not stop there. With the history of homelessness and lack of affordable housing, there have been masses of organizations opening up with the sole mission to help this vulnerable population with not only housing assistance, but other symptoms that derive from homelessness. These organizations have stemmed from church groups, non-profits, even the UC. One of the many UC Berkeley student run programs that focuses on assisting the homeless is the Suitcase Clinic. The Suitcase Clinic was established in the late 1980s by undergraduate and graduate level students to provide direct medical services to the homeless and underrepresented population of Berkeley. Services provided by students have altered over the years to cater to the needs of the homeless population, and now include not only professional medical and dental support, but also health education, foot-washing, child care, a hot meal, and services that promote mental well-being.
Berkeley is located at (37.871775, −122.274603).
According to the United States Census Bureau the city's area includes of land and (40.83%) water, most of it part of San Francisco Bay.
Berkeley borders the cities of Albany, Oakland, and Emeryville and Contra Costa County, including unincorporated Kensington, as well as San Francisco Bay.
Berkeley lies within telephone area code 510 (until September 2, 1991, Berkeley was part of the 415 telephone code that now covers only San Francisco and Marin counties), and the postal ZIP codes are 94701 through 94710, 94712, and 94720 for the University of California campus.
Most of Berkeley lies on a rolling sedimentary plain that rises gently from sea level to the base of the Berkeley Hills. East of the Hayward Fault along the base of the hills, elevation increases more rapidly. The highest peak along the ridge line above Berkeley is Grizzly Peak, elevation . A number of small creeks run from the hills to the Bay through Berkeley: Cerrito, Codornices, Schoolhouse and Strawberry Creeks are the principal streams. Most of these are largely culverted once they reach the plain west of the hills.
The Berkeley Hills are part of the Pacific Coast Ranges, and run in a northwest–southeast alignment. Exposed in the Berkeley Hills are cherts and shales of the Claremont Formation (equivalent to the Monterey Formation), conglomerate and sandstone of the Orinda Formation and lava flows of the Moraga Volcanics. Of similar age to the Moraga Volcanics (extinct), within the Northbrae neighborhood of Berkeley, are outcroppings of erosion resistant rhyolite. These rhyolite formations can be seen in several city parks and in the yards of a number of private residences. Indian Rock Park in the northeastern part of Berkeley near the Arlington/Marin Circle features a large example.
Berkeley is traversed by the Hayward Fault Zone, a major branch of the San Andreas Fault to the west. No large earthquake has occurred on the Hayward Fault near Berkeley in historic times (except possibly in 1836), but seismologists warn about the geologic record of large temblors several times in the deeper past. The current assessment is that a Bay Area earthquake of magnitude 6.7 or greater within the next 30 years is likely, with the Hayward Fault having the highest likelihood among faults in the Bay Area of being the epicenter. Moreover, like much of the Bay Area, Berkeley has many areas of some risk to soil liquefaction, with the flat areas closer to the shore at low to high susceptibility.
The 1868 Hayward earthquake did occur on the southern segment of the Hayward Fault in the vicinity of today's city of Hayward. This quake destroyed the county seat of Alameda County then located in San Leandro and it subsequently moved to Oakland. It was strongly felt in San Francisco, causing major damage. It was regarded as the "Great San Francisco earthquake" prior to 1906. It produced a furrow in the ground along the fault line in Berkeley, across the grounds of the new State Asylum for the Deaf, Dumb and Blind then under construction, which was noted by one early University of California professor. Although no significant damage was reported to most of the few Berkeley buildings of the time, the 1868 quake did destroy the vulnerable adobe home of Domingo Peralta in north Berkeley.
Today, evidence of the Hayward Fault's "creeping" is visible at various locations in Berkeley. Cracked roadways, sharp jogs in streams, and springs mark the fault's path. However, since it cuts across the base of the hills, the creep is often concealed by or confused with slide activity. Some of the slide activity itself, however, results from movement on the Hayward Fault.
A notorious segment of the Hayward Fault runs lengthwise down the middle of Memorial Stadium at the mouth of Strawberry Canyon on the University of California campus. Photos and measurements show the movement of the fault through the stadium.
Berkeley has a cool summer Mediterranean climate (type Csb in the Köppen climate classification), with dry summers and wet winters. Berkeley's location directly opposite the Golden Gate ensures that typical eastward fog flow blankets the city more often than its neighbors. The summers are cooler than a typical Mediterranean climate thanks to upwelling ocean currents along the California coast. These help produce cool and foggy nights and mornings.
Winter is punctuated with rainstorms of varying ferocity and duration, but also produces stretches of bright sunny days and clear cold nights. It does not normally snow, though occasionally the hilltops get a dusting. Spring and fall are transitional and intermediate, with some rainfall and variable temperature. Summer typically brings night and morning low clouds or fog, followed by sunny, warm days. The warmest and driest months are typically June through September, with the highest temperatures occurring in September. Mid-summer (July–August) is often a bit cooler due to the sea breezes and fog common then.
In a year, there are an average of 2.9 days with highs of or higher, and an average of 0.8 days with lows of or lower. The highest recorded temperature was on June 15, 2000 and July 16, 1993, and the lowest recorded temperature was on December 22, 1990.
January is normally the wettest month, averaging of precipitation. Average annual precipitation is , falling on an average of 63.7 days each year. The most rainfall in one month was in February 1998. The most rainfall in 24 hours was on January 4, 1982. As in most of California, the heaviest rainfall years are usually associated with warm water El Niño episodes in the Pacific (e.g., 1982–83; 1997–98), which bring in drenching "pineapple express" storms. In contrast, dry years are often associated with cold Pacific La Niña episodes. Light snow has fallen on rare occasions. Snow has generally fallen every several years on the higher peaks of the Berkeley Hills.
In the late spring and early fall, strong offshore winds of sinking air typically develop, bringing heat and dryness to the area. In the spring, this is not usually a problem as vegetation is still moist from winter rains, but extreme dryness prevails by the fall, creating a danger of wildfires. In September 1923 a major fire swept through the neighborhoods north of the university campus, stopping just short of downtown. ("See" 1923 Berkeley fire). On October 20, 1991, gusty, hot winds fanned a conflagration along the Berkeley–Oakland border, killing 25 people and injuring 150, as well as destroying 2,449 single-family dwellings and 437 apartment and condominium units. ("See" 1991 Oakland firestorm)
The 2010 United States Census reported that Berkeley had a population of 112,580. The population density was 10,752 people per square mile of land area (4,104/km). The racial makeup of Berkeley was 66,996 (59.5%) White, 11,241 (10.0%) Black or African American, 479 (0.4%) Native American, 21,690 (19.3%) Asian (8.4% Chinese, 2.4% Indian, 2.1% Korean, 1.6% Japanese, 1.5% Filipino, 1.0% Vietnamese), 186 (0.2%) Pacific Islander, 4,994 (4.4%) from other races, and 6,994 (6.2%) from two or more races. There were 12,209 people (10.8%) of Hispanic or Latino ancestry, of any race. 6.8% of the city's population was of Mexican ancestry.
The Census reported that 99,731 people (88.6% of the population) lived in households, 12,430 (11.0%) lived in non-institutionalized group quarters, and 419 (0.4%) were institutionalized.
There were 46,029 households, out of which 8,467 (18.4%) had children under the age of 18 living in them, 13,569 (29.5%) were opposite-sex married couples living together, 3,855 (8.4%) had a female householder with no husband present, 1,368 (3.0%) had a male householder with no wife present. There were 2,931 (6.4%) unmarried opposite-sex partnerships, and 961 (2.1%) same-sex married couples or partnerships. 16,904 households (36.7%) were made up of individuals, and 4,578 (9.9%) had someone living alone who was 65 years of age or older. The average household size was 2.17. There were 18,792 families (40.8% of all households); the average family size was 2.81. There were 49,454 housing units at an average density of 2,794.6 per square mile (1,079.0/km), of which 46,029 were occupied, of which 18,846 (40.9%) were owner-occupied, and 27,183 (59.1%) were occupied by renters. The homeowner vacancy rate was 1.0%; the rental vacancy rate was 4.5%. 45,096 people (40.1% of the population) lived in owner-occupied housing units and 54,635 people (48.5%) lived in rental housing units.
The population was spread out, with 13,872 people (12.3%) under the age of 18, 30,295 people (26.9%) aged 18 to 24, 30,231 people (26.9%) aged 25 to 44, 25,006 people (22.2%) aged 45 to 64, and 13,176 people (11.7%) who were 65 years of age or older. The median age was 31.0 years. For every 100 females, there were 95.6 males. For every 100 females age 18 and over, there were 94.2 males.
According to the 2011 American Community Survey 5-Year estimate, the median income for a household in the city was $60,908, and the median income for a family was $102,976. Males had a median income of $67,476 versus $57,319 for females. The per capita income for the city was $38,896. About 7.2% of families and 18.3% of the population were below the poverty line, including 13.2% of those under age 18 and 9.2% of those age 65 or over.
Berkeley has a higher-than-average crime rate, particularly property crime, though the crime rate has fallen significantly since 2000.
According to 2018 US Census Bureau estimates, Berkeley's population was 52.7% White (48.3% Non-Hispanic White and 4.4% Hispanic White), 8.1% Black or African American, 0.6% Native American and Alaskan Native, 24.5% Asian, 0.6% Pacific Islander, 5.5% Some Other Race, and 8.0% from two or more races. 
If Hispanics are treated as a separate category from race, Berkeley's population was 48.3% White, 8.1% Black or African American, 0.3% Native American and Alaskan Native, 24.5% Asian, 0.6% Pacific Islander, 0.1% Some Other Race, 5.9% from two or more races, and 12.3% Hispanic-Latino. 
White Americans remain the largest racial/ethnic group at either 52.7% (including White Hispanics) or 48.3% (excluding White Hispanics).
The Asian population continues to remain the second largest group at 24.5% of the population.
The Black population remains below the national average at 8.1% of the population.
By ethnicity, 12.3% of the total population is Hispanic-Latino (of any race) and 87.7% is Non-Hispanic (of any race). If treated as a category separate from race, Hispanics are the second largest minority group in Berkeley. Most Hispanics self-identify as Other Race (44.1%) with the remainder choosing White (35.8%), Multiracial (17.2%), American Indian and Alaskan Native (1.9%), Asian (0.5%), Black (0.3%), and Hawaiian and Pacific Islander (0.3%).
Berkeley is served by Amtrak (Capitol Corridor), AC Transit, BART (Ashby, Downtown Berkeley Station and North Berkeley) and bus shuttles operated by major employers including UC Berkeley and Lawrence Berkeley National Laboratory. The Eastshore Freeway (Interstate 80 and Interstate 580) runs along the bay shoreline. Each day there is an influx of thousands of cars into the city by commuting UC faculty, staff and students, making parking for more than a few hours an expensive proposition.
Berkeley has one of the highest rates of bicycle and pedestrian commuting in the nation. Berkeley is the safest city of its size in California for pedestrians and cyclists, considering the number of injuries per pedestrian and cyclist, rather than per capita.
Berkeley has modified its original grid roadway structure through use of diverters and barriers, moving most traffic out of neighborhoods and onto arterial streets (visitors often find this confusing, because the diverters are not shown on all maps). Berkeley maintains a separate grid of arterial streets for bicycles, called Bicycle Boulevards, with bike lanes and lower amounts of car traffic than the major streets they often parallel.
Berkeley hosts car sharing networks including Uhaul Car Share, and Zipcar. Rather than owning (and parking) their own cars, members share a group of cars parked nearby. Web- and telephone-based reservation systems keep track of hours and charges. Several "pods" (points of departure where cars are kept) exist throughout the city, in several downtown locations, at the Ashby and North Berkeley BART stations, and at various other locations in Berkeley (and other cities in the region). Using alternative transportation is encouraged.
Berkeley has had recurring problems with parking meter vandalism. In 1999, over 2,400 Berkeley meters were jammed, smashed, or sawed apart. Starting in 2005 and continuing into 2006, Berkeley began to phase out mechanical meters in favor of more centralized electronic meters.
The first commuter service to San Francisco was provided by the Central Pacific's Berkeley Branch Railroad, a standard gauge steam railroad, which terminated in downtown Berkeley, and connected in Emeryville (at a locale then known as "Shellmound") with trains to the Oakland ferry pier as well as with the Central Pacific main line starting in 1876. The Berkeley Branch line was extended from Shattuck and University to Vine Street ("Berryman's Station") in 1878. Starting in 1882, Berkeley trains ran directly to the Oakland Pier. In the 1880s, Southern Pacific assumed operations of the Berkeley Branch under a lease from its own paper affiliate, the Northern Railway. In 1911, Southern Pacific electrified this line and the several others it constructed in Berkeley, creating its East Bay Electric Lines division. The huge and heavy cars specially built for these lines were called the "Red Trains" or the "Big Red Cars." The Shattuck line was extended and connected with two other Berkeley lines (the Ninth Street Line and the California Street line) at Solano and Colusa (the "Colusa Wye"). At this time, the Northbrae Tunnel and Rose Street Undercrossing were constructed, both of which still exist. (The Rose Street Undercrossing is not accessible to the public, being situated between what is now two backyards.) The fourth Berkeley line was the Ellsworth St. line to the university campus. The last Red Trains ran in July 1941.
The first electric rail service in Berkeley was provided by several small streetcar companies starting in 1891. Most of these were eventually bought up by the Key System of Francis "Borax" Smith who added lines and improved equipment. The Key System's streetcars were operated by its East Bay Street Railways division. Principal lines in Berkeley ran on Euclid, The Arlington, College, Telegraph, Shattuck, San Pablo, University, and Grove (today's Martin Luther King Jr. Way). The last streetcars ran in 1948, replaced by buses.
The first electric commuter interurban-type trains to San Francisco from Berkeley were put in operation by the Key System in 1903, several years before the Southern Pacific electrified its steam commuter lines. Like the SP, Key trains ran to a pier serviced by the Key's own fleet of ferryboats, which also docked at the Ferry Building in San Francisco. After the Bay Bridge was built, the Key trains ran to the Transbay Terminal in San Francisco, sharing tracks on the lower deck of the Bay Bridge with the SP's red trains and the Sacramento Northern Railroad. It was at this time that the Key trains acquired their letter designations, which were later preserved by Key's public successor, AC Transit. Today's F bus is the successor of the F train. Likewise, the E, G and the H. Before the Bridge, these lines were simply the Shattuck Avenue Line, the Claremont Line, the Westbrae Line, and the Sacramento Street Line, respectively.
After the Southern Pacific abandoned transbay service in 1941, the Key System acquired the rights to use its tracks and catenary on Shattuck north of Dwight Way and through the Northbrae Tunnel to The Alameda for the F-train. The SP tracks along Monterey Avenue as far as Colusa had been acquired by the Key System in 1933 for the H-train, but were abandoned in 1941. The Key System trains stopped running in April 1958. On December 15, 1962, the Northbrae Tunnel was opened to auto traffic.
According to the city's 2019 Comprehensive Annual Financial Report, the top employers in the city are:
Berkeley is the location of a number of nationally prominent businesses, many of which have been pioneers in their areas of operation. Notable businesses include Chez Panisse, birthplace of California cuisine, Peet's Coffee's original store, the Claremont Resort, punk rock haven 924 Gilman, and Saul Zaentz's Fantasy Studios. Notable former businesses include pioneer bookseller Cody's Books, The Nature Company, Clif Bar energy foods; and the Berkeley Co-op.
Berkeley has relatively few chain stores for a city of its size, due to policies and zoning that promote small businesses and limits to the size of certain types of stores.
Berkeley has a number of distinct neighborhoods. Surrounding the University of California campus are the most densely populated parts of the city. West of the campus is Downtown Berkeley, the city's traditional commercial core; home of the civic center, the city's only public high school, the busiest BART station in Berkeley, as well as a major transfer point for AC Transit buses. South of the campus is Southside, mainly a student ghetto, where much of the university's student housing is located. The busiest stretch of Telegraph Avenue is in this neighborhood. North of the campus is the quieter Northside neighborhood, the location of the Graduate Theological Union.
Farther from the university campus, the influence of the university quickly becomes less visible. Most of Berkeley's neighborhoods are primarily made up of detached houses, often with separate in-law units in the rear, although larger apartment buildings are also common in many neighborhoods. Commercial activities are concentrated along the major avenues and at important intersections and frequently define the neighborhood within which they reside.
In the southeastern corner of the city is the Claremont District, home to the Claremont Hotel. Also in the southeast is the Elmwood District known for its commercial area on College Avenue. West of Elmwood is South Berkeley, known for its weekend flea market at the Ashby Station.
West of (and including) San Pablo Avenue, itself a major commercial and transport corridor, is West Berkeley, the historic commercial center of the city. This neighborhood and area includes the former unincorporated town of Ocean View. West Berkeley contains the remnants of Berkeley's industrial area, much of which has been replaced by retail and office uses, as well as residential live/work loft space, paralleling the decline of manufacturing in the United States. This area abuts the shoreline of the San Francisco Bay and is home to the Berkeley Marina. Also nearby is Berkeley's Aquatic Park, featuring an artificial linear lagoon of San Francisco Bay.
North of downtown is North Berkeley which has its main commercial area nicknamed the "Gourmet Ghetto" because of the concentration of well-known restaurants and other food-related businesses. West of North Berkeley (roughly west of Sacramento and north of Cedar) is Westbrae, a small neighborhood centered on a small commercial area on Gilman Street and through which part of the Ohlone Greenway runs. Meanwhile, further north of North Berkeley are Northbrae, a master-planned subdivision from the early 20th century, and Thousand Oaks. Above these last three neighborhoods, on the western slopes of the Berkeley Hills are the neighborhoods of Cragmont and La Loma Park, notable for their dramatic views, winding streets, and numerous public stairways and paths.
The city has many parks, and promotes greenery and the environment. The city has planted trees for years and is a leader in the nationwide effort to re-tree urban areas. Tilden Regional Park, lies east of the city, occupying the upper extent of Wildcat Canyon between the Berkeley Hills and the San Pablo Ridge. The city is also heavily involved in creek restoration and wetlands restoration, including a planned "daylighting" of Strawberry Creek along Center Street. The Berkeley Marina and East Shore State Park flank its shoreline at San Francisco Bay and organizations like the Urban Creeks Council and Friends of the Five Creeks the former of which is headquartered in Berkeley support the riparian areas in the town and coastlines as well. César Chávez Park, near the Berkeley Marina, was built at the former site of the city dump.
165 buildings in Berkeley are designated as local landmarks or local structures of merit. Of these, 49 are listed in the National Register of Historic Places, including:
Historic districts listed in the National Register of Historic Places:
See "List of Berkeley Landmarks, Structures of Merit, and Historic Districts"
Berkeley is home to the Chilean-American community's La Peña Cultural Center, the largest cultural center for this community in the United States. The Freight and Salvage is the oldest established full-time folk and traditional music venue west of the Mississippi River.
Additionally, Berkeley is home to the off-broadway theater Berkeley Repertory Theater, commonly known as "Berkeley Rep". The Berkeley Repertory Theater consists of two stages, a school, and has received a Tony Award for Outstanding Regional Theatre. The historic Berkeley Art Museum and Pacific Film Archive (BAMPFA) is operated by UC Berkeley, and was moved to downtown Berkeley in January 2016. It offers many exhibitions and screenings of historic films, as well as outreach programs within the community.
University of California, Berkeley's main campus is in the city limits.
The Graduate Theological Union, a consortium of eight independent theological schools, is located a block north of the University of California Berkeley's main campus. The Graduate Theological Union has the largest number of students and faculty of any religious studies doctoral program in the United States. In addition to more theological schools, Zaytuna College, a newly established Muslim liberal arts college, has taken 'Holy Hill' as its new home. The Institute of Buddhist Studies has been located in Berkeley since 1966. Wright Institute, a psychology graduate school, is located in Berkeley. Berkeley City College is a community college in the Peralta Community College District.
The Berkeley Unified School District operates public schools.
The first public school in Berkeley was the Ocean View School, now the site of the Berkeley Adult School located at Virginia Street and San Pablo Avenue. The public schools today are administered by the Berkeley Unified School District. In the 1960s, Berkeley was one of the earliest US cities to voluntarily desegregate, utilizing a system of buses, still in use. The city has one public high school, Berkeley High School (BHS). Established in 1880, BHS currently has over 3,000 students. The Berkeley High campus was designated a historic district by the National Register of Historic Places on January 7, 2008. Saint Mary's College High School, a Catholic school, also has its street address in Berkeley, although most of the grounds and buildings are actually in neighboring Albany. Berkeley has 11 public elementary schools and three middle schools.
The East Bay campus of the German International School of Silicon Valley (GISSV) formerly occupied the Hillside Campus, Berkeley, California; it opened there in 2012. In December 2016, the GISSV closed the building, due to unmet seismic retrofit needs.
There is also the Bay Area Technology School, the only school in the whole Bay Area to offer a technology- and science-based curriculum, with connections to leading universities.
Berkeley also houses Zaytuna College, the first accredited Muslim, liberal-arts college in the United States.
Berkeley Public Library serves as the municipal library. University of California, Berkeley Libraries operates the University of California Berkeley libraries.
Berkeley has a council–manager government. The mayor is elected at-large for a four-year term and is the ceremonial head of the city and the chair of the city council. The Berkeley City Council is composed of the mayor and eight council members elected by district who each serve four-year terms. Districts 2, 3, 5 and 6 hold their elections in years divisible by four while Districts 1, 4, 7 and 8 hold theirs in even-numbered years not divisible by four. The city council appoints a city manager, who is the chief executive of the city. Additionally, the city voters directly elect an independent city auditor, school board, and rent stabilization board. The current council members and auditor are:
Kriss Worthington, elected in 1996 to represent District 7, was the first openly LGBT man elected to Berkeley City Council. Lori Droste, elected in 2014 to represent District 8, is the first openly LGBT woman elected to Berkeley City Council.
Jenny Wong, elected in 2018, is the first Asian American City Auditor in Berkeley.
Nancy Skinner remains the only student to have served on the City Council, elected in 1984 as a graduate student. Today, most of the University housing is located in District 7 (although Foothill and Clark Kerr are in Districts 6 and 8, respectively). Districts 4 and 7 are majority-student. The City of Berkeley in 2014 passed a redistricting measure to create the nation's first student supermajority district in District 7, which in 2018 elected Rigel Robinson, a 22-year-old UC Berkeley graduate and the youngest Councilmember in the city's history.
The city's Public Health Division is one of three municipally-operated public health agencies in California. Though it is part of the city government, it qualifies for the same state funds as a county public health department.
Berkeley is also part of Alameda County, for which the Government of Alameda County is defined and authorized under the California Constitution, California law, and the Charter of the County of Alameda. The county government provides countywide services, such as elections and voter registration, law enforcement, jails, vital records, property records, tax collection, and social services. The county's health department does not cover the city. The county government is primarily composed of the elected five-member Board of Supervisors, other elected offices including the Sheriff/Coroner, the District Attorney, Assessor, Auditor-Controller/County Clerk/Recorder, and Treasurer/Tax Collector, and numerous county departments and entities under the supervision of the County Administrator.
In addition to the Berkeley Unified School District (which is coterminous with the city), Berkeley is also part of the Bay Area Rapid Transit District (BART), the Alameda-Contra Costa Transit District (AC Transit), the East Bay Regional Park District, the East Bay Municipal Utility District, and the Peralta Community College District.
Berkeley has been a Democratic stronghold in presidential elections since 1960, becoming one of the most Democratic cities in the country. The last Republican presidential candidate to receive at least one-quarter of the vote in Berkeley was Richard Nixon in 1968. Consistent with Berkeley's reputation as a strongly liberal and/or progressive city, in the 2016 presidential election more votes were won by Green Party presidential candidate Jill Stein than by Republican candidate Donald Trump.
However, at the local level, Republicans dominated Berkeley city politics into the 1970s, with Republicans holding the mayor's office for all but eight years from 1919 to 1971, with Wallace J.S. Johnson being the last Republican mayor. ("See also": List of mayors of Berkeley, California) 
According to the California Secretary of State, as of February 10, 2019, Berkeley has 79,261 registered voters. Of those, 54,069 (68.2%) are registered Democrats, 2,298 (2.9%) are registered Republicans, and 19,526 (24.6%) have declined to state a political party.
Berkeley became the first city in the United States to pass a sanctuary resolution on November 8, 1971.
Notable individuals who were born in and/or have lived in Berkeley include actors Ben Affleck and Andy Samberg, Billie Joe Armstrong, lead singer of Green Day, rapper Lil B, author Michael Chabon, and EDM producer Kshmr.
Berkeley has 17 sister cities:

</doc>
<doc id="4861" url="https://en.wikipedia.org/wiki?curid=4861" title="Bolventor">
Bolventor

Bolventor () is a hamlet on Bodmin Moor in Cornwall, England, United Kingdom. It is situated in Altarnun civil parish between Launceston and Bodmin.
The hamlet has been said to take its name from the "Bold Venture" that it must have appeared to build a farm in this moorland, but this is probably folk etymology, as "Bol-" is a common prefix in Cornish placenames. It is much more likely that the name derives from the 'Bold Adventure' tin-working area which was in operation near Jamaica Inn during the 1840s-1850s 
Bolventor is the location of the famous Jamaica Inn coaching inn. It is bypassed by a dual carriageway section of the A30 trunk road; before the bypass was built the hamlet straddled the A30 road.
Daphne du Maurier, a former resident, chose Bolventor as the setting for her novel about Cornish smugglers titled "Jamaica Inn". The inn that inspired the novel, Jamaica Inn, has stood beside the main road through the village since 1547. It is now a tourist attraction in its own right and dominates the hamlet. The Jamaica Inn was the subject of a paranormal investigation during a 2004 episode of reality television programme "Most Haunted".
The former Holy Trinity Church that lies to the east of the hamlet closed some years ago. A mile from Bolventor there was a chapel of St Luke (from the 13th to the early 16th century): the font is now at the church of Tideford. Bolventor parish was established in 1846 (before that date the village was in St Neot parish; the new parish was made up of parts of St Neot, Altarnun and Cardinham parishes) but has now been merged with Altarnun.

</doc>
<doc id="4862" url="https://en.wikipedia.org/wiki?curid=4862" title="Bengal">
Bengal

Bengal (; ) is a geopolitical, cultural and historical region in South Asia, specifically in the eastern part of the Indian subcontinent at the apex of the Bay of Bengal. Geographically, it is made up by the Ganges-Brahmaputra delta system, the largest such formation in the world; along with mountains in its north bordering the Himalayan states of Nepal and Bhutan and east bordering Burma.
Politically, Bengal is currently divided between Bangladesh and the Indian state of West Bengal. In the past it included areas of the neighbouring states of Bihar, Odisha, Assam and Tripura. In 2011, the population of Bengal was estimated to be 250 million, making it one of the most densely populated regions in the world. Among them, an estimated 160 million people live in Bangladesh and 91.3 million people live in West Bengal. The predominant ethnolinguistic group is the Bengali people, who speak the Indo-Aryan Bengali language. Bengali Muslims are the majority in Bangladesh and Bengali Hindus are the majority in West Bengal. Outside this region, Indian states of Tripura and Assam's Barak Valley has a Bengali majority population with significant presence in the states of Arunachal Pradesh, Delhi, Chhattisgarh, Jharkhand, Meghalaya, Mizoram, Nagaland and Uttarakhand.
Dense woodlands, including hilly rainforests, cover Bengal's northern and eastern areas; while an elevated forested plateau covers its central area. Highest elevation point of this region is Sandakphu (3636 m; 11,930 ft), which is located in Darjeeling district of West Bengal. In the littoral southwest are the Sundarbans, the world's largest mangrove forest and home of the Bengal tiger. In the coastal southeast lies Cox's Bazar, the longest beach in the world at . The region has a monsoon climate, which the Bengali calendar divides into six seasons.
At times an independent regional empire, Bengal was a leading power in South Asia and later the Islamic East, with extensive trade networks. In antiquity, its kingdoms were known as seafaring nations. Bengal was known to the Greeks as Gangaridai, notable for mighty military power. It was described by Greek historians that Alexander the Great withdrew from India anticipating a counterattack from an alliance of Gangaridai. Later writers noted merchant shipping links between Bengal and Roman Egypt. The Bengali Pala Empire was the last major Buddhist imperial power in the subcontinent, founded in 750 and becoming the dominant power in the northern Indian subcontinent by the 9th century, before being replaced by the Hindu Sena dynasty in the 12th century.
Islam was introduced during the Pala Empire, through trade with the Abbasid Caliphate. Following the formation of the Delhi Sultanate in the 13th century, Islam spread across the Bengal region. During the Islamic Bengal Sultanate, founded in 1352, Bengal was a major trading nation in the world and was often referred to by Europeans as the richest country to trade with. The Khorasanis referred to the land as an "inferno full of gifts", due to its unbearable climate but abundance of wealth. It was later absorbed into the Mughal Empire in 1576. Bengal Subah, described as the "Paradise of the Nations", was the empire's wealthiest province, and became a major global exporter, a center of worldwide industries such as cotton textiles, silk, and shipbuilding. Its economy was worth 12% of the world's GDP, a value bigger than the entirety of Western Europe, and its citizens' living standards were among the world's highest. Bengal's economy underwent a period of proto-industrialization during this period.
The Maratha invasions of Bengal badly affected the economy of Bengal and it is estimated that 400,000 Bengalis were killed by the Maratha bargis, and the genocide has been considered to be among the deadliest massacres in Indian history.
Subsequently, the region was conquered by the British East India Company after the Battle of Plassey in 1757 and became the Bengal Presidency of the British Raj. Bengal made significant contributions to the world's first Industrial Revolution, but experienced its own deindustrialisation. The East India Company increased agriculture tax rates from 10% to up to 50%, which caused multiple famines such as the Great Bengal famine of 1770 which caused the death of 10 million Bengalis and the Bengal famine of 1943 which killed millions.
Bengal played a major role in the Indian independence movement, in which revolutionary groups were dominant. Armed attempts to overthrow the British Raj began with the Sannyasi and Fakir Rebellion, and reached a climax when Subhas Chandra Bose led the Indian National Army allied with Japan to fight against the British. Many Bengalis died in the independence struggle and many were exiled in Cellular Jail, located in Andaman. The United Kingdom Cabinet Mission of 1946 split the region between India and Pakistan, an action popularly known as the partition of Bengal (1947). This was opposed by the Prime Minister of Bengal, Huseyn Shaheed Suhrawardy, and nationalist leader Sarat Chandra Bose. They campaigned for a united and independent nation-state of Bengal. The initiative failed owing to British diplomacy and communal conflict between Muslims and Hindus. Subsequently, Pakistan ruled East Bengal which later became the independent nation of Bangladesh by the Bangladesh War of Independence in 1971.
The name of "Bengal" is derived from the ancient kingdom of Banga,(pronounced Bôngô) the earliest records of which date back to the "Mahabharata" epic in the first millennium BCE. The exact origin of the word "Bangla" is unknown. In Islamic mythology, it is said to come from "Bung/Bang", a son of Hind (son of Hām who was a son of Noah) who colonised the area for the first time. The suffix "al" came to be added to it from the fact that the ancient rajahs of this land raised mounds of earth 10 feet high and 20 in breadth in lowlands at the foot of the hills which were called "al". From this suffix added to the Bung, the name Bengal arose and gained currency". This is also mentioned in Ghulam Husain Salim's Riyaz-us-Salatin.
Other theories on the origin of the term Banga point to the Proto-Dravidian "Bong" tribe that settled in the area circa 1000 BCE and the Austric word "Bong" (Sun-god). The term "Vangaladesa" is used to describe the region in 11th-century South Indian records. The Portuguese referred to the region as "Bengala" in the Age of Discovery.
Most of the Bengal region lies in the Ganges-Brahmaputra delta, but there are highlands in its north, northeast and southeast. The Ganges Delta arises from the confluence of the rivers Ganges, Brahmaputra, and Meghna rivers and their respective tributaries. The total area of Bengal is 232,752  km—West Bengal is and Bangladesh .
The flat and fertile Bangladesh Plain dominates the geography of Bangladesh. The Chittagong Hill Tracts and Sylhet regions are home to most of the mountains in Bangladesh. Most parts of Bangladesh are within above the sea level, and it is believed that about 10% of the land would be flooded if the sea level were to rise by . Because of this low elevation, much of this region is exceptionally vulnerable to seasonal flooding due to monsoons.
The highest point in Bangladesh is in Mowdok range at . A major part of the coastline comprises a marshy jungle, the Sundarbans, the largest mangrove forest in the world and home to diverse flora and fauna, including the royal Bengal tiger. In 1997, this region was declared endangered.
West Bengal is on the eastern bottleneck of India, stretching from the Himalayas in the north to the Bay of Bengal in the south. The state has a total area of . The Darjeeling Himalayan hill region in the northern extreme of the state belongs to the eastern Himalaya. This region contains Sandakfu ()—the highest peak of the state. The narrow Terai region separates this region from the plains, which in turn transitions into the Ganges delta towards the south. The Rarh region intervenes between the Ganges delta in the east and the western plateau and high lands. A small coastal region is on the extreme south, while the Sundarbans mangrove forests form a remarkable geographical landmark at the Ganges delta.
At least nine districts in West Bengal and 42 districts in Bangladesh have arsenic levels in groundwater above the World Health Organization maximum permissible limit of 50 µg/L or 50 parts per billion and the untreated water is unfit for human consumption. The water causes arsenicosis, skin cancer and various other complications in the body.
North Bengal is a term used for the north-western part of Bangladesh and northern part of West Bengal. The Bangladeshi part comprises Rajshahi Division and Rangpur Division. Generally, it is the area lying west of Jamuna River and north of Padma River, and includes the Barind Tract. Politically, West Bengal's part comprises Jalpaiguri Division (Alipurduar, Cooch Behar, Darjeeling, Jalpaiguri, North Dinajpur, South Dinajpur and Malda) together and Bihar's parts include Kishanganj district. Darjeeling Hills are also part of North Bengal. Although only people of Jaipaiguri, Alipurduar and Cooch Behar identifies themselves as North Bengali. North Bengal is divided into Terai and Dooars regions. North Bengal is also noted for its rich cultural heritage, including two UNESCO World Heritage Sites. Aside from the Bengali majority, North Bengal is home to many other communities including Nepalis, Santhal people, Lepchas and Rajbongshis.
Northeast Bengal refers to the Sylhet region, comprising Sylhet Division of Bangladesh and the Karimganj district in the Indian state of Assam. The region is noted for its distinctive fertile highland terrain, extensive tea plantations, rainforests and wetlands. The Surma and Barak river are the geographic markers of the area. The city of Sylhet is its largest urban center, and the region is known for its unique language. The ancient name of the region is Srihatta. The region was ruled by the Kamarupa and Harikela kingdoms as well as the Bengal Sultanate. It later became a district of the Mughal Empire. Alongside the predominant Bengali population resides a small Bishnupriya Manipuri, Khasia and other tribal minorities.
The region is the crossroads of Bengal and northeast India.
Central Bengal refers to the Dhaka Division of Bangladesh. It includes the elevated Madhupur tract with a large Sal tree forest. The Padma River cuts through the southern part of the region, separating the greater Faridpur region. In the north lies the greater Mymensingh and Tangail regions.
South Bengal covers the southern part of the Indian state of West Bengal and southwestern Bangladesh. The Indian part of South Bengal includes 12 districts: Kolkata, Howrah, Hooghly, Burdwan, East Midnapur, West Midnapur, Purulia, Bankura, Birbhum, Nadia, South 24 Parganas, North 24 Parganas. The Bangladeshi part includes the proposed Faridpur Division, Khulna Division and Barisal Division.
The Sundarbans, a major biodiversity hotspot, is located in South Bengal. Bangladesh hosts 60% of the forest, with the remainder in India.
Southeast Bengal refers to the hilly and coastal Bengali-speaking areas of Chittagong Division in southeastern Bangladesh. Southeast Bengal is noted for its thalassocratic and seafaring heritage. The area was dominated by the Bengali Harikela and Samatata kingdoms in antiquity. It was known to Arab traders as "Harkand" in the 9th century. During the medieval period, the region was ruled by the Sultanate of Bengal, the Kingdom of Tripura, the Kingdom of Mrauk U, the Portuguese Empire and the Mughal Empire, prior to the advent of British rule. The Chittagonian dialect of Bengali is prevalent in coastal areas of southeast Bengal. Along with its Bengali population, it is also home to Tibeto-Burman ethnic groups, including the Chakma, Marma, Tanchangya and Bawm peoples.
Southeast Bengal is considered a bridge to Southeast Asia and the northern parts of Arakan are also historically considered to be a part of it.
There are four World Heritage Sites in the region, including the Sundarbans, the Somapura Mahavihara, the Mosque City of Bagerhat and the Darjeeling Himalayan Railway. Other prominent places include the Bishnupur, Bankura temple city, the Adina Mosque, the Caravanserai Mosque, numerous zamindar palaces (like Ahsan Manzil and Cooch Behar Palace), the Lalbagh Fort, the Great Caravanserai ruins, the Shaista Khan Caravanserai ruins, the Kolkata Victoria Memorial, the Dhaka Parliament Building, archaeologically excavated ancient fort cities in Mahasthangarh, Mainamati, Chandraketugarh and Wari-Bateshwar, the Jaldapara National Park, the Lawachara National Park, the Teknaf Game Reserve and the Chittagong Hill Tracts.
Cox's Bazar in southeastern Bangladesh is home to the longest natural sea beach in the world with an unbroken length of 120 km (75 mi). It is also a growing surfing destination. St. Martin's Island, off the coast of Chittagong Division, is home to the sole coral reef in Bengal.
The flat Bengal Plain, which covers most of Bangladesh and West Bengal, is one of the most fertile areas on Earth, with lush vegetation and farmland dominating its landscape. Bengali villages are buried among groves of mango, jack fruit, betel nut and date palm. Rice, jute, mustard and sugarcane plantations are a common sight. Water bodies and wetlands provide a habitat for many aquatic plants in the Ganges-Brahmaputra delta. The northern part of the region features Himalayan foothills ("Dooars") with densely wooded Sal and other tropical evergreen trees. Above an elevation of 1,000 metres (3,300 ft), the forest becomes predominantly subtropical, with a predominance of temperate-forest trees such as oaks, conifers and rhododendrons. Sal woodland is also found across central Bangladesh, particularly in the Bhawal National Park. The Lawachara National Park is a rainforest in northeastern Bangladesh. The Chittagong Hill Tracts in southeastern Bangladesh is noted for its high degree of biodiversity.
The littoral Sundarbans in the southwestern part of Bengal is the largest mangrove forest in the world and a UNESCO World Heritage Site. The region has over 89 species of mammals, 628 species of birds and numerous species of fish. For Bangladesh, the water lily, the oriental magpie-robin, the hilsa and mango tree are national symbols. For West Bengal, the white-throated kingfisher, the chatim tree and the night-flowering jasmine are state symbols. The Bengal tiger is the national animal of Bangladesh and India. The fishing cat is the state animal of West Bengal.
Human settlement in Bengal can be traced back 20,000 years. Remnants of Copper Age settlements date back 4,300 years. Archaeological evidence confirms that by the second millennium BCE, rice-cultivating communities inhabited the region. By the 11th century BCE, the people of the area lived in systemically-aligned housing, used human cemeteries and manufactured copper ornaments and fine black and red pottery. The Ganges, Brahmaputra and Meghna rivers were natural arteries for communication and transportation. Estuaries on the Bay of Bengal allowed for maritime trade. The early Iron Age saw the development of metal weaponry, coinage, permanent field agriculture and irrigation. From 600 BCE, the second wave of urbanisation engulfed the north Indian subcontinent, as part of the Northern Black Polished Ware culture.
Ancient Bengal was divided between the regions of Varendra, Suhma, Anga, Vanga, Samatata and Harikela. Early Indian literature described the region as a thalassocracy, with colonies in Southeast Asia and the Indian Ocean. For example, the first recorded king of Sri Lanka was a Bengali prince called Vijaya. The region was known to the ancient Greeks and Romans as Gangaridai. The Greek ambassador Megasthenes chronicled its military strength and dominance of the Ganges delta. The invasion army of Alexander the Great was deterred by the accounts of Gangaridai's power in 325 BCE. Later Roman accounts noted maritime trade routes with Bengal. A Roman amphora has been found in Purba Medinipur district of West Bengal, made in Aelana (present day Aqaba in Jordan) between the 4th and 7th centuries AD. Another prominent kingdom in Ancient Bengal was Pundravardhana which was located in Northern Bengal with its capital being located in modern-day Bogra, the kingdom was prominently buddhist leaving behind historic Viharas such as Mahasthangarh. In vedic mythology the royal families of Magadha, Anga, Vanga, Suhma and Kalinga were all related and descended from one King.
Ancient Bengal was considered a part of Magadha region, which was the cradle of Indian arts and sciences. Currently the Maghada region is divided into several states that are Bihar, Jharkhand and Bengal (West Bengal and East Bengal) The legacy of Magadha includes the concept of zero, the invention of Chess and the theory of solar and lunar eclipses and the Earth orbiting the Sun. Sanskrit and derived Old Indo-Aryan dialects, was spoken across Bengal. The Bengali language evolved from Old Indo-Aryan Sanskrit dialects. The region was ruled by Hindu, Buddhist and Jain dynasties, including the Mauryans, Guptas, Varmans, Khadgas, Palas, Chandras and Senas among others. In the 9th century, Arab Muslim traders frequented Bengali seaports and found the region to be a thriving seafaring kingdom with well-developed coinage and banking.
The Pala Empire was an imperial power in the Indian subcontinent, which originated in the region of Bengal. They were followers of the Mahayana and Tantric schools of Buddhism. The empire was founded with the election of Gopala as the emperor of Gauda in 750. At its height in the early 9th century, the Pala Empire was the dominant power in the northern subcontinent, with its territory stretching across parts of modern-day eastern Pakistan, northern and northeastern India, Nepal and Bangladesh. The empire enjoyed relations with the Srivijaya Empire, the Tibetan Empire, and the Arab Abbasid Caliphate. Islam first appeared in Bengal during Pala rule, as a result of increased trade between Bengal and the Middle East. The resurgent Hindu Sena dynasty dethroned the Pala Empire in the 12th century, ending the reign of the last major Buddhist imperial power in the subcontinent.
Muslim conquests of the Indian subcontinent absorbed Bengal in 1204. The region was annexed by the Delhi Sultanate. Muslim rule introduced agrarian reform, a new calendar and Sufism. The region saw the rise of important city states in Sonargaon, Satgaon and Lakhnauti. By 1352, Ilyas Shah achieved the unification of an independent Bengal. In the 14th and 15th centuries, the Bengal Sultanate was a major diplomatic, economic and military power in the subcontinent. It developed the subcontinent's relations with China, Egypt, the Timurid Empire and East Africa. In 1540, Sher Shah Suri was crowned Emperor of the northern subcontinent in the Bengali capital Gaur.
The Mughal Empire conquered Bengal in the 16th century. The Bengal Subah province in the Mughal Empire was the wealthiest state in the subcontinent. Bengal's trade and wealth impressed the Mughals so much that it was described as the "Paradise of the Nations" by the Mughal Emperors. The region was also notable for its powerful semi-independent aristocracy, including the Twelve Bhuiyans and the Nawabs of Bengal. It was visited by several world explorers, including Ibn Battuta, Niccolo De Conti and Admiral Zheng He.
Under Mughal rule, Bengal was a center of the worldwide muslin and silk trades. During the Mughal era, the most important center of cotton production was Bengal, particularly around its capital city of Dhaka, leading to muslin being called "daka" in distant markets such as Central Asia. Domestically, much of India depended on Bengali products such as rice, silks and cotton textiles. Overseas, Europeans depended on Bengali products such as cotton textiles, silks and opium; Bengal accounted for 40% of Dutch imports from Asia, for example, including more than 50% of textiles and around 80% of silks. From Bengal, saltpetre was also shipped to Europe, opium was sold in Indonesia, raw silk was exported to Japan and the Netherlands, cotton and silk textiles were exported to Europe, Indonesia, and Japan, cotton cloth was exported to the Americas and the Indian Ocean. Bengal also had a large shipbuilding industry. In terms of shipbuilding tonnage during the 16th–18th centuries, economic historian Indrajit Ray estimates the annual output of Bengal at 223,250 tons, compared with 23,061 tons produced in nineteen colonies in North America from 1769 to 1771.
Since the 16th century, European traders traversed the sea routes to Bengal, following the Portuguese conquests of Malacca and Goa. The Portuguese established a settlement in Chittagong with permission from the Bengal Sultanate in 1528, but were later expelled by the Mughals in 1666. In the 18th-century, the Mughal Court rapidly disintegrated due to Nader Shah's invasion and internal rebellions, allowing European colonial powers to set up trading posts across the territory. The British East India Company eventually emerged as the foremost military power in the region; and defeated the last independent Nawab of Bengal at the Battle of Plassey in 1757.
The Maratha invasions of Bengal badly affected the economy of Bengal and it is estimated that 400,000 Bengali Hindus in western Bengal were killed by the Hindu Maratha bargis, and many women and children gang raped., and the genocide has been considered to be among the deadliest massacres in Indian history.
In Bengal effective political and military power was transferred from the old regime to the British East India Company around 1757–65.
Company rule in India began under the Bengal Presidency. Calcutta was named the capital of British India in 1772. The presidency was run by a military-civil administration, including the Bengal Army, and had the world's sixth earliest railway network. Great Bengal famines struck several times during colonial rule (notably the Great Bengal famine of 1770 and Bengal famine of 1943).
About 50 million were killed in Bengal due to massive plague outbreaks and famines which happened in 1895 to 1920, mostly in western Bengal.
The Indian Rebellion of 1857 was initiated on the outskirts of Calcutta, and spread to Dhaka, Chittagong, Jalpaiguri, Sylhet and Agartala, in solidarity with revolts in North India. The failure of the rebellion led to the abolishment of the Mughal Court and direct rule by the British Raj. The late 19th and early 20th century Bengal Renaissance had a great impact on the cultural and economic life of Bengal and started a great advance in the literature and science of Bengal. Between 1905 and 1912, an abortive attempt was made to divide the province of Bengal into two zones, that included the short-lived province of Eastern Bengal and Assam based in Dacca and Shillong. Under British rule, Bengal experienced deindustrialisation. m
In 1876, about 200,000 people were killed in Bengal by the Great Bangladesh cyclone.
Bengal played a major role in the Indian independence movement, in which revolutionary groups were dominant. Armed attempts to overthrow the British Raj began with the rebellion of Titumir, and reached a climax when Subhas Chandra Bose led the Indian National Army against the British. Bengal was also central in the rising political awareness of the Muslim population—the All-India Muslim League was established in Dhaka in 1906. The Muslim homeland movement pushed for a sovereign state in eastern British India with the Lahore Resolution in 1943. Hindu nationalism was also strong in Bengal, which was home to groups like the Hindu Mahasabha. In spite of a last-ditch effort to form a United Bengal, when India gained independence in 1947, Bengal was partitioned along religious lines. The western part went to India (and was named West Bengal) while the eastern part joined Pakistan as a province called East Bengal (later renamed East Pakistan, giving rise to Bangladesh in 1971). The circumstances of partition were bloody, with widespread religious riots in Bengal.
The 1970 Bhola cyclone took the lives of 500,000 people in Bengal, making it one of the deadliest recorded cyclones.
West Bengal became one of India's most populous states. Calcutta, the former capital of the British Raj, became the state capital of West Bengal and continued to be India's largest city until the late 20th century, when severe power shortages, strikes and a violent Marxist-Naxalite movement damaged much of the state's infrastructure in the 1960s and 70s, leading to a period of economic stagnation. West Bengal politics underwent a major change when the Left Front won the 1977 assembly election, defeating the incumbent Indian National Congress. The Left Front, led by the Communist Party of India (Marxist) (CPI(M)) governed the state for over three decades, which was the world's longest elected Communist administration in history. Since the 2000s, West Bengal has experienced an economic rejuvenation, particularly in its IT industry.
The princely state of Hill Tippera, that was under the suzerainty of British India. Following the death of Maharaja Bir Bikram Kishore Debbarman, the princely state acceded to the Union of India on 15 October 1949 under the Tripura Merger Agreement signed by Maharani Regent Kanchan Prava Devi. By the 1950s, the region had a Bengali majority population due to the influx of Hindu refugees from East Pakistan after partition. It became a Union Territory of India in November 1953. It was granted full statehood with an elected legislature in July 1963. An insurgency by indigenous people affected the state for several years. The Left Front ruled the state between 1978 and 1988, followed by a stint of Indian National Congress rule until 1993, and then a return to the Communists.
Karimganj District joined the union of India after its partition from Sylhet as per the Sylhet referendum in 1947 and has been a part of Assam's Barak Valley. One of the most significant events in the region's history was the language movement in 1961, in which the killing of agitators by state police led to Bengali being recognised as one of the official languages of Assam. The issue of Bengali settlement in the state from has been a contentious part of the Assam conflict.
In 1948, the Government of the Dominion of Pakistan ordained Urdu as the sole national language, sparking extensive protests among the Bengali-speaking majority of East Bengal. Facing rising sectarian tensions and mass discontent with the new law, the government outlawed public meetings and rallies. The students of the University of Dhaka and other political activists defied the law and organised a protest on 21 February 1952. The movement reached its climax when several student demonstrators were shot dead by police firing. As a result of the movement, Pakistan government in 1956 included Bengali as national lanuage along with Urdu. UNESCO in 1999 declared 21 February as International Mother Language Day honouring the 1952 incident.
East Bengal, which was later renamed to East Pakistan in 1955, was home to Pakistan's demographic majority and played an instrumental role in the founding of the new state. Strategically, Pakistan joined the Southeast Asia Treaty Organization under the Bengali prime minister Mohammad Ali of Bogra as a bulwark against communism. However, tensions between East and West Pakistan grew rapidly over political exclusion, economic neglect and ethnic and linguistic discrimination. The State of Pakistan was subjected to years of military rule due to fears of Bengali political supremacy under democracy. Elected Bengali-led governments at the federal and provincial levels, which were led by statesmen such as A. K. Fazlul Huq and H. S. Suhrawardy, were deposed.
East Pakistan witnessed the rise of Bengali self determination calls led by Sheikh Mujibur Rahman and Maulana Bhashani in the 1960s. Rahman launched the Six point movement for autonomy in 1966. After the 1970 national election, Rahman's party, the Awami League, had emerged as the largest party in Pakistan's parliament. The erstwhile Pakistani military junta refused to accept election results which triggered civil disobedience across East Pakistan. The Pakistani military responded by launching a genocide that caused the Bangladesh Liberation War in 1971. The first Government of Bangladesh and the Mukti Bahini waged a guerrilla campaign with support from neighbouring India, which hosted millions of war refugees. Global support for the independence of East Pakistan increased due to the conflict's humanitarian crisis, with the Indian Armed Forces intervening in support of the Bangladesh Forces in the final two weeks of the war and ensuring Pakistan's surrender.
After independence, Bangladesh adopted a secular democracy under its new constitution in 1972. Awami League premier Sheikh Mujibur Rahman became the country's strongman and implemented many socialist policies. A one party state was enacted in 1975. Sheikh Mujibur Rahman was assassinated later that year during a military coup that ushered in sixteen years of military dictatorships and presidential governments. The liberation war commander Ziaur Rahman emerged as Bangladesh's leader in the late 1970s. He reoriented the country's foreign policy towards the West and restored free markets and the multiparty polity. President Zia was assassinated in 1981 during a failed military coup. He was eventually succeeded by his army chief Hussain Muhammad Ershad. Lasting for nine years, Ershad's rule witnessed continued pro-free market reforms and the devolution of some authority to local government. The South Asian Association for Regional Cooperation (SAARC) was founded in Dhaka in 1985. The Jatiya Party government made Islam the state religion in 1988.
A popular uprising restored parliamentary democracy in 1991. Since then, Bangladesh has largely alternated between the premierships of Sheikh Hasina of the Awami League and Khaleda Zia of the Bangladesh Nationalist Party, as well as technocratic caretaker governments. Emergency rule was imposed by the military in 2007 and 2008 after widespread street violence between the League and BNP. The restoration of democratic government in 2009 was followed by the initiation of the International Crimes Tribunal to prosecute surviving collaborators of the 1971 genocide. Today, the country is one of the emerging and growth-leading economies of the world. It is listed as one of the Next Eleven countries, it also has one of the fastest real GDP growth rates. Its gross domestic product ranks 39th largest in the world in terms of market exchange rates and 30th in purchasing power parity. Its per capita income ranks 143th and 136th in two measures. In the field of human development, it has progressed ahead in life expectancy, maternal and child health, and gender equality. But it continues to face challenging problems, including poverty, corruption, terrorism, illiteracy, and inadequate public healthcare.
The Bengal region had been part of major empires and kingdoms like Gangaridai, Nanda Empire, Maurya Empire, Gupta Empire, Pala Empire, Sena dynasty, and Bengal Sultanate. It has also been a regional empire, ruling over neighbouring regions like Bihar, Orissa, Arakan, and parts of North India, Assam and Nepal.
Politically, the region is divided between the People's Republic of Bangladesh, an independent state, and the eastern provinces of the Republic of India, including West Bengal. Politically both Bangladesh and Indian Bengal are socialist, with left wing parties dominating the region's politics.
The state of Bangladesh is a parliamentary republic based on the Westminster system, with a written constitution and a President elected by parliament for mostly ceremonial purposes. The government is headed by a Prime Minister, who is appointed by the President from among the popularly elected 300 Members of Parliament in the Jatiyo Sangshad, the national parliament. The Prime Minister is traditionally the leader of the single largest party in the Jatiyo Sangshad. Under the constitution, while recognising Islam as the country's established religion, the constitution grants freedom of religion to non-Muslims.
Between 1975 and 1990, Bangladesh had a presidential system of government. Since the 1990s, it was administered by non-political technocratic caretaker governments on four occasions, the last being under military-backed emergency rule in 2007 and 2008. The Awami League and the Bangladesh Nationalist Party (BNP) are the two largest political parties in Bangladesh.
Bangladesh is a member of the UN, WTO, IMF, the World Bank, ADB, OIC, IDB, SAARC, BIMSTEC and the IMCTC. Bangladesh has achieved significant strides in human development compared to its neighbours.
West Bengal are provincial states of the Republic of India, with local executives and assemblies- features shared with other states in the Indian federal system. The president of India appoints a governor as the ceremonial representative of the union government. The governor appoints the chief minister on the nomination of the legislative assembly. The chief minister is the traditionally the leader of the party or coalition with most seats in the assembly. President's rule is often imposed in Indian states as a direct intervention of the union government led by the prime minister of India.
Each state has popularly elected members in the Indian lower house of parliament, the Lok Sabha. Each state nominates members to the Indian upper house of parliament, the Rajya Sabha.
The state legislative assemblies also play a key role in electing the ceremonial president of India. The former president of India, Pranab Mukherjee, was a native of West Bengal and a leader of the Indian National Congress.
The two major political forces in the Bengali-speaking zone of India are the Left Front and the Trinamool Congress, with the Bharatiya Janata Party (BJP) and the Indian National Congress being minor players.
India and Bangladesh are the world's second and eighth most populous countries respectively. Bangladesh-India relations began on a high note in 1971 when India played a major role in the liberation of Bangladesh, with the Indian Bengali populace and media providing overwhelming support to the independence movement in the former East Pakistan. The two countries had a twenty five-year friendship treaty between 1972 and 1996. However, differences over river sharing, border security and access to trade have long plagued the relationship. In more recent years, a consensus has evolved in both countries on the importance of developing good relations, as well as a strategic partnership in South Asia and beyond. Commercial, cultural and defence co-operation have expanded since 2010, when Prime Ministers Sheikh Hasina and Manmohan Singh pledged to reinvigorate ties.
The Bangladesh High Commission in New Delhi operates a Deputy High Commission in Kolkata and a consular office in Agartala. India has a High Commission in Dhaka with consulates in Chittagong and Rajshahi. Frequent international air, bus and rail services connect major cities in Bangladesh and Indian Bengal, particularly the three largest cities- Dhaka, Kolkata and Chittagong. Undocumented immigration of Bangladeshi workers is a controversial issue championed by right-wing nationalist parties in India but finds little sympathy in West Bengal. India has since fenced the border which has been criticised by Bangladesh.
The Bengal region is one of the most densely populated areas in the world. With a population of 300 million, Bengalis are the third largest ethnic group in the world after the Han Chinese and Arabs.
According to provisional results of 2011 Bangladesh census, the population of Bangladesh was 149,772,364; however, CIA's "The World Factbook" gives 163,654,860 as its population in a July 2013 estimate. According to the provisional results of the 2011 Indian national census, West Bengal has a population of 91,347,736. "So, the Bengal region, , has at least 241.1 million people, out of which 158.8 million are Muslims (66.4%), 77 million are Hindus (32%) and 5.3 million (1.6%) are others particularly (Buddhists, Christians, Animists etc)". Bangladesh is quite religiously homogeneous in comparison to West Bengal. The Muslim population in Bangladesh is over 134.8 million in 2011, which makes up 90% of the population in the country, while Hinduism is the second largest religious affiliation in Bangladesh, with around 12.5 million people identifying themselves as Hindus making up about 8.5% of the total population according to the 2011 census. West Bengal is quite religiously diverse in comparison to Bangladesh, with regional cultural and religious specificities. As of 2011, Hinduism is the most common religion in West Bengal, with adherents representing 70.54% of the total population, while Muslims are the second-largest community as well as the largest minority group, comprise 27.01% of the total population of the state. The Hindu population of West Bengal is 64,385,546 ,while the Muslim population is 24,654,825, according to the 2011 census. This figures give a population density of 1003.9/km; making it among the most densely populated areas in the world.
Bengali is the main language spoken in Bengal. Many phonological, lexical, and structural differences from the standard variety occur in peripheral varieties of Bengali; these include Sylheti, Chittagonian, Chakma, Rangpuri/Rajbangshi, Hajong, Rohingya, and Tangchangya.
English is often used for official work alongside Bengali. Other major Indo-Aryan languages such as Hindi, Urdu, Assamese, and Nepali are also familiar to Bengalis.
In addition, several minority ethnolinguistic groups are native to the region. These include speakers of other Indo-Aryan languages (e.g., Bishnupriya Manipuri, Oraon Sadri, various Bihari languages), Tibeto-Burman languages (e.g., A'Tong, Chak, Koch, Garo, Megam, Meitei Manipuri, Mizo, Mru, Pangkhua, Rakhine/Marma, Kok Borok, Riang, Tippera, Usoi, various Chin languages), Austroasiatic languages (e.g., Khasi, Koda, Mundari, Pnar, Santali, War), and Dravidian languages (e.g., Kurukh, Sauria Paharia).
Life expectancy is around 72.49 years for Bangladesh and 70.2 for West Bengal. In terms of literacy, West Bengal leads with 77% literacy rate, in Bangladesh the rate is approximately 72.9%. The level of poverty in West Bengal is at 19.98%, while in Bangladesh it stands at 12.9%
West Bengal has one of the lowest total fertility rates in India. West Bengal's TFR of 1.6 roughly equals that of Canada.
About 20,000 people live on "chars". Chars are temporary islands formed by the deposition of sediments eroded off the banks of the Ganges in West Bengal, which often disappear in the monsoon season. They are made of very fertile soil. The inhabitants of the chars are not recognised by the Government of West Bengal on the grounds that it is not known whether they are Indians or Bangladeshis. Consequently, no identification documents are issued to char-dwellers who cannot benefit from health care, barely survive because of very poor sanitation and are prevented from emigrating to the mainland to find jobs when they have turned 14. On a particular char, it was reported that 13% of women died at childbirth.
Historically, Bengal has been the industrial leader of the subcontinent.
The region is one of the largest rice producing areas in the world, with West Bengal being India's largest rice producer and Bangladesh being the world's fourth largest rice producer. Other key crops include jute, tea, sugarcane and wheat. There are significant reserves of limestone, natural gas and coal. Major industries include textiles, leather goods, pharmaceuticals, shipbuilding, banking and information and communication technology.
Three stock exchanges are located in the region, including the Dhaka Stock Exchange, the Chittagong Stock Exchange and the Calcutta Stock Exchange.
Below is a comparison of economies in the region of Bengal
Bangladesh and India are the largest trading partners in South Asia, with two-way trade valued at an estimated US$6.9 billion. Most of this trade relationship is centered on some of the world's busiest land ports on the Bangladesh-India border, particularly the West Bengal section.
The partition of India severed the once strong economic links which integrated the region. Decades later, frequent air, rail and bus services are increasingly connecting cities in Bangladesh and West Bengal, as well as the wider region, including Northeast India, Nepal and Bhutan. However the overall economic relationship remains well below potential.
The following are the largest cities in Bengal (in terms of population):
The Bengal region is located at the crossroads of two huge economic blocs, the SAARC and ASEAN. It gives access to the sea for the landlocked countries of Bhutan and Nepal, as well as the Seven Sister States of North East India. It is also located near China's southern landlocked region, including Yunnan and Tibet.
Both India and Bangladesh plan to expand onshore and offshore oil and gas operations. Bangladesh is Asia's seventh-largest natural gas producer. Its maritime exclusive economic zone potentially holds many of the largest gas reserves in the Asia-Pacific.
The Bay of Bengal is strategically important for its vital shipping lanes and its central location between the Middle East and the Pacific. The Bay of Bengal Initiative, based in Dhaka, brings together Bangladesh, India, Myanmar, Thailand, Nepal, Bhutan and Sri Lanka to promote economic integration in the subregion. Other regional groupings include the Bangladesh-China-India-Myanmar Forum for Regional Cooperation (BCIM) and the Bangladesh Bhutan India Nepal (BBIN) Initiative.
Culturally, Bengal is significant for its huge Hindu and Muslim populations. Bengali Hindus make up the second largest linguistic community in India. Bengali Muslims are the world's second largest Muslim ethnicity (after Arab Muslims), and Bangladesh is the world's third largest Muslim-majority country (after Indonesia and Pakistan).
The Bengali language developed between the 7th and 10th centuries from Apabhraṃśa and Magadhi Prakrit. It is written using the indigenous Bengali alphabet, a descendant of the ancient Brahmi script. Bengali is the 5th most spoken language in the world. It is an eastern Indo-Aryan language and one of the easternmost branches of the Indo-European language family. It is part of the Bengali-Assamese languages. Bengali has greatly influenced other languages in the region, including Odia, Assamese, Chakma, Nepali and Rohingya. It is the sole state language of Bangladesh and the second most spoken language in India. It is also the seventh most spoken language by total number of speakers in the world.
Bengali binds together a culturally diverse region and is an important contributor to regional identity. The 1952 Bengali Language Movement in East Pakistan is commemorated by UNESCO as International Mother Language Day, as part of global efforts to preserve linguistic identity.
In both Bangladesh and West Bengal, currency is commonly denominated as taka. The Bangladesh taka is an official standard bearer of this tradition, while the Indian rupee is also written as taka in Bengali script on all of its banknotes. The history of the taka dates back centuries. Bengal was home one of the world's earliest coin currencies in the first millennium BCE. Under the Delhi Sultanate, the taka was introduced by Muhammad bin Tughluq in 1329. Bengal became the stronghold of the taka. The silver currency was the most important symbol of sovereignty of the Sultanate of Bengal. It was traded on the Silk Road and replicated in Nepal and China's Tibetan protectorate. The Pakistani rupee was scripted in Bengali as taka on its banknotes until Bangladesh's creation in 1971.
Bengali literature has a rich heritage. It has a history stretching back to the 3rd century BCE, when the main language was Sanskrit written in the brahmi script. The Bengali language and script evolved circa 1000 CE from Magadhi Prakrit. Bengal has a long tradition in folk literature, evidenced by the "Chôrjapôdô", "Mangalkavya", "Shreekrishna Kirtana", "Maimansingha Gitika" or "Thakurmar Jhuli". Bengali literature in the medieval age was often either religious (e.g. Chandidas), or adaptations from other languages (e.g. Alaol). During the Bengal Renaissance of the nineteenth and twentieth centuries, Bengali literature was modernised through the works of authors such as Michael Madhusudan Dutta, Ishwar Chandra Vidyasagar, Bankim Chandra Chattopadhyay, Rabindranath Tagore, Sarat Chandra Chattopadhyay, Kazi Nazrul Islam, Satyendranath Dutta and Jibanananda Das. In the 20th century, prominent modern Bengali writers included Syed Mujtaba Ali, Jasimuddin, Manik Bandopadhyay, Tarasankar Bandyopadhyay, Bibhutibhushan Bandyopadhyay, Buddhadeb Bose, Sunil Gangopadhyay and Humayun Ahmed.
Prominent contemporary Bengali writers in English include Amitav Ghosh, Tahmima Anam, Jhumpa Lahiri and Zia Haider Rahman among others.
The Bangamata is a female personification of Bengal which was created during the Bengali Renaissance and later adopted by the Bengali nationalists. Hindu nationalists adopted a modified Bharat Mata as a national personification of India. The Mother Bengal represents not only biological motherness but its attributed characteristics as well – protection, never ending love, consolation, care, the beginning and the end of life. In Amar Sonar Bangla, the national anthem of Bangladesh, Rabindranath Tagore has used the word "Maa" (Mother) numerous times to refer to the motherland i.e. Bengal.
The Pala-Sena School of Art developed in Bengal between the 8th and 12th centuries and is considered a high point of classical Asian art. It included sculptures and paintings.
Islamic Bengal was noted for its production of the finest cotton fabrics and saris, notably the Jamdani, which received warrants from the Mughal court. The Bengal School of painting flourished in Kolkata and Shantiniketan in the British Raj during the early 20th century. Its practitioners were among the harbingers of modern painting in India. Zainul Abedin was the pioneer of modern Bangladeshi art. The country has a thriving and internationally acclaimed contemporary art scene.
Classical Bengali architecture features terracotta buildings. Ancient Bengali kingdoms laid the foundations of the region's architectural heritage through the construction of monasteries and temples (for example, the Somapura Mahavihara). During the sultanate period, a distinct and glorious Islamic style of architecture developed the region. Most Islamic buildings were small and highly artistic terracotta mosques with multiple domes and no minarets. Bengal was also home to the largest mosque in South Asia at Adina. Bengali vernacular architecture is credited for inspiring the popularity of the bungalow.
The Bengal region also has a rich heritage of Indo-Saracenic architecture, including numerous zamindar palaces and mansions. The most prominent example of this style is the Victoria Memorial, Kolkata.
In the 1950s, Muzharul Islam pioneered the modernist terracotta style of architecture in South Asia. This was followed by the design of the Jatiyo Sangshad Bhaban by the renowned American architect Louis Kahn in the 1960s, which was based on the aesthetic heritage of Bengali architecture and geography.
The Gupta dynasty, which is believed to have originated in North Bengal, pioneered the invention of chess, the concept of zero, the theory of Earth orbiting the Sun, the study of solar and lunar eclipses and the flourishing of Sanskrit literature and drama. Bengal was the leader of scientific endeavours in the subcontinent during the British Raj. The educational reforms during this period gave birth to many distinguished scientists in the region. Sir Jagadish Chandra Bose pioneered the investigation of radio and microwave optics, made very significant contributions to plant science, and laid the foundations of experimental science in the Indian subcontinent. IEEE named him one of the fathers of radio science. He was the first person from the Indian subcontinent to receive a US patent, in 1904. In 1924–25, while researching at the University of Dhaka, Prof Satyendra Nath Bose well known for his works in quantum mechanics, provided the foundation for Bose–Einstein statistics and the theory of the Bose–Einstein condensate. Meghnad Saha was the first scientist to relate a star's spectrum to its temperature, developing thermal ionization equations (notably the Saha ionization equation) that have been foundational in the fields of astrophysics and astrochemistry. Amal Kumar Raychaudhuri was a physicist, known for his research in general relativity and cosmology. His most significant contribution is the eponymous Raychaudhuri equation, which demonstrates that singularities arise inevitably in general relativity and is a key ingredient in the proofs of the Penrose–Hawking singularity theorems. In the United States, the Bangladeshi-American engineer Fazlur Rahman Khan emerged as the "father of tubular designs" in skyscraper construction. Ashoke Sen is an Indian theoretical physicist whose main area of work is string theory. He was among the first recipients of the Fundamental Physics Prize “for opening the path to the realisation that all string theories are different limits of the same underlying theory”.
The Baul tradition is a unique heritage of Bengali folk music. The 19th century mystic poet Lalon Shah is the most celebrated practitioner of the tradition. Other folk music forms include Gombhira, Bhatiali and Bhawaiya. Hason Raja is a renowned folk poet of the Sylhet region. Folk music in Bengal is often accompanied by the ektara, a one-stringed instrument. Other instruments include the dotara, dhol, flute, and tabla. The region also has a rich heritage in North Indian classical music.
Bengali cuisine is the only traditionally developed multi-course tradition from the Indian subcontinent. Rice and fish are traditional favourite foods, leading to a saying that "fish and rice make a Bengali". Bengal's vast repertoire of fish-based dishes includes Hilsa preparations, a favourite among Bengalis. Bengalis make distinctive sweetmeats from milk products, including "Rôshogolla", "Chômchôm", and several kinds of "Pithe". The old city of Dhaka is noted for its distinct Indo-Islamic cuisine, including biryani, bakarkhani and kebab dishes.
There are 150 types of Bengali country boats plying the 700 rivers of the Bengal delta, the vast floodplain and many oxbow lakes. They vary in design and size. The boats include the dinghy and sampan among others. Country boats are a central element of Bengali culture and have inspired generations of artists and poets, including the ivory artisans of the Mughal era. The country has a long shipbuilding tradition, dating back many centuries. Wooden boats are made of timber such as "Jarul" (dipterocarpus turbinatus)," sal" (shorea robusta), "sundari" (heritiera fomes), and "Burma teak" (tectons grandis). Medieval Bengal was shipbuilding hub for the Mughal and Ottoman navies. The British Royal Navy later utilised Bengali shipyards in the 19th century, including for the Battle of Trafalgar.
Bengali women commonly wear the "shaŗi" and the salwar kameez, often distinctly designed according to local cultural customs. In urban areas, many women and men wear Western-style attire. Among men, European dressing has greater acceptance. Men also wear traditional costumes such as the "kurta" with "dhoti" or "pyjama", often on religious occasions. The lungi, a kind of long skirt, is widely worn by Bangladeshi men.
Durga Puja is the biggest festival of the Hindus in Bengal as well as the most significant socio-cultural event of the region in general.
The two Eids and Muharram are the important festivals for Muslims. Christmas (called Borodin in Bengali) is also a major festival where people irrespective of their beliefs and faiths participate. Other major festivals include Kali Puja, Saraswati Puja, Holi, Rath Jatra, Janmashtami, Poila Boishakh and Poush Parbon.
Bangladesh has a diverse, outspoken and privately owned press, with the largest circulated Bengali language newspapers in the world. English-language titles are popular in the urban readership. West Bengal had 559 published newspapers in 2005, of which 430 were in Bengali. Bengali cinema is divided between the media hubs of Kolkata and Dhaka.
Cricket and football are popular sports in the Bengal region. Local games include sports such as Kho Kho and Kabaddi, the latter being the national sport of Bangladesh. An Indo-Bangladesh "Bengali Games" has been organised among the athletes of the Bengali speaking areas of the two countries.

</doc>
<doc id="4864" url="https://en.wikipedia.org/wiki?curid=4864" title="Bucket argument">
Bucket argument

Isaac Newton's rotating bucket argument (also known as Newton's bucket) was designed to demonstrate that true rotational motion cannot be defined as the relative rotation of the body with respect to the immediately surrounding bodies. It is one of five arguments from the "properties, causes, and effects" of "true motion and rest" that support his contention that, in general, true motion and rest cannot be defined as special instances of motion or rest relative to other bodies, but instead can be defined only by reference to absolute space. Alternatively, these experiments provide an operational definition of what is meant by "absolute rotation", and do not pretend to address the question of "rotation relative to "what"?" General relativity dispenses with absolute space and with physics whose cause is external to the system, with the concept of geodesics of spacetime.
These arguments, and a discussion of the distinctions between absolute and relative time, space, place and motion, appear in a scholium at the end of Definitions sections in Book I of Newton's work, "The Mathematical Principles of Natural Philosophy" (1687) (not to be confused with General Scholium at the end of Book III), which established the foundations of classical mechanics and introduced his law of universal gravitation, which yielded the first quantitatively adequate dynamical explanation of planetary motion.
Despite their embrace of the principle of rectilinear inertia and the recognition of the kinematical relativity of apparent motion (which underlies whether the Ptolemaic or the Copernican system is correct), natural philosophers of the seventeenth century continued to consider true motion and rest as physically separate descriptors of an individual body. The dominant view Newton opposed was devised by René Descartes, and was supported (in part) by Gottfried Leibniz. It held that empty space is a metaphysical impossibility because space is nothing other than the extension of matter, or, in other words, that when one speaks of the space between things one is actually making reference to the relationship that exists between those things and not to some entity that stands between them. Concordant with the above understanding, any assertion about the motion of a body boils down to a description over time in which the body under consideration is at t found in the vicinity of one group of "landmark" bodies and at some t is found in the vicinity of some other "landmark" body or bodies.
Descartes recognized that there would be a real difference, however, between a situation in which a body with movable parts and originally at rest with respect to a surrounding ring was itself accelerated to a certain angular velocity with respect to the ring, and another situation in which the surrounding ring were given a contrary acceleration with respect to the central object. With sole regard to the central object and the surrounding ring, the motions would be indistinguishable from each other assuming that both the central object and the surrounding ring were absolutely rigid objects. However, if neither the central object nor the surrounding ring were absolutely rigid then the parts of one or both of them would tend to fly out from the axis of rotation.
For contingent reasons having to do with the Inquisition, Descartes spoke of motion as both absolute and relative. 
By the late 19th century, the contention that "all motion is relative" was re-introduced, notably by Ernst Mach (1883).
Newton discusses a bucket () filled with water hung by a cord. If the cord is twisted up tightly on itself and then the bucket is released, it begins to spin rapidly, not only with respect to the experimenter, but also in relation to the water it contains. (This situation would correspond to diagram B above.)
Although the relative motion at this stage is the greatest, the surface of the water remains flat, indicating that the parts of the water have no tendency to recede from the axis of relative motion, despite proximity to the pail. Eventually, as the cord continues to unwind, the surface of the water assumes a concave shape as it acquires the motion of the bucket spinning relative to the experimenter. This concave shape shows that the water is rotating, despite the fact that the water is at rest relative to the pail. In other words, it is not the relative motion of the pail and water that causes concavity of the water, contrary to the idea that motions can only be relative, and that there is no absolute motion. (This situation would correspond to diagram D.) Possibly the concavity of the water shows rotation relative to "something else": say absolute space? Newton says: "One can find out and measure the true and absolute circular motion of the water".
In the 1846 Andrew Motte translation of Newton's words:
The argument that the motion is absolute, not relative, is incomplete, as it limits the participants relevant to the experiment to only the pail and the water, a limitation that has not been established. In fact, the concavity of the water clearly involves gravitational attraction, and by implication the Earth also is a participant. Here is a critique due to Mach arguing that only relative motion is established:
The degree in which Mach's hypothesis is integrated in general relativity is discussed in the article Mach's principle; it is generally held that general relativity is not entirely Machian.
All observers agree that the surface of rotating water is curved. However, the explanation of this curvature involves centrifugal force for all observers with the exception of a truly stationary observer, who finds the curvature is consistent with the rate of rotation of the water as they observe it, with no need for an additional centrifugal force. Thus, a stationary frame can be identified, and it is not necessary to ask "Stationary with respect to what?":
A supplementary thought experiment with the same objective of determining the occurrence of absolute rotation also was proposed by Newton: the example of observing two identical spheres in rotation about their center of gravity and tied together by a string. Occurrence of tension in the string is indicative of absolute rotation; see Rotating spheres.
The historic interest of the rotating bucket experiment is its usefulness in suggesting one can detect absolute rotation by observation of the shape of the surface of the water. However, one might question just how rotation brings about this change. Below are two approaches to understanding the concavity of the surface of rotating water in a bucket.
The shape of the surface of a rotating liquid in a bucket can be determined using Newton's laws for the various forces on an element of the surface. For example, see Knudsen and Hjorth. The analysis begins with the free body diagram in the co-rotating frame where the water appears stationary. The height of the water "h" = "h"("r") is a function of the radial distance "r" from the axis of rotation Ω, and the aim is to determine this function. An element of water volume on the surface is shown to be subject to three forces: the vertical force due to gravity F, the horizontal, radially outward centrifugal force F, and the force normal to the surface of the water F due to the rest of the water surrounding the selected element of surface. The force due to surrounding water is known to be normal to the surface of the water because a liquid in equilibrium cannot support shear stresses. To quote Anthony and Brackett: Moreover, because the element of water does not move, the sum of all three forces must be zero. To sum to zero, the force of the water must point oppositely to the sum of the centrifugal and gravity forces, which means the surface of the water must adjust so its normal points in this direction. (A very similar problem is the design of a , where the slope of the turn is set so a car will not slide off the road. The analogy in the case of rotating bucket is that the element of water surface will "slide" up or down the surface unless the normal to the surface aligns with the vector resultant formed by the vector addition F + F.)
As "r" increases, the centrifugal force increases according to the relation (the equations are written per unit mass):
where "Ω" is the constant rate of rotation of the water. The gravitational force is unchanged at
where "g" is the acceleration due to gravity. These two forces add to make a resultant at an angle "φ" from the vertical given by
which clearly becomes larger as "r" increases. To ensure that this resultant is normal to the surface of the water, and therefore can be effectively nulled by the force of the water beneath, the normal to the surface must have the same angle, that is,
leading to the ordinary differential equation for the shape of the surface:
or, integrating:
where "h"(0) is the height of the water at "r" = 0. In other words, the surface of the water is parabolic in its dependence upon the radius.
The shape of the water's surface can be found in a different, very intuitive way using the interesting idea of the potential energy associated with the centrifugal force in the co-rotating frame.
In a reference frame uniformly rotating at angular rate Ω, the fictitious centrifugal force is conservative and has a potential energy of the form:
where "r" is the radius from the axis of rotation. This result can be verified by taking the gradient of the potential to obtain the radially outward force:
The meaning of the potential energy is that movement of a test body from a larger radius to a smaller radius involves doing work against the centrifugal force.
The potential energy is useful, for example, in understanding the concavity of the water surface in a rotating bucket. Notice that at equilibrium the surface adopts a shape such that an element of volume at any location on its surface has the same potential energy as at any other. That being so, no element of water on the surface has any incentive to move position, because all positions are equivalent in energy. That is, equilibrium is attained. On the other hand, were surface regions with lower energy available, the water occupying surface locations of higher potential energy would move to occupy these positions of lower energy, inasmuch as there is no barrier to lateral movement in an ideal liquid.
We might imagine deliberately upsetting this equilibrium situation by somehow momentarily altering the surface shape of the water to make it different from an equal-energy surface. This change in shape would not be stable, and the water would not stay in our artificially contrived shape, but engage in a transient exploration of many shapes until non-ideal frictional forces introduced by sloshing, either against the sides of the bucket or by the non-ideal nature of the liquid, killed the oscillations and the water settled down to the equilibrium shape.
To see the principle of an equal-energy surface at work, imagine gradually increasing the rate of rotation of the bucket from zero. The water surface is flat at first, and clearly a surface of equal potential energy because all points on the surface are at the same height in the gravitational field acting upon the water. At some small angular rate of rotation, however, an element of surface water can achieve lower potential energy by moving outward under the influence of the centrifugal force. Because water is incompressible and must remain within the confines of the bucket, this outward movement increases the depth of water at the larger radius, increasing the height of the surface at larger radius, and lowering it at smaller radius. The surface of the water becomes slightly concave, with the consequence that the potential energy of the water at the greater radius is increased by the work done against gravity to achieve the greater height. As the height of water increases, movement toward the periphery becomes no longer advantageous, because the reduction in potential energy from working with the centrifugal force is balanced against the increase in energy working against gravity. Thus, at a given angular rate of rotation, a concave surface represents the stable situation, and the more rapid the rotation, the more concave this surface. If rotation is arrested, the energy stored in fashioning the concave surface must be dissipated, for example through friction, before an equilibrium flat surface is restored.
To implement a surface of constant potential energy quantitatively, let the height of the water be formula_10: then the potential energy per unit mass contributed by gravity is formula_11 and the total potential energy per unit mass on the surface is
with formula_13 the background energy level independent of "r". In a static situation (no motion of the fluid in the rotating frame), this energy is constant independent of position "r". Requiring the energy to be constant, we obtain the parabolic form:
where "h(0)" is the height at "r" = 0 (the axis). See Figures 1 and 2.
The principle of operation of the centrifuge also can be simply understood in terms of this expression for the potential energy, which shows that it is favorable energetically when the volume far from the axis of rotation is occupied by the heavier substance.

</doc>
<doc id="4865" url="https://en.wikipedia.org/wiki?curid=4865" title="Roman Breviary">
Roman Breviary

The Roman Breviary (Latin: "Breviarium Romanum") is the liturgical book of the Latin liturgical rites of the Catholic Church containing the public or canonical prayers, hymns, the Psalms, readings, and notations for everyday use, especially by bishops, priests, and deacons in the Divine Office (i.e., at the canonical hours or Liturgy of the Hours, the Christians' daily prayer). 
The volume containing the daily hours of Catholic prayer was published as the "Breviarium Romanum" (Roman Breviary) from its "editio princeps" in 1568 under Pope Pius V until the reforms of Paul VI (1974), when it became known as the Liturgy of the Hours. 
In the course of the Catholic Counter-Reformation, Pope Pius V (r. 1566–1572) imposed the use of the Roman Breviary, mainly based on the "Breviarium secundum usum Romanae Curiae", on the whole Roman Catholic Church. Exceptions are the Benedictines and Dominicans, who have Breviaries of their own, and
two surviving local breviaries,
The Latin word "breviarium" generally signifies "abridgement, compendium". This wider sense has often been used by Christian authors, e.g. "Breviarium fidei, Breviarium in psalmos, Breviarium canonum, Breviarium regularum". 
In liturgical language specifically, "breviary" ("breviarium") has a special meaning, indicating a book furnishing the regulations for the celebration of Mass or the canonical Office, and may be met with under the titles 
"Breviarium Ecclesiastici Ordinis", or "Breviarium Ecclesiæ Romanæ". 
In the 9th century, Alcuin uses the word to designate an office abridged or simplified for the use of the laity. Prudentius of Troyes, about the same period, composed a "Breviarium Psalterii". 
In an ancient inventory occurs "Breviarium Antiphonarii", meaning "Extracts from the Antiphonary". In the "Vita Aldrici" occurs "sicut in plenariis et breviariis Ecclesiæ ejusdem continentur". 
Again, in the inventories in the catalogues, such notes as these may be met with: "Sunt et duo cursinarii et tres benedictionales Libri; ex his unus habet obsequium mortuorum et unus Breviarius", or, "Præter Breviarium quoddam quod usque ad festivitatem S. Joannis Baptistæ retinebunt", etc. 
Monte Cassino in c. 1100 obtained a book titled "Incipit Breviarium sive Ordo Officiorum per totam anni decursionem".
From such references, and from others of a like nature, Quesnel gathers that by the word "Breviarium" was at first designated a book furnishing the rubrics, a sort of Ordo. The title Breviary, as we employ it—that is, a book containing the entire canonical office—appears to date from the 11th century.
Pope Gregory VII (r. 1073–1085) having, indeed, abridged the order of prayers, and having simplified the Liturgy as performed at the Roman Court, this abridgment received the name of Breviary, which was suitable, since, according to the etymology of the word, it was an abridgment. 
The name has been extended to books which contain in one volume, or at least in one work, liturgical books of different kinds, such as the Psalter, the Antiphonary, the Responsoriary, the Lectionary, etc. 
In this connection it may be pointed out that in this sense the word, as it is used nowadays, is illogical; it should be named a Plenarium rather than a Breviarium, since, liturgically speaking, the word Plenarium exactly designates such books as contain several different compilations united under one cover.
The canonical hours of the Breviary owe their remote origin to the Old Covenant when God commanded the Aaronic priests to offer morning and evening sacrifices. Other inspiration may have come from David's words in the Psalms "Seven times a day I praise you" (Ps. 119:164), as well as, "the just man meditates on the law day and night" (Ps. 1:2). Regarding Daniel "Three times daily he was kneeling and offering prayers and thanks to his God" (Dan. 6:10).
In the early days of Christian worship the Sacred Scriptures furnished all that was thought necessary, containing as it did the books from which the lessons were read and the psalms that were recited. The first step in the evolution of the Breviary was the separation of the Psalter into a choir-book. At first the president of the local church (bishop) or the leader of the choir chose a particular psalm as he thought appropriate. From about the 4th century certain psalms began to be grouped together, a process that was furthered by the monastic practice of daily reciting the 150 psalms. This took so much time that the monks began to spread it over a week, dividing each day into hours, and allotting to each hour its portion of the Psalter. St Benedict in the 6th century drew up such an arrangement, probably, though not certainly, on the basis of an older Roman division which, though not so skilful, is the one in general use. Gradually there were added to these psalter choir-books additions in the form of antiphons, responses, collects or short prayers, for the use of those not skilful at improvisation and metrical compositions. Jean Beleth, a 12th-century liturgical author, gives the following list of books necessary for the right conduct of the canonical office: the Antiphonarium, the Old and New Testaments, the "Passionarius (liber)" and the "Legendarius" (dealing respectively with martyrs and saints), the "Homiliarius" (homilies on the Gospels), the "Sermologus" (collection of sermons) and the works of the Fathers, besides, of course, the "Psalterium" and the "Collectarium". To overcome the inconvenience of using such a library the Breviary came into existence and use. Already in the 9th century Prudentius, bishop of Troyes, had in a "Breviarium Psalterii" made an abridgment of the Psalter for the laity, giving a few psalms for each day, and Alcuin had rendered a similar service by including a prayer for each day and some other prayers, but no lessons or homilies. 
The Breviary, rightly so called, only dates from the 11th century; the earliest MS. containing the whole canonical office, is of the year 1099 and is in the Mazarin library. Gregory VII (pope 1073–1085), too, simplified the liturgy as performed at the Roman court, and gave his abridgment the name of Breviary, which thus came to denote a work which from another point of view might be called a Plenary, involving as it did the collection of several works into one. There are several extant specimens of 12th-century Breviaries, all Benedictine, but under Innocent III (pope 1198–1216) their use was extended, especially by the newly founded and active Franciscan order. These preaching friars, with the authorization of Gregory IX, adopted (with some modifications, e.g. the substitution of the "Gallican" for the "Roman" version of the Psalter) the Breviary hitherto used exclusively by the Roman court, and with it gradually swept out of Europe all the earlier partial books (Legendaries, Responsories), etc., and to some extent the local Breviaries, like that of Sarum. Finally, Nicholas III (pope 1277–1280) adopted this version both for the curia and for the basilicas of Rome, and thus made its position secure.
Before the rise of the mendicant orders (wandering friars) in the 13th century, the daily services were usually contained in a number of large volumes. The first occurrence of a single manuscript of the daily office was written by the Benedictine order at Monte Cassino in Italy in 1099. The Benedictines were not a mendicant order, but a stable, monastery-based order, and single-volume breviaries are rare from this early period.
The arrangement of the Psalms in the Rule of St. Benedict had a profound impact upon the breviaries used by secular and monastic clergy alike, until 1911 when Pope Pius X introduced his reform of the Roman Breviary. In many places, every diocese, order or ecclesiastical province maintained its own edition of the breviary.
However, mendicant friars travelled frequently and needed a shortened, or abbreviated, daily office contained in one portable book, and single-volume breviaries flourished from the thirteenth century onwards. These abbreviated volumes soon became very popular and eventually supplanted the Catholic Church's Curia office, previously said by non-monastic clergy.
Before the advent of printing, breviaries were written by hand and were often richly decorated with initials and miniature illustrations telling stories in the lives of Christ or the saints, or stories from the Bible. Later printed breviaries usually have woodcut illustrations, interesting in their own right but with poor relation to the beautifully illuminated breviaries.
The beauty and value of many of the Latin Breviaries were brought to the notice of English churchmen by one of the numbers of the Oxford "Tracts for the Times", since which time they have been much more studied, both for their own sake and for the light they throw upon the English Prayer-Book.
From a bibliographical point of view some of the early printed Breviaries are among the rarest of literary curiosities, being merely local. The copies were not spread far, and were soon worn out by the daily use made of them. Doubtless many editions have perished without leaving a trace of their existence, while others are known by unique copies. In Scotland the only one which has survived the convulsions of the 16th century is "Aberdeen Breviary", a Scottish form of the Sarum Office (the Sarum Rite was much favoured in Scotland as a kind of protest against the jurisdiction claimed by the diocese of York), revised by William Elphinstone (bishop 1483–1514), and printed at Edinburgh by Walter Chapman and Androw Myllar in 1509–1510. Four copies have been preserved of it, of which only one is complete; but it was reprinted in facsimile in 1854 for the Bannatyne Club by the munificence of the Duke of Buccleuch. It is particularly valuable for the trustworthy notices of the early history of Scotland which are embedded in the lives of the national saints. Though enjoined by royal mandate in 1501 for general use within the realm of Scotland, it was probably never widely adopted. The new Scottish "Proprium" sanctioned for the Catholic province of St Andrews in 1903 contains many of the old Aberdeen collects and antiphons.
The Sarum or Salisbury Breviary itself was very widely used. The first edition was printed at Venice in 1483 by Raynald de Novimagio in folio; the latest at Paris, 1556, 1557. While modern Breviaries are nearly always printed in four volumes, one for each season of the year, the editions of the Sarum never exceeded two parts.
Until the Council of Trent (1545–1563) and the Catholic Counter-Reformation, every bishop had full power to regulate the Breviary of his own diocese; and this was acted upon almost everywhere. Each monastic community, also, had one of its own. Pope Pius V (r. 1566–1572), however, while sanctioning those which could show at least 200 years of existence, made the Roman obligatory in all other places. But the influence of the Roman rite has gradually gone much beyond this, and has superseded almost all the local uses. The Roman has thus become nearly universal, with the allowance only of additional offices for saints specially venerated in each particular diocese. 
The Roman Breviary has undergone several revisions: The most remarkable of these is that by Francis Quignonez, cardinal of Santa Croce in Gerusalemme (1536), which, though not accepted by Rome (it was approved by Clement VII and Paul III, and permitted as a substitute for the unrevised Breviary, until Pius V in 1568 excluded it as too short and too modern, and issued a reformed edition of the old Breviary, the "Breviarium Pianum" or "Pian Breviary"), formed the model for the still more thorough reform made in 1549 by the Church of England, whose daily morning and evening services are but a condensation and simplification of the Breviary offices. 
Some parts of the prefaces at the beginning of the English Prayer-Book are free translations of those of Quignonez. The Pian Breviary was again altered by Sixtus V in 1588, who introduced the revised Vulgate, in 1602 by Clement VIII (through Baronius and Bellarmine), especially as concerns the rubrics, and by Urban VIII (1623–1644), a purist who altered the text of certain hymns.
In the 17th and 18th centuries a movement of revision took place in France, and succeeded in modifying about half the Breviaries of that country. Historically, this proceeded from the labours of Jean de Launoy (1603–1678), "le dénicheur des saints", and Louis Sébastien le Nain de Tillemont, who had shown the falsity of numerous lives of the saints; theologically it was produced by the Port Royal school, which led men to dwell more on communion with God as contrasted with the invocation of the saints. This was mainly carried out by the adoption of a rule that all antiphons and responses should be in the exact words of Scripture, which, of course, cut out the whole class of appeals to created beings. The services were at the same time simplified and shortened, and the use of the whole Psalter every week (which had become a mere theory in the Roman Breviary, owing to its frequent supersession by saints' day services) was made a reality. These reformed French Breviaries—e.g. the Paris Breviary of 1680 by Archbishop François de Harlay (1625–1695) and that of 1736 by Archbishop Charles-Gaspard-Guillaume de Vintimille du Luc (1655–1746)—show a deep knowledge of Holy Scripture, and much careful adaptation of different texts.
During the pontificate of Pius IX a strong Ultramontane movement arose against the French Breviaries of 1680 and 1736. This was inaugurated by Montalembert, but its literary advocates were chiefly Dom Gueranger, a learned Benedictine monk, abbot of Solesmes, and Louis Veuillot (1813–1883) of the Univers; and it succeeded in suppressing them everywhere, the last diocese to surrender being Orleans in 1875. The Jansenist and Gallican influence was also strongly felt in Italy and in Germany, where Breviaries based on the French models were published at Cologne, Münster, Mainz and other towns. Meanwhile, under the direction of Benedict XIV (pope 1740–1758), a special congregation collected much material for an official revision, but nothing was published. In 1902, under Leo XIII, a commission under the presidency of Monsignor Louis Duchesne was appointed to consider the Breviary, the Missal, the Pontifical and the Ritual.
Significant changes came in 1910 with the reform of the Roman Breviary by Pope Pius X. This revision modified the traditional psalm scheme so that, while all 150 psalms were used in the course of the week, these were said without repetition. Those assigned to the Sunday office underwent the least revision, although noticeably fewer psalms are recited at Matins, and both Lauds and Compline are slightly shorter due to psalms (or in the case of Compline the first few verses of a psalm) being removed. Pius X was probably influenced by earlier attempts to eliminate repetition in the psalter, most notably the liturgy of the Benedictine congregation of St. Maur. However, since Cardinal Quignonez's attempt to reform the Breviary employed this principle—albeit with no regard to the traditional scheme—such notions had floated around in the western Church, and can particularly be seen in the Paris Breviary.
Pope Pius XII introduced optional use of a new translation of the Psalms from the Hebrew to a more classical Latin. Most breviaries published in the late 1950s and early 1960s used this "Pian Psalter".
Pope John XXIII also revised the Breviary in 1960, introducing changes drawn up by his predecessor Pope Pius XII. The most notable alteration is the shortening of most feasts from nine to three lessons at Matins, keeping only the Scripture readings (the former lesson i, then lessons ii and iii together), followed by either the first part of the patristic reading (lesson vii) or, for most feasts, a condensed version of the former second Nocturn, which was formerly used when a feast was reduced in rank and commemorated.
At the beginning stands the usual introductory matter, such as the tables for determining the date of Easter, the calendar, and the general rubrics. The Breviary itself is divided into four seasonal parts—winter, spring, summer, autumn—and comprises under each part:
These parts are often published separately.
This psalm book is the very backbone of the Breviary, the groundwork of the Catholic prayer-book; out of it have grown the antiphons, responsories and versicles. Until the 1911 reform, the psalms were arranged according to a disposition dating from the 8th century, as follows: Psalms 1-108, with some omissions, were recited at Matins, twelve each day from Monday to Saturday, and eighteen on Sunday. The omissions were said at Lauds, Prime and Compline. Psalms 109-147 (except 117, 118, and 142) were said at Vespers, five each day. Psalms 148-150 were always used at Lauds, and give that hour its name. The text of this Psalter is that commonly known as the Gallican. The name is misleading, for it is simply the second revision (A.D. 392) made by Jerome of the old "Itala" version originally used in Rome. Jerome's first revision of the "Itala" (A.D. 383), known as the Roman, is still used at St Peter's in Rome, but the "Gallican", thanks especially to St Gregory of Tours, who introduced it into Gaul in the 6th century, has ousted it everywhere else. The Antiphonary of Bangor proves that Ireland accepted the Gallican version in the 7th century, and the English Church did so in the 10th.
Following the 1911 reform, Matins was reduced to nine Psalms every day, with the other psalms redistributed throughout Prime, Terce, Sext, and Compline. For Sundays and special feasts Lauds and Vespers largely remained the same, Psalm 118 remained distributed at the Little Hours and Psalms 4, 90, and 130 were kept at Compline.
This contains the office of the seasons of the Christian year (Advent to Trinity), a conception that only gradually grew up. There is here given the whole service for every Sunday and weekday, the proper antiphons, responsories, hymns, and especially the course of daily Scripture reading, averaging about twenty verses a day, and (roughly) arranged thus:
This contains the lessons, psalms and liturgical formularies for saints' festivals, and depends on the days of the secular month. The readings of the second Nocturn are mainly hagiological biography, with homilies or papal documents for certain major feasts, particularly those of Jesus and Mary. Some of this material has been revised by Leo XIII, in view of archaeological and other discoveries. The third Nocturn consists of a homily on the Gospel which is read at that day's Mass. Covering a great stretch of time and space, they do for the worshipper in the field of church history what the Scripture readings do in that of biblical history.
This comprises psalms, antiphons, lessons, &c., for feasts of various groups or classes (twelve in all); e.g. apostles, martyrs, confessors, virgins, and the Blessed Virgin Mary. These offices are of very ancient date, and many of them were probably in origin proper to individual saints. They contain passages of great literary beauty. The lessons read at the third nocturn are patristic homilies on the Gospels, and together form a rough summary of theological instruction.
Here are found the Little Office of the Blessed Virgin Mary, the Office for the Dead (obligatory on All Souls' Day), and offices peculiar to each diocese.
It has already been indicated, by reference to Matins, Lauds, &c., that not only each day, but each part of the day, has its own office, the day being divided into liturgical "hours." A detailed account of these will be found in the article Canonical Hours. Each of the hours of the office is composed of the same elements, and something must be said now of the nature of these constituent parts, of which mention has here and there been already made. They are: psalms (including canticles), antiphons, responsories, hymns, lessons, little chapters, versicles and collects.
Before the 1911 reform, the multiplication of saints' festivals, with practically the same festal psalms, tended to repeat the about one-third of the Psalter, with a correspondingly rare recital of the remaining two-thirds. Following this reform, the entire Psalter is again generally recited each week, with the festal psalms restricted to only the highest-ranking feasts. As in the Greek usage and in the Benedictine, certain canticles like the Song of Moses (Exodus xv.), the Song of Hannah (1 Sam. ii.), the prayer of Habakkuk (iii.), the prayer of Hezekiah (Isaiah xxxviii.) and other similar Old Testament passages, and, from the New Testament, the Magnificat, the Benedictus and the Nunc dimittis, are admitted as psalms.
The antiphons are short liturgical forms, sometimes of biblical, sometimes of patristic origin, used to introduce a psalm. The term originally signified a chant by alternate choirs, but has quite lost this meaning in the Breviary.
The responsories are similar in form to the antiphons, but come at the end of the psalm, being originally the reply of the choir or congregation to the precentor who recited the psalm.
The hymns are short poems going back in part to the days of Prudentius, Synesius, Gregory of Nazianzus and Ambrose (4th and 5th centuries), but mainly the work of medieval authors.
The lessons, as has been seen, are drawn variously from the Bible, the Acts of the Saints and the Fathers of the Church. In the primitive church, books afterwards excluded from the canon were often read, e.g. the letters of Clement of Rome and the Shepherd of Hermas. In later days the churches of Africa, having rich memorials of martyrdom, used them to supplement the reading of Scripture. Monastic influence accounts for the practice of adding to the reading of a biblical passage some patristic commentary or exposition. Books of homilies were compiled from the writings of SS. Augustine, Hilary, Athanasius, Isidore, Gregory the Great and others, and formed part of the library of which the Breviary was the ultimate compendium. In the lessons, as in the psalms, the order for special days breaks in upon the normal order of ferial offices and dislocates the scheme for consecutive reading. The lessons are read at Matins (which is subdivided into three nocturns).
The little chapters are very short lessons read at the other "hours."
The versicles are short responsories used after the little chapters in the minor hours. They appear after the hymns in Lauds and Vespers.
The collects come at the close of the office and are short prayers summing up the supplications of the congregation. They arise out of a primitive practice on the part of the bishop (local president), examples of which are found in the Didachē (Teaching of the Apostles) and in the letters of Clement of Rome and Cyprian. With the crystallization of church order, improvisation in prayer largely gave place to set forms, and collections of prayers were made which later developed into Sacramentaries and Orationals. The collects of the Breviary are largely drawn from the Gelasian and other Sacramentaries, and they are used to sum up the dominant idea of the festival in connection with which they happen to be used.
Before 1910, the difficulty of harmonizing the "Proprium de Tempore" and the "Proprium Sanctorum", to which reference has been made, was only partly met in the thirty-seven chapters of general rubrics. Additional help was given by a kind of Catholic Churchman's Almanack, called the "Ordo Recitandi Divini Officii", published in different countries and dioceses, and giving, under every day, minute directions for proper reading. In 1960, John XXIII simplified the rubrics governing the Breviary in order to make it easier to use.
Every cleric in Holy Orders, and many other members of religious orders, must publicly join in or privately read aloud (i.e. using the lips as well as the eyes—it takes about two hours in this way) the whole of the Breviary services allotted for each day. In large churches where they were celebrated the services were usually grouped; e.g. Matins and Lauds (about 7.30 A.M.); Prime, Terce (High Mass), Sext, and None (about 10 A.M.); Vespers and Compline (4 P.M.); and from four to eight hours (depending on the amount of music and the number of high masses) are thus spent in choir.
Lay use of the Breviary has varied throughout the Church's history. In some periods laymen did not use the Breviary as a manual of devotion to any great extent. The late Medieval period saw the recitation of certain hours of the Little Office of the Blessed Virgin, which was based on the Breviary in form and content, becoming popular among those who could read, and Bishop Challoner did much to popularise the hours of Sunday Vespers and Compline (albeit in English translation) in his "Garden of the Soul" in the eighteenth century. The Liturgical Movement in the twentieth century saw renewed interest in the Offices of the Breviary and several popular editions were produced, containing the vernacular as well as the Latin.
The complete pre-Pius X Roman Breviary was translated into English (by the Marquess of Bute in 1879; new ed. with a trans, of the Martyrology, 1908), French and German. Bute's version is noteworthy for its inclusion of the skilful renderings of the ancient hymns by J.H. Newman, J.M. Neale and others. Several editions of the Pius X Breviary were produced during the twentieth century, including a notable edition prepared with the assistance of the sisters of Stanbrook Abbey in the 1950s. Two editions in English and Latin were produced in the following decade, which conformed to the rubrics of 1960, published by Liturgical Press and Benziger in the United States. These used the Pius XII psalter. Baronius Press's revised edition of the Liturgical Press edition uses the older Gallican psalter of St. Jerome. This edition was published and released in 2012 for pre-orders only. In 2013, the publication has resumed printing and is available on Baronius' website.
Under Pope Benedict XVI's motu proprio Summorum Pontificum, Catholic bishops, priests, and deacons are again permitted to use the 1961 edition of the Roman Breviary, promulgated by Pope John XXIII to satisfy their obligation to recite the Divine Office every day.
In 2008, an "i-breviary" was launched, which combines the ancient breviaries with the latest computer technology.

</doc>
<doc id="4866" url="https://en.wikipedia.org/wiki?curid=4866" title="Boomer">
Boomer

Boomer may refer to:

</doc>
<doc id="4868" url="https://en.wikipedia.org/wiki?curid=4868" title="B. F. Skinner">
B. F. Skinner

Burrhus Frederic Skinner (March 20, 1904 – August 18, 1990) was an American psychologist, behaviorist, author, inventor, and social philosopher. He was a professor of psychology at Harvard University from 1958 until his retirement in 1974.
Considering free will to be an illusion, Skinner saw human action as dependent on consequences of previous actions, a theory he would articulate as the "principle of reinforcement": If the consequences to an action are bad, there is a high chance the action will not be repeated; if the consequences are good, the probability of the action being repeated becomes stronger.
Skinner developed behavior analysis, especially the philosophy of "radical behaviorism", and founded the experimental analysis of behavior, a school of experimental research psychology. He also used operant conditioning to strengthen behavior, considering the rate of response to be the most effective measure of response strength. To study operant conditioning, he invented the operant conditioning chamber (aka the Skinner Box), and to measure rate he invented the cumulative recorder. Using these tools, he and Charles Ferster produced Skinner's most influential experimental work, outlined in their book "Schedules of Reinforcement" (1957).
Skinner was a prolific author, having published 21 books and 180 articles. He imagined the application of his ideas to the design of a human community in his utopian novel, "Walden Two" (1948), while his analysis of human behavior culminated in his work, "Verbal Behavior".
Contemporary academia considers Skinner, along with John B. Watson and Ivan Pavlov, a pioneer of modern behaviorism. Accordingly, a June 2002 survey listed Skinner as the most influential psychologist of the 20th century.
Skinner was born in Susquehanna, Pennsylvania, to Grace and William Skinner, the latter of whom was a lawyer. Skinner became an atheist after a Christian teacher tried to assuage his fear of the hell that his grandmother described. His brother Edward, two and a half years younger, died at age 16 of a cerebral hemorrhage.
Skinner's closest friend as a young boy was Raphael Miller, whom he called Doc because his father was a doctor. Doc and Skinner became friends due to their parents’ religiousness and both had an interest in contraptions and gadgets. They had set up a telegraph line between their houses to send messages to each other, although they had to call each other on the telephone due to the confusing messages sent back and forth. During one summer, Doc and Skinner started an elderberry business to gather berries and sell them door to door. They had found that out when they picked the ripe berries, the unripe ones came off the branches too, so they built a device that was able to separate them. The device was a bent piece of metal to form a trough. They would pour water down the trough into a bucket, and the ripe berries would sink into the bucket and the unripe ones would be pushed over the edge to be thrown away.
Skinner attended Hamilton College in New York with the intention of becoming a writer. He found himself at a social disadvantage at the College because of his intellectual attitude.
The school was known for being a strong fraternity college, and Skinner joined Lambda Chi Alpha fraternity while attending. Skinner had thought that his fraternity brothers would be respectful and would not haze or mistreat the newcomers, instead helping out the other boys with courses or other activities. Contrary to his expectations, at Lambda Chi Alpha freshmen were called “slimers” who had to wear small green knit hats and greet everyone that they passed for punishment. The year before Skinner entered Hamilton, there was a hazing accident that caused the death of a student. The freshman was asleep in his bed when he was pushed onto the floor, where he smashed his head, resulting in his death. Skinner had a similar incident where two freshmen captured him and tied him to a pole, where he should have stayed all night, but he had a razor blade in his shoe for emergency and managed to cut himself free.
He wrote for the school paper, but, as an atheist, he was critical of the traditional mores of his college. After receiving his Bachelor of Arts in English literature in 1926, he attended Harvard University, where he would later research, teach, and eventually become a prestigious board member. While attending Harvard, a fellow student, Fred Keller, convinced Skinner that he could make an experimental science from the study of behavior. This led Skinner to invent a prototype for the Skinner Box and to join Keller in the creation of other tools for small experiments.
After graduation, Skinner unsuccessfully tried to write a great novel while he lived with his parents, a period that he later called the 'Dark Years.' He became disillusioned with his literary skills despite encouragement from the renowned poet Robert Frost, concluding that he had little world experience and no strong personal perspective from which to write. His encounter with John B. Watson's "Behaviorism" led him into graduate study in psychology and to the development of his own version of behaviorism.
Skinner received a PhD from Harvard in 1931, and remained there as a researcher until 1936. He then taught at the University of Minnesota at Minneapolis and later at Indiana University, where he was chair of the psychology department from 1946–1947, before returning to Harvard as a tenured professor in 1948. He remained at Harvard for the rest of his life. In 1973, Skinner was one of the signers of the Humanist Manifesto II.
In 1936, Skinner married Yvonne (Eve) Blue. The couple had two daughters, Julie (m. Vargas) and Deborah (m. Buzan). Yvonne died in 1997, and is buried in Mount Auburn Cemetery, Cambridge, Massachusetts.
Skinner's public exposure had increased in the 1970s, he remained active even after his retirement in 1974, until his death. In 1989, Skinner was diagnosed with leukemia and died on August 18, 1990, in Cambridge, Massachusetts. Ten days before his death, he was given the lifetime achievement award by the American Psychological Association and gave a talk in an auditorium concerning his work.
Skinner referred to his approach to the study of behavior as "radical behaviorism". This philosophy of behavioral science assumes that behavior is a consequence of environmental histories of reinforcement (see applied behavior analysis). In his words: 
Skinner's ideas about behaviorism were largely set forth in his first book, "Behavior of Organisms" (1938). Here, he gives a systematic description of the manner in which environmental variables control behavior. He distinguished two sorts of behavior which are controlled in different ways:
Both of these sorts of behavior had already been studied experimentally, most notably: respondents, by Ivan Pavlov; and operants, by Edward Thorndike. Skinner's account differed in some ways from earlier ones, and was one of the first accounts to bring them under one roof.
The idea that behavior is strengthened or weakened by its consequences raises several questions. Among the most important are these:
Skinner's answer to the first question was very much like Darwin's answer to the question of the origin of a 'new' bodily structure, namely, variation and selection. Similarly, the behavior of an individual varies from moment to moment; a variation that is followed by reinforcement is strengthened and becomes prominent in that individual's behavioral repertoire. "Shaping" was Skinner's term for the gradual modification of behavior by the reinforcement of desired variations. Skinner believed that 'superstitious' behavior can arise when a response happens to be followed by reinforcement to which it is actually unrelated.
The second question, "how is operant behavior controlled?" arises because, to begin with, the behavior is "emitted" without reference to any particular stimulus. Skinner answered this question by saying that a stimulus comes to control an operant if it is present when the response is reinforced and absent when it is not. For example, if lever-pressing only brings food when a light is on, a rat, or a child, will learn to press the lever only when the light is on. Skinner summarized this relationship by saying that a discriminative stimulus (e.g. light) sets the occasion for the reinforcement (food) of the operant (lever-press). This three-term contingency (stimulus-response-reinforcer) is one of Skinner's most important concepts, and sets his theory apart from theories that use only pair-wise associations.
Most behavior of humans cannot easily be described in terms of individual responses reinforced one by one, and Skinner devoted a great deal of effort to the problem of behavioral complexity. Some complex behavior can be seen as a sequence of relatively simple responses, and here Skinner invoked the idea of "chaining". Chaining is based on the fact, experimentally demonstrated, that a discriminative stimulus not only sets the occasion for subsequent behavior, but it can also reinforce a behavior that precedes it. That is, a discriminative stimulus is also a "conditioned reinforcer". For example, the light that sets the occasion for lever pressing may also be used to reinforce "turning around" in the presence of a noise. This results in the sequence "noise – turn-around – light – press lever – food." Much longer chains can be built by adding more stimuli and responses.
However, Skinner recognized that a great deal of behavior, especially human behavior, cannot be accounted for by gradual shaping or the construction of response sequences. Complex behavior often appears suddenly in its final form, as when a person first finds his way to the elevator by following instructions given at the front desk. To account for such behavior, Skinner introduced the concept of rule-governed behavior. First, relatively simple behaviors come under the control of verbal stimuli: the child learns to "jump," "open the book," and so on. After a large number of responses come under such verbal control, a sequence of verbal stimuli can evoke an almost unlimited variety of complex responses.
Reinforcement, a key concept of behaviorism, is the primary process that shapes and controls behavior, and occurs in two ways: "positive" and "negative". In "The Behavior of Organisms" (1938), Skinner defines "negative reinforcement" to be synonymous with "punishment", i.e. the presentation of an aversive stimulus. This definition would subsequently be re-defined in "Science and Human Behavior" (1953).
In what has now become the standard set of definitions, "positive" reinforcement is the strengthening of behavior by the occurrence of some event (e.g., praise after some behavior is performed), whereas "negative" reinforcement is the strengthening of behavior by the removal or avoidance of some aversive event (e.g., opening and raising an umbrella over your head on a rainy day is reinforced by the cessation of rain falling on you).
Both types of reinforcement strengthen behavior, or increase the probability of a behavior reoccurring; the difference being in whether the reinforcing event is something applied (positive reinforcement) or something removed or avoided (negative reinforcement). Punishment can be the "application" of an aversive stimulus/event (positive punishment or punishment by contingent stimulation) or the "removal" of a desirable stimulus (negative punishment or punishment by contingent withdrawal). Though punishment is often used to suppress behavior, Skinner argued that this suppression is temporary and has a number of other, often unwanted, consequences. Extinction is the absence of a rewarding stimulus, which weakens behavior.
Writing in 1981, Skinner pointed out that Darwinian natural selection is, like reinforced behavior, "selection by consequences." Though, as he said, natural selection has now "made its case," he regretted that essentially the same process, "reinforcement", was less widely accepted as underlying human behavior.
Skinner recognized that behavior is typically reinforced more than once, and, together with Charles Ferster, he did an extensive analysis of the various ways in which reinforcements could be arranged over time, calling it the "schedules of reinforcement".
The most notable schedules of reinforcement studied by Skinner were continuous, interval (fixed or variable), and ratio (fixed or variable). All are methods used in operant conditioning.
Skinnerian principles have been used to create token economies in a number of institutions, such as psychiatric hospitals. When participants behave in desirable ways, their behavior is reinforced with tokens that can be changed for such items as candy, cigarettes, coffee, or the exclusive use of a radio or television set.
Challenged by Alfred North Whitehead during a casual discussion while at Harvard to provide an account of a randomly provided piece of verbal behavior, Skinner set about attempting to extend his then-new functional, inductive approach to the complexity of human verbal behavior. Developed over two decades, his work appeared in the book "Verbal Behavior". Although Noam Chomsky was highly critical of "Verbal Behavior", he conceded that Skinner's "S-R psychology" was worth a review. (Behavior analysts reject the "S-R" characterization: operant conditioning involves the emission of a response which then becomes more or less likely depending upon its consequence.)
"Verbal Behavior" had an uncharacteristically cool reception, partly as a result of Chomsky's review, partly because of Skinner's failure to address or rebut any of Chomsky's criticisms. Skinner's peers may have been slow to adopt the ideas presented in "Verbal Behavior" because of the absence of experimental evidence—unlike the empirical density that marked Skinner's experimental work.
An operant conditioning chamber (also known as a Skinner Box) is a laboratory apparatus used in the experimental analysis of animal behavior. It was invented by Skinner while he was a graduate student at Harvard University. As used by Skinner, the box had a lever (for rats), or a disk in one wall (for pigeons). A press on this "manipulandum" could deliver food to the animal through an opening in the wall, and responses reinforced in this way increased in frequency. By controlling this reinforcement together with discriminative stimuli such as lights and tones, or punishments such as electric shocks, experimenters have used the operant box to study a wide variety of topics, including schedules of reinforcement, discriminative control, delayed response ("memory"), punishment, and so on. By channeling research in these directions, the operant conditioning chamber has had a huge influence on course of research in animal learning and its applications. It enabled great progress on problems that could be studied by measuring the rate, probability, or force of a simple, repeatable response. However, it discouraged the study of behavioral processes not easily conceptualized in such terms—spatial learning, in particular, which is now studied in quite different ways, for example, by the use of the water maze.
The cumulative recorder makes a pen-and-ink record of simple repeated responses. Skinner designed it for use with the operant chamber as a convenient way to record and view the rate of responses such as a lever press or a key peck. In this device, a sheet of paper gradually unrolls over a cylinder. Each response steps a small pen across the paper, starting at one edge; when the pen reaches the other edge, it quickly resets to the initial side. The slope of the resulting ink line graphically displays the rate of the response; for example, rapid responses yield a steeply sloping line on the paper, slow responding yields a line of low slope. The cumulative recorder was a key tool used by Skinner in his analysis of behavior, and it was very widely adopted by other experimenters, gradually falling out of use with the advent of the laboratory computer. Skinner's major experimental exploration of response rates, presented in his book with Charles Ferster, "Schedules of Reinforcement", is full of cumulative records produced by this device.
The air crib is an easily cleaned, temperature- and humidity-controlled box-bed intended to replace the standard infant crib. Skinner invented the device to help his wife cope with the day-to-day tasks of child rearing. It was designed to make early childcare simpler (by reducing laundry, diaper rash, cradle cap, etc.), while allowing the baby to be more mobile and comfortable, and less prone to cry. Reportedly it had some success in these goals.
The air crib was a controversial invention. It was popularly mischaracterized as a cruel pen, and it was often compared to Skinner's operant conditioning chamber (aka the 'Skinner Box'). This association with laboratory animal experimentation discouraged its commercial success, though several companies attempted production.
Psychologist Lauren Slater's 2004 book, "Opening Skinner's Box", caused a stir by mentioning the rumors that Skinner had used his baby daughter, Deborah, in some of his experiments, and that she had subsequently committed suicide. Although Slater's book rejected such rumors as false, a reviewer in "The Observer" in March 2004 misquoted Slater's book as supporting the rumors. This review was read by Deborah Skinner (now Deborah Buzan), who wrote a vehement riposte in "The Guardian".
The teaching machine was a mechanical device whose purpose was to administer a curriculum of programmed learning. The machine embodies key elements of Skinner's theory of learning and had important implications for education in general and classroom instruction in particular.
In one incarnation, the machine was a box that housed a list of questions that could be viewed one at a time through a small window. (see picture.) There was also a mechanism through which the learner could respond to each question. Upon delivering a correct answer, the learner would be rewarded.
Skinner advocated the use of teaching machines for a broad range of students (e.g., preschool aged to adult) and instructional purposes (e.g., reading and music). For example, one machine that he envisioned could teach rhythm. He wrote:A relatively simple device supplies the necessary contingencies. The student taps a rhythmic pattern in unison with the device. "Unison" is specified very loosely at first (the student can be a little early or late at each tap) but the specifications are slowly sharpened. The process is repeated for various speeds and patterns. In another arrangement, the student echoes rhythmic patterns sounded by the machine, though not in unison, and again the specifications for an accurate reproduction are progressively sharpened. Rhythmic patterns can also be brought under the control of a printed score.The instructional potential of the teaching machine stemmed from several factors: it provided automatic, immediate and regular reinforcement without the use of aversive control; the material presented was coherent, yet varied and novel; the pace of learning could be adjusted to suit the individual. As a result, students were interested, attentive, and learned efficiently by producing the desired behavior, "learning by doing."
Teaching machines, though perhaps rudimentary, were not rigid instruments of instruction. They could be adjusted and improved based upon the students' performance. For example, if a student made many incorrect responses, the machine could be reprogrammed to provide less advanced prompts or questions—the idea being that students acquire behaviors most efficiently if they make few errors. Multiple-choice formats were not well-suited for teaching machines because they tended to increase student mistakes, and the contingencies of reinforcement were relatively uncontrolled.
Not only useful in teaching explicit skills, machines could also promote the development of a repertoire of behaviors that Skinner called self-management. Effective self-management means attending to stimuli appropriate to a task, avoiding distractions, reducing the opportunity of reward for competing behaviors, and so on. For example, machines encourage students to pay attention before receiving a reward. Skinner contrasted this with the common classroom practice of initially capturing students’ attention (e.g., with a lively video) and delivering a reward (e.g., entertainment) before the students have actually performed any relevant behavior. This practice fails to reinforce correct behavior and actually counters the development of self-management.
Skinner pioneered the use of teaching machines in the classroom, especially at the primary level. Today computers run software that performs similar teaching tasks, and there has been a resurgence of interest in the topic related to the development of adaptive learning systems.
During World War II, the US Navy required a weapon effective against surface ships, such as the German "Bismarck" class battleships. Although missile and TV technology existed, the size of the primitive guidance systems available rendered automatic guidance impractical. To solve this problem, Skinner initiated Project Pigeon, which was intended to provide a simple and effective guidance system. This system divided the nose cone of a missile into three compartments, with a pigeon placed in each. Lenses projected an image of distant objects onto a screen in front of each bird. Thus, when the missile was launched from an aircraft within sight of an enemy ship, an image of the ship would appear on the screen. The screen was hinged, such that pecks at the image of the ship would guide the missile toward the ship.
Despite an effective demonstration, the project was abandoned, and eventually more conventional solutions, such as those based on radar, became available. Skinner complained that "our problem was no one would take us seriously."
Early in his career Skinner became interested in "latent speech" and experimented with a device he called the "verbal summator". This device can be thought of as an auditory version of the Rorschach inkblots. When using the device, human participants listened to incomprehensible auditory "garbage" but often read meaning into what they heard. Thus, as with the Rorschach blots, the device was intended to yield overt behavior that projected subconscious thoughts. Skinner's interest in projective testing was brief, but he later used observations with the summator in creating his theory of verbal behavior. The device also led other researchers to invent new tests such as the tautophone test, the auditory apperception test, and the Azzageddi test.
Along with psychology, education has also been influenced by Skinner's views, which are extensively presented in his book "The Technology of Teaching", as well as reflected in Fred S. Keller's "Personalized System of Instruction" and Ogden R. Lindsley's "Precision Teaching".
Skinner argued that education has two major purposes:
He recommended bringing students’ behavior under appropriate control by providing reinforcement only in the presence of stimuli relevant to the learning task. Because he believed that human behavior can be affected by small consequences, something as simple as "the opportunity to move forward after completing one stage of an activity" can be an effective reinforcer. Skinner was convinced that, to learn, a student must engage in behavior, and not just passively receive information.
Skinner believed that effective teaching must be based on positive reinforcement which is, he argued, more effective at changing and establishing behavior than punishment. He suggested that the main thing people learn from being punished is how to avoid punishment. For example, if a child is forced to practice playing an instrument, the child comes to associate practicing with punishment and thus learns to hate and avoid practicing the instrument. This view had obvious implications for the then widespread practice of rote learning and punitive discipline in education. The use of educational activities as punishment may induce rebellious behavior such as vandalism or absence.
Because teachers are primarily responsible for modifying student behavior, Skinner argued that teachers must learn effective ways of teaching. In "The Technology of Teaching" (1968), Skinner has a chapter on why teachers fail: He says that teachers have not been given an in-depth understanding of teaching and learning. Without knowing the science underpinning teaching, teachers fall back on procedures that work poorly or not at all, such as:
Skinner suggests that any age-appropriate skill can be taught. The steps are
Skinner is popularly known mainly for his books "Walden Two" (1948) and "Beyond Freedom and Dignity," (for which he made the cover of "TIME" Magazine). The former describes a fictional "experimental community" in 1940s United States. The productivity and happiness of citizens in this community is far greater than in the outside world because the residents practice scientific social planning and use operant conditioning in raising their children.
"Walden Two", like Thoreau's "Walden", champions a lifestyle that does not support war, or foster competition and social strife. It encourages a lifestyle of minimal consumption, rich social relationships, personal happiness, satisfying work, and leisure. In 1967, Kat Kinkade and others founded the Twin Oaks Community, using Walden Two as a blueprint. The community still exists and continues to use the Planner-Manager system and other aspects of the community described in Skinner's book, though behavior modification is not a community practice.
In "Beyond Freedom and Dignity", Skinner suggests that a technology of behavior could help to make a better society. We would, however, have to accept that an autonomous agent is not the driving force of our actions. Skinner offers alternatives to punishment, and challenges his readers to use science and modern technology to construct a better society.
Skinner's political writings emphasized his hopes that an effective and human science of behavioral control – a technology of human behavior – could help with problems as yet unsolved and often aggravated by advances in technology such as the atomic bomb. Indeed, one of Skinner's goals was to prevent humanity from destroying itself. He saw political activity as the use of aversive or non-aversive means to control a population. Skinner favored the use of positive reinforcement as a means of control, citing Jean-Jacques Rousseau's novel "" as an example of literature that "did not fear the power of positive reinforcement."
Skinner's book, "Walden Two", presents a vision of a decentralized, localized society, which applies a practical, scientific approach and behavioral expertise to deal peacefully with social problems. (For example, his views led him to oppose corporal punishment in schools, and he wrote a letter to the California Senate that helped lead it to a ban on spanking.) Skinner's utopia is both a thought experiment and a rhetorical piece. In "Walden Two", Skinner answers the problem that exists in many utopian novels – "What is the Good Life?" The book's answer is a life of friendship, health, art, a healthy balance between work and leisure, a minimum of unpleasantness, and a feeling that one has made worthwhile contributions to a society in which resources are ensured, in part, by minimizing consumption. 
Skinner described his novel as "my New Atlantis", in reference to Bacon's utopia. 
One of Skinner's experiments examined the formation of superstition in one of his favorite experimental animals, the pigeon. Skinner placed a series of hungry pigeons in a cage attached to an automatic mechanism that delivered food to the pigeon "at regular intervals with no reference whatsoever to the bird's behavior." He discovered that the pigeons associated the delivery of the food with whatever chance actions they had been performing as it was delivered, and that they subsequently continued to perform these same actions.One bird was conditioned to turn counter-clockwise about the cage, making two or three turns between reinforcements. Another repeatedly thrust its head into one of the upper corners of the cage. A third developed a 'tossing' response, as if placing its head beneath an invisible bar and lifting it repeatedly. Two birds developed a pendulum motion of the head and body, in which the head was extended forward and swung from right to left with a sharp movement followed by a somewhat slower return.Skinner suggested that the pigeons behaved as if they were influencing the automatic mechanism with their "rituals", and that this experiment shed light on human behavior:The experiment might be said to demonstrate a sort of superstition. The bird behaves as if there were a causal relation between its behavior and the presentation of food, although such a relation is lacking. There are many analogies in human behavior. Rituals for changing one's fortune at cards are good examples. A few accidental connections between a ritual and favorable consequences suffice to set up and maintain the behavior in spite of many unreinforced instances. The bowler who has released a ball down the alley but continues to behave as if she were controlling it by twisting and turning her arm and shoulder is another case in point. These behaviors have, of course, no real effect upon one's luck or upon a ball half way down an alley, just as in the present case the food would appear as often if the pigeon did nothing—or, more strictly speaking, did something else.Modern behavioral psychologists have disputed Skinner's "superstition" explanation for the behaviors he recorded. Subsequent research (e.g. Staddon and Simmelhag, 1971), while finding similar behavior, failed to find support for Skinner's "adventitious reinforcement" explanation for it. By looking at the timing of different behaviors within the interval, Staddon and Simmelhag were able to distinguish two classes of behavior: the "terminal response", which occurred in anticipation of food, and "interim responses", that occurred earlier in the interfood interval and were rarely contiguous with food. Terminal responses seem to reflect classical (as opposed to operant) conditioning, rather than adventitious reinforcement, guided by a process like that observed in 1968 by Brown and Jenkins in their "autoshaping" procedures. The causation of interim activities (such as the schedule-induced polydipsia seen in a similar situation with rats) also cannot be traced to adventitious reinforcement and its details are still obscure (Staddon, 1977).
Noam Chomsky, a prominent critic of Skinner, published a review of Skinner's "Verbal Behavior" two years after it was published. Chomsky argued that Skinner's attempt to use behaviorism to explain human language amounted to little more than word games. Conditioned responses could not account for a child's ability to create or understand an infinite variety of novel sentences. Chomsky's review has been credited with launching the cognitive revolution in psychology and other disciplines. Skinner, who rarely responded directly to critics, never formally replied to Chomsky's critique. Many years later, Kenneth MacCorquodale's reply was endorsed by Skinner.
Chomsky also reviewed Skinner's "Beyond Freedom and Dignity", using the same basic motives as his "Verbal Behavior" review. Among Chomsky's criticisms were that Skinner's laboratory work could not be extended to humans, that when it was extended to humans it represented 'scientistic' behavior attempting to emulate science but which was not scientific, that Skinner was not a scientist because he rejected the hypothetico-deductive model of theory testing, and that Skinner had no science of behavior.
Skinner has been repeatedly criticized for his supposed animosity towards Sigmund Freud, psychoanalysis, and psychodynamic psychology. Some have argued, however, that Skinner shared several of Freud's assumptions, and that he was influenced by Freudian points of view in more than one field, among them the analysis of defense mechanisms, such as repression. To study such phenomena, Skinner even designed his own projective test, the "verbal summator" described above.
As understood by Skinner, ascribing "dignity" to individuals involves giving them credit for their actions. To say "Skinner is brilliant" means that Skinner is an originating force. If Skinner's determinist theory is right, he is merely the focus of his environment. He is not an originating force and he had no choice in saying the things he said or doing the things he did. Skinner's environment and genetics both allowed and compelled him to write his book. Similarly, the environment and genetic potentials of the advocates of freedom and dignity cause them to resist the reality that their own activities are deterministically grounded. J. E. R. Staddon has argued the compatibilist position; Skinner's determinism is not in any way contradictory to traditional notions of reward and punishment, as he believed.
Skinner received honorary degrees from:

</doc>
<doc id="4869" url="https://en.wikipedia.org/wiki?curid=4869" title="Bill">
Bill

Bill may refer to:

</doc>
<doc id="4870" url="https://en.wikipedia.org/wiki?curid=4870" title="Bill Macy">
Bill Macy

Wolf Martin Garber (May 18, 1922 – October 17, 2019), known professionally as Bill Macy, was an American television, film and stage actor, best known for his role in the CBS television series "Maude" (1972–78).
Macy was born in May 1922 in Revere, Massachusetts to Mollie (née Friedopfer; 1889–1986) and Michael Garber (1884–1974), a manufacturer. He was raised Jewish in Brooklyn, New York. He worked as a cab driver for a decade before being cast as Walter Matthau's understudy in "Once More, with Feeling" on Broadway in 1958. He portrayed a cab driver on the soap opera "The Edge of Night" in 1966.
Macy was an original cast member of the 1969-1972 Off-Broadway sensation "Oh! Calcutta!", performing in the show from 1969 to 1971. He later appeared in the 1972 movie version of the musical. Of appearing fully nude with the rest of the cast in the stage show, he said "The nudity didn't bother me. I'm from Brooklyn."
Macy performed on the P.D.Q. Bach album "The Stoned Guest" (1970).
Appreciating Macy's comedic skills Off-Broadway, Norman Lear brought him to Hollywood, where he first got a small part as a police officer in "All in the Family." He was cast in the role of Walter Findlay, the long-suffering husband of the title character on the 1970s television sitcom "Maude", starring Bea Arthur. The show ran for six seasons from 1972 to 1978.
Strangers on the street often called him "Mr. Maude", consoling him for having such a difficult wife. "I used to tell them that people like that really existed," Macy explained.
In 1977, Macy & Samantha Harper-Macy appeared on the game show "Tattletales".
In 1986, Macy was a guest on the fourth episode of "L.A. Law", playing an older man whose young wife wants a music career. Macy appeared in the television movie "Perry Mason: The Case of the Murdered Madam" (1987) as banker Richard Wilson. He occasionally appeared on "Seinfeld" as one of the residents of the Florida retirement community where Jerry Seinfeld's parents lived. Macy made a guest appearance as a patient on "Chicago Hope" and as an aging gambler on the series "Las Vegas". Macy's last television role occurred in a 2010 episode of Jada Pinkett Smith's series "Hawthorne".
Macy appeared as the jury foreman in "The Producers" in 1967, with the memorable sole line "We find the defendants INCREDIBLY guilty". Other memorable roles include the co-inventor of the Opti-grab in the 1979 Steve Martin comedy "The Jerk" and as the head television writer in "My Favorite Year" (1982).
Other film credits included roles in "Death at Love House" (1976), "The Late Show" (1977), "Serial" (1980), "Movers & Shakers" (1985), "Bad Medicine" (1985), "Tales from the Darkside" (1986), "Sibling Rivalry" (1990), "The Doctor" (1991), "Me Myself & I" (1992), "Analyze This" (1999), "Surviving Christmas" (2004), "The Holiday" (2006), and "Mr. Woodcock" (2007).
Macy met his future wife, Samantha Harper, on the set of "Oh! Calcutta!" in 1969. They married in 1975.
Macy died on October 17, 2019, at the age of 97; no cause was given. He is survived by his wife Samantha Harper Macy.

</doc>
<doc id="4871" url="https://en.wikipedia.org/wiki?curid=4871" title="Bob Knight">
Bob Knight

Robert Montgomery Knight (born October 25, 1940) is an American former basketball coach. Often referred to as "Bobby Knight" and nicknamed "the General", Knight won 902 NCAA Division I men's college basketball games, a record at the time of his retirement, and currently third all-time, behind his former player and assistant coach Mike Krzyzewski of Duke and Jim Boeheim of Syracuse, who are both still active. Knight is best known as the head coach of the Indiana Hoosiers from 1971 to 2000. He also coached at Texas Tech (2001–2008) and at Army (1965–1971).
While at Indiana, Knight led his teams to three NCAA championships, one National Invitation Tournament (NIT) championship, and 11 Big Ten Conference championships. His 1975–76 team went undefeated during the regular season and won the 1976 NCAA tournament. The 1976 Indiana squad is the last men's college basketball team to go undefeated for the entire season. Knight received National Coach of the Year honors four times and Big Ten Coach of the Year honors eight times. In 1984, he coached the USA men's Olympic team to a gold medal, becoming one of only three basketball coaches to win an NCAA title, NIT title, and an Olympic gold medal.
Knight was one of college basketball's most successful and innovative coaches, having popularized the motion offense. He has also been praised for running good programs (none of his teams was ever sanctioned by the NCAA for recruiting violations), and nearly all of his players graduated. Knight sparked controversy with his outspoken nature and demonstrative behavior. He once famously threw a chair across the court during a game, which was rewarded with an ejection. Knight was once arrested in Puerto Rico following a physical confrontation with a police officer. Knight regularly displayed a volatile nature and was sometimes accused of verbal conflicts with members of the press. He was also recorded on videotape appearing to have possibly grabbed one of his players by the neck. Knight remains "the object of near fanatical devotion" from many of his former players and Indiana fans. Nevertheless, Knight was accused of choking a player during practice. Following the incident, a "zero tolerance" policy was instituted specifically for coach Knight. After an ensuing run-in with a student, university president Myles Brand fired Knight in the fall of 2000.
In 2008, Knight joined ESPN as a men's college basketball studio analyst during Championship Week and for coverage of the NCAA Tournament. He continued covering college basketball for ESPN through the 2014–15 season.
Knight was born in 1940 Massillon, Ohio, and grew up in Orrville, Ohio. He began playing organized basketball at Orrville High School. Knight continued at Ohio State in 1958 when he played for Basketball Hall of Fame coach Fred Taylor. Despite being a star player in high school, he played a reserve role as a forward on the 1960 Ohio State Buckeyes team that won the NCAA Championship and featured future Hall of Fame players John Havlicek and Jerry Lucas. The Buckeyes lost to the Cincinnati Bearcats in each of the next two NCAA Championship games, of which Knight was also a part.
Due in part to the star power of those Ohio State teams, Knight usually received scant playing time, but that did not prevent him from making an impact. In the 1961 NCAA Championship game, Knight came off the bench with 1:41 on the clock and Cincinnati leading Ohio State, 61–59. In the words of then-Ohio State assistant coach Frank Truitt,
Knight got the ball in the left front court and faked a drive into the middle. Then [he] crossed over like he worked on it all his life and drove right in and laid it up. That tied the game for us, and Knight ran clear across the floor like a 100-yard dash sprinter and ran right at me and said, 'See there, coach, I should have been in that game a long time ago!'
To which Truitt replied, "Sit down, you hot dog. You're lucky you're even on the floor."
In addition to lettering in basketball at Ohio State, it has been claimed that Knight also lettered in football and baseball; however, the official list of Ohio State football letter earners does not include Knight. Knight graduated with a degree in history and government in 1962.
After completion of graduation from Ohio State University in 1962, he coached junior varsity basketball at Cuyahoga Falls High School in Ohio for one year. Knight then enlisted in the United States Army and accepted an assistant coaching position with the Army Black Knights in 1963, where, two years later, he was named head coach at the relatively young age of 24. In six seasons at West Point, Knight won 102 games, with his first as a head coach coming against Worcester Polytechnic Institute. One of his players was Mike Krzyzewski, who later served as his assistant before becoming a Hall of Fame head coach at Duke. Mike Silliman was another of Knight's players at Army, and Knight was quoted as saying, "Mike Silliman is the best player I have ever coached."
During his tenure at Army, Knight gained a reputation for having an explosive temper. For example, after Army's 66–60 loss to BYU and Hall of Fame coach Stan Watts in the semifinals of the 1966 NIT, Knight completely lost control, kicking lockers and verbally blasting the officials. Embarrassed, he later went to Watts' hotel room and apologized. Watts forgave him, and is quoted as saying, "I want you to know that you're going to be one of the bright young coaches in the country, and it's just a matter of time before you win a national championship."
Knight was one of seven candidates vying to fill the Wisconsin men's basketball head coaching vacancy after John Erickson resigned to become the Milwaukee Bucks' first-ever general manager on April 3, 1968. He was offered the position but requested more time to think it over. By the time he returned to West Point, news that he was to become the Badgers' new coach was prematurely leaked to the local media. After consulting with Bo Schembechler who the previous year also had a negative experience as a Wisconsin football coaching candidate, Knight withdrew his candidacy and continued to coach at Army for three more seasons. Erickson's assistant coach John Powless was promoted instead.
In 1971, Indiana University hired Knight as head coach. During his 29 years at the school, the Hoosiers won 662 games, including 22 seasons of 20 or more wins, while losing 239, a .735 winning percentage. In 24 NCAA tournament appearances at Indiana, Hoosier teams under Knight won 42 of 63 games (.667), winning titles in 1976, 1981, and 1987, while losing in the semi-finals in 1973 and 1992.
In 1972–73, Knight's second year as coach, Indiana won the Big Ten championship and reached the Final Four, but lost to UCLA, who was on its way to its seventh consecutive national title. The following season, 1973–74, Indiana once again captured a Big Ten title. In the two following seasons, 1974–75 and 1975–76, the Hoosiers were undefeated in the regular season and won 37 consecutive Big Ten games, including two more Big Ten championships. The 1974–75 Hoosiers swept the entire Big Ten by an average of 22.8 points per game. However, in an 83–82 win against Purdue they lost consensus All-American forward Scott May to a broken left arm. With May's injury keeping him to 7 minutes of play, the No. 1 Hoosiers lost to Kentucky 92–90 in the Mideast Regional. The Hoosiers were so dominant that four starters – Scott May, Steve Green, Kent Benson and Quinn Buckner – would make the five-man All-Big Ten team. The following season, 1975–76, the Hoosiers went the entire season and 1976 NCAA tournament without a single loss, beating Michigan 86–68 in the title game. Immediately after the game, Knight lamented that "it should have been two." The 1976 Hoosiers remain the last undefeated NCAA Division I men's basketball team. Through these two seasons, Knight's teams were undefeated in the regular season, including a perfect 37–0 record in Big Ten games on their way to their third and fourth conference titles in a row. Behind the play of Mike Woodson, Indiana won the 1979 NIT championship.
The 1979–80 Hoosiers, led by Mike Woodson and Isiah Thomas, won the Big Ten championship and advanced to the 1980 Sweet Sixteen. The following season, in 1980–81, Thomas and the Hoosiers once again won a conference title and won the 1981 NCAA tournament, Knight's second national title. In 1982–1983, with the strong play of Uwe Blab and All-Americans Ted Kitchel and Randy Wittman, the No. 1 ranked Hoosiers were favorites to win another national championship. However, with an injury to All-American Ted Kitchel mid-season, the Hoosiers' prospects were grim. Knight asked for fan support to rally around the team and, when the team ultimately won the Big Ten title, he ordered that a banner be hung for the team in Assembly Hall as a tribute to the fans, who he credited with inspiring the team to win its final three home games. Nevertheless, in the tournament Kitchel's absence was felt and the team lost to Kentucky in the 1983 Sweet Sixteen.
The 1985–86 Hoosiers were profiled in a best-selling book "A Season on the Brink". To write it Knight granted author John Feinstein almost unprecedented access to the Indiana basketball program, as well as insights into Knight's private life. The following season, in 1986–87, the Hoosiers were led by All-American Steve Alford and captured a share of the Big Ten title. The team won Knight's third national championship (the school's fifth) against Syracuse in the 1987 NCAA tournament with a game-winning jump shot by Keith Smart with five seconds of play remaining in the championship game. In the 1988–1989 season the Hoosiers were led by All-American Jay Edwards and won a Big Ten championship.
From 1990–91 through 1992–93, the Hoosiers posted 87 victories, the most by any Big Ten team in a three-year span, breaking the mark of 86 set by Knight's Indiana teams of 1974–76. Teams from these three seasons spent all but two of the 53 poll weeks in the top 10, and 38 of them in the top 5. They captured two Big Ten crowns in 1990–91 and 1992–93, and during the 1991–92 season reached the Final Four. During the 1992–93 season, the 31–4 Hoosiers finished the season at the top of the AP Poll, but were defeated by Kansas in the Elite Eight. Teams from this era included Greg Graham, Pat Knight, All-Americans Damon Bailey and Alan Henderson Brian Evans, and National Player of the Year Calbert Cheaney.
Throughout the mid and late 1990s Knight continued to experience success with continual NCAA tournament appearances and a minimum of 19 wins each season. However, 1993 would be Knight's last conference championship and 1994 would be his last trip to the Sweet Sixteen.
On March 14, 2000 (just before Indiana was to begin play in the NCAA tournament), the CNN Sports Illustrated network ran a piece on Robert Abbott's investigation of Knight in which former player Neil Reed claimed he had been choked by Knight during a practice in 1997. Knight denied the claims in the story. However, less than a month later, the network aired a tape of an Indiana practice from 1997 that appeared to show Knight placing his hand on the neck of Reed.
In response, Indiana University president Myles Brand announced that he had adopted a "zero tolerance" policy with regard to Knight's behavior. Later in the year, in September 2000, Indiana freshman Kent Harvey (not a basketball player) reportedly said, "Hey, Knight, what's up?" to Knight. According to Harvey, Knight then grabbed him by the arm and lectured him for not showing him respect, insisting that Harvey address him as either "Mr. Knight" or "Coach Knight" instead of simply "Knight." Brand stated that this incident was only one of numerous complaints that occurred after the zero-tolerance policy had been put into place. Brand asked Knight to resign on September 10, and when Knight refused, Brand relieved him of his coaching duties effective immediately. Knight's dismissal was met with outrage from students. That night, thousands of Indiana students marched from Indiana University's Assembly Hall to Brand's home, burning Brand in effigy.
Harvey was supported by some and vilified by many who claim he had intentionally set up Knight. Kent Harvey's stepfather, Mark Shaw, was a former Bloomington-area radio talk show host and Knight critic. On September 13, Knight said goodbye to a crowd of some 6,000 supporters in Dunn Meadow at Indiana University. He asked that they not hold a grudge against Harvey and that they continue to support the basketball team. Knight's firing made national headlines, including the cover of "Sports Illustrated" and around the clock coverage on ESPN.
In a March 2017 interview on "The Dan Patrick Show", Knight stated that he had no interest in ever returning to Indiana. When host Dan Patrick commented that most of the administration that had fired Knight seventeen years earlier were no longer there, Knight said, "I hope they’re all dead."
Following his dismissal from Indiana, Knight took a season off and was on the lookout for coaching vacancies. He accepted the head coaching position at Texas Tech, although his hiring was opposed by a faculty group that was led by Walter Schaller. When he was introduced at the press conference, Knight quipped, "This is without question the most comfortable red sweater I've had on in six years."
Knight quickly improved the program, which had not been to an NCAA tournament since 1996. He led the team to postseason appearances in each of his first four years at the school (three NCAA Championship tournaments and one NIT). After a rough 2006 season, the team improved in 2007, finishing 21–13 and again making it to the NCAA tournament, where it lost to Boston College in the first round. The best performance by the Red Raiders under Knight came in 2005 when they advanced as far as the Sweet Sixteen. In both 2006 and 2007 under Knight, Texas Tech defeated two Top 10-ranked teams in consecutive weeks. During Knight's first six years at Texas Tech, the Red Raiders won 126 games, an average of 21 wins per season.
On February 4, 2008, Knight announced his retirement. His son Pat Knight, the head coach designate since 2005, was immediately named as his successor at Texas Tech. The younger Knight had said that after many years of coaching, his father was exhausted and ready to retire. Just after achieving his 900th win, Knight handed the job over to Pat in the mid-season in part to allow him to get acquainted with coaching the team earlier, instead of having him wait until October, the start of the next season. Knight continued to live in Lubbock after he retired.
In 1979 Knight guided the United States Pan American team to a gold medal in Puerto Rico. In 1984 Knight led the U.S. national team to a gold medal in the Olympic Games as coach of the 1984 basketball team (coaches do not receive medals in the Olympics). Players on the team included Michael Jordan and Knight's Indiana player and protege Steve Alford.
In 2008, ESPN hired Knight as a studio analyst and occasional color commentator. In November 2012, he called an Indiana men's basketball game for the first time, something he had previously refused to do. Former Indiana men's basketball coach Tom Crean reached out to Knight in an attempt to get him to visit the school again. 
On April 2, 2015, ESPN announced that it would not renew its contract with Knight.
On February 27, 2019, Don Fischer, an IU radio announcer since 1974, said during an interview that Knight was in ill health. He continued by saying Knight's health “has declined” but did not offer any specifics.
On April 4, 2019, Knight made his first public appearance since Fischer made his comments. He appeared with longtime friend and journalist Bob Hammel and spoke about different aspects of his career. During the presentation, Knight seemed to struggle with his memory: he re-introduced his wife to the audience after doing so only 10 minutes earlier, he mistakenly said that former IU basketball player Landon Turner had died, and, after telling a story about Michael Jordan, he later told the same story, replacing Jordan with former IU basketball player Damon Bailey.
On July 10, 2019, the "Indiana Daily Student", IU's campus newspaper, reported that on July 2, 2019 Knight and his wife, Karen, purchased a home in Bloomington for $572,500, suggesting that Knight had decided to return to Bloomington to live.
On February 8, 2020, Knight was honored at an Indiana basketball game. It was the first Indiana game attended by Knight since his dismissal by the school 20 years prior.
Knight was an innovator of the motion offense, which he perfected and popularized. The system emphasizes post players setting screens and perimeter players passing the ball until a teammate becomes open for an uncontested jump shot or lay-up. This required players to be unselfish, disciplined, and effective in setting and using screens to get open.
Knight's motion offense did not take shape until he began coaching at Indiana. Prior to that, at Army, he ran a "reverse action" that involved reversing the ball from one side of the floor to the other and screening along with it. According to Knight, it was a "West Coast offense" that Pete Newell used exclusively during his coaching career. After being exposed to the Princeton offense, Knight instilled more cutting with the offense he employed, which evolved into the motion offense that he ran for most of his career. Knight continued to develop the offense, instituting different cuts over the years and putting his players in different scenarios.
Knight was well known for the extreme preparation he put into each game and practice. He was often quoted as saying, "Most people have the will to win, few have the will to prepare to win." Often during practice, Knight would instruct his players to a certain spot on the floor and give them options of what to do based on how the defense might react. In contrast to set plays, Knight's offense was designed to react according to the defense.
The 3-point shot was adopted by the NCAA in 1986, which was midway through Knight's coaching career. Although he opposed the rule change throughout his life, it did complement his offense well by improving the spacing on the floor. He sardonically said at the time that he supported institution of the three point shot because if a team's offense was functioning efficiently enough to get a layup the team should be rewarded with three points for that basket. Knight's offense also emphasized a two-count. Players in the post are expected to try to post in the paint for two seconds and if they do not receive the ball they go set a screen. Players with the ball are expected to hold the ball for two seconds to see where they are going to take it. Screens are supposed to be held for two seconds, as well.
On defense Knight was known for emphasizing tenacious "man-to-man" defense where defenders contest every pass and every shot, and help teammates when needed. However, Knight has also incorporated a zone defense periodically after eschewing that defense for the first two decades of his coaching career.
Knight's coaching also included a firm emphasis on academics. All but four of his four-year players completed their degrees, which was a ratio of nearly 98 percent. Nearly 80 percent of his players graduated; this figure was much higher than the national average of 42 percent for Division 1 schools.
Knight's all time coaching record is 902–371. His 902 wins in NCAA Division I men's college basketball games is third all-time to Knight's former player Mike Krzyzewski, and Syracuse head coach Jim Boeheim. Knight achieved his 880th career win on January 1, 2007 and passed retired North Carolina coach Dean Smith for most career victories, a title he held until his win total was surpassed by Krzyzewski on November 15, 2011, and by Jim Boeheim on December 30, 2012. Knight is the youngest coach to reach 200 (age 35), 300 (age 40) and 400 (age 44) wins. He was also among the youngest to reach other milestones of 500 (age 48) and 600 (age 52) wins.
Texas Tech's participation in the 2007 NCAA Tournament gave Knight more NCAA tournament appearances than any other coach. He is the only coach to win the NCAA, the NIT, an Olympic Gold medal, and a Pan American Games Gold medal. Knight is also one of only three people, along with Dean Smith and Joe B. Hall, who had both played on and coached an NCAA Tournament championship basketball team.
Knight received a number of personal honors during and after his coaching career. He was named the National Coach of the Year four times (1975, 1976, 1987, 1989) and Big Ten Coach of the Year eight times (1973, 1975, 1976, 1980, 1981, 1989, 1992, 1993). In 1975 he was a unanimous selection as National Coach of the Year, an honor he was accorded again in 1976 by the Associated Press, United Press International, and "Basketball Weekly". In 1987 he was the first person to be honored with the Naismith Coach of the Year Award. In 1989 he garnered National Coach of the Year honors by the AP, UPI, and the United States Basketball Writers Association. Knight was inducted into the Basketball Hall of Fame in 1991.
On November 17, 2006, Knight was recognized for his impact on college basketball as a member of the founding class of the National Collegiate Basketball Hall of Fame. The following year, he was the recipient of the Naismith Award for Men's Outstanding Contribution to Basketball. Knight was also inducted into the Army Sports Hall of Fame (Class of 2008) and the Indiana Hoosiers athletics Hall of Fame (Class of 2009). In August 2003, he was honored as the first inductee in The Vince Lombardi Titletown Legends.
A number of Knight's assistant coaches, players, and managers have gone on to be coaches. Among them are Hall of Fame Duke coach Mike Krzyzewski, former UCLA coach Steve Alford, Murry Bartow and NBA coaches Randy Wittman, Mike Woodson, Keith Smart, Isiah Thomas, former Evansville Coach Marty Simmons, former St. Louis Coach Jim Crews, Lawrence Frank, and Texas Tech coach Chris Beard.
In 1986 author John Feinstein published "A Season on the Brink", which detailed the 1985–86 season of the Indiana Hoosiers. Granted almost unprecedented access to the Indiana basketball program, as well as insights into Knight's private life, the book quickly became a major best-seller and spawned a new genre, as a legion of imitators wrote works covering a single year of a sports franchise. In the book Feinstein depicts a coach who is quick with a violent temper, but also one who never cheats and strictly follows all of the NCAA's rules.
Two years later, author Joan Mellen penned the book "Bob Knight: His Own Man" (), in part to rebut Feinstein's "A Season on the Brink". Mellen deals with seemingly all the causes celebres in Knight's career and presents the view that he is more sinned against than sinning.
In 1990 Robert P. Sulek wrote "Hoosier Honor: Bob Knight and Academic Success at Indiana University" which discusses the academic side of the basketball program. The book details all of the players that have played for Knight and what degree they earned.
Only a month following his termination from IU, Rich J. Wolfe wrote "Oh, What a Knight: Knightmares" which is a two part book. Part One includes stories from people who have had positive interactions with Knight such as friends and former players, and Part Two is stories from people who have had negative interactions with Knight, such as the police officer who arrested Knight in Puerto Rico, and a Purdue basketball player who was playing in the game where Knight threw the chair.
A number of close associates and friends of Knight have also written books about him. Former player and current Nevada Wolf Pack head basketball coach Steve Alford wrote "Playing for Knight: My Six Seasons with Bobby Knight", published in 1990. Former player Kirk Haston wrote "Days of Knight: How the General Changed My Life," published in 2016.
Knight's autobiography, written with longtime friend and sports journalist Bob Hammel, was titled "Knight: My Story" and published in 2003. Three years later Steve Delsohn and Mark Heisler wrote "Bob Knight: An Unauthorized Biography".
In 2013 Knight and Bob Hammel published "The Power of Negative Thinking: An Unconventional Approach to Achieving Positive Results". Knight discusses his approach to preparing for a game by anticipating all of the things that could go wrong and trying to prevent it or having a plan to deal with it. In the book Knight also shares one of his favorite sayings, "Victory favors the team making the fewest mistakes."
In 2017 sports reporter Terry Hutchens published "Following the General: Why Three Coaches Have Been Unable to Return Indiana Basketball to Greatness" which discussed Knight's coaching legacy with Indiana and how none of the coaches following him have been able to reach his level of success.
Knight has appeared or been featured in numerous films and television productions. In 1994 a feature film titled "Blue Chips" featured a character named Pete Bell, a volatile but honest college basketball coach under pressure to win who decides to blatantly violate NCAA rules to field a competitive team after a sub-par season. It starred Nick Nolte as Bell and NBA star Shaquille O'Neal as Neon Bodeaux, a once-in-a-lifetime player that boosters woo to his school with gifts and other perks. The coach's temper and wardrobe were modeled after Knight's, though at no time had Knight been known to illegally recruit. Knight himself appears in the movie and coaches against Nolte in the film's climactic game.
ESPN's first feature-length film was "A Season on the Brink", a 2002 TV adaptation from John Feinstein's book. In the movie Knight is played by veteran character actor Brian Dennehy. ESPN also featured Knight in a reality show titled "Knight School", which followed a handful of Texas Tech students as they competed for the right to join the basketball team as a non-scholarship player.
Knight made a cameo appearance as himself in the 2003 film "Anger Management". In 2008, Knight appeared in a commercial as part of Volkswagen's Das Auto series where Max, a 1964 black Beetle interviews famous people. When Knight talked about Volkswagen winning the best resale value award in 2008, Max replied, "At least one of us is winning a title this year." This prompted Knight to throw his chair off the stage and walk out saying, "I may not be retired."
Knight also made an appearance in a TV commercial for with fellow coaches Mike Krzyzewski, Rick Pitino, and Roy Williams, in a parody of Tom Cruise in "Risky Business".
In 2009, Knight produced three instructional coaching DVD libraries—on motion offense, man-to-man defense, and instilling mental toughness—with Championship Productions.
Knight married the former Nancy Falk on April 17, 1963. They had two sons, Tim and Pat, but the couple divorced in 1985. Pat played at Indiana from 1991 to 1995 and served as head coach at Lamar from the time of his father's retirement until he was dismissed in 2014. Pat Knight coached Texas Tech after his father's retirement before he moved to Lamar. In 1988, Knight married his second wife, Karen Vieth Edgar, a former Oklahoma high school basketball coach.
Knight has a high regard for education and has made generous donations to the schools he has been a part of, particularly libraries. At Indiana University Knight endowed two chairs, one in history and one in law. He also raised nearly $5 million for the Indiana University library system by championing a library fund to support the library's activities. The fund was ultimately named in his honor.
When Knight came to Texas Tech in 2001, he gave $10,000 to the library, the first gift to the Coach Knight Library Fund which has now collected over $300,000. On November 29, 2007, the Texas Tech library honored this with "A Legacy of Giving: The Bob Knight Exhibit".
Knight supported Donald Trump's 2016 presidential campaign, and later made an appearance at his rally in Indianapolis for the 2018 midterms. At the rally, Knight called Trump "a great defender of the United States of America".
It was reported years after the incident that Knight choked and punched IU's longtime sports information director, Kit Klingelhoffer, in the 1970s, over a news release that upset the coach.

</doc>
<doc id="4874" url="https://en.wikipedia.org/wiki?curid=4874" title="Black metal">
Black metal

Black metal is an extreme subgenre of heavy metal music. Common traits include fast tempos, a shrieking vocal style, heavily distorted guitars played with tremolo picking, raw (lo-fi) recording, unconventional song structures, and an emphasis on atmosphere. Artists often appear in corpse paint and adopt pseudonyms.
During the 1980s, several thrash metal and death metal bands formed a prototype for black metal. This so-called first wave included bands such as Venom, Bathory, Mercyful Fate, Hellhammer and Celtic Frost. A second wave arose in the early 1990s, spearheaded by Norwegian bands such as Mayhem, Darkthrone, Burzum, Immortal, Emperor, Satyricon and Gorgoroth. The early Norwegian black metal scene developed the style of their forebears into a distinct genre. Norwegian-inspired black metal scenes emerged throughout Europe and North America, although some other scenes developed their own styles independently. Some prominent Swedish bands spawned during this second wave, the second generation in Sweden being led by Dissection, Abruptum, Marduk, and Nifelheim.
Initially a synonym for "Satanic metal", black metal has often sparked controversy, due to the actions and ideologies associated with the genre. Many artists express extreme anti-Christian and misanthropic views, advocating various forms of Satanism or ethnic paganism. In the 1990s, members of the scene were responsible for a spate of church burnings and murders. There is also a small neo-Nazi movement within black metal, although it has been shunned by many prominent artists. Generally, black metal strives to remain an underground phenomenon.
Although "contemporary black metal" typically refers to the Norwegian style with shrieking vocals and raw production, the term has traditionally been applied to bands with widely differing sounds, such as Death SS, Mercyful Fate, Mayhem, Blasphemy, and the Greek and Finnish bands that emerged around the same time as the Norwegian scene.
Norwegian-inspired black metal guitarists usually favor high-pitched or trebly guitar tones and heavy distortion. The guitar is usually played with fast, un-muted tremolo picking and power chords. Guitarists often use dissonance—along with specific scales, intervals and chord progressions—to create a sense of dread. The tritone, or flat-fifth, is often used. Guitar solos and low guitar tunings are rare in black metal. The bass guitar is seldom used to play stand-alone melodies. It is not uncommon for the bass to be muted against the guitar, or for it to homophonically follow the low-pitched riffs of the guitar. While electronic keyboards are not a standard instrument, some bands, like Dimmu Borgir, use keyboards "in the background" or as "proper instruments" for creating atmosphere. Some newer black metal bands began raising their production quality and introducing additional instruments such as synthesizers and even orchestras.
The drumming is usually fast and relies on double-bass and blast beats to maintain tempos that can sometimes approach 300 beats per minute. These fast tempos require great skill and physical stamina, typified by black metal drummers Frost (Kjetil-Vidar Haraldstad) and Hellhammer (Jan Axel Blomberg). Even still, authenticity is still prioritized over technique. "This professionalism has to go," insists well-respected drummer and metal historian Fenriz (Gylve Fenris Nagell) of Darkthrone. "I want to "de-learn" playing drums, I want to play primitive and simple, I don't want to play like a drum solo all the time and make these complicated riffs".
Black metal songs often stray from conventional song structure and often lack clear verse-chorus sections. Instead, many black metal songs contain lengthy and repetitive instrumental sections. The Greek style—established by Rotting Christ, Necromantia and Varathron—has more traditional heavy metal and death metal traits than Norwegian black metal.
Traditional black metal bands tend to favor raspy, high-pitched vocals which include techniques such as shrieking, screaming, and snarling, a vocal style influenced by Quorthon of Bathory. Death growls, common in the death metal genre, are sometimes used, but less frequently than the characteristic black metal shriek.
Black metal lyrics typically attack Christianity and the other institutional religions, often using apocalyptic language. Satanic lyrics are common, and many see them as essential to black metal. For Satanist black metal artists, "Black metal songs are meant to be like Calvinist sermons; deadly serious attempts to unite the true believers". Misanthropy, global catastrophe, war, death, destruction and rebirth are also common themes. Another topic often found in black metal lyrics is that of the wild and extreme aspects and phenomena of the natural world, particularly the wilderness, forests, mountains, winter, storms, and blizzards. Black metal also has a fascination with the distant past. Many bands write about the mythology and folklore of their homelands and promote a revival of pre-Christian, pagan traditions. A significant number of bands write lyrics only in their native language and a few (e.g. Arckanum and early Ulver) have lyrics in archaic languages. Some doom metal-influenced artists' lyrics focus on depression, nihilism, introspection, self-harm and suicide.
Many bands choose not to play live. Many of those who do play live maintain that their performances "are not for entertainment or spectacle. Sincerity, authenticity and extremity are valued above all else". Some bands consider their concerts to be rituals and often make use of stage props and theatrics. Bands such as Mayhem, Gorgoroth, and Watain are noted for their controversial shows, which have featured impaled animal heads, mock crucifixions, medieval weaponry and band members doused in animal blood. A few vocalists, such as Dead, Maniac and Kvarforth, are known for cutting themselves while singing onstage.
Black metal artists often appear dressed in black with combat boots, bullet belts, spiked wristbands and inverted crosses and inverted pentagrams to reinforce their anti-Christian or anti-religious stance. However, the most stand-out trait is their use of corpse paint—black and white face paint sometimes mixed with real or fake blood, which is used to create a corpse-like or demonic appearance.
The imagery of black metal reflects its lyrics and ideology. In the early 1990s, most pioneering black metal artists had minimalist album covers featuring xeroxed black-and-white pictures and/or writing. This was partly a reaction against death metal bands, who at that time had begun to use brightly colored album artwork. Many purist black metal artists have continued this style. Black metal album covers are typically dark and tend to be atmospheric or provocative; some feature natural or fantasy landscapes (for example Burzum's "Filosofem" and Emperor's "In the Nightside Eclipse") while others are violent, sexually transgressive, sacrilegious, or iconoclastic (for example Marduk's "Fuck Me Jesus" and Dimmu Borgir's "In Sorte Diaboli").
The earliest black metal artists had very limited resources, which meant that recordings would often be done in homes or basements, giving their recordings a distinctive "lo-fi" quality. However, even when success allowed access to professional studios, many artists instead chose to continue making lo-fi recordings. Artists believed that by doing so, they would both stay true to the genre's underground roots as well as make the music sound more "raw" or "cold". A well-known example of this approach is on the album "Transilvanian Hunger" by Darkthrone, a band who Johnathan Selzer of "Terrorizer" magazine says "represent the DIY aspect of black metal." In addition, lo-fi production was used to keep black metal inaccessible or unappealing to mainstream music fans and those who are not committed. Many have claimed that black metal was originally intended only for those who were part of the scene and not for a wider audience. Vocalist Gaahl said that during its early years, "Black metal was never meant to reach an audience, it was purely for our own satisfaction".
The conventional history of black metal is that pioneers like Venom, Bathory and Hellhammer were part of a "first wave", and that a "second wave" was begun by the early Norwegian scene, especially by Mayhem vocalist, Dead's suicide; Mayhem's leader, Euronymous, who founded the Norwegian scene after Dead's suicide; and Darkthrone's album "A Blaze in the Northern Sky". There are also some who argue that albums like Sarcófago's "I.N.R.I." or Samael's "Worship Him" began the second wave.
Occult and Satanic lyrical themes were present in the music of heavy metal and rock bands of the late 1960s and early 1970s such as Black Sabbath and Coven.
In the late 1970s, the form of rough and aggressive heavy metal played by the British band Motörhead gained popularity. Many first wave black metal bands would cite Motörhead as an influence. Also popular in the late 1970s, punk rock came to influence the birth of black metal. Tom G. Warrior of Hellhammer and Celtic Frost credited English punk group Discharge as "a revolution, much like Venom", saying, "When I heard the first two Discharge records, I was blown away. I was just starting to play an instrument and I had no idea you could go so far."
The use of corpse paint in the black metal imagery was mainly influenced by the American 1970s rock band Kiss.
The first wave of black metal refers to those bands during the 1980s who influenced the black metal sound and formed a prototype for the genre. They were often speed metal or thrash metal bands.
The term "black metal" was coined by the English band Venom with their second album "Black Metal" (1982). Although deemed speed metal or thrash metal rather than black metal by today's standards, the album's lyrics and imagery focused more on anti-Christian and Satanic themes than any before it. Their music was fast, unpolished in production and with raspy or grunted vocals. Venom's members also adopted pseudonyms, a practice that would become widespread among black metal musicians.
Another major influence on black metal was the Swedish band Bathory. The band, led by Thomas Forsberg (a.k.a. Quorthon), created "the blueprint for Scandinavian black metal". Not only was Bathory's music dark, fast, heavily distorted, lo-fi and with anti-Christian themes, Quorthon was also the first to use the shrieked vocals that later became a common trait. The band played in this style on their first four albums: "Bathory" (1984), "The Return……" (1985), "Under the Sign of the Black Mark" (1987) and "Blood Fire Death" (1988). With "Blood Fire Death" and the two following albums, Bathory pioneered the style that would become known as Viking metal.
Hellhammer, from Switzerland, "made truly raw and brutal music" with Satanic lyrics, and became an important influence on later black metal; "Their simple yet effective riffs and fast guitar sound were groundbreaking, anticipating the later trademark sound of early Swedish death metal". In 1984, members of Hellhammer formed Celtic Frost, whose music "explored more orchestral and experimental territories. The lyrics also became more personal, with topics about inner feelings and majestic stories. But for a couple of years, Celtic Frost was one of the world's most extreme and original metal bands, with a huge impact on the mid-1990s black metal scene".
The Danish band Mercyful Fate influenced the Norwegian scene with their imagery and lyrics. Frontman King Diamond, who wore ghoulish black-and-white facepaint on stage, may be one of the inspirators of what became known as 'corpse paint'. Other acts which adopted a similar appearance on stage were the horror punk band Misfits, Celtic Frost and the Brazilian extreme metal band Sarcófago. Other artists usually considered part of this movement include Kreator, Sodom and Destruction (from Germany), Bulldozer and Death SS (from Italy), whose vocalist Steve Sylvester was a member of the Ordo Templi Orientis.
In 1987, in the fifth issue of his "Slayer" fanzine, Jon 'Metalion' Kristiansen wrote that "the latest fad of black/Satanic bands seems to be over", the tradition being continued by a few bands like Incubus and Morbid Angel (from the United States), Sabbat (from Great Britain), Tormentor (from Hungary), Sarcófago (from Brazil), Grotesque, Treblinka and early Tiamat (from Sweden). Other early black metal bands include Sabbat (formed 1983 in Japan), Parabellum (formed 1983 in Colombia), Salem (formed 1985 in Israel) and Mortuary Drape (formed 1986 in Italy). Japanese band Sigh formed in 1990 and was in regular contact with key members of the Norwegian scene. Their debut album, "Scorn Defeat", became "a cult classic in the black metal world". In the years before the Norwegian black metal scene arose, important recordings were released by Root and Master's Hammer (from Czechoslovakia), Von (from the United States), Rotting Christ (from Greece), Samael (from Switzerland) and Blasphemy (from Canada), whose debut album "Fallen Angel of Doom" (1990) is considered one of the most influential records for the war metal style. Fenriz of the Norwegian band Darkthrone called Master's Hammer's debut album "Ritual" "the first Norwegian black metal album, even though they are from Czechoslovakia".
In 1990 and 1991, Northern European metal acts began to release music influenced by these bands or the older ones from the first wave. In Sweden, this included Marduk, Dissection, Nifelheim and Abruptum. In Finland, there emerged a scene that mixed the first wave black metal style with elements of death metal and grindcore; this included Beherit, Archgoat and Impaled Nazarene, whose debut album "Tol Cormpt Norz Norz Norz" "Rock Hard" journalist Wolf-Rüdiger Mühlmann considers a part of war metal's roots. Bands such as Demoncy and Profanatica emerged during this time in the United States, when death metal was more popular among extreme metal fans. The Norwegian band Mayhem's concert in Leipzig with Eminenz and Manos in 1990, later released as "Live in Leipzig", was said to have had a strong influence on the East German scene and is even called the unofficial beginning of German black metal.
The second wave of black metal began in the early 1990s and was spearheaded by the Norwegian black metal scene. During , a number of Norwegian artists began performing and releasing a new kind of black metal music; this included Mayhem, Darkthrone, Burzum, Immortal, Emperor, Satyricon, Enslaved, Thorns, Carpathian Forest and Gorgoroth. They developed the style of their 1980s forebears into a distinct genre. This was partly thanks to a new kind of guitar playing developed by Snorre 'Blackthorn' Ruch of Stigma Diabolicum/Thorns and Øystein 'Euronymous' Aarseth of Mayhem. Fenriz of Darkthrone described it as being "derived from Bathory" and noted that "those kinds of riffs became the new order for a lot of bands in the '90s".
The wearing of corpse paint became standard, and was a way for many black metal artists to distinguish themselves from other metal bands of the era. The scene also had an ideology and ethos. Artists were bitterly opposed to Christianity and presented themselves as misanthropic Devil worshippers who wanted to spread terror, hatred and evil. They professed to be serious in their views and vowed to act on them. Ihsahn of Emperor said that they sought to "create fear among people" and "be in opposition to society". The scene was exclusive and created boundaries around itself, incorporating only those who were "true" and attempting to expel all "poseurs". Some members of the scene were responsible for a spate of church burnings and murder, which eventually drew attention to it and led to a number of artists being imprisoned.
On 8 April 1991, Mayhem vocalist Per Yngve Ohlin (who called himself "Dead") died by suicide while alone in a house shared by the band. Fellow musicians described Dead as odd, introverted and depressed. Before going onstage he went to great lengths to make himself look like a corpse and would cut his arms while singing. Mayhem's drummer, Hellhammer, said that Dead was the first to wear the distinctive corpse paint that became widespread in the scene. He was found with slit wrists and a shotgun wound to the head. Dead's suicide note apologized for firing the weapon indoors and ended: "Excuse all the blood". Before calling the police, Euronymous got a disposable camera and photographed the body, after re-arranging some items. One of these photographs was later used as the cover of a bootleg live album, "Dawn of the Black Hearts".
Euronymous made necklaces with bits of Dead's skull and gave some to musicians he deemed worthy. Rumors also spread that he had made a stew with bits of his brain. Euronymous used Dead's suicide to foster Mayhem's evil image and claimed Dead had killed himself because extreme metal had become trendy and commercialized. Mayhem bassist Jørn 'Necrobutcher' Stubberud noted that "people became more aware of the black metal scene after Dead had shot himself ... I think it was Dead's suicide that really changed the scene".
Two other members of the early Norwegian scene would later die by suicide: Erik 'Grim' Brødreskift (of Immortal, Borknagar, Gorgoroth) in 1999 and Espen 'Storm' Andersen (of Strid) in 2001.
During May–June 1991, Euronymous of Mayhem opened an independent record shop named "Helvete" (Norwegian for "Hell") at Schweigaards gate 56 in Oslo. It quickly became the focal point of Norway's emerging black metal scene and a meeting place for many of its musicians; especially the members of Mayhem, Burzum, Emperor and Thorns. Jon 'Metalion' Kristiansen, writer of the fanzine "Slayer", said that the opening of Helvete was "the creation of the whole Norwegian black metal scene". In its basement, Euronymous founded an independent record label named Deathlike Silence Productions. With the rising popularity of his band and others like it, the underground success of Euronymous's label is often credited for encouraging other record labels, who had previously shunned black metal acts, to then reconsider and release their material.
In 1992, members of the Norwegian black metal scene began a wave of arson attacks on Christian churches. By 1996, there had been at least 50 such attacks in Norway. Some of the buildings were hundreds of years old and seen as important historical landmarks. The first to be burnt down was Norway's Fantoft stave church. Police believe Varg Vikernes of Burzum was responsible. The cover of Burzum's EP "Aske" ("ashes") is a photograph of the destroyed church. In May 1994, Vikernes was found guilty for burning down Holmenkollen Chapel, Skjold Church and Åsane Church. To coincide with the release of Mayhem's "De Mysteriis Dom Sathanas", Vikernes and Euronymous had also allegedly plotted to bomb Nidaros Cathedral, which appears on the album cover. The musicians Faust, Samoth, (both of Emperor) and Jørn Inge Tunsberg (of Hades Almighty) were also convicted for church arsons. Members of the Swedish scene started to burn churches in 1993.
Those convicted for church burnings showed no remorse and described their actions as a symbolic "retaliation" against Christianity in Norway. Mayhem drummer Hellhammer said he had called for attacks on mosques and Hindu temples, on the basis that they were more foreign. Today, opinions on the church burnings differ within the black metal community. Many, such as Infernus and Gaahl of Gorgoroth, continue to praise the church burnings, with the latter saying "there should have been more of them, and there will be more of them". Others, such as Necrobutcher and Kjetil Manheim of Mayhem and Abbath of Immortal, see the church burnings as having been futile. Manheim claimed that many arsons were "just people trying to gain acceptance" within the black metal scene. Watain vocalist Erik Danielsson respected the attacks, but said of those responsible: "the only Christianity they defeated was the last piece of Christianity within themselves. Which is a very good beginning, of course".
In early 1993, animosity arose between Euronymous and Vikernes. On the night of 10 August 1993, Varg Vikernes (of Burzum) and Snorre 'Blackthorn' Ruch (of Thorns) drove from Bergen to Euronymous's apartment in Oslo. When they arrived a confrontation began and Vikernes stabbed Euronymous to death. His body was found outside the apartment with 23 cut wounds—two to the head, five to the neck, and sixteen to the back.
It has been speculated that the murder was the result of either a power struggle, a financial dispute over Burzum records or an attempt at outdoing a stabbing in Lillehammer the year before by Faust. Vikernes denies all of these, claiming that he attacked Euronymous in self-defense. He says that Euronymous had plotted to stun him with an electroshock weapon, tie him up and torture him to death while videotaping the event. He said Euronymous planned to use a meeting about an unsigned contract to ambush him. Vikernes claims he intended to hand Euronymous the signed contract that night and "tell him to fuck off", but that Euronymous panicked and attacked him first. He also claims that most of the cuts were from broken glass Euronymous had fallen on during the struggle. The self-defense story is doubted by Faust, while Necrobutcher confirmed that Vikernes killed Euronymous in self-defense due to the death threats he received from him.
Vikernes was arrested on 19 August 1993, in Bergen. Many other members of the scene were taken in for questioning around the same time. Some of them confessed to their crimes and implicated others. In May 1994, Vikernes was sentenced to 21 years in prison (Norway's maximum penalty) for the murder of Euronymous, the arson of four churches, and for possession of 150 kg of explosives. However, he only confessed to the latter. Two churches were burnt the day he was sentenced, "presumably as a statement of symbolic support". Vikernes smiled when his verdict was read and the picture was widely reprinted in the news media. Blackthorn was sentenced to eight years in prison for being an accomplice to the murder. That month saw the release of Mayhem's album "De Mysteriis Dom Sathanas", which featured Euronymous on guitar and Vikernes on bass guitar. Euronymous's family had asked Mayhem's drummer, Hellhammer, to remove the bass tracks recorded by Vikernes, but Hellhammer said: "I thought it was appropriate that the murderer and victim were on the same record. I put word out that I was re-recording the bass parts. But I never did". In 2003, Vikernes failed to return to Tønsberg prison after being given a short leave. He was re-arrested shortly after while driving a stolen car with various weapons. Vikernes was released on parole in 2009.
Black metal scenes also emerged on the European mainland during the early 1990s, inspired by the Norwegian scene or the older bands, or both. In Poland, a scene was spearheaded by Graveland and Behemoth. In France, a close-knit group of musicians known as Les Légions Noires emerged; this included artists such as Mütiilation, Vlad Tepes, Belketre and Torgeist. In Belgium, there were acts such as Ancient Rites and Enthroned. Bands such as Black Funeral, Grand Belial's Key and Judas Iscariot emerged during this time in the United States. Black Funeral, from Houston, formed in 1993, was associated with black magic and Satanism.
A notable black metal group in England was Cradle of Filth, who released three demos in a black/death metal style with symphonic flourishes, followed by the album "The Principle of Evil Made Flesh", which featured a then-unusual hybrid style of black and gothic metal. The band then abandoned black metal for gothic metal, becoming one of the most successful extreme metal bands to date. John Serba of AllMusic commented that their first album "made waves in the early black metal scene, putting Cradle of Filth on the tips of metalheads' tongues, whether in praise of the band's brazen attempts to break the black metal mold or in derision for its 'commercialization' of an underground phenomenon that was proud of its grimy heritage". Some black metal fans did not consider Cradle of Filth to be black metal. When asked if he considers Cradle of Filth a black metal band, vocalist Dani Filth said he considers them black metal in terms of philosophy and atmosphere, but not in other ways. Another English band called Necropolis never released any music, but "began a desecratory assault against churches and cemeteries in their area" and "almost caused Black Metal to be banned in Britain as a result". Dayal Patterson says successful acts like Cradle of Filth "provoked an even greater extremity [of negative opinion] from the underground" scene due to concerns about "selling out".
The controversy surrounding the Thuringian band Absurd drew attention to the German black metal scene. In 1993, the members murdered a boy from their school, Sandro Beyer. A photo of Beyer's gravestone is on the cover of one of their demos, "Thuringian Pagan Madness", along with pro-Nazi statements. It was recorded in prison and released in Poland by Graveland drummer Capricornus. The band's early music was more influenced by Oi! and Rock Against Communism (RAC) than by black metal, and described as being "more akin to '60s garage punk than some of the […] Black Metal of their contemporaries". Alexander von Meilenwald from German band Nagelfar considers Ungod's 1993 debut "Circle of the Seven Infernal Pacts", Desaster's 1994 demo "Lost in the Ages", Tha-Norr's 1995 album "Wolfenzeitalter", Lunar Aurora's 1996 debut "Weltengänger" and Katharsis's 2000 debut "666" to be the most important recordings for the German scene. He said they were "not necessarily the best German releases, but they all kicked off something".
In the beginning of the second wave, the different scenes developed their own styles; as Alan 'Nemtheanga' Averill says, "you had the Greek sound and the Finnish sound, and the Norwegian sound, and there was German bands and Swiss bands and that kind of thing." By the mid-1990s, the style of the Norwegian scene was being adopted by bands worldwide, and in 1998, "Kerrang!" journalist Malcolm Dome said that "black metal as we know it in 1998 owes more to Norway and to Scandinavia than any other particular country". Newer black metal bands also began raising their production quality and introducing additional instruments such as synthesizers and even full-symphony orchestras. By the late 1990s, the underground concluded that several of the Norwegian pioneers—like Emperor, Immortal, Dimmu Borgir, Ancient, Covenant/The Kovenant, and Satyricon—had commercialized or sold out to the mainstream and "big bastard labels." Dayal Patterson states that successful acts like Dimmu Borgir "provoked and even greater extremity [of negative opinion] from the underground" regarding the view that these bands had "sold out."
After Euronymous's death, "some bands went more towards the Viking metal and epic style, while some bands went deeper into the abyss." Since 1993, the Swedish scene had carried out church burnings, grave desecration, and other violent acts. In 1995, Jon Nödtveidt of Dissection joined the Misanthropic Luciferian Order (MLO). In 1997, he and another MLO member were arrested and charged with shooting dead a 37-year-old man. It was said he was killed "out of anger" because he had "harassed" the two men. Nödtveidt received a 10-year sentence. As the victim was a homosexual immigrant, Dissection was accused of being a Nazi band, but Nödtveidt denied this and dismissed racism and nationalism.
The Swedish band Shining, founded in 1996, began writing music almost exclusively about depression and suicide, musically inspired by Strid and by Burzum's albums "Hvis lyset tar oss" and "Filosofem". Vocalist Niklas Kvarforth wanted to "force-feed" his listeners "with self-destructive and suicidal imagery and lyrics." In the beginning, he used the term "suicidal black metal" for his music. However, he stopped using the term in 2001 because it had begun to be used by a slew of other bands, whom he felt had misinterpreted his vision and were using the music as a kind of therapy rather than a weapon against the listener as Kvarforth intended. He said that he "wouldn't call Shining a black metal band" and called the "suicidal black metal" term a "foolish idea."
According to Erik Danielsson, when his band Watain formed in 1998, there were very few bands who took black metal as seriously as the early Norwegian scene had. A newer generation of Swedish Satanic bands like Watain and Ondskapt, supposedly inspired by Ofermod, the new band of Nefandus member Belfagor, put this scene "into a new light." Kvarforth said, "It seems like people actually [got] afraid again." "The current Swedish black metal scene has a particularly ambitious and articulate understanding of mysticism and its validity to black metal. Many Swedish black metal bands, most notably Watain and Dissection, are [or were] affiliated with the Temple of the Black Light, or Misanthropic Luciferian Order […] a Theistic, Gnostic, Satanic organization based in Sweden". Upon his release in 2004, Jon Nödtveidt restarted Dissection with new members whom he felt were able to "stand behind and live up to the demands of Dissection's Satanic concept." He started calling Dissection "the sonic propaganda unit of the MLO" and released a third full-length album, "Reinkaos". The lyrics contain magical formulae from the "Liber Azerate" and are based on the organization's teachings. After the album's release and a few concerts, Nödtveidt said that he had "reached the limitations of music as a tool for expressing what I want to express, for myself and the handful of others that I care about" and disbanded Dissection before dying by suicide.
A part of the underground scene adopted a Jungian interpretation of the church burnings and other acts of the early scene as the re-emergence of ancient archetypes, which Kadmon of Allerseelen and the authors of "Lords of Chaos" had implied in their writings. They mixed this interpretation with Paganism and Nationalism. Varg Vikernes was seen as "an ideological messiah" by some, although Vikernes had disassociated himself from black metal and his neo-Nazism had nothing to do with that subculture. This led to the rise of National Socialist black metal (NSBM), which Hendrik Möbus of Absurd calls "the logical conclusion" of the Norwegian black metal "movement". Other parts of the scene oppose NSBM as it is "indelibly linked with Asá Trŭ and opposed to Satanism", or look upon Nazism "with vague skepticism and indifference". Members of the NSBM scene, among others, see the Norwegian bands as poseurs whose "ideology is cheap", although they still respect Vikernes and Burzum, whom Grand Belial's Key vocalist Richard Mills called "the only Norwegian band that remains unapologetic and literally convicted of his beliefs."
In France, besides Les Légions Noires (The Black Legions), an NSBM scene arose. Members of French band Funeral desecrated a grave in Toulon in June 1996, and a 19-year-old black metal fan stabbed a priest to death in Mulhouse on Christmas Eve 1996. According to MkM of Antaeus and Aosoth, the early French scene "was quite easy to divide: either you were NSBM, and you had the support from zine and the audience, or you were part of the black legions, and you had that 'cult' aura", whereas his band Antaeus, not belonging to either of these sub-scenes, "did not fit anywhere." Many French bands, like Deathspell Omega and Aosoth, have an avantgarde approach and a disharmonic sound that is representative of that scene.
The early American black metal bands remained underground. Some of them—like Grand Belial's Key and Judas Iscariot—joined an international NSBM organization called the Pagan Front, although Judas Iscariot's sole member Akhenaten left the organization. Other bands like Averse Sefira never had any link with Nazism. The US bands have no common style. Many were musically inspired by Burzum but did not necessarily adopt Vikernes's ideas. Profanatica's music is close to death metal, while Demoncy were accused of ripping off Gorgoroth riffs. There also emerged bands like Xasthur and Leviathan (whose music is inspired by Burzum and whose lyrics focus on topics such as depression and suicide), Nachtmystium, Krallice, Wolves in the Throne Room (a band linked to the crust punk scene and the environmental movement), and Liturgy (the style of whom frontwoman Hunter Hunt-Hendrix describes as 'trancendental black metal'). These bands eschew black metal's traditional lyrical content for "something more Whitman-esque" and have been rejected by some traditional black-metallers for their ideologies and the post-rock and shoegazing influences some of them have adopted. Also, some bands like Agalloch,
In Australia, a scene led by bands like Deströyer 666, Vomitor, Hobbs' Angel of Death, Nocturnal Graves and Gospel of the Horns arose. This scene's typical style is a mixture of old school black metal and raw thrash metal influenced by old Celtic Frost, Bathory, Venom, and Sodom but also with its own elements.
Melechesh was formed in Jerusalem in 1993, "the first overtly anti-Christian band to exist in one of the holiest cities in the world". Melechesh began as a straightforward black metal act with their first foray into folk metal occurring on their 1996 EP "The Siege of Lachish". Their subsequent albums straddled black, death, and thrash metal. Another band, Arallu, was formed in the late 1990s and has relationships with Melechesh and Salem. Melechesh and Arallu perform a style they call "Mesopotamian Black Metal", a blend of black metal and Mesopotamian folk music.
Since the 2000s, a number of anti-Islamic and anti-religious black metal bands—whose members come from Muslim backgrounds—have emerged in the Middle East. Janaza, believed to be Iraq's first female black metal artist, released the demo "Burning Quran Ceremony" in 2010. Its frontwoman, Anahita, claimed her parents and brother were killed by a suicide bomb during the Iraq War. Another Iraqi band, Seeds of Iblis, also fronted by Anahita, released their debut EP "Jihad Against Islam" in 2011 through French label Legion of Death. Metal news website Metalluminati suggests that their claims of being based in Iraq are a hoax. These bands, along with Tadnees (from Saudi Arabia), Halla (from Iran), False Allah (from Bahrain), and Mosque of Satan (from Lebanon), style themselves as the "Arabic Anti-Islamic Legion." Another Lebanese band, Ayat, drew much attention with their debut album "Six Years of Dormant Hatred", released through North American label Moribund Records in 2008. Some European bands have also begun expressing anti-Islamic views, most notably the Norwegian band Taake.
Regarding the sound of black metal, there are two conflicting groups within the genre: "those that stay true to the genre's roots, and those that introduce progressive elements". The former believe that the music should always be minimalist—performed only with the standard guitar-bass-drums setup and recorded in a low fidelity style. One supporter of this train of thought is Blake Judd of Nachtmystium, who has rejected labeling his band black metal for its departure from the genre's typical sound. Snorre Ruch of Thorns, on the other hand, has said that modern black metal is "too narrow" and believes that this was "not the idea at the beginning".
Since the 1990s, different styles of black metal have emerged and some have melded Norwegian-style black metal with other genres:
Ambient black metal is a style of black metal which relies on heavy incorporation of atmospheric, sometimes dreamy textures, and is therefore less aggressive. It often features synthesizers or classical instrumentation, typically for melody or ethereal "shimmering" over the wall of sound provided by the guitars. The music is usually slow to mid paced with rare blast beat usage, without any abrupt changes and generally features slowly developing, sometimes repetitive melodies and riffs, which separate it from other black metal styles. Subject matter usually concerns nature, folklore, mythology, and personal introspection. Artists include Agalloch and Wolves in the Throne Room.
Black-doom, also known as blackened doom, is a style that combines the slowness and thicker, bassier sound of doom metal with the shrieking vocals and heavily distorted guitar sound of black metal. Black-doom bands maintain the Satanic ideology associated with black metal, while melding it with moodier themes more related to doom metal, like depression, nihilism and nature. They also use the slower pace of doom metal in order to emphasize the harsh atmosphere present in black metal. Examples of black-doom bands include Barathrum, Forgotten Tomb, Woods of Ypres, Deinonychus, Shining, Nortt, Bethlehem, early Katatonia. Tiamat, Dolorian, and October Tide.
Pioneered by black-doom bands like Ophthalamia, Katatonia, Bethlehem, Forgotten Tomb and Shining, depressive suicidal black metal, also known as suicidal black metal, depressive black metal or DSBM, is a style that melds the second wave-style of black metal with doom metal, with lyrics revolving around themes such as depression, self-harm, misanthropy, suicide and death. DSBM bands draws the lo-fi recording and highly distorted guitars of black metal, while employing the usage of acoustic instruments and non-distorted electric guitar's timbres present in doom metal, interchanging the slower, doom-like, sections with faster tremolo picking. Vocals are usually high-pitched like in black metal, but lacking of energy, simulating feelings like hopelessness, desperation and plea. The presence of one-man bands is more prominent in this genre compared to others. Examples of bands include Xasthur, Leviathan, Strid, Silencer, Make a Change… Kill Yourself, and I Shalt Become.
Black 'n' roll is a style of black metal that incorporates elements from 1970s hard rock and rock and roll music. Examples of black 'n' roll bands include Midnight, Kvelertak, Vreid, and Khold. Bands such as Satyricon, Darkthrone, Nachtmystium, Nidingr, Craft, and Sarke also experimented with the genre.
Crust punk groups, such as Antisect, Sacrilege and Anti System took some influence from early black metal bands like Venom, Hellhammer, and Celtic Frost, while Amebix's lead vocalist and guitarist sent his band's early demo tape to Cronos of Venom, who replied by saying "We’ll rip you off." Similarly, Bathory was initially inspired by crust punk as well as heavy metal. Crust punk was affected by a second wave of black metal in the 1990s, with some bands emphasizing these black metal elements. Iskra are probably the most obvious example of second wave black metal-influenced crust punk; Iskra coined their own phrase "blackened crust" to describe their new style. The Japanese group Gallhammer also fused crust punk with black metal while the English band Fukpig has been said to have elements of crust punk, black metal, and grindcore. North Carolina's Young and in the Way have been playing blackened crust since their formation in 2009. In addition, Norwegian band Darkthrone have incorporated crust punk traits in their more recent material. As Daniel Ekeroth wrote in 2008,
Blackened death-doom is a genre that combines the slow tempos and monolithic drumming of doom metal, the complex and loud riffage of death metal and the shrieking vocals of black metal. Examples of blackened death-doom bands include Morast, Faustcoven, The Ruins of Beverast, Bölzer, Necros Christos, Harvest Gulgaltha, Dragged Into Sunlight, Hands of Thieves, and Soulburn.
Blackened death metal is commonly death metal that incorporates musical, lyrical or ideological elements of black metal, such as an increased use of tremolo picking, anti-Christian or Satanic lyrical themes and chord progressions similar to those used in black metal. Blackened death metal bands are also more likely to wear corpse paint and suits of armour, than bands from other styles of death metal. Lower range guitar tunings, death growls and abrupt tempo changes are common in the genre. Examples of blackened death metal bands are Belphegor, Behemoth, Akercocke, and Sacramentum.
Melodic black-death (also known as blackened melodic death metal or melodic blackened death metal) is a genre of extreme metal that describes the style created when melodic death metal bands began being inspired by black metal and European romanticism. However, unlike most other black metal, this take on the genre would incorporate an increased sense of melody and narrative. Some bands who have played this style include Dissection, Sacramentum, Embraced, Naglfar, Satariel, Throes of Dawn, Obscurity, Dawn, "Cries of the Past"-era Underoath, Catamenia, Midvinter, Twin Obscenity, Nokturnal Mortum Unanimated, Epoch of Unlight, This Ending, Suidakra, Oathean, Thulcandra, Skeletonwitch, and Cardinal Sin.
War metal (also known as war black metal or bestial black metal) is an aggressive, cacophonous, and chaotic subgenre of blackened death metal, described by "Rock Hard" journalist Wolf-Rüdiger Mühlmann as "rabid" and "hammering". Important influences include early black and death metal bands, such as Sodom, Possessed, Autopsy, Sarcófago, and the first two Sepultura releases, as well as seminal grindcore acts like Repulsion. War metal bands include Blasphemy, Archgoat, Impiety, Beherit, Crimson Thorn, and Bestial Warlust.
Blackened grindcore is a fusion genre that combines elements of black metal and grindcore. Notable bands include Vomit Fist, Dendritic Arbor, Sunlight's Bane, Scumpulse, Malevich, Absvrdist, and early Rotting Christ.
Blackened thrash metal, also known as black-thrash, is a fusion genre that combines elements of black metal and thrash metal. Being considered as one of the first fusions of extreme metal, it was inspired by bands such as Venom, Sodom, and Sarcófago. Notable bands include Aura Noir, Witchery, Black Fast, Sathanas, and Deströyer 666.
Folk black metal, pagan metal and Viking metal are styles that incorporates elements of folk music, with pagan metal bands focusing on pagan lyrics and imagery, and Viking metal bands giving thematic focus on Norse mythology, Norse paganism, and the Viking Age, more influenced by Nordic folk music. While not focused on Satanism, the bands' use of ancient folklore and mythologies still express anti-Christian views, with folk black metal doing it as part of a "rebellion to the status quo", that developed concurrently along with the rise of folk metal in Europe in the 1990s, Notable artist include Negură Bunget, Windir, Primordial, In the Woods..., Cruachan, and Bathory, to whose albums "Blood Fire Death" (1988) and "Hammerheart" (1990) the origin of Viking metal can be traced.
Industrial black metal is a style of black metal that incorporates elements of industrial music. Mysticum, formed in 1991, was the first of these groups. DHG (Dødheimsgard), Thorns from Norway and Blut Aus Nord, N.K.V.D. and Blacklodge from France, have been acclaimed for their incorporation of industrial elements. Other industrial black metal musicians include Samael, The Axis of Perdition, Aborym, and ...And Oceans. In addition, The Kovenant, Mortiis and Ulver emerged from the Norwegian black metal scene, but later chose to experiment with industrial music.
Post-black metal is an umbrella term for genres that experiment beyond black metal's conventions and broaden their sounds, evolving past the genre's limits. Notable bands include Myrkur, Alcest, Bosse-de-Nage, and Wildernessking.
Blackgaze incorporates common black metal and post-black metal elements such as blast beat drumming and high-pitched screamed vocals with the melodic and heavily distorted guitar styles typically associated with shoegazing. It is associated with bands such as Deafheaven, Alcest, Vaura, Amesoeurs, Bosse-de-Nage, Oathbreaker, and Fen.
Psychedelic black metal is a subgenre of black metal which employs the usage of psychedelic elements. Notable acts include Oranssi Pazuzu, Nachtmystium, Deafheaven, Woe, Amesoeurs, and In the Woods...
Symphonic black metal is a style of black metal that incorporates symphonic and orchestral elements. This may include the usage of instruments found in symphony orchestras (piano, violin, cello, flute and keyboards), "clean" or operatic vocals and guitars with less distortion.
Unlike other metal genres, black metal is associated with an ideology and ethos. It is fiercely opposed to Christianity and the other main institutional religions, Islam and Judaism. Many black metal bands are Satanists and see Satanism as a key part of black metal. Others advocate ethnic Paganism, "often coupled with nationalism", although the early Pagan bands did not call themselves 'black metal'.
Black metal tends to be misanthropic and hostile to modern society. It is "a reaction against the mundanity, insincerity and emotional emptiness that participants feel is intrinsic to modern secular culture". The black metal scene tends to oppose political correctness, humanitarianism, consumerism, globalization and homogeneity. Aaron Weaver from Wolves in the Throne Room said: "I think that black metal is an artistic movement that is critiquing modernity on a fundamental level, saying that the modern world view is missing something". As part of this, some parts of the scene glorify nature and have a fascination with the distant past. Black metal has been likened to Romanticism and there is an undercurrent of romantic nationalism in the genre. Sam Dunn noted that "unlike any other heavy metal scene, the culture and the place is incorporated into the music and imagery". Individualism is also an important part of black metal, with Fenriz of Darkthrone describing black metal as "individualism above all". Unlike other kinds of metal, black metal has numerous one-man bands. However, it is argued that followers of Euronymous were anti-individualistic, and that "Black Metal is characterized by a conflict between radical individualism and group identity and by an attempt to accept both polarities simultaneously".
In his master's thesis, Benjamin Hedge Olson wrote that some artists can be seen as transcendentalists. Dissatisfied with a "world that they feel is devoid of spiritual and cultural significance", they try to leave or "transcend" their "mundane physical forms" and become one with the divine. This is done through their concerts, which he describes as "musical rituals" that involve self-mortification and taking on an alternative, "spiritual persona" (for example by the wearing of costume and face paint).
Black metal was originally a term for extreme metal bands with Satanic lyrics and imagery. However, most of the 'first wave' bands (including Venom, who coined the term 'black metal') were not Satanists and rather used Satanic themes to provoke controversy or gain attention. One of the few exceptions was Mercyful Fate singer and Church of Satan member King Diamond, whom Michael Moynihan calls "one of the only performers of the '80s Satanic metal who was more than just a poseur using a devilish image for shock value".
In the early 1990s, many Norwegian black-metallers presented themselves as genuine Devil worshippers. Mayhem's Euronymous was the key figure behind this. They attacked the Church of Satan for its "freedom and life-loving" views; the theistic Satanism they espoused was an inversion of Christianity. Benjamin Hedge Olson wrote that they "transform[ed] Venom's quasi-Satanic stage theatrics into a form of cultural expression unique from other forms of metal or Satanism" and "abandoned the mundane identities and ambitions of other forms of metal in favor of religious and ideological fanaticism". Some prominent scene members—such as Euronymous and Faust—stated that only bands who are Satanists can be called 'black metal'. Bands with a Norwegian style, but without Satanic lyrics, tended to use other names for their music. This view is still held by many artists—such as Infernus, Arioch, Nornagest and Erik Danielsson. Some bands, like the reformed Dissection and Watain, insist that all members must be of the same Satanic belief, whereas Michael Ford of Black Funeral and MkM of Antaeus believe black metal must be Satanic but not all band members need to be Satanists. Others—such as Jan Axel Blomberg, Sigurd Wongraven and Eric Horner—believe that black metal does not need to be Satanic. An article in Metalion's "Slayer" fanzine attacked musicians that "care more about their guitars than the actual essence onto which the whole concept was and is based upon", and insisted that "the music itself doesn't come as the first priority". Bands with a similar style but with Pagan lyrics tend to be referred to as 'Pagan Metal' by many 'purist' black-metallers.
Others shun Satanism, seeing it as Christian or "Judeo-Christian" in origin, and regard Satanists as perpetuating the "Judeo-Christian" worldview. Quorthon of Bathory said he used 'Satan' to provoke and attack Christianity. However, with his third and fourth albums, "Under the Sign of the Black Mark" and "Blood Fire Death", he began "attacking Christianity from a different angle", realizing that Satanism is a "Christian product". Nevertheless, some artists use Satan as a symbol or metaphor for their beliefs, such as LaVeyan Satanists (who are atheist). Vocalist Gaahl, who considers himself a Norse Shaman, said: "We use the word 'Satanist' because it is Christian world and we have to speak their language ... When I use the word 'Satan', it means the natural order, the will of a man, the will to grow, the will to become the superman". Varg Vikernes called himself a Satanist in early interviews but "now downplays his former interest in Satanism", saying he was using Satan as a symbol for Odin as the 'adversary' of the Christian God. He saw Satanism as "an introduction to more indigenous heathen beliefs". Some bands such as Carach Angren, Immortal and Enslaved do not have Satanic lyrics.
A wide range of political views are found in the black metal scene. The vast majority of black metal bands are not openly political, although there is said to be an undercurrent of romantic and ethnic nationalism in black metal.
National Socialist black metal (also known as NSBM) promotes neo-Nazi or similar beliefs through its lyrics and imagery. Artists typically meld neo-Nazi ideology with ethnic European paganism, but a few meld these beliefs with Satanism or occultism. Some commentators see this ideology as a natural development of the black metal worldview. Members of the early Norwegian scene flirted with Nazi themes, but this was largely an attempt to provoke. Varg Vikernes—who now refers to his ideology as 'Odalism'—is credited with popularizing such views within the scene. NSBM emerged in the mid-1990s and was spearheaded by artists such as Absurd (from Germany), Graveland, Infernum, and Veles (from Poland), and Grand Belial's Key (from the US). It is particularly strong in the former Eastern Bloc. There are dozens of NSBM bands, several independent record labels and zines devoted to NSBM, and festivals associated with it. Some black metal bands have been wrongly labeled as NSBM for exploring Nazi Germany in their lyrics or referencing it for shock value.
NSBM artists are a small minority within the genre. While some black metal fans boycott NSBM artists, many are indifferent or appreciate the music without supporting the musicians. NSBM has been criticized by some prominent and influential black metal artists—including Jon Nödtveidt, Gorgoroth, Dark Funeral, Richard Lederer, Michael Ford, and Arkhon Infaustus. Some liken Nazism to Christianity in that it is authoritarian, collectivist, and a "herd mentality". Olson writes that the shunning of Nazism by some black-metallers "has nothing to do with notions of a 'universal humanity' or a rejection of hate" but that Nazism is shunned "because its hatred is too specific and exclusive".
Red and Anarchist black metal (also known as RABM or Anarchist black metal) consists of a small number of artists who promote ideologies such as anarchism, green anarchism, or Marxism. It was born partly as a reaction to NSBM and from the melding of black metal with anarchist crust punk. Artists labelled RABM include Iskra, Panopticon, Skagos, Storm of Sedition, Not A Cost, Black Kronstadt, and Vidargangr. Others with similar outlook, such as Wolves in the Throne Room, are not overtly political and do not endorse the label.
'Unblack metal' (or 'Christian black metal') promotes Christianity through its lyrics and imagery. The first unblack metal record, "Hellig Usvart" (1994) by Australian artist Horde, was a provocative parody of Norwegian black metal. It sparked controversy, and death threats were issued against Horde. Norwegian unblack metal band Antestor was originally formed as a death/doom band bearing a different name, Crush Evil.
Many black-metallers see "Christian black metal" as an oxymoron and believe black metal cannot be Christian. In fact, the early unblack metal groups Horde and Antestor refused to call their music "black metal" because they did not share its ethos. Horde called its music "holy unblack metal" and Antestor called theirs "sorrow metal". Horde's Jayson Sherlock later said "I will never understand why Christians think they can play Black Metal. I really don't think they understand what true Black Metal is". However, current unblack metal bands such as Crimson Moonlight feel that black metal has changed from an ideological movement to a purely musical genre, and thus call their music 'black metal'.

</doc>
<doc id="4875" url="https://en.wikipedia.org/wiki?curid=4875" title="Bin Laden (disambiguation)">
Bin Laden (disambiguation)

bin Laden () is an Arabic language surname synonymous with Osama bin Laden (1957–2011); it may also pertain to the Saudi Binladin Group, a holding company for the assets of the bin Laden family, and other notable members of Osama's family.

</doc>
<doc id="4876" url="https://en.wikipedia.org/wiki?curid=4876" title="Blizzard Entertainment">
Blizzard Entertainment

Blizzard Entertainment, Inc. is an American video game developer and publisher based in Irvine, California. A subsidiary of Activision Blizzard, the company was founded on February 8, 1991, under the name Silicon & Synapse, Inc. by three graduates of the University of California, Los Angeles: Michael Morhaime, Frank Pearce and Allen Adham. The company originally concentrated on the creation of game ports for other studios' games before beginning development of their own software in 1993 with games like "Rock n' Roll Racing" and "The Lost Vikings". In 1994, the company became Chaos Studios, Inc., then Blizzard Entertainment after being acquired by distributor Davidson & Associates.
Shortly thereafter, Blizzard released '. Blizzard created several other video games, including "Warcraft" sequels, the "Diablo" series, the "StarCraft" series, and in 2004, the massively multiplayer online role-playing game "World of Warcraft". Their most recent projects include the first expansion for "Diablo III", ', the online collectible card game "Hearthstone," the seventh expansion for "World of Warcraft", ', the multiplayer online battle arena "Heroes of the Storm," the third and final expansion for ', "", and the multiplayer first-person hero shooter "Overwatch".
On July 9, 2008, Activision merged with Vivendi Games, culminating in the inclusion of the Blizzard brand name in the title of the resulting holding company. On July 25, 2013, Activision Blizzard announced the purchase of 429 million shares from majority owner Vivendi. As a result, Activision Blizzard became a completely independent company.
Blizzard Entertainment hosts conventions for fans to meet and to promote their games: the BlizzCon in California, United States, and the Blizzard Worldwide Invitational in other countries, including South Korea and France.
Blizzard Entertainment was founded by Michael Morhaime, Allen Adham, and Frank Pearce as Silicon & Synapse in February 1991, after all three had earned their bachelor's degrees from the University of California, Los Angeles, the year prior. The name "Silicon & Synapse" was a high concept from the three founders, with "silicon" representing the building block of a computer, while "synapse" the building block of the brain. The initial logo created by Stu Rose. To fund the company, each of them contributed about $10,000, Morhaime borrowing the sum interest-free from his grandmother. During the first two years, the company focused on creating game ports for other studios. Ports include titles such as "J.R.R. Tolkien's The Lord of the Rings, Vol. I" and "Battle Chess II: Chinese Chess". In 1993, the company developed games such as "Rock n' Roll Racing" and "The Lost Vikings" (published by Interplay Productions).
Around 1993, co-founder Adham told the other executives that he did not like the name "Silicon & Synapse" anymore, as people outside the company were confusing the meaning of silicon the chemical element used in microchips with silicone the materials used in breast implants. By the end of 1993, Adham changed the name to "Chaos Studios", reflecting on the haphazardness of their development processes.
In early 1994, they were acquired by distributor Davidson & Associates for $6.75 million ($ million today). Shortly after this point, they were contacted by a Florida company, Chaos Technologies, who wanted the company to pay to keep the name. Not wanting to pay that sum, the executives decided to change the studio's name to "Ogre Studios" by April 1994. However, Davidson & Associated did not like this name, and forced the company to change it. According to Morhaime, Adham began running through a dictionary from the start, writing down any word that seemed interesting and passing it to the legal department to see if it had any complications. One of the first words they found to be interest and cleared the legal check was "blizzard", leading them to change their name to "Blizzard Entertainment" by May 1994. By May, Chaos Studios was renamed Blizzard Entertainment.
Shortly thereafter, Blizzard shipped their breakthrough hit "", a real-time strategy (RTS) game in a high-fantasy setting.
Blizzard has changed hands several times since then. Davidson was acquired along with Sierra On-Line by a company called CUC International in 1996. CUC then merged with a hotel, real-estate, and car-rental franchiser called HFS Corporation to form Cendant in 1997. In 1998 it became apparent that CUC had engaged in accounting fraud for years before the merger. Cendant's stock lost 80% of its value over the next six months in the ensuing widely discussed accounting scandal. The company sold its consumer software operations, Sierra On-line (which included Blizzard) to French publisher Havas in 1998, the same year Havas was purchased by Vivendi. Blizzard was part of the Vivendi Games group of Vivendi.
In 1996, Blizzard acquired Condor Games of San Mateo, California, which had been working on the action role-playing game (ARPG) "Diablo" for Blizzard at the time. Condor was renamed Blizzard North, with Blizzard's main headquarters in Irving renamed to Blizzard South to distinguish the two. "Diablo" was released at the very start of 1997 alongside Battle.net, a matchmaking service for the game. Blizzard North developed the sequel "Diablo II" (2000), and its expansion pack "" (2001). Following these releases, a number of key staff from Blizzard North departed for other opportunities, such as Bill Roper. Vivendi made the decision in August 2005 to consolidate Blizzard North into Blizzard South, relocating staff to the main Blizzard offices in Irvine, and subsequently dropping the "Blizzard South" name.
Following the success of "Warcraft II", Blizzard began development on a science-fiction themed RTS "StarCraft" and released the title in March 1998. The title was the top-selling PC game for the year, and led to further growth of the Battle.net service and the use of the game for esports. Around 2000, Blizzard engaged with Nihilistic Software to work on a version of "StarCraft" for home consoles for Blizzard. Nihilisitic was co-founded by Robert Huebner, who had worked on "StarCraft" and other games while a Blizzard employee before leaving to found the studio. The game, ', was a stealth-oriented game compared to the RTS features of "StarCraft", and was a major feature of the 2002 Tokyo Game Show. However, over the next few years, the game entered development hell with conflicts between Nihilisitic and Blizzard on its direction. Blizzard ordered Nihilistic to stop work on "StarCraft: Ghost" in July 2004, and instead brought on Swingin' Ape Studios, a third-party studio that had just successfully released ' in 2003, to reboot the development of "Ghost". Blizzard fully acquired Swingin' Ape Studios in May 2005 to continue on "Ghost". However, while the game was scheduled to be released in 2005, it was targeted at the consoles of the sixth generation, such as the PlayStation 2 and original Xbox, while the industry was transitioning to the seventh generation. Blizzard decided to cancel "Ghost" rather than extend its development period to work on the newer consoles.
In 2002, Blizzard was able to reacquire rights for three of its earlier Silicon & Synapse titles, "The Lost Vikings", "Rock n' Roll Racing" and "Blackthorne", from Interplay Entertainment and re-release them for Game Boy Advance, a handheld console.
In 2004, Blizzard opened European offices in the Paris suburb of Vélizy, Yvelines, France.
Blizzard released "World of Warcraft", a massively multiplayer online role-playing game (MMORPG) based on the "Warcraft" franchise, on November 23, 2004 in North America, and on February 11, 2005 in Europe. By December 2004, the game was the fastest-selling PC game in the United States, and by March 2005, had reached 1.5 million subscribers worldwide. Blizzard partnered with Chinese publisher The9 to publish and distribute "World of Warcraft" in China, as foreign companies could not directly publish into the country themselves. "World of Warcraft" launched in China in June 2005. By the end of 2007, "World of Warcraft" was considered a global phenomenon, having reached over 9 million subscribers and exceeded in revenue since its release. In April 2008, "World of Warcraft" was estimated to hold 62 percent of the MMORPG subscription market. Blizzard's staff quadrupled from around 400 employees in 2004 to 1600 by 2006 to provide more resources to the game and its various expansions, and Blizzard moved their headquarters to 16215 Alton Parkway in Irvine, California in 2007 to support the additional staff.
With the success of "World of Warcraft", Blizzard organized the first BlizzCon fan convention in October 2005 held at the Anaheim Convention Center. The inaugural event drew about 6,000 people and became an annual event which Blizzard uses to announce new games, expansions, and content for its properties.
Up through 2006, Bobby Kotick, the CEO of Activision, had been working to rebound the company from near-bankruptcy, and had established a number of new studios. However, Activision lacked anything in the MMO market. Kotick saw that "World of Warcraft" was bringing in over a year in subscription fees, and began approaching Vivendi's CEO Jean-Bernard Lévy about potential acquisition of their struggling Vivendi Games division, which included Blizzard Entertainment. Lévy was open to a merger, but would only allow it if he controlled the majority of the combined company, knowing the value of "World of Warcraft" to Kotick. Among those Kotick spoke to for advice included Blizzard's Morhaime, who told Kotick that they had begun establishing lucrative in-roads into the Chinese market. Kotick accepted Lévy's deal, with the deal approved by shareholders in December 2007. By July 2008, the merger was complete, with Vivendi Games effectively dissolved except for Blizzard, and the new company was named Activision Blizzard.
Blizzard established a distribution agreement with the Chinese company NetEase in August 2008 to publish Blizzard's games in China. The deal focused on "StarCraft II" which was gaining popularity as an esport within southeast Asia, as well as for other Blizzard games with the exception of "World of Warcraft", still being handled by The9. The two companies established the Shanghai EaseNet Network Technology for managing the games within China. Blizzard and The9 prepared to launch the "World of Warcraft" expansion "", but the expansion came under scrutiny by China's content regulation board, the General Administration of Press and Publication, which rejected publication of it within China in March 2009, even with preliminary modifications made by The9 to clear it. Rumors of Blizzard's dissatisfaction with The9 from this and other previous complications with "World of Warcraft" came to a head when, in April 2009, Blizzard announced it was terminating its contract with The9, and transferred operation of "World of Warcraft" in China to NetEase.
They released an improved version of Battle.net (Battle.net 2.0) in March 2009 which included improved matchmaking, storefront features, and better support for all of Blizzard's existing titles particularly "World of Warcraft".
Having peaked at 12 million monthly subscriptions in 2010, "World of Warcraft" subscriptions sunk to 6.8 million in 2014, the lowest number since the end of 2006, prior to "" expansion. However, "World of Warcraft" is still the world's most-subscribed MMORPG, and holds the Guinness World Record for the most popular MMORPG by subscribers. In 2008, Blizzard was honored at the 59th Annual Technology & Engineering Emmy Awards for the creation of "World of Warcraft". Mike Morhaime accepted the award.
Following the merger, Blizzard found it was relying on its well-established properties, but at the same time, the industry was experiencing a shift towards indie games. Blizzard established a few small teams within the company to work on developing new concepts based on the indie development approach that it could potentially use. One of these teams quickly came onto the idea of a collectible card game based on the "Warcraft" narrative universe, which ultimately became "Hearthstone", released as a free-to-play title in March 2014. "Hearthstone" reached over 25 million players by the end of 2014, and exceeded 100 million players by 2018.
Another small internal team began work around 2008 on a new intellectual property known as "Titan", a more contemporary or near-future MMORPG that would have co-existed alongside "World of Warcraft". The project gained more visibility in 2010 as a result of some information leaks. Blizzard continued to speak on "Titan"s development over the next few years, with over 100 people within Blizzard working on the project. However, "Titan"s development was troubled, and, internally, in May 2013, Blizzard cancelled the project (publicly reporting this in 2014), and reassigned most of the staff but left about 40 people, led by Jeff Kaplan, to either come up with a fresh idea within a few weeks or have their team reassigned to Blizzard's other departments. The small team came upon the idea of a team-based multiplayer shooter game, reusing many of the assets from "Titan" but set in a new near-future narrative. The new project was greenlit by Blizzard and became known as "Overwatch", which was released in May 2016. "Overwatch" became the fourth main intellectual property of Blizzard, following "Warcraft", "Starcraft", and "Diablo".
In addition to "Hearthstone" and "Overwatch", Blizzard continued to produce sequels and expansions to its established properties during this period, including ' (2010) and "Diablo III" (2012). Blizzard's major crossover title, "Heroes of the Storm", was released as a MOBA game in 2015. The game featured various characters from Blizzard's franchises as playable heroes, as well as different battlegrounds based on "Warcraft", "Diablo", "StarCraft", and "Overwatch" universes. In the late 2010s, Blizzard released ' (2017) and "" (2020)"," remastered versions of the original "StarCraft" and "Warcraft III," respectively"."
In 2012, Blizzard had 4,700 employees, with offices across 11 cities including Austin, Texas, and countries around the globe. , the company's headquarters in Irvine, California had 2,622 employees.
The May 2016 release of "Overwatch" was highly successful, and was the highest-selling game on PC for 2016. Several traditional esport events had been established within the year of "Overwatch" release, such as the Overwatch World Cup, but Blizzard continued to expand this and announced the first esports professional league, the Overwatch League at the 2016 BlizzCon event. The company purchased a studio at The Burbank Studios in Burbank, California, that it converted into a dedicated esports venue, the Blizzard Arena, to be used for the Overwatch League and other events. The inaugural season of the Overwatch League launched on January 10, 2018 with 12 global teams playing. By the second season in 2019 it had expanded the League to 20 teams, and with its third season in 2020, it will have these teams traveling across the globe in a transitional home/away-style format.
On October 3, 2018, Mike Morhaime announced he was stepping down as the company president and CEO, but will still remain an advisor to the company. Morhaime was replaced by J. Allen Brack, the executive producer on "World of Warcraft". In January 2019 it was announced that Morhaime would leave the company on April 7, 2019. Frank Pearce announced he would be stepping down as Blizzard's Chief Development Officer on July 19, 2019, though will remain in an advisory role similar to Morhaime. Michael Chu, lead writer on many of Blizzard's franchises including "Diablo", "Warcraft", and "Overwatch", announced he was leaving the company after 20 years in March 2020.
Blizzard Entertainment has developed 19 games since 1991, in addition to developing 8 ports between 1992 and 1993; 11 of those games are in the "Warcraft", "Diablo", and "StarCraft" series. Since the release of "" (1994), "Diablo" (1997), and "StarCraft" (1998), Blizzard has focused almost exclusively on those three series. The sole exception has been "Overwatch" (2016).
Currently, Blizzard has four main franchises: "Warcraft", "Diablo", "StarCraft", and "Overwatch". Each franchise is supported by other media based around its intellectual property such as novels, collectible card games, comics and video shorts. Blizzard Entertainment announced in 2006 that they would be producing a "Warcraft" live-action movie. The movie was directed by Duncan Jones, financed and produced by Legendary Pictures, Atlas Entertainment, and others, and distributed by Universal Pictures. It was released in June 2016.
Notable unreleased titles include ', which was canceled on May 22, 1998, "Shattered Nations", and ', which was "Postponed indefinitely" on March 24, 2006 after being in development hell for much of its lifespan. After seven years of development, Blizzard revealed the cancellation of an unannounced MMO codenamed "Titan" on September 23, 2014. The company also has a history of declining to set release dates, choosing to instead take as much time as needed, generally saying a given product is "done when it's done."
"Pax Imperia II" was originally announced as a title to be published by Blizzard. Blizzard eventually dropped "Pax Imperia II", though, when it decided it might be in conflict with their other space strategy project, which became known as "StarCraft". THQ eventually contracted with Heliotrope and released the game in 1997 as "".
Blizzard has made use of a special form of software known as the 'Warden Client'. The Warden client is known to be used with Blizzard's online games such as "Diablo" and "World of Warcraft", and the Terms of Service contain a clause consenting to the Warden software's RAM scans while a Blizzard game is running.
The Warden client scans a small portion of the code segment of running processes in order to determine whether any third-party programs are running. The goal of this is to detect and address players who may be attempting to run unsigned code or third party programs in the game. This determination of third party programs is made by hashing the scanned strings and comparing the hashed value to a list of hashes assumed to correspond to banned third party programs. The Warden's reliability in correctly discerning legitimate versus illegitimate actions was called into question when a large scale incident happened. This incident banned many Linux users after an update to Warden caused it to incorrectly detect Cedega as a cheat program. Blizzard issued a statement claiming they had correctly identified and restored all accounts and credited them with 20 days play. Warden scans all processes running on a computer, not just the game, and could possibly run across what would be considered private information and other personally identifiable information. It is because of these peripheral scans that Warden has been accused of being spyware and has run afoul of controversy among privacy advocates.
Blizzard released its revamped Battle.net service in 2009. This service allows people who have purchased Blizzard products ("StarCraft", ', "Diablo II", and ', as well as their expansions) to download digital copies of games they have purchased, without needing any physical media.
On November 11, 2009, Blizzard required all "World of Warcraft" accounts to switch over to Battle.net accounts. This transition means that all current Blizzard titles can be accessed, downloaded, and played with a singular Battle.net login.
Battle.net 2.0 is the platform for matchmaking service for Blizzard games, which offers players a host of additional features. Players are able to track their friend's achievements, view match history, avatars, etc. Players are able to unlock a wide range of achievements for Blizzard games.
The service allows players to chat simultaneously with players from other Blizzard games. For example, players no longer need to create multiple user names or accounts for most Blizzard products. To enable cross-game communication, players need to become either Battletag or Real ID friends.
On July 6, 2010, Blizzard announced that they were changing the way their forums worked to require that users identify themselves with their real name. The reaction from the community was overwhelmingly negative with multiple game magazines calling the change "foolhardy" and an "epic fail". It resulted in a significant user response on the Blizzard forums, including one thread on the issue reaching over 11,000 replies. This included personal details of a Blizzard employee who gave his real name "to show it wasn't a big deal". Shortly after revealing his real name, forum users posted personal information including his phone number, picture, age, and home address.
Some technology media outlets suggested that displaying real names through Real ID is a good idea and would benefit both Battle.net and the Blizzard community. But others were worried that Blizzard was opening their fans up to real-life dangers such as stalking, harassment, and employment issues, since a simple Internet search by someone's employer can reveal their online activities.
Blizzard initially responded to some of the concerns by saying that the changes would not be retroactive to previous posts, that parents could set up the system so that minors cannot post, and that posting to the forums is optional. However, due to the huge negative response, Blizzard President Michael Morhaime issued a statement rescinding the plan to use real names on Blizzard's forums for the time being. The idea behind this plan was to allow players who had a relationship outside of the games to find each other easier across all the Blizzard game titles. They also planned to add several other features designed to make reading the forums more enjoyable and to empower players with tools to improve the quality of forum discussions.
Apart from the negative side effects of Real ID relating to privacy, the addition boasts features for current Blizzard titles. For instance, real names for friends, cross-realm and cross-game chat, rich presence and broadcasts are included with the Real ID system.
During an October 2019 "Hearthstone Grandmasters" streaming event in Taiwan, one player Ng Wai Chung, going by his online alias "Blitzchung" used an interview period to show support for the protestors in the 2019–20 Hong Kong protests. Shortly afterwards, on October 7, 2019, Blitzchung was disqualified from the current tournament and forfeited his winnings to date, and banned for a one-year period. The two shoutcasters engaged in the interview were also penalized with similar bans. Blizzard justified the ban as from its "Grandmasters" tournament rules that prevents players from anything that "brings [themselves] into public disrepute, offends a portion or group of the public, or otherwise damages [Blizzard's] image".
Blizzard's response led to several protests from current "Hearthstone" players, other video game players, and criticism from Blizzard's employees, fearing that Blizzard was giving into the censorship of the Chinese government. Protests were held, including through the 2019 BlizzCon in early November, to urge Blizzard to reverse their bans. The situation also drew the attention of several U.S. lawmakers, fearing that Blizzard, as a U.S. company, was letting China dictate how it handled speech and also urged the bans to be reversed.
Blizzard CEO J. Allen Brack wrote an open letter on October 11, 2019, apologizing for the way Blizzard handled the situation, and reduced the bans for both Blitzchung and the casters to six months. Brack reiterated that while they support free speech and their decision was in no way tied to the Chinese government, they want players and casters to avoid speaking beyond the tournament and the games in such interviews.
In 1998, Donald P. Driscoll, an Albany, California attorney filed a suit on behalf of Intervention, Inc., a California consumer group, against Blizzard Entertainment for "unlawful business practices" for the action of collecting data from a user's computer without their permission.
On June 20, 2003, Blizzard issued a cease and desist letter to the developers of an open-source clone of the Warcraft engine called "FreeCraft", claiming trademark infringement. This hobby project had the same gameplay and characters as "Warcraft II", but came with different graphics and music.
As well as a similar name, "FreeCraft" enabled players to use "Warcraft II" graphics, provided they had the "Warcraft II" CD. The programmers of the clone shut down their site without challenge. Soon after that the developers regrouped to continue the work by the name of "Stratagus".
On August 14, 2007, Beijing University Founder Electronics Co., Ltd. sued Blizzard Entertainment Limited for copyright infringement claiming 100 million yuan in damages. The lawsuit alleged the Chinese edition of "World of Warcraft" reproduced a number of Chinese typefaces made by Founder Electronics without permission.
On July 14, 2008, the United States District Court for the District of Arizona ruled on the case "MDY Industries, LLC v. Blizzard Entertainment, Inc.". The Court found that MDY was liable for copyright infringement since users of its Glider bot program were breaking the End User License Agreement and Terms of Use for "World of Warcraft". MDY Industries appealed the judgment of the district court, and a judgment was delivered by the Ninth Circuit Court of Appeals on December 14, 2010, in which the summary judgment against MDY for contributory copyright infringement was reversed. Nevertheless, they ruled that the bot violated the DMCA and the case was sent back to the district court for review in light of this decision.
On December 5, 2008, Blizzard issued a cease and desist letter to many administrators of high population "World of Warcraft" private servers (essentially slightly altered hosting servers of the actual "World of Warcraft" game, that players do not have to pay for). Blizzard used the Digital Millennium Copyright Act to influence many private servers to fully shut down and cease to exist.
Shortly after Valve filed its trademark for "Dota" to secure the franchising rights for "Dota 2", DotA-Allstars, LLC, run by former contributors to the games's predecessor, "Defense of the Ancients", filed an opposing trademark in August 2010. DotA All-Stars, LLC was sold to Blizzard Entertainment in 2011. After the opposition was over-ruled in Valve's favor, Blizzard filed an opposition against Valve in November 2011, citing their license agreement with developers, as well as their ownership of DotA-Allstars, LLC. Blizzard conceded their case in May 2012, however, giving Valve undisputed commercial rights to "Dota" name, while Blizzard would rename their "StarCraft II: Heart of the Swarm" mod "Blizzard All-Stars", which would become the stand-alone game, "Heroes of the Storm".
Over the years, some former Blizzard employees have moved on and established gaming companies of their own:

</doc>
<doc id="4878" url="https://en.wikipedia.org/wiki?curid=4878" title="Robert Bellarmine">
Robert Bellarmine

Robert Bellarmine (; 4 October 1542 – 17 September 1621) was an Italian Jesuit and a cardinal of the Catholic Church. He was canonized a saint in 1930 and named Doctor of the Church, one of only 36. He was one of the most important figures in the Counter-Reformation.
Bellarmine was a professor of theology and later rector of the Roman College, and in 1602 became Archbishop of Capua. He supported the reform decrees of the Council of Trent. He is also widely remembered for his role in the Giordano Bruno affair, the Galileo affair, and the trial of Friar Fulgenzio Manfredi.
Bellarmine was born in Montepulciano, the son of noble, albeit impoverished, parents, Vincenzo Bellarmino and his wife Cinzia Cervini, who was the sister of Pope Marcellus II. As a boy he knew Virgil by heart and composed a number of poems in Italian and Latin. One of his hymns, on Mary Magdalene, is included in the Roman Breviary.
He entered the Roman Jesuit novitiate in 1560, remaining in Rome for three years. He then went to a Jesuit house at Mondovì, in Piedmont, where he learned Greek. While at Mondovì, he came to the attention of Francesco Adorno, the local Jesuit Provincial Superior, who sent him to the University of Padua.
Bellarmine's systematic studies of theology began at Padua in 1567 and 1568, where his teachers were adherents of Thomism. In 1569 he was sent to finish his studies at the University of Leuven in Flanders. There he was ordained and obtained a reputation both as a professor and as a preacher. He was the first Jesuit to teach at the university, where the subject of his course was the "Summa Theologica" of Thomas Aquinas. His residency in Leuven lasted seven years. In poor health, in 1576 he made a journey to Italy. Here he remained, commissioned by Pope Gregory XIII to lecture on polemical theology in the new Roman College, now known as the Pontifical Gregorian University. Later, he would promote the cause of the beatification of Aloysius Gonzaga, who had been a student at the college during Bellarmine's tenure.
Until 1589, Bellarmine was occupied as professor of theology. After the murder in that year of Henry III of France, Pope Sixtus V sent Enrico Caetani as legate to Paris to negotiate with the Catholic League of France, and chose Bellarmine to accompany him as theologian. He was in the city during its siege by Henry of Navarre.
The next pope, Clement VIII, said of him, "the Church of God had not his equal in learning". Bellarmine was made rector of the Roman College in 1592, examiner of bishops in 1598, and cardinal in 1599. Immediately after his appointment as Cardinal, Pope Clement made him a Cardinal Inquisitor, in which capacity he served as one of the judges at the trial of Giordano Bruno, and concurred in the decision which condemned Bruno to be burned at the stake as a heretic.
Upon the death of Pope Sixtus V in 1590, the Count of Olivares wrote to King Philip III of Spain, "Bellarmine ... would not do for a Pope, for he is mindful only of the interests of the Church and is unresponsive to the reasons of princes." In 1602 he was made archbishop of Capua. He had written against pluralism and non-residence of bishops within their dioceses. As bishop he put into effect the reforming decrees of the Council of Trent. He received some votes in the 1605 conclaves which elected Pope Leo XI, Pope Paul V, and in 1621 when Pope Gregory XV was elected, but him being a Jesuit stood against him in the judgment of many of the cardinals.
Thomas Hobbes saw Bellarmine in Rome at a service on All Saints Day (1 November) 1614 and, exempting him alone from a general castigation of cardinals, described him as "a little lean old man" who lived "more retired".
In 1616, on the orders of Paul V, Bellarmine summoned Galileo, notified him of a forthcoming decree of the Congregation of the Index condemning the Copernican doctrine of the mobility of the Earth and the immobility of the Sun, and ordered him to abandon it. Galileo agreed to do so.
When Galileo later complained of rumours to the effect that he had been forced to abjure and do penance, Bellarmine wrote out a certificate denying the rumours, stating that Galileo had merely been notified of the decree and informed that, as a consequence of it, the Copernican doctrine could not be "defended or held". Unlike the previously mentioned formal injunction (see earlier footnote), this certificate would have allowed Galileo to continue using and teaching the mathematical content of Copernicus's theory as a purely theoretical device for predicting the apparent motions of the planets. According to some of his letters, Cardinal Bellarmine believed that a demonstration for heliocentrism could not be found because it would contradict the unanimous consent of the Fathers' scriptural exegesis, to which the Council of Trent, in 1546, defined all Catholics must adhere. In other passages, Bellarmine argued that he did not support the heliocentric model for the lack of evidence of the time ("I will not believe that there is such a demonstration, until it is shown me").
Bellarmine wrote to heliocentrist Paolo Antonio Foscarini in 1615:
The Council [of Trent] prohibits interpreting Scripture against the common consensus of the Holy Fathers; and if Your Paternity wants to read not only the Holy Fathers, but also the modern commentaries on Genesis, the Psalms, Ecclesiastes, and Joshua, you will find all agreeing in the literal interpretation that the sun is in heaven and turns around the earth with great speed, and that the earth is very far from heaven and sits motionless at the center of the world.andI say that if there were a true demonstration that the sun is at the center of the world and the earth in the third heaven, and that the sun does not circle the earth but the earth circles the sun, then one would have to proceed with great care in explaining the Scriptures that appear contrary, and say rather that we do not understand them, than that what is demonstrated is false. But I will not believe that there is such a demonstration, until it is shown me. Nor is it the same to demonstrate that by supposing the sun to be at the center and the earth in heaven one can save the appearances, and to demonstrate that in truth the sun is at the center and the earth in heaven; for I believe the first demonstration may be available, but I have very great doubts about the second, and in case of doubt one must not abandon the Holy Scripture as interpreted by the Holy Fathers.
In 1633, nearly twelve years after Bellarmine's death, Galileo was again called before the Inquisition in this matter. Galileo produced Bellarmine's certificate for his defense at the trial.
In his article on Bellarmine in the "Complete Dictionary of Scientific Biography", Ernan McMullin cites Pierre Duhem and Karl Popper as prominent adherents to an "often repeated" view that "in one respect, at least, Bellarmine had shown himself a better scientist than Galileo", insofar as he supposedly denied that a "strict proof" of the Earth's motion could be possible on the grounds that an astronomical theory merely 'saves the appearances' without necessarily revealing what 'really happens. McMullin himself emphatically rejects that view as untenable.
Bellarmine retired to the Jesuit college of St. Andrew in Rome, where he died on 17 September 1621, aged 78.
Bellarmine's books bear the stamp of their period; the effort for literary elegance (so-called "maraviglia") had given place to a desire to pile up as much material as possible, to embrace the whole field of human knowledge, and incorporate it into theology. His controversial works provoked many replies, and were studied for some decades after his death. At Leuven he made extensive studies in the Church Fathers and scholastic theologians, which gave him the material for his book "De scriptoribus ecclesiasticis" (Rome, 1613). It was later revised and enlarged by Sirmond, Labbeus, and Casimir Oudin. Bellarmine wrote the preface to the new Sixto-Clementine Vulgate. St. Robert Bellarmine also prepared for posterity his very own commentary on each of the Psalms. An English translation from the Latin was published in 1866.
From his research grew "Disputationes de controversiis christianae fidei" (also called "Controversiae"), first published at Ingolstadt in 1581–1593. This major work was the earliest attempt to systematize the various religious disputes between Catholics and Protestants. Bellarmine calmly and fairly reviewed the issues and devoted eleven years to it while at the Roman College. In August 1590 Pope Sixtus V decided to place the first volume of the "Disputationes" on the Index because Bellarmine argued in it that the Pope is not the temporal ruler of the whole world and that temporal rulers do not derive their authority to rule from God but from the consent of the governed. However Sixtus died before the revised Index was published, and the next Pope, Urban VII, removed the book from the Index during his brief twelve-day reign.In 1597 and 1598 he published a "Catechism" in two versions ( and ) which has been translated to 50 languages, becoming one of the greatest bestsellers and the official teaching of the Catholic Church in the 17th to 19th centuries.
Under Pope Paul V (reigned 1605–1621), a major conflict arose between Venice and the Papacy. Paolo Sarpi, as spokesman for the Republic of Venice, protested against the papal interdict, and reasserted the principles of the Council of Constance and of the Council of Basel, denying the pope's authority in secular matters. Bellarmine wrote three rejoinders to the Venetian theologians, and may have warned Sarpi of an impending murderous attack, when in September 1607, an unfrocked friar and brigand by the name of Rotilio Orlandini planned to kill Sarpi for the sum of 8,000 crowns. Orlandini's plot was discovered, and when he and his accomplices crossed from Papal into Venetian territory they were arrested.
Bellarmine also became involved in controversy with King James I of England. From a point of principle for English Catholics, this debate drew in figures from much of Western Europe. It raised the profile of both protagonists, King James as a champion of his own restricted Calvinist Protestantism, and Bellarmine for Tridentine Catholicism.
During his retirement, he wrote several short books intended to help ordinary people in their spiritual life: "De ascensione mentis in Deum per scalas rerum creatorum opusculum" ("The Mind's Ascent to God –" 1614) which was translated into English as "Jacob's Ladder" (1638) without acknowledgement by Henry Isaacson, "The Art of Dying Well" (1619) (in Latin, English translation under this title by Edward Coffin), and "The Seven Words on the Cross".
Bellarmine was canonized by Pope Pius XI in 1930; the following year he was declared a Doctor of the Church. His remains, in a cardinal's red robes, are displayed behind glass under a side altar in the Church of Saint Ignatius, the chapel of the Roman College, next to the body of his student, Aloysius Gonzaga, as he himself had wished. In the General Roman Calendar Saint Robert Bellarmine's feast day is on 17 September, the day of his death; but some continue to use pre-1969 calendars, in which for 37 years his feast day was on 13 May. The rank assigned to his feast has been "double" (1932–1959), "third-class feast" (1960–1968), and since the 1969 revision "memorial".
Bellarmine University in Louisville, Kentucky, is named after him, as are Bellarmine College Preparatory in San Jose, California, Saint Robert Bellarmine Parish in Chicago, Illinois, and Bellarmine Preparatory School in Tacoma, Washington. Saint Joseph's University, Fairfield University, and Seattle University all have a Bellarmine Hall dedicated to the saint. The Jesuit Sogang University in Seoul, South Korea, has a Bellarmino Dormitory, named after his Italian name. The Ateneo de Manila University, another Jesuit institution in the Philippines, also has a Bellarmine Hall, which serves as a classroom building and home of the University Press.
St. Robert Bellarmine Church in Warrington, Pennsylvania, in the Archdiocese of Philadelphia, is also the site of St. Joseph's/St. Robert's School, a K-8 grade private, Roman-Catholic school. St. Robert Bellarmine Church in Omaha, Nebraska in the Archdiocese of Omaha is also the site of St. Robert Bellarmine School, a k-8 private, Roman-Catholic grade school.
Works of Bellarmine
Works about Bellarmine

</doc>
<doc id="4880" url="https://en.wikipedia.org/wiki?curid=4880" title="Bildungsroman">
Bildungsroman

In literary criticism, a Bildungsroman (, plural Bildungsromane, ) is a literary genre that focuses on the psychological and moral growth of the protagonist from youth to adulthood (coming of age), in which character change is important. The term comes from the German words "Bildung" ("education") and "Roman" ("novel").
The term was coined in 1819 by philologist Karl Morgenstern in his university lectures, and was later famously reprised by Wilhelm Dilthey, who legitimized it in 1870 and popularized it in 1905. The genre is further characterized by a number of formal, topical, and thematic features. The term "coming-of-age novel" is sometimes used interchangeably with "Bildungsroman", but its use is usually wider and less technical.
The birth of the Bildungsroman is normally dated to the publication of "Wilhelm Meister's Apprenticeship" by Johann Wolfgang Goethe in 1795–96, or, sometimes, to Christoph Martin Wieland's "Geschichte des Agathon" of 1767. Although the "Bildungsroman" arose in Germany, it has had extensive influence first in Europe and later throughout the world. Thomas Carlyle translated Goethe's novel into English, and after its publication in 1824, many British authors wrote novels inspired by it. In the 20th century, it spread to Germany, Britain, France, and several other countries around the globe.
The genre translates fairly directly into the cinematic form, the coming-of-age film.
A "Bildungsroman" relates the growing up or "coming of age" of a sensitive person who goes in search of answers to life's questions with the expectation that these will result in gaining experience of the world. The genre evolved from folklore tales of a dunce or youngest child going out in the world to seek his fortune. Usually in the beginning of the story there is an emotional loss which makes the protagonist leave on his or her journey. In a "Bildungsroman", the goal is maturity, and the protagonist achieves it gradually and with difficulty. The genre often features a main conflict between the main character and society. Typically, the values of society are gradually accepted by the protagonist and he or she is ultimately accepted into society—the protagonist's mistakes and disappointments are over. In some works, the protagonist is able to reach out and help others after having achieved maturity.
Franco Moretti "argues that the main conflict in the "Bildungsroman" is the myth of modernity with its overvaluation of youth and progress as it clashes with the static teleological vision of happiness and reconciliation found in the endings of Goethe's "Wilhelm Meister" and even Jane Austen's "Pride and Prejudice"".
There are many variations and subgenres of "Bildungsroman" that focus on the growth of an individual. An "Entwicklungsroman" ("development novel") is a story of general growth rather than self-cultivation. An "Erziehungsroman" ("education novel") focuses on training and formal schooling, while a "Künstlerroman" ("artist novel") is about the development of an artist and shows a growth of the self. Furthermore, some memoirs and published journals can be regarded as "Bildungsroman" although being predominantly factual (e.g. "The Dharma Bums" by Jack Kerouac or" The Motorcycle Diaries" by Ernesto "Che" Guevara). The term is also more loosely used to describe coming-of-age films and related works in other genres.

</doc>
<doc id="4881" url="https://en.wikipedia.org/wiki?curid=4881" title="Bachelor">
Bachelor

A bachelor is a man who is not and has never been married.
A bachelor is first attested as the 12th-century "bacheler", a knight bachelor, a knight too young or poor to gather vassals under his own banner. The Old French ' presumably derives from Provençal ' and Italian ', but the ultimate source of the word is uncertain. The proposed Medieval Latin *' ("vassal", "field hand") is only attested late enough that it may have derived from the vernacular languages, rather than from the southern French and northern Spanish Latin '. Alternatively, it has been derived from Latin ' ("a stick"), in reference to the wooden sticks used by knights in training.
From the 14th century, the term was also used for a junior member of a guild (otherwise known as "yeomen") or university and then for low-level ecclesiastics, as young monks and recently appointed canons. As an inferior grade of scholarship, it came to refer to one holding a "bachelor's degree". This sense of ' or ' is first attested at the University of Paris in the 13th century in the system of degrees established under the auspices of Pope Gregory IX as applied to scholars still '. There were two classes of ': the ', theological candidates passed for admission to the divinity course, and the ', who had completed the course and were entitled to proceed to the higher degrees.
In the Victorian era, the term eligible bachelor was used in the context of upper class matchmaking, denoting a young man who was not only unmarried and eligible for marriage, but also considered "eligible" in financial and social terms for the prospective bride under discussion. Also in the Victorian era, the term "confirmed bachelor" denoted a man who was resolute to remain unmarried.
By the later 19th century, the term "bachelor" had acquired the general sense of "unmarried man". The expression bachelor party is recorded 1882. In 1895, a feminine equivalent "bachelor-girl" was coined, replaced in US English by "bachelorette" by the mid-1930s. After World War II, this terminology came to be seen as antiquated and has been mostly replaced by the gender-neutral term "single" (first recorded 1964). In England and Wales, the term "bachelor" remained the official term used for the purpose of marriage registration until 2005, when it was abolished in favor of "single."
In certain Gulf Arab countries, "bachelor" can refer to men who are single as well as immigrant men married to a spouse residing in their country of origin (due to the high added cost of sponsoring a spouse onsite), and a colloquial term "executive bachelor" is also used in rental and sharing accommodation advertisements to indicate availability to white-collar bachelors in particular.
Bachelors have been subject to penal laws in many countries, most notably in Ancient Sparta and Rome. At Sparta, men unmarried after a certain age were subject to various penalties (, "atimía"): they were forbidden to watch women's gymnastics; during the winter, they were made to march naked through the agora singing a song about their dishonor; and they were not provided with the traditional respect due to the elderly. Some Athenian laws were similar. Bachelors in Rome fell under the Lex Julia of 18  and the Lex Papia Poppaea of  9: these lay heavy fines on unmarried or childless people while providing certain privileges to those with several children. 
In Britain, taxes occasionally fell heavier on bachelors than other persons: examples include 6 & 7 Will. III, the 1785 Tax on Servants, and the 1798 Income Tax. Over time, some punishments developed into no more than a teasing game. In some parts of Germany, for instance, men who were still unmarried by their 30th birthday were made to sweep the stairs of the town hall until kissed by a "virgin".
Listed chronologically by date of birth.

</doc>
<doc id="4882" url="https://en.wikipedia.org/wiki?curid=4882" title="Background radiation">
Background radiation

Background radiation is a measure of the level of ionizing radiation present in the environment at a particular location which is not due to deliberate introduction of radiation sources.
Background radiation originates from a variety of sources, both natural and artificial. These include both cosmic radiation and environmental radioactivity from naturally occurring radioactive materials (such as radon and radium), as well as man-made medical X-rays, fallout from nuclear weapons testing and nuclear accidents.
Background radiation is defined by the International Atomic Energy Agency as "Dose or dose rate (or an observed measure related to the dose or dose rate) attributable to all sources other than the one(s) specified. So a distinction is made between dose which is already in a location, which is defined here as being "background", and the dose due to a deliberately introduced and specified source. This is important where radiation measurements are taken of a specified radiation source, where the existing background may affect this measurement. An example would be measurement of radioactive contamination in a gamma radiation background, which could increase the total reading above that expected from the contamination alone.
However, if no radiation source is specified as being of concern, then the total radiation dose measurement at a location is generally called the background radiation, and this is usually the case where an ambient dose rate is measured for environmental purposes.
Background radiation varies with location and time, and the following table gives examples:
Radioactive material is found throughout nature. Detectable amounts occur naturally in soil, rocks, water, air, and vegetation, from which it is inhaled and ingested into the body. In addition to this "internal exposure", humans also receive "external exposure" from radioactive materials that remain outside the body and from cosmic radiation from space. The worldwide average natural dose to humans is about per year. This is four times the worldwide average artificial radiation exposure, which in 2008 amounted to about per year. In some developed countries, like the US and Japan, artificial exposure is, on average, greater than the natural exposure, due to greater access to medical imaging. In Europe, average natural background exposure by country ranges from under annually in the United Kingdom to more than annually for some groups of people in Finland.
The International Atomic Energy Agency states:
Terrestrial radiation, for the purpose of the table above, only includes sources that remain external to the body. The major radionuclides of concern are potassium, uranium and thorium and their decay products, some of which, like radium and radon are intensely radioactive but occur in low concentrations. Most of these sources have been decreasing, due to radioactive decay since the formation of the Earth, because there is no significant amount currently transported to the Earth. Thus, the present activity on earth from uranium-238 is only half as much as it originally was because of its 4.5 billion year half-life, and potassium-40 (half-life 1.25 billion years) is only at about 8% of original activity. But during the time that humans have existed the amount of radiation has decreased very little.
Many shorter half-life (and thus more intensely radioactive) isotopes have not decayed out of the terrestrial environment because of their on-going natural production. Examples of these are radium-226 (decay product of thorium-230 in decay chain of uranium-238) and radon-222 (a decay product of radium-226 in said chain).
Thorium and uranium (and their daughters) primarily undergo alpha and beta decay, and aren't easily detectable. However, many of their daughter products are strong gamma emitters. Thorium-232 is detectable via a 239 keV peak from lead-212, 511, 583 and 2614 keV from thallium-208, and 911 and 969 keV from actinium-228. Uranium-238 manifests as 609, 1120, and 1764 keV peaks of bismuth-214 ("cf." the same peak for atmospheric radon). Potassium-40 is detectable directly via its 1461 keV gamma peak.
The level over the sea and other large bodies of water tends to be about a tenth of the terrestrial background. Conversely, coastal areas (and areas by the side of fresh water) may have an additional contribution from dispersed sediment.
The biggest source of natural background radiation is airborne radon, a radioactive gas that emanates from the ground. Radon and its isotopes, parent radionuclides, and decay products all contribute to an average inhaled dose of 1.26 mSv/a (millisievert per year). Radon is unevenly distributed and varies with weather, such that much higher doses apply to many areas of the world, where it represents a significant health hazard. Concentrations over 500 times the world average have been found inside buildings in Scandinavia, the United States, Iran, and the Czech Republic. Radon is a decay product of uranium, which is relatively common in the Earth's crust, but more concentrated in ore-bearing rocks scattered around the world. Radon seeps out of these ores into the atmosphere or into ground water or infiltrates into buildings. It can be inhaled into the lungs, along with its decay products, where they will reside for a period of time after exposure.
Although radon is naturally occurring, exposure can be enhanced or diminished by human activity, notably house construction. A poorly sealed basement in an otherwise well insulated house can result in the accumulation of radon within the dwelling, exposing its residents to high concentrations. The widespread construction of well insulated and sealed homes in the northern industrialized world has led to radon becoming the primary source of background radiation in some localities in northern North America and Europe. Basement sealing and suction ventilation reduce exposure. Some building materials, for example lightweight concrete with alum shale, phosphogypsum and Italian tuff, may emanate radon if they contain radium and are porous to gas.
Radiation exposure from radon is indirect. Radon has a short half-life (4 days) and decays into other solid particulate radium-series radioactive nuclides. These radioactive particles are inhaled and remain lodged in the lungs, causing continued exposure. Radon is thus assumed to be the second leading cause of lung cancer after smoking, and accounts for 15,000 to 22,000 cancer deaths per year in the US alone. However, the discussion about the opposite experimental results is still going on.
About 100,000 Bq/m of radon was found in Stanley Watras's basement in 1984. He and his neighbours in Boyertown, Pennsylvania, United States may hold the record for the most radioactive dwellings in the world. International radiation protection organizations estimate that a committed dose may be calculated by multiplying the equilibrium equivalent concentration (EEC) of radon by a factor of 8 to 9 and the EEC of thoron by a factor of 40 .
Most of the atmospheric background is caused by radon and its decay products. The gamma spectrum shows prominent peaks at 609, 1120, and 1764 keV, belonging to bismuth-214, a radon decay product. The atmospheric background varies greatly with wind direction and meteorological conditions. Radon also can be released from the ground in bursts and then form "radon clouds" capable of traveling tens of kilometers.
The Earth and all living things on it are constantly bombarded by radiation from outer space. This radiation primarily consists of positively charged ions from protons to iron and larger nuclei derived from outside the Solar System. This radiation interacts with atoms in the atmosphere to create an air shower of secondary radiation, including X-rays, muons, protons, alpha particles, pions, electrons, and neutrons. The immediate dose from cosmic radiation is largely from muons, neutrons, and electrons, and this dose varies in different parts of the world based largely on the geomagnetic field and altitude. For example, the city of Denver in the United States (at 1650 meters elevation) receives a cosmic ray dose roughly twice that of a location at sea level. This radiation is much more intense in the upper troposphere, around 10 km altitude, and is thus of particular concern for airline crews and frequent passengers, who spend many hours per year in this environment. During their flights airline crews typically get an additional occupational dose between per year and 2.19 mSv/year, according to various studies.
Similarly, cosmic rays cause higher background exposure in astronauts than in humans on the surface of Earth. Astronauts in low orbits, such as in the International Space Station or the Space Shuttle, are partially shielded by the magnetic field of the Earth, but also suffer from the Van Allen radiation belt which accumulates cosmic rays and results from the Earth's magnetic field. Outside low Earth orbit, as experienced by the Apollo astronauts who traveled to the Moon, this background radiation is much more intense, and represents a considerable obstacle to potential future long term human exploration of the moon or Mars.
Cosmic rays also cause elemental transmutation in the atmosphere, in which secondary radiation generated by the cosmic rays combines with atomic nuclei in the atmosphere to generate different nuclides. Many so-called cosmogenic nuclides can be produced, but probably the most notable is carbon-14, which is produced by interactions with nitrogen atoms. These cosmogenic nuclides eventually reach the Earth's surface and can be incorporated into living organisms. The production of these nuclides varies slightly with short-term variations in solar cosmic ray flux, but is considered practically constant over long scales of thousands to millions of years. The constant production, incorporation into organisms and relatively short half-life of carbon-14 are the principles used in radiocarbon dating of ancient biological materials, such as wooden artifacts or human remains.
The cosmic radiation at sea level usually manifests as 511 keV gamma rays from annihilation of positrons created by nuclear reactions of high energy particles and gamma rays. At higher altitudes there is also the contribution of continuous bremsstrahlung spectrum.
Two of the essential elements that make up the human body, namely potassium and carbon, have radioactive isotopes that add significantly to our background radiation dose. An average human contains about 17 milligrams of potassium-40 (K) and about 24 nanograms (10 g) of carbon-14 (C), (half-life 5,730 years). Excluding internal contamination by external radioactive material, these two are the largest components of internal radiation exposure from biologically functional components of the human body. About 4,000 nuclei of K decay per second, and a similar number of C. The energy of beta particles produced by K is about 10 times that from the beta particles from C decay.
C is present in the human body at a level of about 3700 Bq (0.1 μCi) with a biological half-life of 40 days. This means there are about 3700 beta particles per second produced by the decay of C. However, a C atom is in the genetic information of about half the cells, while potassium is not a component of DNA. The decay of a C atom inside DNA in one person happens about 50 times per second, changing a carbon atom to one of nitrogen.
The global average internal dose from radionuclides other than radon and its decay products is 0.29 mSv/a, of which 0.17 mSv/a comes from K, 0.12 mSv/a comes from the uranium and thorium series, and 12 μSv/a comes from C.
Some areas have greater dosage than the country-wide averages. In the world in general, exceptionally high natural background locales include Ramsar in Iran, Guarapari in Brazil, Karunagappalli in India, Arkaroola in Australia and Yangjiang in China.
The highest level of purely natural radiation ever recorded on the Earth's surface was 90 µGy/h on a Brazilian black beach ("areia preta" in Portuguese) composed of monazite. This rate would convert to 0.8 Gy/a for year-round continuous exposure, but in fact the levels vary seasonally and are much lower in the nearest residences. The record measurement has not been duplicated and is omitted from UNSCEAR's latest reports. Nearby tourist beaches in Guarapari and Cumuruxatiba were later evaluated at 14 and 15 µGy/h. Note that the values quoted here are in Grays. To convert to Sieverts (Sv) a radiation weighting factor is required; these weighting factors vary from 1 (beta & gamma) to 20 (alpha particles).
The highest background radiation in an inhabited area is found in Ramsar, primarily due to the use of local naturally radioactive limestone as a building material. The 1000 most exposed residents receive an average external effective radiation dose of per year, six times the ICRP recommended limit for exposure to the public from artificial sources. They additionally receive a substantial internal dose from radon. Record radiation levels were found in a house where the effective dose due to ambient radiation fields was per year, and the internal committed dose from radon was per year. This unique case is over 80 times higher than the world average natural human exposure to radiation.
Epidemiological studies are underway to identify health effects associated with the high radiation levels in Ramsar. It is much too early to draw unambiguous statistically significant conclusions. While so far support for beneficial effects of chronic radiation (like longer lifespan) has been observed in few places only, a protective and adaptive effect is suggested by at least one study whose authors nonetheless caution that data from Ramsar are not yet sufficiently strong to relax existing regulatory dose limits. However, the recent statistical analyses discussed that there is no correlation between the risk of negative health effects and elevated level of natural background radiation.
Background radiation doses in the immediate vicinity of particles of high atomic number materials, within the human body, have a small enhancement due to the photoelectric effect.
Most of the natural neutron background is a product of cosmic rays interacting with the atmosphere. The neutron energy peaks at around 1 MeV and rapidly drops above. At sea level, the production of neutrons is about 20 neutrons per second per kilogram of material interacting with the cosmic rays (or, about 100–300 neutrons per square meter per second). The flux is dependent on geomagnetic latitude, with a maximum near the magnetic poles. At solar minimums, due to lower solar magnetic field shielding, the flux is about twice as high vs the solar maximum. It also dramatically increases during solar flares. In the vicinity of larger heavier objects, e.g. buildings or ships, the neutron flux measures higher; this is known as "cosmic ray induced neutron signature", or "ship effect" as it was first detected with ships at sea.
Frequent above-ground nuclear explosions between the 1940s and 1960s scattered a substantial amount of radioactive contamination. Some of this contamination is local, rendering the immediate surroundings highly radioactive, while some of it is carried longer distances as nuclear fallout; some of this material is dispersed worldwide. The increase in background radiation due to these tests peaked in 1963 at about 0.15 mSv per year worldwide, or about 7% of average background dose from all sources. The Limited Test Ban Treaty of 1963 prohibited above-ground tests, thus by the year 2000 the worldwide dose from these tests has decreased to only 0.005 mSv per year.
The International Commission on Radiological Protection recommends limiting occupational radiation exposure to 50 mSv (5 rem) per year, and 100 mSv (10 rem) in 5 years.
However, background radiation for occupational doses includes radiation that is not measured by radiation dose instruments in potential occupational exposure conditions. This includes both offsite "natural background radiation" and any medical radiation doses. This value is not typically measured or known from surveys, such that variations in the total dose to individual workers is not known. This can be a significant confounding factor in assessing radiation exposure effects in a population of workers who may have significantly different natural background and medical radiation doses. This is most significant when the occupational doses are very low.
At an IAEA conference in 2002, it was recommended that occupational doses below 1–2 mSv per year do not warrant regulatory scrutiny.
Under normal circumstances, nuclear reactors release small amounts of radioactive gases, which cause small radiation exposures to the public. Events classified on the International Nuclear Event Scale as incidents typically do not release any additional radioactive substances into the environment. Large releases of radioactivity from nuclear reactors are extremely rare. To the present day, there were two major "civilian" accidents – the Chernobyl accident and the Fukushima I nuclear accidents – which caused substantial contamination. The Chernobyl accident was the only one to cause immediate deaths.
Total doses from the Chernobyl accident ranged from 10 to 50 mSv over 20 years for the inhabitants of the affected areas, with most of the dose received in the first years after the disaster, and over 100 mSv for liquidators. There were 28 deaths from acute radiation syndrome.
Total doses from the Fukushima I accidents were between 1 and 15 mSv for the inhabitants of the affected areas. Thyroid doses for children were below 50 mSv. 167 cleanup workers received doses above 100 mSv, with 6 of them receiving more than 250 mSv (the Japanese exposure limit for emergency response workers).
The average dose from the Three Mile Island accident was 0.01 mSv.
Non-civilian: In addition to the civilian accidents described above, several accidents at early nuclear weapons facilities – such as the Windscale fire, the contamination of the Techa River by the nuclear waste from the Mayak compound, and the Kyshtym disaster at the same compound – released substantial radioactivity into the environment. The Windscale fire resulted in thyroid doses of 5–20 mSv for adults and 10–60 mSv for children. The doses from the accidents at Mayak are unknown.
The Nuclear Regulatory Commission, the United States Environmental Protection Agency, and other U.S. and international agencies, require that licensees limit radiation exposure to individual members of the public to 1 mSv (100 mrem) per year.
Coal plants emit radiation in the form of radioactive fly ash which is inhaled and ingested by neighbours, and incorporated into crops. A 1978 paper from Oak Ridge National Laboratory estimated that coal-fired power plants of that time may contribute a whole-body committed dose of 19 µSv/a to their immediate neighbours in a radius of 500 m. The United Nations Scientific Committee on the Effects of Atomic Radiation's 1988 report estimated the committed dose 1 km away to be 20 µSv/a for older plants or 1 µSv/a for newer plants with improved fly ash capture, but was unable to confirm these numbers by test. When coal is burned, uranium, thorium and all the uranium daughters accumulated by disintegration — radium, radon, polonium — are released. Radioactive materials previously buried underground in coal deposits are released as fly ash or, if fly ash is captured, may be incorporated into concrete manufactured with fly ash.
The global average human exposure to artificial radiation is 0.6 mSv/a, primarily from medical imaging. This medical component can range much higher, with an average of 3 mSv per year across the USA population. Other human contributors include smoking, air travel, radioactive building materials, historical nuclear weapons testing, nuclear power accidents and nuclear industry operation.
A typical chest x-ray delivers 20 µSv (2 mrem) of effective dose. A dental x-ray delivers a dose of 5 to 10 µSv. A CT scan delivers an effective dose to the whole body ranging from 1 to 20 mSv (100 to 2000 mrem). The average American receives about 3 mSv of diagnostic medical dose per year; countries with the lowest levels of health care receive almost none. Radiation treatment for various diseases also accounts for some dose, both in individuals and in those around them.
Cigarettes contain polonium-210, originating from the decay products of radon, which stick to tobacco leaves. Heavy smoking results in a radiation dose of 160 mSv/year to localized spots at the bifurcations of segmental bronchi in the lungs from the decay of polonium-210. This dose is not readily comparable to the radiation protection limits, since the latter deal with whole body doses, while the dose from smoking is delivered to a very small portion of the body.
In a radiation metrology laboratory, background radiation refers to the measured value from any incidental sources that affect an instrument when a specific radiation source sample is being measured. This background contribution, which is established as a stable value by multiple measurements, usually before and after sample measurement, is subtracted from the rate measured when the sample is being measured.
This is in accordance with the International Atomic Energy Agency definition of background as being "Dose or dose rate (or an observed measure related to the dose or dose rate) attributable to all sources other than the one(s) specified.
The same issue occurs with radiation protection instruments, where a reading from an instrument may be affected by the background radiation. An example of this is a scintillation detector used for surface contamination monitoring. In an elevated gamma background the scintillator material will be affected by the background gamma, which will add to the reading obtained from any contamination which is being monitored. In extreme cases it will make the instrument unusable as the background swamps the lower level of radiation from the contamination. In such instruments the background can be continually monitored in the "Ready" state, and subtracted from any reading obtained when being used in "Measuring" mode.
Regular Radiation measurement is carried out at multiple levels. Government agencies compile radiation readings as part of environmental monitoring mandates, often making the readings available to the public and sometimes in near-real-time. Collaborative groups and private individuals may also make real-time readings available to the public. Instruments used for radiation measurement include the Geiger–Müller tube and the Scintillation detector. The former is usually more compact and affordable and reacts to several radiation types, while the latter is more complex and can detect specific radiation energies and types. Readings indicate radiation levels from all sources including background, and real-time readings are in general unvalidated, but correlation between independent detectors increases confidence in measured levels.
List of near-real-time government radiation measurement sites, employing multiple instrument types:
List of international near-real-time collaborative/private measurement sites, employing primarily Geiger-Muller detectors:

</doc>
<doc id="4884" url="https://en.wikipedia.org/wiki?curid=4884" title="Balmoral">
Balmoral

Balmoral Castle is a residence of Queen Elizabeth II in Aberdeenshire, Scotland.
Balmoral may also refer to:

</doc>
<doc id="4885" url="https://en.wikipedia.org/wiki?curid=4885" title="Bannock">
Bannock

Bannock may mean:

</doc>
<doc id="4886" url="https://en.wikipedia.org/wiki?curid=4886" title="Banquo">
Banquo

Lord Banquo , the Thane of Lochaber, is a character in William Shakespeare's 1606 play "Macbeth". In the play, he is at first an ally to Macbeth (both are generals in the King's army) and they meet the Three Witches together. After prophesying that Macbeth will become king, the witches tell Banquo that he will not be king himself, but that his descendants will be. Later, Macbeth in his lust for power sees Banquo as a threat and has him murdered by three hired assassins; Banquo's son, Fleance, escapes. Banquo's ghost returns in a later scene, causing Macbeth to react with alarm during a public feast.
Shakespeare borrowed the character Banquo from "Holinshed's Chronicles", a history of Britain published by Raphael Holinshed in 1587. In "Chronicles" Banquo is an accomplice to Macbeth in the murder of the king, rather than a loyal subject of the king who is seen as an enemy by Macbeth. Shakespeare may have changed this aspect of his character to please King James, who was thought at the time to be a descendant of the real Banquo. Critics often interpret Banquo's role in the play as being a foil to Macbeth, resisting evil whereas Macbeth embraces it. Sometimes, however, his motives are unclear, and some critics question his purity. He does nothing to accuse Macbeth of murdering the king, even though he has reason to believe Macbeth is responsible.
Shakespeare often used Raphael Holinshed's "Chronicles of England, Scotland, and Ireland"—commonly known as "Holinshed's Chronicles"—as a source for his plays, and in "Macbeth" he borrows from several of the tales in that work. Holinshed portrays Banquo as an historical figure: he is an accomplice in Mac Bethad mac Findlaích's (Macbeth's) murder of Donnchad mac Crínáin (King Duncan) and plays an important part in ensuring that Macbeth, not Máel Coluim mac Donnchada (Malcolm), takes the throne in the coup that follows. Holinshed in turn used an earlier work, the "Scotorum Historiae" (1526–7) by Hector Boece, as his source. Boece's work is the first known record of Banquo and his son Fleance; and scholars such as David Bevington generally consider them fictional characters invented by Boece. In Shakespeare's day, however, they were considered historical figures of great repute, and the king, James I, based his claim to the throne in part on a descent from Banquo. The House of Stuart was descended from Walter fitz Alan, Steward of Scotland, and he was believed to have been the grandson of Fleance and Gruffydd ap Llywelyn's daughter, Nesta ferch Gruffydd. In reality, Walter fitz Alan was the son of Alan fitz Flaad, a Breton knight.
Unlike his sources, Shakespeare gives Banquo no role in the King's murder, making it a deed committed solely by Macbeth and his wife, Lady Macbeth. Why Shakespeare's Banquo is so different from the character described by Holinshed and Boece is not known, though critics have proposed several possible explanations. First among them is the risk associated with portraying the king's ancestor as a murderer and conspirator in the plot to overthrow a rightful king, as well as the author's desire to flatter a powerful patron. But Shakespeare may also simply have altered Banquo's character because there was no dramatic need for another accomplice to the murder. There was, however, a need to provide a dramatic contrast to Macbeth; a role that many scholars argue is filled by Banquo. Similarly, when Jean de Schelandre wrote about Banquo in his "Stuartide" in 1611, he also changed the character by portraying him as a noble and honourable man—the critic D.W. Maskell describes him as "...Schelandre's paragon of valour and virtue"—probably for reasons similar to Shakespeare's.
Banquo's role in the coup that follows the murder is harder to explain. Banquo's loyalty to Macbeth, rather than Malcolm, after Duncan's death makes him a passive accomplice in the coup: Malcolm, as Prince of Cumberland, is the rightful heir to the throne and Macbeth a usurper. Daniel Amneus argued that "Macbeth" as it survives is a revision of an earlier play, in which Duncan granted Macbeth not only the title of Thane of Cawdor, but the "greater honor" of Prince of Cumberland (i.e. heir to the throne of Scotland). Banquo's silence may be a survival from the posited earlier play, in which Macbeth was the legitimate successor to Duncan.
Banquo is in a third of the play's scenes, as both a human and a ghost. As significant as he is to the plot, he has fewer lines than the relatively insignificant Ross, a Scottish nobleman who survives the play. In the second scene of the play, a wounded soldier describes the manner in which Macbeth, Thane of Glamis, and Banquo, Thane of Lochaber, resisted invading forces, fighting side by side. In the next scene, Banquo and Macbeth, returning from the battle together, encounter the Three Witches, who predict that Macbeth will become Thane of Cawdor, and then king. Banquo, sceptical of the witches, challenges them to predict his own future, and they foretell that Banquo will never himself take the throne, but will beget a line of kings. Banquo remains sceptical after the encounter, wondering aloud if evil can ever speak the truth. He warns Macbeth that evil will offer men a small, hopeful truth only to catch them in a deadly trap.
When Macbeth kills the king and takes the throne, Banquo—the only one aware of this encounter with the witches—reserves judgment for God. He is unsure whether Macbeth committed regicide to gain the throne, but muses in a soliloquy that "I fear / Thou play'dst most foully for 't". He offers his respects to the new King Macbeth and pledges loyalty. Later, worried that Banquo's descendants and not his own will rule Scotland, Macbeth sends two men, and then a Third Murderer, to kill Banquo and his son Fleance. During the melee, Banquo holds off the assailants so that Fleance can escape, but is himself killed. The ghost of Banquo later returns to haunt Macbeth at the banquet in Act Three, Scene Four. A terrified Macbeth sees him, while the apparition is invisible to his guests. He appears again to Macbeth in a vision granted by the Three Witches, wherein Macbeth sees a long line of kings descended from Banquo.
Many scholars see Banquo as a foil and a contrast to Macbeth. Macbeth, for example, eagerly accepts the Three Witches' prophecy as true and seeks to help it along. Banquo, on the other hand, doubts the prophecies and the intentions of these seemingly evil creatures. Whereas Macbeth places his hope in the prediction that he will be king, Banquo argues that evil only offers gifts that lead to destruction. Banquo steadily resists the temptations of evil within the play, praying to heaven for help, while Macbeth seeks darkness, and prays that evil powers will aid him. This is visible in act two; after Banquo sees Duncan to bed, he says: "There's husbandry in heaven, / Their candles are all out". This premonition of the coming darkness in association with Macbeth's murders is repeated just before Banquo is killed: "it will be rain to-night", Banquo tells his son Fleance.
Banquo's status as a contrast to Macbeth makes for some tense moments in the play. In act two, scene one, Banquo meets his son Fleance and asks him to take both his sword and his dagger ("Hold, take my sword ... Take thee that too"). He also explains that he has been having trouble sleeping due to "cursed thoughts that nature / gives way to in repose!" On Macbeth's approach, he demands the sword returned to him quickly. Scholars have interpreted this to mean that Banquo has been dreaming of murdering the king as Macbeth's accomplice to take the throne for his own family, as the Three Witches prophesied to him. In this reading, his good nature is so revolted by these thoughts that he gives his sword and dagger to Fleance to be sure they do not come true, but is so nervous at Macbeth's approach that he demands them back. Other scholars have responded that Banquo's dreams have less to do with killing the king and more to do with Macbeth. They argue that Banquo is merely setting aside his sword for the night. Then, when Macbeth approaches, Banquo, having had dreams about Macbeth's deeds, takes back his sword as a precaution in this case.
Macbeth eventually sees that Banquo can no longer be trusted to aid him in his evil, and considers his friend a threat to his newly acquired throne; thus, he has him murdered. Banquo's ability to live on in different ways is another oppositional force, in this case to Macbeth's impending death. His spirit lives on in Fleance, his son, and in his ghostly presence at the banquet.
When Macbeth returns to the witches later in the play, they show him an apparition of the murdered Banquo, along with eight of his descendants. The scene carries deep significance: King James, on the throne when "Macbeth" was written, was believed to be separated from Banquo by nine generations. What Shakespeare writes here thus amounts to a strong support of James' right to the throne by lineage, and for audiences of Shakespeare's day, a very real fulfilment of the witches' prophecy to Banquo that his sons would take the throne. This apparition is also deeply unsettling to Macbeth, who not only wants the throne for himself, but also desires to father a line of kings.
Banquo's other appearance as a ghost during the banquet scene serves as an indicator of Macbeth's conscience returning to plague his thoughts. Banquo's triumph over death appears symbolically, insofar as he literally takes Macbeth's seat during the feast. Shocked, Macbeth uses words appropriate to the metaphor of usurpation, describing Banquo as "crowned" with wounds. The spirit drains Macbeth's manhood along with the blood from his cheeks; as soon as Banquo's form vanishes, Macbeth announces: "Why, so; being gone, / I am a man again."
Like the vision of Banquo's lineage, the banquet scene has also been the subject of criticism. Critics have questioned whether not one, but perhaps two ghosts appear in this scene: Banquo and Duncan. Scholars arguing that Duncan attends the banquet state that Macbeth's lines to the Ghost could apply equally well to the slain king. "Thou canst not say I did it", for example, can mean that Macbeth is not the man who actually killed Banquo, or it can mean that Duncan, who was asleep when Macbeth killed him, cannot claim to have seen his killer. To add to the confusion, some lines Macbeth directs to the ghost, such as "Thy bones are marrowless", cannot rightly be said of Banquo, who has only recently died.
Scholars debate whether Macbeth's vision of Banquo is real or a hallucination. Macbeth had already seen a hallucination before murdering Duncan: a knife hovering in the air. Several performances of the play have even ignored the stage direction to have the Ghost of Banquo enter at all, heightening the sense that Macbeth is growing mad, since the audience cannot see what he claims to see. Scholars opposing this view claim that while the dagger is unusual, ghosts of murdered victims are more believable, having a basis in the audience's superstitions. Spirits in other Shakespeare plays—notably "Hamlet" and "Midsummer Night's Dream"—exist in ambiguous forms, occasionally even calling into question their own presence.
The concept of a character being confronted at a triumphant feast with a reminder of their downfall is not unique to Shakespeare and may originate from Belshazzar's feast, as portrayed in the Bible. The term 'ghost at the feast' has entered popular culture, and is often used as a metaphor for a subject a person would rather avoid considering, or (considering the general plot of "Macbeth") a reminder of a person's unpleasant past or likely future.
Banquo's role, especially in the banquet ghost scene, has been subject to a variety of interpretations and mediums. Shakespeare's text states: "Enter Ghost of Banquo, and sits in Macbeth's place." Several television versions have altered this slightly, having Banquo appear suddenly in the chair, rather than walking onstage and into it. Special effects and camera tricks also allow producers to make the ghost disappear and reappear, highlighting the fact that "only" Macbeth can see it.
Stage directors, unaided by post-production effects and camera tricks, have used other methods to depict the ghost. In the late 19th century, elaborate productions of the play staged by Henry Irving employed a wide variety of approaches for this task. In 1877 a green silhouette was used to create a ghostlike image; ten years later a trick chair was used to allow an actor to appear in the middle of the scene, and then again from the midst of the audience. In 1895 a shaft of blue light served to indicate the presence of Banquo's spirit. In 1933 a Russian director named Theodore Komisarjevsky staged a modern retelling of the play (Banquo and Macbeth were told of their future through palmistry); he used Macbeth's shadow as the ghost. In 1936, Orson Welles directed the Federal Theatre Project production of the play, with an African-American cast that included Canada Lee in the role of Banquo.
Film adaptations have approached Banquo's character in a variety of ways. Akira Kurosawa's 1957 adaptation "Throne of Blood" makes the character into Capitan Miki (played by Minoru Chiaki), slain by Macbeth's equivalent (Captain Washizu) when his wife explains that she is with child. News of Miki's death does not reach Washizu until after he has seen the ghost in the banquet scene. In Roman Polanski's 1971 adaptation, Banquo is played by acclaimed stage actor Martin Shaw, in a style reminiscent of earlier stage performances. Polanski's version also emphasises Banquo's objection to Macbeth's ascendency by showing him remaining silent as the other thanes around him hail Macbeth as king. In the 1990 film "Men of Respect", a reimagining of "Macbeth" as taking place among a New York Mafia crime family, the character of Banquo is named "Bankie Como" and played by American actor Dennis Farina.

</doc>
<doc id="4887" url="https://en.wikipedia.org/wiki?curid=4887" title="British Army">
British Army

The British Army is the principal land warfare force of the United Kingdom, a part of British Armed Forces, under the Army Act of the Parliament of the United Kingdom. , the British Army comprises just over 79,300 trained regular (full-time) personnel and just over 27,200 trained reserve (part-time) personnel.
The modern British Army traces back to 1707, with an antecedent in the English Army that was created during the Restoration in 1660. The term "British Army" was adopted in 1707 after the Acts of Union between England and Scotland. Members of the British Army swear allegiance to the monarch as their commander-in-chief, but the Bill of Rights of 1689 requires parliamentary consent for the Crown to maintain a peacetime standing army. Therefore, Parliament approves the army by passing an Armed Forces Act at least once every five years. The army is administered by the Ministry of Defence and commanded by the Chief of the General Staff.
The British Army has seen action in major wars between the world's great powers, including the Seven Years' War, the Napoleonic Wars, the Crimean War and the First and Second World Wars. Britain's victories in these decisive wars allowed it to influence world events and establish itself as one of the world's leading military and economic powers. Since the end of the Cold War, the British Army has been deployed to a number of conflict zones, often as part of an expeditionary force, a coalition force or part of a United Nations peacekeeping operation.
Until the English Civil War, England never had a standing army with professional officers and careerist corporals and sergeants. It relied on militia organised by local officials or private forces mobilised by the nobility, or on hired mercenaries from Europe. From the later Middle Ages until the English Civil War, when a foreign expeditionary force was needed, such as the one that Henry V of England took to France and that fought at the Battle of Agincourt (1415), the army, a professional one, was raised for the duration of the expedition.
During the English Civil War, the members of the Long Parliament realised that the use of county militia organised into regional associations (such as the Eastern Association), often commanded by local members of parliament (both from the House of Commons and the House of Lords), while more than able to hold their own in the regions which Parliamentarians controlled, were unlikely to win the war. So Parliament initiated two actions. The Self-denying Ordinance forbade members of parliament (with the notable exception of Oliver Cromwell) from serving as officers in the Parliamentary armies. This created a distinction between the civilians in Parliament, who tended to be Presbyterian and conciliatory to the Royalists in nature, and a corps of professional officers, who tended to Independent (Congregational) in theology, to whom they reported. The second action was legislation for the creation of a Parliamentary-funded army, commanded by Lord General Thomas Fairfax, which became known as the New Model Army (originally new-modelled Army).
While this proved to be a war-winning formula, the New Model Army, being organised and politically active, went on to dominate the politics of the Interregnum and by 1660 was widely disliked. The New Model Army was paid off and disbanded at the Restoration of the monarchy in 1660. For many decades the alleged excesses of the New Model Army under the Protectorate of Oliver Cromwell were used as propaganda (and still feature in Irish folklore) and the Whig element recoiled from allowing a standing army. The militia acts of 1661 and 1662 prevented local authorities from calling up militia and oppressing their own local opponents. Calling up the militia was possible only if the king and local elites agreed to do so.
Charles II and his Cavalier supporters favoured a new army under royal control, and immediately after the Restoration began working on its establishment. The first English Army regiments, including elements of the disbanded New Model Army, were formed between November 1660 and January 1661 and became a standing military force for England (financed by Parliament). The Royal Scots and Irish Armies were financed by the parliaments of Scotland and Ireland. Parliamentary control was established by the Bill of Rights 1689 and Claim of Right Act 1689, although the monarch continued to influence aspects of army administration until at least the end of the nineteenth century.
After the Restoration Charles II pulled together four regiments of infantry and cavalry, calling them his guards, at a cost of £122,000 from his general budget. This became the foundation of the permanent English Army. By 1685 it had grown to 7,500 soldiers in marching regiments, and 1,400 men permanently stationed in garrisons. A rebellion in 1685 allowed James II to raise the forces to 20,000 men. There were 37,000 in 1678 when England played a role in the closing stage of the Franco-Dutch War. After William and Mary's accession to the throne, England involved itself in the War of the Grand Alliance, primarily to prevent a French invasion restoring James II (Mary's father). In 1689, William III expanded the army to 74,000, and then to 94,000 in 1694. Parliament was very nervous and reduced the cadre to 7000 in 1697. Scotland and Ireland had theoretically separate military establishments, but they were unofficially merged with the English force.
By the time of the 1707 Acts of Union, many regiments of the English and Scottish armies were combined under one operational command and stationed in the Netherlands for the War of the Spanish Succession. Although all the regiments were now part of the new British military establishment, they remained under the old operational-command structure and retained much of the institutional ethos, customs and traditions of the standing armies created shortly after the restoration of the monarchy 47 years earlier. The order of seniority of the most-senior British Army line regiments is based on that of the English army. Although technically the Scots Royal Regiment of Foot was raised in 1633 and is the oldest Regiment of the Line, Scottish and Irish regiments were only allowed to take a rank in the English army on the date of their arrival in England (or the date when they were first placed on the English establishment). In 1694, a board of general officers was convened to decide the rank of English, Irish and Scots regiments serving in the Netherlands; the regiment which became known as the Scots Greys were designated the 4th Dragoons because there were three English regiments raised prior to 1688 when the Scots Greys were first placed in the English establishment. In 1713, when a new board of general officers was convened to decide the rank of several regiments, the seniority of the Scots Greys was reassessed and based on their June 1685 entry into England. At that time there was only one English regiment of dragoons, and the Scots Greys eventually received the British Army rank of 2nd Dragoons.
After 1700 British continental policy was to contain expansion by competing powers such as France and Spain. Although Spain was the dominant global power during the previous two centuries and the chief threat to England's early transatlantic ambitions, its influence was now waning. The territorial ambitions of the French, however, led to the War of the Spanish Succession and the Napoleonic Wars.
Although the Royal Navy is widely regarded as vital to the rise of the British Empire, the British Army played an important role in the formation of colonies, protectorates and dominions in the Americas, Africa, Asia, India and Australasia. British soldiers captured strategically important territories, and the army was involved in wars to secure the empire's borders and support friendly governments. Among these actions were the Seven Years' War, the American Revolutionary War, the Napoleonic Wars, the First and Second Opium Wars, the Boxer Rebellion, the New Zealand Wars, the Australian frontier wars, the Sepoy Rebellion of 1857, the first and second Boer Wars, the Fenian raids, the Irish War of Independence, interventions in Afghanistan (intended to maintain a buffer state between British India and the Russian Empire) and the Crimean War (to keep the Russian Empire at a safe distance by aiding Turkey). Like the English Army, the British Army fought the kingdoms of Spain, France (including the Empire of France) and the Netherlands for supremacy in North America and the West Indies. With native and provincial assistance, the army conquered New France in the North American theatre of the Seven Years' War and suppressed a Native American uprising in Pontiac's War. The British Army was defeated in the American Revolutionary War, losing the Thirteen Colonies but retaining The Canadas and The Maritimes as British North America, as well as Bermuda (originally part of Virginia, and which had been strongly sympathetic to the rebels early in the war).
The British Army was heavily involved in the Napoleonic Wars, participating in a number of campaigns in Europe (including continuous deployment in the Peninsular War), the Caribbean, North Africa and North America. The war between the British and the First French Empire of Napoleon Bonaparte stretched around the world; at its peak in 1813, the regular army contained over 250,000 men. A coalition of Anglo-Dutch and Prussian armies under the Duke of Wellington and Field Marshal von Blücher finally defeated Napoleon at Waterloo in 1815.
The English were involved politically and militarily in Ireland since receiving the Lordship of Ireland from the pope in 1171. The campaign of English republican Protector Oliver Cromwell involved uncompromising treatment of the Irish towns (most notably Drogheda and Wexford) which supported the Royalists during the English Civil War. The English Army (and the subsequent British Army) remained in Ireland primarily to suppress Irish revolts or disorder. In addition to its conflict with Irish nationalists, it was faced with the prospect of battling Anglo-Irish and Ulster Scots in Ireland who were angered by unfavourable taxation of Irish produce imported into Britain. With other Irish groups, they raised a volunteer army and threatened to emulate the American colonists if their conditions were not met. Learning from their experience in America, the British government sought a political solution. The British Army fought Irish rebels—Protestant and Catholic—primarily in Ulster and Leinster (Wolfe Tone's United Irishmen) in the 1798 rebellion.
In addition to battling the armies of other European empires (and its former colonies, the United States, in the War of 1812), the British Army fought the Chinese in the first and second Opium Wars and the Boxer Rebellion, Māori tribes in the first of the New Zealand Wars, Nawab Shiraj-ud-Daula's forces and British East India Company mutineers in the Sepoy Rebellion of 1857, the Boers in the first and second Boer Wars, Irish Fenians in Canada during the Fenian raids and Irish separatists in the Anglo-Irish War. The increasing demands of imperial expansion and the inadequacy and inefficiency of the underfunded British Army, Militia, Yeomanry and Volunteer Force after the Napoleonic Wars led to the late-19th-century Cardwell and Childers Reforms, which gave the army its modern shape and redefined its regimental system. The 1907 Haldane Reforms created the Territorial Force as the army's volunteer reserve component, merging and reorganising the Volunteer Force, Militia and Yeomanry.
Great Britain was challenged by other powers, primarily the German Empire and the Third Reich, during the 20th century. A century earlier it vied with Napoleonic France for global pre-eminence, and Hanoverian Britain's natural allies were the kingdoms and principalities of northern Germany. By the middle of the 19th century, Britain and France were allies in preventing Russia's appropriation of the Ottoman Empire, although the fear of French invasion led shortly afterwards to the creation of the Volunteer Force. By the first decade of the 20th century, the United Kingdom was allied with France (by the Entente Cordiale) and Russia (which had a secret agreement with France for mutual support in a war against the Prussian-led German Empire and the Austro-Hungarian Empire).
When the First World War broke out in August 1914 the British Army sent the British Expeditionary Force (BEF), consisting mainly of regular army troops, to France and Belgium. The fighting bogged down into static trench warfare for the remainder of the war. In 1915 the army created the Mediterranean Expeditionary Force to invade the Ottoman Empire via Gallipoli, an unsuccessful attempt to capture Constantinople and secure a sea route to Russia.
The First World War was the most devastating in British military history, with nearly 800,000 men killed and over two million wounded. Early in the war, the BEF was virtually destroyed and was replaced first by volunteers and then by a conscript force. Major battles included those at the Somme and Passchendaele. Advances in technology saw the advent of the tank (and the creation of the Royal Tank Regiment) and advances in aircraft design (and the creation of the Royal Flying Corps) which would be decisive in future battles. Trench warfare dominated Western Front strategy for most of the war, and the use of chemical weapons (disabling and poison gases) added to the devastation.
The Second World War broke out in September 1939 with the Russian and German Army's invasion of Poland. British assurances to the Poles led the British Empire to declare war on Germany. As in the First World War, a relatively small BEF was sent to France but then hastily evacuated from Dunkirk as the German forces swept through the Low Countries and across France in May 1940.
After the British Army recovered from its earlier defeats, it defeated the Germans and Italians at the Second Battle of El Alamein in North Africa in 1942–1943 and helped drive them from Africa. It then fought through Italy and, with the help of American, Canadian, Australian, New Zealand, Indian and Free French forces, and took part in the D-Day invasion of Normandy on 6 June 1944; nearly half the Allied soldiers were British. In the Far East, the British Army rallied against the Japanese in the Burma Campaign and regained the British Far Eastern colonial possessions.
After the Second World War the British Army was significantly reduced in size, although National Service continued until 1960. This period saw decolonisation begin with the partition and independence of India and Pakistan, followed by the independence of British colonies in Africa and Asia. Although the British Army was a major participant in Korea in the early 1950s and Suez in 1956, during this period Britain's role in world events was reduced and the army was downsized. The British Army of the Rhine, consisting of I (BR) Corps, remained in Germany as a bulwark against Soviet invasion. The Cold War continued, with significant technological advances in warfare, and the army saw the introduction of new weapons systems. Despite the decline of the British Empire, the army was engaged in Aden, Indonesia, Cyprus, Kenya and Malaya. In 1982, the British Army and the Royal Marines helped liberate the Falkland Islands during the conflict with Argentina after that country's invasion of the British territory.
In the three decades following 1969, the army was heavily deployed in Northern Ireland's Operation Banner to support the Royal Ulster Constabulary (later the Police Service of Northern Ireland) in their conflict with republican paramilitary groups. The locally recruited Ulster Defence Regiment was formed, becoming home-service battalions of the Royal Irish Regiment in 1992 before it was disbanded in 2006. Over 700 soldiers were killed during the Troubles. Following the 1994–1996 IRA ceasefires and since 1997, demilitarisation has been part of the peace process and the military presence has been reduced. On 25 June 2007 the 2nd Battalion of the Princess of Wales's Royal Regiment left the army complex in Bessbrook, County Armagh, ending the longest operation in British Army history.
The British Army contributed 50,000 troops to the coalition which fought Iraq in the Persian Gulf War, and British forces controlled Kuwait after its liberation. Forty-seven British military personnel died during the war.
The army was deployed to Yugoslavia in 1992. Initially part of the United Nations Protection Force, in 1995 its command was transferred to the Implementation Force (IFOR) and then to the Stabilisation Force in Bosnia and Herzegovina (SFOR); the commitment rose to over 10,000 troops. In 1999, British forces under SFOR command were sent to Kosovo and the contingent increased to 19,000 troops. Between early 1993 and June 2010, 72 British military personnel died during operations in the former Yugoslavian countries of Bosnia, Kosovo and Macedonia.
Although there have been permanent garrisons in Northern Ireland throughout its history, the British Army was deployed as a peacekeeping force from 1969 to 2007 in Operation Banner. Initially, this was (in the wake of unionist attacks on nationalist communities in Derry and Belfast) to prevent further loyalist attacks on Catholic communities; it developed into support of the Royal Ulster Constabulary (RUC) and its successor, the Police Service of Northern Ireland (PSNI) against the Provisional Irish Republican Army (PIRA). Under the 1998 Good Friday Agreement, there was a gradual reduction in the number of soldiers deployed. In 2005, after the PIRA declared a ceasefire, the British Army dismantled posts, withdrew many troops and restored troop levels to those of a peacetime garrison.
Operation Banner ended at midnight on 31 July 2007 after about 38 years of continuous deployment, the longest in British Army history. According to an internal document released in 2007, the British Army had failed to defeat the IRA but made it impossible for them to win by violence. Operation Helvetic replaced Operation Banner in 2007, maintaining fewer service personnel in a more-benign environment. Of the 300,000 troops who served in Northern Ireland since 1969, there were 763 British military personnel killed and 306 killed by the British military, mostly civilians. An estimated 100 soldiers committed suicide during Operation Banner or soon afterwards and a similar number died in accidents. A total of 6,116 were wounded.
In November 2001, as part of Operation Enduring Freedom with the United States, the United Kingdom deployed forces in Afghanistan to topple the Taliban in Operation Herrick. The 3rd Division were sent to Kabul to assist in the liberation of the capital and defeat Taliban forces in the mountains. In 2006 the British Army began concentrating on fighting Taliban forces and bringing security to Helmand Province, with about 9,500 British troops (including marines, airmen and sailors) deployed at its peak—the second-largest force after that of the US. In December 2012 Prime Minister David Cameron announced that the combat mission would end in 2014, and troop numbers gradually fell as the Afghan National Army took over the brunt of the fighting. Between 2001 and 26 April 2014 a total of 453 British military personnel died in Afghan operations. Operation Herrick ended with the handover of Camp Bastion on 26 October 2014, but the British Army maintains a deployment in Afghanistan as part of Operation Toral.
In 2003 the United Kingdom was a major contributor to the invasion of Iraq, sending a force of over 46,000 military personnel. The British Army controlled southern Iraq, and maintained a peace-keeping presence in Basra. All British troops were withdrawn from Iraq by 30 April 2009, after the Iraqi government refused to extend their mandate. One hundred and seventy-nine British military personnel died in Iraqi operations. The British Armed Forces returned to Iraq in 2014 as part of Operation Shader to counter the Islamic State (ISIL).
The British Army maintains a standing liability to support the civil authorities in certain circumstances, usually in either niche capabilities (e.g. explosive ordnance removal) or in general support of the civil authorities when their capacity is exceeded. In recent years this has been seen as army personnel supporting the civil authorities in the face of the 2001 United Kingdom foot-and-mouth outbreak, the 2002 firefighters strike, widespread flooding in 2005, 2007, 2009, 2013 and 2014 and most recently supporting the security services in "Operation Temperer" following the Manchester Arena bombing.
The British Army has been a volunteer force since national service ended during the 1960s. Since the creation of the part-time, reserve Territorial Force in 1908 (renamed the Army Reserve in 2014), the full-time British Army has been known as the Regular Army. In October 2019 there were just over 79,330 trained Regulars and 27,250 Army Reservists.
Following the Strategic Defence and Security Review 2010 (SDSR) and the Strategic Defence and Security Review 2015 the British Army adopted an evolving structure (known as Army 2020 Refine) that would see the number of Regular personnel set at 82,000 and see an increase in the number of Reservists to 30,000. This would bring the ratio of regular to part-time personnel in line with the US and Canada and better integrate the Army Reserve into the Regular Army.
In addition to the active Regular and Reserve force all former Regular Army personnel may be recalled for duty if required (known as the Regular Reserve). The Regular Reserve has two categories: A and D. Category A is mandatory, with the length of time in the category dependent on time spent in Regular Army service. Category D is voluntary, and consists of personnel who are no longer required to serve in category A. Regular Reserves in both categories serve under a fixed-term reserve contract and may report for training or service overseas and at home, similar to the Army Reserve. From 2013 the MOD only reports the numbers of Regular Reserve who fall into Category A, which in 2019 stood at 27,540.
The table below illustrates British Army personnel figures from 1710 to 2020.
The British Army's basic weapon is the L85A2 or L85A3 assault rifle, with some specialist personnel using the L22A2 carbine variant (pilots and some tank crew). The weapon was traditionally equipped with either iron sights or an optical SUSAT, although other optical sights have been subsequently purchased to supplement these. The weapon can be enhanced further utilising the Picatinny rail with attachments such as the L17A2 under-barrel grenade launcher.
Some soldiers are equipped with the L129A1 sharpshooter rifle, which in 2018 formally replaced the L86A2 Light Support Weapon. Support fire is provided by the L7 general-purpose machine gun (GPMG), and indirect fire is provided by L16 81mm mortars. Sniper rifles include the L118A1 7.62 mm, L115A3 and the AW50F, all manufactured by Accuracy International. The British Army utilises the Glock 17 as its side arm.
The army's main battle tank is the Challenger 2. It is supported by the Warrior Infantry Fighting Vehicle as the primary armoured personnel carrier and the many variants of the Combat Vehicle Reconnaissance (Tracked) and Bulldog. Light armoured units often utilise the Supacat "Jackal" MWMIK and Coyote for reconnaissance and fire support.
The army has three main artillery systems: the Multi Launch Rocket System (MLRS), the AS-90 and the L118 light gun. The MLRS, first used in Operation Granby, has an range. The AS-90 is a 155 mm self-propelled armoured gun with a range. The L118 light gun is a 105 mm towed gun. To identify artillery targets, the army operates weapon locators such as the MAMBA Radar and utilises artillery sound ranging. For air defence it uses the Short-Range Air Defence (SHORAD) Rapier FSC missile system, widely deployed since the Falklands War, and the Very Short-Range Air Defence (VSHORAD) Starstreak HVM (high-velocity missile) launched by a single soldier or from a vehicle-mounted launcher.
Where armour is not required or mobility and speed are favoured the British Army utilises protected patrol vehicles, such as the Panther variant of the Iveco LMV, the Foxhound, and variants of the Cougar family (such as the Ridgeback, Husky and Mastiff). For day-to-day utility work the army commonly uses the Land Rover Wolf, which is based on the Land Rover Defender.
Specialist engineering vehicles include bomb-disposal robots and the modern variants of the Armoured Vehicle Royal Engineers, including the Titan bridge-layer, Trojan combat-engineer vehicle, Terrier Armoured Digger and Python Minefield Breaching System. Day-to-day utility work uses a series of support vehicles, including six-, nine- and fifteen-tonne trucks (often called "Bedfords", after a historic utility vehicle), heavy-equipment transporters (HET), close-support tankers, quad bikes and ambulances. Tactical communication uses the Bowman radio system, and operational or strategic communication is controlled by the Royal Corps of Signals.
The Army Air Corps (AAC) provides direct aviation support, with the Royal Air Force providing support helicopters. The primary attack helicopter is the Westland WAH-64 Apache, a licence-built, modified version of the US AH-64 Apache which replaced the Westland Lynx AH7 in the anti-tank role. Other helicopters include the Westland Gazelle (a light surveillance aircraft), the Bell 212 (in jungle "hot and high" environments) and the AgustaWestland AW159 Wildcat, a dedicated intelligence, surveillance, target acquisition, and reconnaissance (ISTAR) helicopter. The Eurocopter AS 365N Dauphin is used for special operations aviation, and the Britten-Norman Islander is a light, fixed-wing aircraft used for airborne reconnaissance and command and control. The army operates two unmanned aerial vehicles ('UAV's) in a surveillance role: the small Lockheed Martin Desert Hawk III and the larger Thales Watchkeeper WK450.
Army Headquarters is located in Andover, Hampshire. The army's structure is broadly similar to the Royal Navy and Royal Air Force, in that the four-star (general-equivalent) field commands have been eliminated. Under the Army 2020 Command structure, the Chief of the General Staff is in charge of Army Headquarters. There are four lieutenant-general posts in Army headquarters: the Deputy Chief of the General Staff, the Commander Field Army, the Commander Home Command and the Commander Allied Rapid Reaction Corps. Army Headquarters is responsible for providing forces at operational readiness for employment by the Permanent Joint Headquarters.
The command structure is hierarchical, with divisions and brigades controlling groups of units. Major units are regiment/battalion-sized, and minor units are company-sized units (or platoons). All units are Regular (full-time) or Army Reserve (part-time).
Naming conventions of units differ for historical reasons, creating some confusion; the term "battalion" in the infantry is synonymous with cavalry, artillery or engineer regiment, and the infantry "company" is synonymous with an engineer or cavalry squadron and an artillery battery. The table below illustrates the different names for equivalent units.
Adding to the confusion is the tendency of units (again for historical reasons) to misuse titles for larger administrative structures. Although the Royal Artillery consists of 13 Regular regiments (equivalent to infantry battalions), it calls itself the Royal Regiment of Artillery when referring to the units as a whole. The Royal Logistic Corps and Intelligence Corps are not corps-sized, but corps in this instance are administrative branches consisting of several battalions or regiments.
The forces of the British Army after the Army 2020 Refine reforms are organised in garrison as:
For operational tasks, the most common unit is the battlegroup, formed around a combat unit and supported by units (or sub-units) from other areas. An example of a battlegroup in the Reactive Force (e.g. the 1st Brigade) would be 2 companies of armoured infantry (e.g. from the 1st Battalion of the Mercian Regiment), one squadron of heavy armour (e.g. A Squadron of the Royal Tank Regiment), a company of engineers (e.g. B Company of the 22nd Engineer Regiment), a battery of artillery (e.g. D Battery of the 1st Regiment of the Royal Horse Artillery) and smaller attachments from medical, logistic and intelligence units. Typically organised and commanded by a battlegroup headquarters and named after the unit which provided the most combat units, in this example, it would be the 1 Mercian Battlegroup. This creates a self-sustaining mixed formation of armour, infantry, artillery, engineers and support units, typically 600 to 1,000 soldiers commanded by a lieutenant colonel.
The table below demonstrates how 3 or 4 battlegroups make up a brigade and 3 or 4 brigades make up a division. A division is currently the largest unit the British Army is capable of deploying independently, although it could be grouped with 3 or 4 other divisions from a multi-national coalition to form a corps.
The British Army contributes 2 of the 3 special forces formations to the United Kingdom Special Forces directorate: the Special Air Service (SAS) and Special Reconnaissance Regiment (SRR).
The SAS consists of one regular and two reserve regiments. The regular regiment, 22 SAS, has its headquarters at Stirling Lines, Credenhill, Herefordshire. It consists of 5 squadrons (A, B, D, G and Reserve) and a training wing. 22 SAS is supported by 2 reserve regiments, 21 SAS and 23 SAS, which collectively form the Special Air Service (Reserve) (SAS [R]), under the command of the 1st Intelligence, Surveillance and Reconnaissance Brigade.
The SRR, formed in 2005, performs close reconnaissance and special surveillance tasks. The Special Forces Support Group, under the operational control of the Director of Special Forces, provides operational manoeuvring support to the United Kingdom Special Forces.
The British Army historically included many units from what are now separate Commonwealth realms. When the English Empire was established in North America, Bermuda, and the West Indies in the early 17th century there was no standing English Army, only the Militia, and this was extended to the colonies. Colonial militias defended colonies single-handedly at first against indigenous peoples and European competitors. Once the standing English Army, later the British Army, came into existence, the colonial militias fought side by side with it in a number of wars, including the Seven Years' War. Some of the colonial militias rebelled during the American War of Independence. The militia fought alongside the regular British Army (and native allies) in defending British North America from their former countrymen during the War of 1812. With the growth of the empire around the world, "non-European" (i.e. non-white, except for officers) units were recruited in many colonies and protectorates, but most were deemed auxiliaries and not part of the British Army. The West India Regiments were an exception, as they were fully incorporated into the British Army, but were kept outside of Europe and non-whites were denied commissions. Locally raised units in strategically-located colonies (including Bermuda, Gibraltar, Malta) and the Channel Islands were generally more fully integrated into the British Army as evident from their appearances in British Army lists, unlike units such as the King's African Rifles. The larger colonies (Australia, New Zealand, Canada, South Africa, etc.) mostly achieved Commonwealth Dominion status before or after the First World War and were granted full legislative independence in 1931. While remaining within the British Empire, this placed their governments on a par with the British government, and hence their military units comprised separate armies (e.g. the Australian Army), although Canada retained the term "militia" for its military forces until the Second World War. From the 1940s, these dominions and many colonies chose full independence, usually becoming Commonwealth realms (as member states of the Commonwealth are known today).
Units raised in self-governing and Crown colonies that are part of the British realm remain under British control. The UK retains responsibility for the defence of the fourteen remaining British Overseas Territories, of which six have locally raised regiments:
Although the army primarily recruits within the United Kingdom, it accepts applications from Commonwealth citizens and (occasionally) those from friendly nations who meet certain criteria. In 2016, it was decided to open all roles to women in 2018; women had not previously been permitted to join the Combat Arms. The British Army is an equal-opportunity employer (with some exceptions due to its medical standards), and does not discriminate based on race, religion or sexual orientation.
The minimum age is 16 (after the end of GCSEs), although soldiers under 18 may not serve in operations. The maximum recruitment age is 35 years and 6 months, and the maximum age for Army Reserve soldiers is 49 years old, . A soldier would traditionally enlist for a term of 22 years, although recently there has been a shift towards 12-year terms with a 22-year option. A soldier is not normally permitted to leave until they have served for at least four years, and must give 12 months' notice.
All soldiers and commissioned officers must take an oath of allegiance upon joining the Army, a process known as attestation. Those who wish to swear by God use the following words:
Others replace the words "swear by Almighty God" with "solemnly, sincerely and truly declare and affirm".
Royal Military Academy Sandhurst (RMAS) is the officer-training school, and Royal School of Artillery (RSA) trains the Royal Artillery. Royal School of Military Engineering (RSME) trains the Corps of Royal Engineers.
The Army Training Regiment, Grantham provides training for Army Reserve recruits, and the Army Training Regiment, Pirbright provides training for the Army Air Corps, the Royal Artillery, the Royal Corps of Signals, the Royal Logistic Corps, the Royal Electrical and Mechanical Engineers, the Adjutant General's Corps, the Royal Army Medical Corps and the Intelligence Corps. The Army Training Regiment, Winchester trains the Royal Armoured Corps, the Army Air Corps, the Royal Artillery, the Royal Engineers, the Royal Corps of Signals, the Royal Logistic Corps, the Royal Electrical and Mechanical Engineers, the Adjutant General's Corps, the Royal Army Medical Corps and the Intelligence Corps.
There is an Infantry Training Centre at Catterick and an Infantry Battle School in Brecon. Other training centres are the Army Foundation College (Harrogate) and Army Training Units.
The army's official flag is the 3:5 ratio Union Jack, although a non-ceremonial flag flies at the Ministry of Defence building in Whitehall and is often used at recruiting and military events and exhibitions. It represents the army on the Cenotaph at Whitehall in London, the UK memorial to its war dead. Each British Army unit has a set of flags, known as the colours—normally a Regimental Colour and a Queen's Colour (the Union Jack).
Each regiment and corps has distinctive insignia, such as a cap badge, beret, tactical recognition flash or stable belt. Many units also call soldiers of different ranks by different names, for example; a NATO OR-1 (private) is called a guardsman in Guards regiments, a gunner in artillery units, a sapper in engineer units and a trooper in the SAS. These names do not affect a soldier's pay or role.
The British Army uniform has sixteen categories, ranging from ceremonial uniforms to combat dress to evening wear. No. 8 Dress, the day-to-day uniform, is known as "Personal Clothing System – Combat Uniform" (PCS-CU) and consists of a Multi-Terrain Pattern (MTP) windproof smock, a lightweight jacket and trousers with ancillary items such as thermals and waterproofs. The army has introduced tactical recognition flashes (TRFs); worn on the right arm of a combat uniform, the insignia denotes the wearer's regiment or corps.
Working headdress is typically a beret, whose colour indicates its wearer's type of regiment. Beret colours are:
In addition to working dress, the army has a number of parade uniforms for ceremonial and non-ceremonial occasions. The most-commonly-seen uniforms are No.1 Dress (full ceremonial, seen at formal occasions such as at the changing of the guard at Buckingham Palace) and No.2 Dress (Service Dress), a brown khaki uniform worn for non-ceremonial parades.

</doc>
<doc id="4888" url="https://en.wikipedia.org/wiki?curid=4888" title="Bruin">
Bruin

Bruin, (from Dutch for "brown"), is an English folk term for brown bear.
Bruin, Bruins or BRUIN may also refer to:

</doc>
<doc id="4890" url="https://en.wikipedia.org/wiki?curid=4890" title="Bayesian probability">
Bayesian probability

Bayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief.
The Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, that is to say, with propositions whose truth or falsity is unknown. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.
Bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies a prior probability. This, in turn, is then updated to a posterior probability in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.
The term "Bayesian" derives from the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability.
Bayesian methods are characterized by concepts and procedures as follows:
Broadly speaking, there are two interpretations on Bayesian probability. For objectivists, interpreting probability as extension of logic, "probability" quantifies the reasonable expectation everyone (even a "robot") sharing the same knowledge should share in accordance with the rules of Bayesian statistics, which can be justified by Cox's theorem. For subjectivists, "probability" corresponds to a personal belief. Rationality and coherence allow for substantial variation within the constraints they pose; the constraints are justified by the Dutch book argument or by decision theory and de Finetti's theorem. The objective and subjective variants of Bayesian probability differ mainly in their interpretation and construction of the prior probability.
The term "Bayesian" derives from Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem in a paper titled "An Essay towards solving a Problem in the Doctrine of Chances". In that special case, the prior and posterior distributions were beta distributions and the data came from Bernoulli trials. It was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called "inverse probability" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, "inverse probability" was largely supplanted by a collection of methods that came to be called frequentist statistics.
In the 20th century, the ideas of Laplace developed in two directions, giving rise to "objective" and "subjective" currents in Bayesian practice.
Harold Jeffreys' "Theory of Probability" (first published in 1939) played an important role in the revival of the Bayesian view of probability, followed by works by Abraham Wald (1950) and Leonard J. Savage (1954). The adjective "Bayesian" itself dates to the 1950s; the derived "Bayesianism", "neo-Bayesianism" is of 1960s coinage. In the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. No subjective decisions need to be involved. In contrast, "subjectivist" statisticians deny the possibility of fully objective analysis for the general case.
In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods and the consequent removal of many of the computational problems, and to an increasing interest in nonstandard, complex applications. While frequentist statistics remains strong (as seen by the fact that most undergraduate teaching is still based on it ), Bayesian methods are widely accepted and used, e.g., in the field of machine learning.
The use of Bayesian probabilities as the basis of Bayesian inference has been supported by several arguments, such as Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.
Richard T. Cox showed that Bayesian updating follows from several axioms, including two functional equations and a hypothesis of differentiability. The assumption of differentiability or even continuity is controversial; Halpern found a counterexample based on his observation that the Boolean algebra of statements may be finite. Other axiomatizations have been suggested by various authors with the purpose of making the theory more rigorous.
The Dutch book argument was proposed by de Finetti; it is based on betting. A Dutch book is made when a clever gambler places a set of bets that guarantee a profit, no matter what the outcome of the bets. If a bookmaker follows the rules of the Bayesian calculus in the construction of his odds, a Dutch book cannot be made.
However, Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. For example, Hacking writes "And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."
In fact, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "probability kinematics" following the publication of Richard C. Jeffreys' rule, which is itself regarded as Bayesian). The additional hypotheses sufficient to (uniquely) specify Bayesian updating are substantial and not universally seen as satisfactory.
A decision-theoretic justification of the use of Bayesian inference (and hence of Bayesian probabilities) was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures. Conversely, every Bayesian procedure is admissible.
Following the work on expected utility theory of Ramsey and von Neumann, decision-theorists have accounted for rational behavior using a probability distribution for the agent. Johann Pfanzagl completed the "Theory of Games and Economic Behavior" by providing an axiomatization of subjective probability and utility, a task left uncompleted by von Neumann and Oskar Morgenstern: their original theory supposed that all the agents had the same probability distribution, as a convenience. Pfanzagl's axiomatization was endorsed by Oskar Morgenstern: "Von Neumann and I have anticipated ... [the question whether probabilities] might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility together with a number for the probabilities (cf. p. 19 of The Theory of Games and Economic Behavior). We did not carry this out; it was demonstrated by Pfanzagl ... with all the necessary rigor".
Ramsey and Savage noted that the individual agent's probability distribution could be objectively studied in experiments. Procedures for testing hypotheses about probabilities (using finite samples) are due to Ramsey (1931) and de Finetti (1931, 1937, 1964, 1970). Both Bruno de Finetti and Frank P. Ramsey acknowledge their debts to pragmatic philosophy, particularly (for Ramsey) to Charles S. Peirce.
The "Ramsey test" for evaluating probability distributions is implementable in theory, and has kept experimental psychologists occupied for a half century.
This work demonstrates that Bayesian-probability propositions can be falsified, and so meet an empirical criterion of Charles S. Peirce, whose work inspired Ramsey. (This falsifiability-criterion was popularized by Karl Popper.)
Modern work on the experimental evaluation of personal probabilities uses the randomization, blinding, and Boolean-decision procedures of the Peirce-Jastrow experiment. Since individuals act according to different probability judgments, these agents' probabilities are "personal" (but amenable to objective study).
Personal probabilities are problematic for science and for some applications where decision-makers lack the knowledge or time to specify an informed probability-distribution (on which they are prepared to act). To meet the needs of science and of human limitations, Bayesian statisticians have developed "objective" methods for specifying prior probabilities.
Indeed, some Bayesians have argued the prior state of knowledge defines "the" (unique) prior probability-distribution for "regular" statistical problems; cf. well-posed problems. Finding the right method for constructing such "objective" priors (for appropriate classes of regular problems) has been the quest of statistical theorists from Laplace to John Maynard Keynes, Harold Jeffreys, and Edwin Thompson Jaynes. These theorists and their successors have suggested several methods for constructing "objective" priors (Unfortunately, it is not clear how to assess the relative "objectivity" of the priors proposed under these methods):
Each of these methods contributes useful priors for "regular" one-parameter problems, and each prior can handle some challenging statistical models (with "irregularity" or several parameters). Each of these methods has been useful in Bayesian practice. Indeed, methods for constructing "objective" (alternatively, "default" or "ignorance") priors have been developed by avowed subjective (or "personal") Bayesians like James Berger (Duke University) and José-Miguel Bernardo (Universitat de València), simply because such priors are needed for Bayesian practice, particularly in science. The quest for "the universal method for constructing priors" continues to attract statistical theorists.
Thus, the Bayesian statistician needs either to use informed priors (using relevant expertise or previous data) or to choose among the competing methods for constructing "objective" priors.

</doc>
<doc id="4892" url="https://en.wikipedia.org/wiki?curid=4892" title="Bert Bell">
Bert Bell

De Benneville "Bert" Bell (February 25, 1895 – October 11, 1959) was the National Football League (NFL) commissioner from 1946 until his death in 1959. As commissioner, he introduced competitive parity into the NFL to improve the league's commercial viability and promote its popularity, and he helped make the NFL the most financially sound sports enterprise and preeminent sports attraction in the United States (US). He was posthumously inducted into the charter class of the Pro Football Hall of Fame.
Bell played football at the University of Pennsylvania, where as quarterback, he led his team to an appearance in the 1917 Rose Bowl. After being drafted into the US Army during World War I, he returned to complete his collegiate career at Penn and went on to become an assistant football coach with the Quakers in the 1920s. During the Great Depression, he was an assistant coach for the Temple Owls and a co-founder and co-owner of the Philadelphia Eagles.
With the Eagles, Bell led the way in cooperating with the other NFL owners to establish the National Football League Draft in order to afford the weakest teams the first opportunity to sign the best available players. He subsequently became sole proprietor of the Eagles, but the franchise suffered financially. Eventually, he sold the team and bought a share in the Pittsburgh Steelers. During World War II, Bell astutely argued against the league suspending operations until the war's conclusion.
After the war, he was elected NFL commissioner and sold his ownership in the Steelers. As commissioner, he implemented a proactive anti-gambling policy, negotiated a merger with the All-America Football Conference (AAFC), and unilaterally crafted the entire league schedule with an emphasis on enhancing the dramatic effect of late-season matches. During the Golden Age of Television, he tailored the game's rules to strengthen its appeal to mass media and enforced a policy of blacking out local broadcasts of home contests to safeguard ticket receipts. Amid criticism from franchise owners and under pressure from Congress, he unilaterally recognized the NFLPA and facilitated in the development of the first pension plan for the players. He survived to oversee the "Greatest Game Ever Played" and to envision what the league would become in the future.
Bell was born de Benneville Bell, on February 25, 1895, in Philadelphia to John C. Bell and Fleurette de Benneville Myers. His father was an attorney who served a term as the Pennsylvania Attorney General. His older brother, John C., Jr., was born in 1892. Bert's parents were very wealthy, and his mother's lineage predated the American Revolutionary War. His father, a Quaker of the University of Pennsylvania (class of 1884) during the early days of American football, accompanied him to his first football game when Bell was six years old. Thereafter, Bell regularly engaged in football games with childhood friends.
In 1904, Bell matriculated at the Episcopal Academy, the Delancey School from 1909 to 1911 and then the Haverford School until 1914. About this time, his father was installed as athletics director at Penn and helped form the National Collegiate Athletic Association (NCAA). At Haverford, Bell captained the school's football, basketball, and baseball teams, and "was awarded The Yale Cup [for being] 'The pupil who has done the most to promote athletics in the school.'" Although he excelled at baseball, his devotion was to football. His father, who was named a trustee at Penn in 1911, said of Bell's plans for college, "Bert will go to Penn or he will go to hell."
Bell entered Penn in the fall of 1914 as an English major and joined Phi Kappa Sigma. In a rare occurrence for a sophomore, he became the starting quarterback for Penn's coach George H. Brooke. On the team, he also was as a defender, punter, and punt returner. After the team's 3–0 start, Bell temporarily shared possession of his quarterbacking duties until he subsequently reclaimed them later in the season, as Penn finished with a record of 3–5–2.
Prior to Penn's 1916 season, his mother died while he was en route to her bedside. Nevertheless, he started the first game for the Quakers under new coach Bob Folwell, but mixed results left him platooned for the rest of the season. Penn finished with a record of 7–2–1. However, the Quakers secured an invitation to the 1917 Rose Bowl against the Oregon Ducks. Although the best offensive gain for Penn during their 20–14 loss to Oregon was a 20-yard run by Bell, he was replaced late in the game at quarterback after throwing an interception.
In the 1917 season, Bell led Penn to a 9–2 record. Afterwards, he registered with a Mobile Hospital Unit of the US Army for World War I and was deployed to France in May 1918. As a result of his unit participating in hazardous duty, it received a congratulatory letter for bravery from General John J. Pershing, and Bell was promoted to first sergeant. After the war, Bell returned to the United States in March 1919. He returned to Penn as captain of the team in the fall and again performed erratically. The Quakers finished 1919 with a 6–2–1 record. Academically, his aversion to attending classes forced him to withdraw from Penn without a degree in early 1920. His collegiate days ended with his having been a borderline All-American, but this period of his life had proven that he "possessed the qualities of a leader."
Bell assembled the Stanley Professionals in Chicago in 1920, but he disbanded it prior to playing any games because of negative publicity received by Chicago due to the Black Sox Scandal. He joined John Heisman's staff at Penn as an assistant coach in 1920, and Bell would remain there for several years. At Penn, he was well regarded as a football coach, and after its 1924 season, he drew offers for, but declined, head-coaching assignments at other universities. At least as early as 1926, his avocation was socializing and frequenting Saratoga Race Course, where he counted as friends Tim Mara, Art Rooney, and George Preston Marshall. In 1928, Bell tendered his resignation at Penn in protest over the emphasis of in-season scrimmages during practices by Lud Wray, a fellow assistant coach. Bell's resignation was accommodated prior to the start of the 1929 season.
Bell was then an employee of the Ritz-Carlton in Philadelphia. At one point, he tried his hand as a stock broker and lost $50,000 (presently, $) during the Wall Street Crash of 1929. His father bailed him out of his deprivation, and he returned to working at the Ritz. From 1930 until 1932, he was a backfield coach for the Temple Owls football team. In 1932, Marshall tried to coax Bell into buying the rights to an NFL franchise, but Bell disparaged the league and ridiculed the idea. When Pop Warner was hired to coach Temple for the 1933 season, Warner chose to hire his own assistants and Bell was let go.
By early 1933, Bell's opinion on the NFL had changed, and he wanted to become an owner of a team based in Philadelphia. After being advised by the NFL that a prerequisite to a franchise being rendered in Philadelphia was that the Pennsylvania Blue Laws would have to be mollified, he was the "force majeure" in lobbying to getting the laws deprecated. He borrowed funds from Frances Upton, partnered with Wray, and he procured the rights to a franchise in Philadelphia which he christened as the Philadelphia Eagles.
After the inaugural 1933 Philadelphia Eagles season, Bell married Upton at St. Madeleine Sophie Roman Catholic Church in Philadelphia. Days later, his suggestion to bestow the winner of the NFL championship game with the Ed Thorp Memorial Trophy was affirmed. In 1934, the Eagles finished with a 4–7 record, The Eagles' inability to seriously challenge other teams made it difficult to sell tickets, and his failure to sign a talented college prospect led him to adduce that the only way to bring stability to the league was to institute a draft to ensure the weakest teams had an advantage in signing the preeminent players. In 1935, his proposal for a draft was accepted, and in February 1936, the first draft kicked off, at which he acted as Master of Ceremonies. Later that month, his first child, Bert Jr., was born.
In the Eagles' first three years, the partners exhausted $85,000 (presently, $), and at a public auction, Bell became sole owner of the Eagles with a bid of $4,500 (presently, $). Austerity measures forced him to supplant Wray as head coach of the Eagles, wherein Bell led the Eagles to a 1–11 finish, their worst record ever. In December, an application for a franchise in Los Angeles was obstructed by Bell and Pittsburgh Steelers owner Rooney as they deemed it too far of a distance to travel for games. During the Eagles' 2-8-1 1937 season, his second child, John "Upton", was born. In the Eagles' first profitable season, 1938, they posted a 5–6 record. The Eagles finished 1–9–1 in 1939 and 1–10 in 1940.
In December 1940, Bell conciliated the sale of Rooney's Steelers to Alexis Thompson, and then Rooney acquired half of Bell's interest in the Eagles. In a series of events known as the "Pennsylvania Polka", Rooney and Bell exchanged their entire Eagles roster and their "territorial rights" in Philadelphia to Thompson for his entire Steelers roster and his rights in Pittsburgh. Ostensibly, Rooney had provided assistance to Bell by rewarding him with a 20% commission on the sale of the Steelers. Bell became the Steelers head coach and Rooney became the general manager.
During the training camp of the Pittsburgh's inaugural season with the nickname Steelers, Bell was buoyant with optimism about the team's prospect, but he became crestfallen after Rooney denigrated the squad and flippantly remarked that they looked like the "[s]ame old Steelers" (SOS). After losing the first two games of the 1941 season, Rooney compelled Bell into resigning as head coach. Bell's coaching career ended with a 10–46–2 record, his 0.179 winning percentage is second lowest in NFL history to only Phil Handler's 0.105 for coaches with at least five seasons. And at 36 games under .500 he held the record for futility until John McKay passed him in 1983 and Marion Campbell passed him in 1988. His first daughter and last child, Jane Upton, was born several months after the season's conclusion.
By 1943, 40% of the NFL rosters had been drafted into the United States Armed Forces for World War II. The resulting difficulty in fielding a full-strength squad led some owners to recommend the league should shut down until the war ended. Bell auspiciously argued against this as he feared they might not be able to resume operations easily after the war, and since Major League Baseball was continuing unabated, then they should also.
Throughout Bell's affiliation with the Steelers, he suffered monetarily and Rooney bought an increasing allotment of the franchise from him. Compounding Bell's problems, Arch Ward organized the All-America Football Conference (AAFC) in 1944 to displace the NFL's sovereignty in professional football. Ward's AAFC promptly began luring players to join the league, which resulted in salaries being driven up drastically. In Bill Dudley's contract proceedings with the Steelers, he attributed Bell's anxiety during the negotiations to the rivalry from the AAFC. Furthermore, by the end of 1945, the Steelers were in their most economically perilous situation in its history.
Elmer Layden was appointed the first NFL commissioner in 1941, but Ward appeared as dictating his hiring. Layden tendered his resignation for personal reasons January 1946. Bell, who was not well respected in Pittsburgh, was elected to replace him. He received a three-year contract at $20,000 ($) per year, and transacted a sale of his stake in the Steelers to Rooney, albeit for a price Bell did not construe was full-value. He was then immediately placed at the center of a controversy wherein the owners denied Dan Reeves permission to relocate the Cleveland Rams to Los Angeles. Bell moderated a settlement, and, as a result, the Los Angeles Rams were formed. As a precondition to the Rams leasing the Los Angeles Coliseum, they signed Kenny Washington, which marked the beginning of the end of racial segregation on the field, but also caused "'all hell to break loose'" amidst the owners.
The drawing up of a regular season schedule had been a perennial source of contention among the NFL owners since the league's inception. The crux of the problem was the scheduling of games meant weighing the interest of owners who, early in the season, wanted their franchises to confront teams that drew the largest crowds, versus owners who wanted to play the weaker franchises to pad their team's win-loss record. The resultant impasse coerced the owners, in 1946, to confer upon Bell the sole discretion in developing the league's schedule. He utilized this responsibility to, early in the season, pit the weaker teams against other weak teams, and the strong teams against other strong teams. His goal was to augment game attendances by keeping the difference in team standings to a minimum as deep into the season as possible.
On the eve of the 1946 championship game, Bell was notified that Merle Hapes and Frank Filchock of the New York Giants had been implicated in a bribing scandal. Filchock was sanctioned by Bell to play in the game but Hapes was suspended. At the next NFL owners' meeting, Bell was worried the repercussions from this event would lead to his firing. However, he was pleasantly surprised to learn that his contract would be elevated to five years at $30,000 per year. Reinvigorated with renewed support, he persuaded the owners to allow him to put sudden-death overtime into the playoffs.
Subsequently, he wrote an anti-gambling resolution into the league constitution, which empowered him with the ability to permanently ban any NFL associated personnel for betting on a game or for withholding information on a game being possibly fixed. Furthermore, to obstruct gamblers from getting inside information, he secreted the names of officials he would assign to games, and he directed each team to promulgate a precursory injury report which listed anyone who might not participate in a game. Eventually, he lobbied to get every state in the US to criminalize the fixing of sporting events and put employees on the payroll of the NFL to investigate potential betting scams.
The NFL's struggle against the AAFC generated stress on wages, attendance, marketing, and by 1949, it had prevented the NFL for showing a profit for three consecutive years. Bell and representatives from both leagues met to attempt a merger, but their efforts were fruitless. In an unrelated matter, he apprised the owners that attendance records had shown televising games locally had a negative impact on the sale of home tickets. Nevertheless, he actualized the NFL's first television contract—the 1949 championship game. Simultaneously, he dealt with a lawsuit from Bill Radovich, who had been blacklisted for leaving the Lions and gaining employment with the AAFC. Bell and the owners were advised by John C. Jr. that this lawsuit was potentially not winnable, and the ramifications from the outcome of the case weighed heavily on Bell.
One of the primary impediments in an AAFC-NFL merger was the supposed violation of "territorial rights" claimed by Marshall. Eventually, Bell gathered enough support to effectuate a compromise with the AAFC. In late 1949, the leagues merged, and Bell would stay on as commissioner with his contract extended from five to ten years as three AAFC teams (the Cleveland Browns, San Francisco 49ers, and Baltimore Colts) were subsumed. Seeking to capitalize on the publicity of the residual rivalry, he utilized "exquisite dramatic" and business sense and allocated the 1950 opening game to a contest between the 1949 champion Eagles versus the perennial AAFC champion Browns. Feeling financially secure after the merger, he purchased his first home for himself and his family in Narberth, Pennsylvania.
In 1950, Bell originated a blackout rule into the NFL which forbid all teams to televise their home games within a 75-mile radius of their stadium – except for the Rams. Consequently, the United States Department of Justice (DOJ) opened an investigation into a violation of the Sherman Antitrust Act. Ensuingly, the Rams attendance for 1950 dropped off by 50%, and this signaled a potential financial disaster. In 1951, he licensed the DuMont Television Network to air the championship games for the next five years, and he stipulated that teams were free to develop their own television contracts independently.
However, preceding 1951 season, he reimposed the blackout rule on all teams in the league. The DOJ filed suit over this and Bell publicly retorted, "You can't give fans a game for free on TV and also expect them to go to the ballpark"; nevertheless, the suit was ordered to trial for January 1952. After the 1951 season ended, he gained unilateral control over the setting of a television strategy for the NFL. He negotiated a deal with DuMont, which granted it the rights to nationally broadcast one regular season game every week, and he directed that the income from this contract was to be shared equally between all the teams. In the DOJ's case, the judge ruled that the blackout policy was legal, but both Bell, and the franchises collectively, were enjoined from negotiating a TV contract; Bell was ecstatic. Later that year, Bell forced one of the owners of the Cleveland Browns to sell all of his shares in the team after Bell determined the owner had bet on Browns' football games. Although he hated to fly, at some indeterminate point, he visited the training camps of every team and lectured on the danger gamblers posed to the league.
Bell authorized a Pro Bowl to be held at the end of each season in order to showcase the talents of the best players. But in the early 1950s, on the field activities sometimes denigrated to borderline "assault and battery" with teams' star players being viciously targeted by opposing players. He answered charges the league was too savage by saying, "'I have never seen a maliciously dirty football player in my life and I don't believe there are any.'" Nevertheless, he ordered broadcasts to follow a strict rule of conduct whereby TV announcers would not be permitted to criticize the game, and neither fights, nor injuries, could be televised by virtue in his belief that announcers were "'salesman for professional football [and] we do not want kids believing that engaging in fights is the way to play football.'"
Bell was criticized for censoring TV broadcasts, a charge he dismissed as not pertinent because he believed he was not impeding the print media but only advertising a product. After CBS and NBC gained the rights to broadcast the games in 1956, he advised the franchises to avoid criticizing the games or the officials, and forewarned that TV would give "'us our greatest opportunity to sell the NFL and everyone must present to the public the greatest games ... combined with the finest sportsmanship.'" This relationship with television was the beginning of the NFL's rise to becoming America's most popular sport.
In Radovich v. National Football League, the Supreme Court ruled in Radovich's favor and declared the NFL was subject to antitrust laws, and the implication was that the legality of the draft and reserve clause were dubious. Bell pressed a case in the media that the NFL should be exempted from antitrust regulations and proffered the league was a sport and not a business. He invited an investigation from Congress with respect to the court's ruling. The House Judiciary committee, chaired by Emanuel Celler—who believed the draft was illegal and should be abolished, convened in July 1957 to discuss the ramifications of the Radovich decision. Red Grange and Bell testified at the committee's solicitation and argued the draft was essential to the sport's success. Representatives of the NFLPA contradicted these statements and said the draft and the reserve clause were anti-labor, and it seemed as if Congress was going to accept their position. Faced with Congressional opposition, Bell formally recognized the NFLPA and declared he would negotiate with its representatives.
However, Bell was speaking only for himself and without the auspices of the owners. At the next owners' meeting, Rooney admonished they either had to recognize the NFLPA or remove Bell as commissioner. In order to do this, they had to agree in a vote that required a "super-majority". Bell unsuccessfully attempted to persuade the owners to permit the NFLPA to act as a bargaining agent for the players. However, he did reach a compromise with the owners to get them to acquiesce to some of the NFLPA's requests for salary standards and health benefits.
For the 1958 season, the durations of timeouts was extended from 60 to 90 seconds and Bell mandated officials call a few "TV timeouts" during each game — a change which triggered criticism from sportswriters. The 1958 championship game became the first NFL championship game decided in overtime, and it was considered to be the greatest football game ever played. The game further increased football's marketability to television advertising, and the drama associated with overtime was the catalyst. Years later, after witnessing Bell openly crying after the game, Raymond Berry attributed it to Bell's realization of the impact the game would have on the prevalence of the sport.
The death of Mara in February unsettled Bell and he experienced a heart attack later that month. He converted to Catholicism that summer because of the lifelong urging of his wife, Mara's death, and his enduring friendship with Rooney, a practicing Catholic. Bell was advised by his doctor to avoid going to football games, to which he quipped, "I'd rather die watching football than in my bed with my boots off." Bell and his children attended an Eagles game on October 11 at Franklin Field against the Steelers (both his old teams). The Eagles held complimentary box seats for him and guests to watch the game, but he preferred to buy his own tickets and sit with the other fans. Sitting behind the end zone during the fourth quarter of the game, he suffered a fatal heart attack and died later that day at the nearby university hospital. League Treasurer Austin Gunsel was named interim NFL commissioner for the rest of the season.
Afterwards, he was remembered as "a man of buoyant joviality, with a rough and ready wit, laughter and genuine humility and honesty, clearly innocent of pretense and [pretension]." His funeral was held at Narberth's St. Margaret Roman Catholic Church and Monsignor Cornelius P. Brennan delivered the eulogy, as close friends and admirers attended the mass. Dominic Olejniczak and all the extant owners of the NFL franchises were pallbearers. Bell was interred at Calvary Cemetery in West Conshohocken, Pennsylvania, northwest of Philadelphia.
Bell was inducted into the Professional Football Hall of Fame, the Penn Athletics Hall of Fame, the Philadelphia Sports Hall of Fame, and Haverford's Athletic Hall of Fame. The Maxwell Football Club, which he founded in 1937, has presented the best NFL player of the year with the Bert Bell Award since 1959. The Bert Bell Benefit Bowl was exhibited in his honor from 1960 through 1969.
Though his career spanned the desegregation and reintegration of the NFL, as an owner, he never had an African American on any of his teams, but Bert Jr. believed the mere discussion of whether his father was prejudiced was absurd. Bell's handling of the merger with the AAFC was acclaimed as a personal triumph. Although he did not have the wherewithal to prevent the wholesale betting on games, he was proactive in ensuring games were not tampered with by gamblers, and he created the foundation of the contemporary NFL anti-gambling policy.
Bell was criticized as being too strict with his refusal to let sold-out games to be televised locally. Nevertheless, his balancing of television broadcasts against protecting game attendance made the NFL the "healthiest professional sport in America", and he was the "leading protagonist in pro football's evolution into America's major sport." He had understood that the league needed a cooperative television contract with revenue-sharing, but he failed to overcome the obstacles to achieve it. He was portrayed by sportswriters as ensuring the owners treated the players fairly, and his decision to recognize the NFLPA in the face of adversity from owners was a "master stroke" in thwarting Congressional intervention. After he initiated terms for a pension plan with the players in 1959, little progress was made with the NFLPA, however, the first players' pension plan-the Bert Bell National Football League Retirement Plan, was approved in 1962.
Bell's implementation of the draft did not show immediate results, but it was "the single greatest contributor to the [league]'s prosperity" in its first eighty-four years. His original version of the draft was later ruled unconstitutional, but his anchoring of the success of the league to competitive balance has been "hailed by contemporaries and sports historians". Bell had often said, "[o]n any given Sunday, any team in the NFL can beat any other team."

</doc>
<doc id="4893" url="https://en.wikipedia.org/wiki?curid=4893" title="Bob Costas">
Bob Costas

Robert Quinlan Costas (born March 22, 1952) is an American sportscaster who is known for his long tenure with NBC Sports, from 1980 through 2019. He has received several Emmy awards for his work. He was the prime-time host of 11 Olympic Games from 1992 until 2016. He is employed by MLB Network, where he does play-by-play and once hosted an interview show called "Studio 42 with Bob Costas".
Costas was born in Queens, New York City, and grew up in Commack, New York. He is the son of Jayne (Quinlan), of Irish descent, and John George Costas, an electrical engineer of Greek descent. His father's ancestry can be traced back to the island of Kalymnos in the Aegean Sea in Greece. As Costas stated on Ken Burns' "Baseball", he had a very poor relationship with his father. Costas graduated from Commack High School South and attended Syracuse University in Syracuse, New York. He graduated with a communications degree in 1974 from their S.I. Newhouse School of Public Communications.
In 1973, Costas began his professional career at WSYR TV and radio in Syracuse while still completing his communications degree at the S.I. Newhouse School of Public Communications. His sportscasting career began while attending Syracuse University, serving as an announcer for the Syracuse Blazers minor-league hockey team playing in the Eastern Hockey League and North American Hockey League.
After graduating in 1974 at the age of 22, Costas went to KMOX radio in St. Louis, Missouri, calling play-by-play for the Spirits of St. Louis of the American Basketball Association in 1974. He was a prominent contributor to the ABA book "Loose Balls: The Short, Wild Life of the American Basketball Association". He is extensively quoted on many topics. The book includes his reflections of ABA life during his tenure as radio voice of the Spirits of St. Louis.
Later, Costas would call Missouri Tigers basketball and co-host KMOX's "Open Line" call-in program. He did play-by-play for Chicago Bulls broadcasts on WGN-TV during the 1979–1980 NBA season. From 1978 to 1981, Costas worked as a fill-in play-by-play man on St. Louis Blues radio broadcasts on KMOX whenever the regular play-by-play announcer, Dan Kelly, was unavailable. He was also employed by CBS Sports as a regional CBS NFL and CBS NBA announcer from 1976 to 1979, after which he moved to NBC.
In 1980, Costas was hired by NBC. Don Ohlmeyer, who at the time ran the network's sports division, told 28-year-old Costas he looked like a 14-year-old. Costas would recite this anecdote during an appearance on "Late Night with Conan O'Brien". Ohlmeyer based his reaction on Costas' modest stature (Costas is ) and boyish, baby-faced appearance.
For many years, Costas hosted NBC's National Football League coverage and NBA coverage. He also did play-by-play for National Basketball Association and Major League Baseball coverage. With the introduction of the NBC Sports Network, Costas also became the host of the new monthly interview program "Costas Tonight".
On March 30, 2015, it was announced that Costas would join forces with Marv Albert (blow-by-blow) and Al Michaels (host) on the April 11, 2015, edition of NBC's primetime "PBC on NBC" boxing series. Costas was added to serve as a special contributor for the event from Barclays Center in Brooklyn. He would narrate and write a feature on the storied history of boxing in New York City.
Costas has also hosted NBC's coverage of the U.S. Open golf tournament from 2003 to 2014.
For baseball telecasts, Costas teamed with Sal Bando (1982), Tony Kubek (from 1983 to 1989), and Joe Morgan and Bob Uecker (from 1994 to 2000). One of his most memorable broadcasts occurred on June 23, 1984 (in what would go down in baseball lore as "The Sandberg Game"). Costas, along with Tony Kubek, was calling the Saturday baseball "Game of the Week" from Chicago's Wrigley Field. The game between the Chicago Cubs and St. Louis Cardinals in particular was cited for putting Ryne Sandberg (as well as the 1984 Cubs in general, who would go on to make their first postseason appearance since 1945) "on the map". In the ninth inning, the Cubs, trailing 9–8, faced the premier relief pitcher of the time, Bruce Sutter. Sandberg, then not known for his power, slugged a home run to left field against the Cardinals' ace closer. Despite this dramatic act, the Cardinals scored two runs in the top of the tenth. Sandberg came up again in the tenth inning, facing a determined Sutter with one man on base. Sandberg then shocked the national audience by hitting a second home run, even farther into the left field bleachers, to tie the game again. The Cubs went on to win in the 11th inning. When Sandberg hit that second home run, Costas said, "Do you believe it?!" The Cardinals' Willie McGee also hit for the cycle in the same game.
While hosting Game 4 of the 1988 World Series between the Los Angeles Dodgers and Oakland Athletics on NBC, Costas angered many members of the Dodgers (especially the team's manager, Tommy Lasorda) by commenting before the start of the game that the Dodgers quite possibly were about to put up the weakest-hitting lineup in World Series history. That comment ironically fired up the Dodgers' competitive spirit. Later (while being interviewed by NBC's Marv Albert), after the Dodgers had won Game4 (en route to a 4–1 series victory), Lasorda sarcastically suggested the MVP of the 1988 World Series should be Bob Costas.
Besides calling the 1989 American League Championship Series for NBC, Costas also filled in for a suddenly ill Vin Scully, who had come down with laryngitis, for Game2 of the 1989 National League Championship Series alongside Tom Seaver. Game2 of the NLCS took place on Thursday, October 5, which was an off day for the ALCS. NBC then decided to fly Costas from Toronto to Chicago to substitute for Scully on Thursday night. Afterward, Costas flew back to Toronto, where he resumed work on the ALCS the next night.
Costas anchored NBC's pre- and post-game shows for NFL broadcasts and the pre and post-game shows for numerous World Series and Major League Baseball All-Star Games during the 1980s (the first being for the 1982 World Series). Costas did not get a shot at doing play-by-play (as the games on NBC were previously called by Vin Scully) for an All-Star Game until 1994 and a World Series until 1995 (when NBC split the coverage with ABC under "The Baseball Network" umbrella), when NBC regained Major League Baseball rights after a four-year hiatus (when the broadcast network television contract moved over to CBS, exclusively). It was not until 1997 when Costas finally got to do play-by-play for a World Series from start to finish. Costas ended up winning a Sports Emmy Award for Outstanding Sports Personality, Play-by-Play.
In 1999, Costas teamed with his then-NBC colleague Joe Morgan to call two weekday night telecasts for ESPN. The first was on Wednesday, August 25 with Detroit Tigers playing against the Seattle Mariners. The second was on Tuesday, September 21 with the Atlanta Braves playing against the New York Mets.
On August 3, 2019, Costas alongside Paul O'Neill and David Cone called both games of a double-header between the New York Yankees and Boston Red Sox for the YES Network. Costas was filling in for Michael Kay, who was recovering from vocal cord surgery.
In November 2017, it was announced that Costas would alongside Krista Voda, co-anchor NBC's pre-race coverage leading into the NASCAR Cup Series finale from Homestead. In addition to hosting pre-race coverage, Costas would conduct a live interview with incoming NBC broadcaster Dale Earnhardt Jr., who was running his final race.
When NBC gained the NBA network contract from CBS in 1990, Costas hosted the telecasts and was teamed in the studio with ex-Lakers coach Pat Riley. He also hosted the studio program "Showtime" and did play-by-play for the 1991 All-Star Game. In 1997, Costas began a three-year stint as the lead play-by-play man for "The NBA on NBC". NBC enlisted Costas' services after they were forced to (temporarily) remove Marv Albert from their broadcasts due to lingering personal and legal problems at the time. Costas teamed with Isiah Thomas and Doug Collins for NBA telecasts from 1997 to 2000. He stepped aside following the 2000 NBA Finals in favor of a returning Albert. While this, in essence, ended his active role on the "NBA on NBC" program (by this point, Hannah Storm and briefly, Ahmad Rashād had replaced Costas on studio anchoring duties), Costas would return to do play-by-play for selected playoff games. He returned to call some games of the 2002 NBA Playoffs after Albert was injured in a car accident two days before the playoffs. Costas also co-anchored (with Hannah Storm) NBC's NBA Finals coverage in 2002, which was their last to-date (before the NBA's network television contract moved to ABC).
NBC Sports allowed Costas to opt out from having to cover the XFL. He publicly denigrated the league throughout its existence and remains a vocal critic of the league and its premise.
In 2006, Costas returned to NFL studio hosting duties for NBC's new "Sunday Night Football", hosting its pre-game show "Football Night in America". Costas last hosted NFL telecasts for NBC in 1992 before being replaced in the studio by Jim Lampley and subsequently, Greg Gumbel. Before becoming the studio host for "The NFL on NBC" in 1984, Costas did play-by-play of NFL games with analyst Bob Trumpy.
Costas is nicknamed "Rapping Roberto" by New York City's "Daily News" sports media columnist Bob Raissman. Al Michaels also called him "Rapping Roberto" during the telecast between the Indianapolis Colts and the New York Giants on September 10, 2006, in response to Costas calling him "Alfalfa".
Costas hosted NBC's coverage of the 2008, 2009, and the 2010 NHL Winter Classic. He was scheduled to host coverage of the 2011 event as well but, due to the game's postponement, Costas hosted only pre-game coverage before leaving to go to Seattle for his duties with NBC's NFL coverage the next night. He hosted the event in 2012 as well as a post-game edition of "NHL Live" on the NBC Sports Network.
Costas has frontlined many Olympics broadcasts for NBC. They include the 1988 Winter Olympics in Calgary and 1988 Summer Olympics in Seoul, Barcelona in 1992, Atlanta in 1996, Sydney in 2000, Salt Lake City in 2002, Athens in 2004, Torino in 2006, Beijing in 2008, Vancouver in 2010, London in 2012, Sochi in 2014 and Rio in 2016. He discusses his work on the Olympic telecasts extensively in a book by Andrew Billings entitled "Olympic Media: Inside the Biggest Show on Television". A personal influence on Costas has been legendary ABC Sports broadcaster Jim McKay, who hosted many Olympics for ABC from the 1960s to the 1980s.
During the 1992 Barcelona and 1996 Atlanta Opening Ceremonies, Costas's remarks on China's teams' possible drug use caused an uproar among the American Chinese and international communities. Thousands of dollars were raised to purchase ads in "The Washington Post" and Sunday "The New York Times", featuring an image of the head of a statue of Apollo and reading: "Costas Poisoned Olympic Spirit, Public Protests NBC". However, Costas' comments were made subsequent to the suspension of Chinese coach Zhou Ming after seven of his swimmers were caught using steroids in 1994. Further evidence of Chinese athletes' drug use came in 1997 when Australian authorities confiscated 13 vials of Somatropin, a human growth hormone, from the bag of Chinese swimmer Yuan Yuan upon her arrival for the 1997 World Swimming Championships. At the World Championships, four Chinese swimmers tested positive for the banned substance Triamterene, a diuretic used to dilute urine samples to mask the presence of anabolic steroids. Including these failed drug tests, 27 Chinese swimmers were caught using performance-enhancing drugs from 1990 through 1997; more than the rest of the world combined.
Along with co-host Meredith Vieira and Matt Lauer, Costas' commentary of the 2012 Summer Olympics Opening Ceremonies came under fierce criticism, with Costas being described as making "a series of jingoistic remarks, including a joke about Idi Amin when Uganda's team appeared" and the combined commentary as being "ignorant" and "banal".
Following the Olympics, Costas appeared on Conan O'Brien's talk show and jokingly criticized his employer for its decision to air a preview of the upcoming series "Animal Practice" over a performance by The Who during the London closing ceremonies. "So here is the balance NBC has to consider: The Who, 'Animal Practice'. Roger Daltrey, Pete Townshend—monkey in a lab coat. I'm sure you'd be the first to attest, Conan, that when it comes to the tough calls, NBC usually gets 'em right," Costas said, alluding at the end to O'Brien's involvement in the 2010 "Tonight Show" conflict.
An eye infection Costas had at the start of the 2014 Winter Olympics forced him, on February 11, 2014, to cede his Olympic hosting duties to Matt Lauer (four nights) and Meredith Vieira (two nights), the first time Costas had not done so at all since the 1998 Winter Olympics (as rights were not held by NBC).
From 2001 until 2018, Costas co-hosted the Kentucky Derby. In 2009, he hosted Bravo's coverage of the 2009 Kentucky Oaks. After Costas officially departed from NBC Sports, his role on NBC's thoroughbred racing coverage was essentially filled-in by Rebecca Lowe, beginning with the 2019 Kentucky Derby.
On February 9, 2017, Costas announced during "Today" that he had begun the process of stepping down from his main on-air roles at NBC Sports, announcing in particular that he would cede his role as primetime host for NBC's Olympics coverage to Mike Tirico (who joined the network from ESPN in 2016), and that he would host Super Bowl LII as his final Super Bowl. However, Costas ultimately dropped out of the coverage entirely.
"USA Today" reported that he would similarly step down from "Football Night in America" in favor of Tirico. Costas explained that he was not outright retiring and expected to take on a role at NBC Sports similar to that of Tom Brokaw, being an occasional special correspondent to the division. He explained that his decision "opens up more time to do the things that I feel I'm most connected to; there will still be events, features, and interviews where I can make a significant contribution at NBC, but it will also leave more time for baseball (on MLB Network), and then, at some point down the road, I'll have a chance to do more of the long-form programming I enjoy." Costas told "USA Today" his gradual retirement was planned in advance, and that he did not want to announce it during the 2016 Summer Olympics or the NFL season because it would be too disruptive, and joked: "I'm glad that Sochi wasn't the last one. You wouldn't want your pink-eye Olympics to be your last Olympics."
Costas's final major on-air broadcast for NBC was hosting the 2018 Belmont Stakes, where Justify won the Triple Crown.
On January 15, 2019, it was announced that Costas had officially departed from NBC Sports after forty years.
Costas hosted the syndicated radio program "Costas Coast to Coast" from 1986 to 1996, which was revived as "Costas on the Radio". "Costas on the Radio", which ended its three-year run on May 31, 2009, aired on 200 stations nationwide each weekend and syndicated by the Clear Channel owned Premiere Radio Networks. During that period, Costas also served as the imaging voice of Clear Channel-owned KLOU in St. Louis, Missouri, during that station's period as "My 103.3". Like "Later", Costas' radio shows have focused on a wide variety of topics and have not been limited to sports discussion.
Costas hosted "Later with Bob Costas" on NBC from 1988 until 1994. This late night show created by Dick Ebersol, coming on at 1:30 a.m. as the third program in NBC's nightly lineup after "The Tonight Show Starring Johnny Carson" and "Late Night with David Letterman", was something of a break from the typical TV talk show format of the era, featuring Costas and a single guest conversing for the entire half hour, without a band, opening monologue or studio audience. On several occasions, Costas held the guest over for multiple nights. The show was taped in GE Building's studios 3B or 8H at the Rockefeller Plaza, with Costas interviewing the guest for 45 minutes to an hour before turning the material over to editors who condensed it down to 22 minutes plus commercial breaks. More popular guests were given two- or three-part interviews; in August 1991 Mel Brooks was the "only" guest on "four" consecutive nights. The program was critically acclaimed and twice nominated for Emmys during its -year run, winning the Emmy for Best Informational Series in 1993.
Costas decided to leave "Later" after six seasons, having grown tired of the commute to New York City from his home in St. Louis and wishing to lighten his workload in order to spend more time with his family. He also turned down an offer from David Letterman, who moved to CBS in 1995, to follow him there and become the first host of "The Late Late Show", which was being developed by Letterman's company to air at 12:30 after the "Late Show with David Letterman".
In June 2005, Costas was named by CNN president Jonathan Klein as a regular substitute anchor for Larry King's "Larry King Live" for one year. Costas, as well as Klein, have said Costas was not trying out for King's position on a permanent basis. Nancy Grace was also named a regular substitute host for the show. On August 18, 2005, Costas refused to host a "Larry King Live" broadcast where the subject was missing teenager Natalee Holloway. Costas said that because there were no new developments in the story, he felt it had no news value, and he was uncomfortable with television's drift in the direction of tabloid-type stories.
Beginning in October 2011, Costas was a correspondent for "Rock Center with Brian Williams". He gained acclaim for his November 2011 live interview of former Pennsylvania State University assistant coach Jerry Sandusky concerning charges of sexual abuse of minors, in which Sandusky called in to deny the charges.
Costas hosted a monthly talk show "Costas Tonight" on NBC Sports Network.
In 2001, Costas was hired by HBO to host a 12-week series called "On the Record with Bob Costas". "On the Record with Bob Costas" was similar to the format of the old "Later" program as they both concentrated on in-depth interviews. In 2005, "On the Record with Bob Costas" was revamped to become "Costas Now", a monthly issue-oriented sports program that occasionally employed a town hall style format.
In 2002, Costas began a stint as co-host of HBO's long-running series "Inside the NFL". Costas remained host of "Inside the NFL" through the end of the 2007 NFL season. He hosted the show with Cris Collinsworth and former NFL legends Dan Marino and Cris Carter. The program aired each week during the NFL season.
Costas left HBO to sign with MLB Network in February 2009.
At the channel's launch on January 1, 2009, Costas hosted the premiere episode of "All Time Games", a presentation of the recently discovered kinescope of Game5 of the 1956 World Series. During the episode, he held a forum with Don Larsen, who pitched MLB's only postseason perfect game during that game, and Yogi Berra, who caught the game.
Costas joined the network full-time on February 3, 2009. He hosted a regular interview show titled "MLB Network Studio 42 with Bob Costas" as well as special programming and provides play-by-play for select live baseball game telecasts. In 2017, Costas called Game1 of the American League Division Series between the Boston Red Sox and the Houston Astros on MLB Network. The Astros went on to win 8–2. Unfortunately, Costas and his color commentator Jim Kaat received criticism for their "bantering about minutia" and misidentification of plays. Costas also went on to become an internet meme after using the term the "sacks were juiced" to describe the bases being loaded.
As aforementioned, Costas hosted "Thursday Night Football" on NBC and NFL Network in 2016, having returned to broadcasting after a brief absence. He was replaced by Liam McHugh in 2017.
Costas provided significant contributions to the Ken Burns, PBS mini series "Baseball" as well as its follow-up "The 10th Inning". He also appears in another PBS film, "A Time for Champions", produced by St. Louis's Nine Network of Public Media.
In July 2020, it was announced that Costas would join CNN as a contributor. According to CNN, Costas would provide commentary "on a wide range of sports-related issues as the industry adapts to new challenges posed by the coronavirus and the frequent intersection of sports with larger societal issues." Costas, who would continue working on MLB Network, said of joining CNN: “CNN’s willingness to devote time and attention to sports related topics, makes it a good fit for me.”
June 23, 1984: Costas called NBC's "Game of the Week" with Tony Kubek, where Ryne Sandberg hit two separate home runs in the 9th and 10th innings against Bruce Sutter to tie the game. This game is known as "The Sandberg Game".
Costas's call of the first home run:
Into left center field, and deep. This is a tie ball game!
Costas's call of the second home run:
Costas: 1–1 pitch. [Sandberg swings] <br>
Kubek: OHHH BOY! <br>
Costas: [Over Kubek] And he hits it to deep left center! Look out! Do you believe it, it's gone! We will go to the 11th, tied at 11.
October 26, 1997: Costas called Game 7 of the 1997 World Series, where Édgar Rentería hit a walk off single to give the Marlins their first World Series championship. Costas's call:
The 0–1 pitch. A liner... off Nagy's glove, into center field. The Florida Marlins have won... the World Series.
June 14, 1998: Costas called Game 6 of the 1998 NBA Finals, Michael Jordan and Phil Jackson's final game with the Chicago Bulls where Jordan hit a 20 foot jumpshot to put the Bulls up 87–86 with 5.2 seconds remaining. The Bulls would win the game by that score, giving them their sixth championship and third consecutive. Costas's call:
Jordan with 43. Malone is doubled. They swat at him and steal it! Here comes Chicago. 17 seconds. 17 seconds, from Game 7, or from championship #6. Jordan, open, CHICAGO WITH THE LEAD! Timeout Utah, 5.2 seconds left. Michael Jordan, running on fumes, with 45 points.
June 4, 2000: Costas called Game 7 of the 2000 Western Conference Finals for NBC's NBA coverage. Kobe Bryant threw an alley oop pass to Shaquille O'Neal to give the Lakers a six-point lead with 41.3 seconds remaining. Costas's call of the play:
Portland has three timeouts left, the Lakers have two. Bryant... TO SHAQ!
September 25, 2014: Costas called Derek Jeter's final game at Yankee Stadium for MLB Network, where he hit an RBI single to win the game. Costas's call:
A base hit to right! Here comes Richardson, they're waving him home! The throw, it's close but he scores! On a walk off hit by Derek Jeter!
Costas is a devoted baseball fan. He's been suggested as a potential commissioner and wrote "Fair Ball: A Fan's Case for Baseball" in 2000. For his 40th birthday, then Oakland Athletics manager Tony La Russa allowed Costas to manage the club during a spring training game. The first time Costas visited baseball legend Stan Musial's St. Louis eatery, he left a $3.31 tip on a ten dollar tab in homage to Musial's lifetime batting average (.331). Costas delivered the eulogy at Mickey Mantle's funeral. In eulogizing Mantle, Costas described the baseball legend as ""a fragile hero to whom we had an emotional attachment so strong and lasting that it defied logic"." Costas has even carried a 1958 Mickey Mantle baseball card in his wallet. Costas also delivered the eulogy for Musial after his death in early 2013.
Costas was outspoken about his disdain for Major League Baseball instituting a playoff wild card. Costas believed it diminishes the significance and drama of winning a divisional championship. He prefers a system in which winning the wild card puts a team at some sort of disadvantage, as opposed to an equal level with teams who outplayed them over a 162-game season. Or, as explained in his book "Fair Ball", have only the three division winners in each league go to the postseason, with the team with the best record receiving a bye to the League Championship Series. Once, on the air on HBO's "Inside the NFL", he mentioned that the NFL regular season counted for something, but baseball's was beginning to lose significance. With the advent of the second wild card, Costas has said he feels the format has improved, since there is now a greater premium placed on finishing first. He has suggested a further tweak: Make the wild card round a best two of three, instead of a single game, with all three games, if necessary, on the homefield of the wild card of the better record.
He also has disdained the Designated Hitter rule, saying baseball would be a better game without it.
Costas serves as a member of the advisory board of the Baseball Assistance Team, a 501(c)(3) non-profit organization dedicated to helping former Major League, Minor League, and Negro League players through financial and medical difficulties.
On May 26, 2007, Costas discussed the presidency of George W. Bush on his radio show, stating he liked Bush personally, and had been optimistic about his presidency, but said the course of the Iraq war, and other mis-steps have led him to conclude Bush's presidency had "tragically failed" and considered it "overwhelmingly evident, even if you're a conservative Republican, if you're honest about it, this is a failed administration." The following summer, Costas interviewed Bush during the president's appearance at the 2008 Summer Olympics in Beijing.
During a segment on the "Sunday Night Football" halftime show on December 2, 2012, Costas paraphrased Fox Sports columnist Jason Whitlock in regard to Jovan Belcher's murder-suicide the day prior, saying the United States' gun culture was causing more domestic disputes to result in death, and that it was likely Belcher and his girlfriend would not have died had he not possessed a gun.
Critics interpreted his remarks as support for gun control, resulting in mostly negative reactions. Many (including former Republican Presidential candidates Mike Huckabee and Herman Cain) felt Costas should not have used a program typically viewed as entertainment to publicize political views on sensitive topics, Lou Dobbs criticized his remarks for supporting the abolition of the Second Amendment by quoting a sports writer, while Andrew Levy remarked that he had been given a civics lecture by someone who had "gotten rich thanks in part to a sport that destroys men's bodies and brains". However, liberal reporter Erik Wemple of "The Washington Post" praised Costas for speaking out for gun control on the broadcast, commenting that the incident's connection to the NFL provided him with an obligation to acknowledge the incident during the halftime show, stating that "the things that [NFL players] do affect the public beyond whether their teams cover the point spread. And few cases better exemplify that dynamic as powerfully as the Belcher incident."
During the following week, Costas defended his remarks in an appearance on MSNBC's program "The Last Word with Lawrence O'Donnell", where he said the remarks were related to the country's gun culture, and not about gun control as critics had inferred. Costas did suggest that more regulation be placed on America's gun culture:
Now, do I believe that we need more comprehensive and more sensible gun control legislation? Yes I do. That doesn't mean repeal the Second Amendment. That doesn't mean a prohibition on someone having a gun to protect their home and their family. It means sensible and more comprehensive gun control legislation. But even if you had that, you would still have the problem of what Jason Whitlock wrote about, and what I agree with. And that is a gun culture in this country.
During his coverage of the 2014 Winter Olympics, Costas was criticized by some conservative members of the media, including Michelle Malkin and Glenn Beck for supposedly praising Vladimir Putin's role in defusing tensions surrounding Syria, and Iran. Several media commentators, including Bill O'Reilly and Bernard Goldberg, defended Costas' remarks as factually correct and pointed out that Costas had also voiced considerable criticism of both Russia and Putin while broadcasting from Sochi. During an interview on Fox News Goldberg said "...the idea that Costas somehow portrayed Vladimir Putin as a benign figure is ridiculous." Costas defended himself on O'Reilly's broadcast on March 3, reiterating that he criticized Putin immediately preceding, and following, the statements that were questioned. O'Reilly then aired a portion of an Olympic commentary in which Costas was pointedly critical of the Russian leader. Costas also indicated that Senator John McCain, who had been among those who had initially criticized Costas, had called Costas to apologize after hearing the full segment in context.
While visiting the University of Maryland in November 2017 for a roundtable discussion on various sports topics, Costas said the sport of football was in a decline, with evidence mounting that the repetition of concussions "destroys people's brains" and he wouldn't allow a son with athletic talent to play it. Costas had been scheduled to work Super Bowl LII, his eighth as a host (despite stepping down from "Football Night in America" in favor of his successor Mike Tirico, Costas was to return while Tirico prepped to lead NBC's coverage of the 2018 Winter Olympics, set to begin a few days later). However, the network announced shortly before the game that Liam McHugh would instead join Dan Patrick as a co-host, leading to speculation that NBC removed Costas from the NFL's biggest game over his comments. Costas originally denied such, saying it made more sense for McHugh, who had been hosting Thursday night games on NBC, to serve in that capacity. However, he later admitted in an interview with ESPN's "Outside the Lines" that the comments were indeed the basis of his removal, ultimately resulting in his departure from the network after forty years.
Costas was married from 1983 to 2001 to Carole "Randy" Randall Krummenacher. They had two children, son Keith (born 1986) and daughter Taylor (born 1989). Costas once jokingly promised Minnesota Twins center fielder Kirby Puckett that, if he was batting over .350 by the time his child was born, he would name the baby Kirby. Kirby was hitting better than .350, but Bob's son initially was not given a first (or second) name of Kirby. After Puckett reminded Costas of the agreement, the birth certificate was changed to "Keith Michael Kirby Costas".
On March 12, 2004, Costas married his second wife, Jill Sutton. Costas and his wife now reside primarily in New York, but he has often said he thinks of St. Louis as his hometown.
Costas's children have also won Sports Emmys; Keith has won two as an associate producer on MLB Network's "MLB Tonight", and Taylor as an associate producer on NBC's coverage of the 2012 Summer Olympics.
Costas has won eight National Sportcaster of the Year awards from the National Sportscasters and Sportswriters Association. He was inducted into that organization's Hall of Fame in 2012. He has also won four Sportscaster of the Year awards from the American Sportscasters Association and well over twenty Sports Emmy Awards for announcing. He is the only person in television history to have won Emmys for Sports, News (Sandusky interview), "and" Entertainment ("Later").
In 1995, Costas received a star on the St. Louis Walk of Fame. In 1999, he was a recipient of the Curt Gowdy Media Award from the Basketball Hall of Fame, which is awarded to members of the electronic and print media for outstanding contributions to the sport. In 2000, he won a "TV Guide" Award for Favorite Sportscaster. In 2001, Syracuse University honored Costas with the George Arents Award, SU's highest alumni honor, for "Excellence in Sports Broadcasting". He was selected as the Dick Schaap Award for Outstanding Journalism recipient in 2004. In 2006, he was also awarded an honorary doctorate in humane letters from Loyola College in Maryland. In 2012, Costas was awarded the Walter Cronkite Award for Excellence in Journalism. In 2013, the S.I. Newhouse School of Public Communications honored him with the first Marty Glickman Award for Leadership in Sports Media.
On December 13, 2017, it was announced that Costas would receive the Ford C. Frick Award from the National Baseball Hall of Fame on July 28, 2018. In August 2018, the Sports Broadcasting Hall of Fame announced that Costas would be inducted to its body at a ceremony on December 11, 2018 honoring ten other sports figures including Dick Vitale, Jim Nantz and Bud Greenspan.
Costas is an honorary trustee of Webster University, a private college located in Webster Groves, Missouri. He is a frequent supporter of the school and has been in numerous radio commercials for them. He is also an honorary board member of the Multiple Myeloma Research Foundation.
In 1994, Costas appeared as the play-by-play announcer for the World Series (working alongside Tim McCarver) in the movie "The Scout". In 1998, he appeared as himself along with his rival/counterpart Al Michaels (who now works for NBC) from ABC in the movie "BASEketball". Costas voiced an animated car version of himself, Bob Cutlass, in the movies "Cars" (2006) and "Cars 3" (2017). He also appeared as himself in the 2001 movie "Pootie Tang", where he remarks that he saw "the longest damn clip ever".
Costas' voice appeared in the 2011 documentary film "Legendary: When Baseball Came to the Bluegrass", which detailed the humble beginnings of the Lexington Legends, a minor league baseball team located in Lexington, Kentucky.
Costas has been alluded to several times in popular music. The songs "Mafioso" by Mac Dre, "We Major" by Domo Genesis and "The Last Huzzah" by Mr. Muthafuckin' eXquire, all refer to Costas. He was also mentioned in a Ludacris song after Costas mentioned the rapper on the late night talk show "Last Call with Carson Daly".
In June 2013, Costas provided the voice of God in the Monty Python musical "Spamalot" at The Muny Repertory in St. Louis.
Apart from his normal sportscasting duties, Costas has also presented periodic sports blooper reels, and announced dogsled and elevator races, on "Late Night with David Letterman".
In 1985, Costas appeared on "The War to Settle the Score", a pre-"WrestleMania" program that the World Wrestling Federation aired on MTV.
In 1993, Costas hosted the "pregame" show for the final episode of "Cheers". Costas once appeared on the television program "NewsRadio" as himself. He hosted an award show and later had some humorous encounters with the crew of WNYX. Costas also once appeared as a guest on the faux talk show cartoon "Space Ghost Coast to Coast". He also had a recurring guest role as himself on the HBO series "Arli$$".
Costas has been impersonated several times by Darrell Hammond on "Saturday Night Live". Costas was "supposed" to appear in the fourth-season premiere of "Celebrity Deathmatch" (ironically titled "Where is Bob Costas?") as a guest-commentator, but about halfway through the episode it was revealed that John Tesh had killed him before the show to take his place.
On June 13, 2008, Costas appeared on MSNBC's commercial-free special coverage of "Remembering Tim Russert (1950–2008)".
On January 30, 2009, Costas guest-starred as himself on the television series "Monk" in an episode titled "Mr. Monk Makes the Playoffs"'. He mentions to Captain Stottlemeyer about how Adrian Monk once helped him out of a problem several years ago with regards to a demented cat salesman. He apparently sold Costas a cat that allegedly tried to kill him with a squeeze toy. (In fact when he signs off he says, "The cat was definitely trying to kill me.")
Costas guest-voiced as himself in 2010 "Simpsons" episode, "Boy Meets Curl", when Homer and Marge make the U.S. Olympic curling team. Costas also guest-voiced as himself on the "Family Guy" episode "Turban Cowboy" in an interview with Peter after he wins the Boston Marathon by hitting everyone with his car.
On February 11, 2010, Stephen Colbert jokingly expressed his desire to stab Costas with an ice pick at the upcoming 2010 Winter Olympics in Vancouver so Colbert could take over as host. Costas later made a cameo appearance on the February 25, 2010, edition of Colbert's show.
In January 2013, Costas appeared as himself in the "Go On" episode "Win at All Costas" with Matthew Perry, wherein Ryan King auditions with him for a TV Show.
Real footage of Costas from NBC's pregame show before Game5 of the 1994 NBA Finals was used in the second episode of "".
Costas appeared on the September 22, 2017 episode of "Real Time with Bill Maher" to discuss issues such as concussions and the role of political activism in professional sports (namely by Colin Kaepernick).
In 2002, Costas was the play-by-play announcer, alongside Harold Reynolds, for "Triple Play 2002" during the ballgame for PlayStation 2 and Xbox.
 
 

</doc>
<doc id="4896" url="https://en.wikipedia.org/wiki?curid=4896" title="Bamberg">
Bamberg

Bamberg (, , ) is a town in Upper Franconia, Germany, on the river Regnitz close to its confluence with the river Main. The town dates back to the 9th century, when its name was derived from the nearby "" castle. Cited as one of Germany's most beautiful towns, its Old Town has been a UNESCO World Heritage Site since 1993, with Bamberg being home to Europe's largest intact old city wall.
Starting in the 10th century and leading onwards, Bamberg became a key link with the Slav peoples, notably those of Poland and Pomerania. It experienced a period of great prosperity from the 12th century onwards, during which time it was briefly the centre of the Holy Roman Empire. Emperor Henry II was also buried in the old town, alongside his wife Kunigunde. The town's architecture from this period strongly influenced that in Northern Germany and Hungary. From the middle of the 13th century onward, the bishops were princes of the Empire and ruled Bamberg, overseeing the construction of monumental buildings. This growth was complemented by the obtaining of large portions of the estates of the Counts of Meran in 1248 and 1260 by the see, partly through purchase and partly through the appropriation of extinguished fiefs. 
Bamberg lost its independence in 1802, following the secularization of church lands, becoming part of Bavaria in 1803. The town was first connected to the German rail system in 1844, which has been an important part of its infrastructure ever since. After a communist uprising took control over Bavaria in the years following World War I, the state government fled to Bamberg and stayed there for almost two years before the Bavarian capital of Munich was retaken by "Freikorps" units (see Bavarian Soviet Republic). The first republican constitution of Bavaria was passed in Bamberg, becoming known as the "Bamberger Verfassung" (Bamberg Constitution).
Following the Second World War, Bamberg was an important base for the Bavarian, German, and then American military stationed at Warner Barracks, only closing in 2014.
During the post-Roman centuries of Germanic migration and settlement, the region later included in the Diocese of Bamberg was inhabited for the most part by Slavs. The town, first mentioned in 902, grew up by the castle "" which gave its name to the Babenberg family. On their extinction it passed to the Saxon house. The area was Christianized chiefly by the monks of the Benedictine Fulda Abbey, and the land was under the spiritual authority of the Diocese of Würzburg.
In 1007, Holy Roman Emperor Henry II made Bamberg a family inheritance, the seat of a separate diocese. The Emperor's purpose in this was to make the Diocese of Würzburg less unwieldy in size and to give Christianity a firmer footing in the districts of Franconia, east of Bamberg. In 1008, after long negotiations with the Bishops of Würzburg and Eichstätt, who were to cede portions of their dioceses, the boundaries of the new diocese were defined, and Pope John XVIII granted the papal confirmation in the same year. Henry II ordered the building of a new cathedral, which was consecrated 6 May 1012. The church was enriched with gifts from the pope, and Henry had it dedicated in honor of him. In 1017 Henry also founded Michaelsberg Abbey on the Michaelsberg ("Mount St. Michael"), near Bamberg, a Benedictine abbey for the training of the clergy. The emperor and his wife Kunigunde gave large temporal possessions to the new diocese, and it received many privileges out of which grew the secular power of the bishop. Pope Benedict VIII visited Bamberg in 1020 to meet Henry II for discussions concerning the Holy Roman Empire. While he was here he placed the diocese in direct dependence on the Holy See. He also personally consecrated some of Bamberg's churches. For a short time Bamberg was the centre of the Holy Roman Empire. Henry and Kunigunde were both buried in the cathedral.
From the middle of the 13th century onward the bishops were princes of the Empire and ruled Bamberg, overseeing the construction of monumental buildings. In 1248 and 1260 the see obtained large portions of the estates of the Counts of Meran, partly through purchase and partly through the appropriation of extinguished fiefs. The old Bishopric of Bamberg was composed of an unbroken territory extending from Schlüsselfeld in a northeasterly direction to the Franconian Forest, and possessed in addition estates in the Duchies of Carinthia and Salzburg, in the Nordgau (the present Upper Palatinate), in Thuringia, and on the Danube. By the changes resulting from the Reformation, the territory of this see was reduced nearly one half in extent. Since 1279 the coat of arms of the city of Bamberg is known in form of a seal.
The witch trials of the 17th century claimed about one thousand victims in Bamberg, reaching a climax between 1626 and 1631, under the rule of Prince-Bishop Johann Georg II Fuchs von Dornheim. The famous "Drudenhaus" (witch prison), built in 1627, is no longer standing today; however, detailed accounts of some cases, such as that of Johannes Junius, remain.
In 1647, the University of Bamberg was founded as ".
Bambrzy (") are German Poles who are descended from settlers from the Bamberg area who settled in villages around Poznań in the years 1719–1753.
In 1759, the possessions and jurisdictions of the diocese situated in Austria were sold to that state. When the secularization of church lands took place (1802) the diocese covered and had a population of 207,000. Bamberg thus lost its independence in 1802, becoming part of Bavaria in 1803.
Bamberg was first connected to the German rail system in 1844, which has been an important part of its infrastructure ever since. After a communist uprising took control over Bavaria in the years following World War I, the state government fled to Bamberg and stayed there for almost two years before the Bavarian capital of Munich was retaken by "Freikorps" units (see Bavarian Soviet Republic). The first republican constitution of Bavaria was passed in Bamberg, becoming known as the "Bamberger Verfassung" (Bamberg Constitution).
In February 1926 Bamberg served as the venue for the Bamberg Conference, convened by Adolf Hitler in his attempt to foster unity and to stifle dissent within the then-young Nazi party. Bamberg was chosen for its location in Upper Franconia, reasonably close to the residences of the members of the dissident northern Nazi faction but still within Bavaria.
In 1973, the town celebrated the 1,000th anniversary of its founding.
Bamberg is located in Franconia, north of Nuremberg by railway and east of Würzburg, also by rail. It is situated on the Regnitz river, before it flows into the Main river.
Its geography is shaped by the Regnitz and by the foothills of the Steigerwald, part of the German uplands. From northeast to southwest, the town is divided into first the Regnitz plain, then one large and several small islands formed by two arms of the Regnitz ('), and finally the part of town on the hills, the "Hill Town" (').
Bamberg extends over seven hills, each crowned by a beautiful church. This has led to Bamberg being called the "Franconian Rome" — although a running joke among Bamberg's tour guides is to refer to Rome instead as the "Italian Bamberg". The hills are Cathedral Hill, Michaelsberg, Kaulberg/Obere Pfarre, Stefansberg, Jakobsberg, Altenburger Hill and Abtsberg.
Climate in this area has mild differences between highs and lows, and there is adequate rainfall year-round. The Köppen climate classification subtype for this climate is "" (Marine West Coast Climate/Oceanic climate), with a certain continental influence as indicated by average winter nighttime temperatures well below zero.
In 2013 (latest data available) the GDP per inhabitant was €56,723. This places the district 10th out of 96 districts (rural and urban) in Bavaria (overall average: €39,691).
The old town of Bamberg is listed as a UNESCO World Heritage Site, primarily because of its authentic medieval appearance. The town established a documentation centre in 2005 to support World Heritage activities.
Some of the main sights are:
Bamberg Cathedral is a late Romanesque building with four towers. It was founded in 1004 by Emperor Henry II, finished in 1012 and consecrated on 6 May 1012. It was later partially destroyed by fire in 1081. The new cathedral, built by Saint Otto of Bamberg, was consecrated in 1111 and in the 13th century received its present late-Romanesque form.
The cathedral is long, wide, high, and the four towers are each about high. It contains many historic works of art, such as the marble tomb of the founder and his wife, considered one of the greatest works of the sculptor Tilman Riemenschneider, and carved between 1499 and 1513. Another treasure of the cathedral is an equestrian statue known as the Bamberg Horseman (""). This statue, possibly depicting the emperor Conrad III, most likely dates to the second quarter of the 13th century. The statue also serves as a symbol of the town of Bamberg.
The ' (New Residence) (1698–1704) was initially occupied by the prince-bishops, and from 1864 to 1867 by the deposed King Otto of Greece. Its ' (Rose Garden) overlooks the town. It has over 4500 roses.
The is located on the highest of Bamberg's seven hills. It was mentioned for the first time in 1109. Between 1251 and 1553 it was the residence of Bamberg's bishops. Destroyed in 1553 by Albert Alcibiades, Margrave of Brandenburg-Kulmbach, it was used, after scanty repairs, only as a prison, and increasingly decayed.
In 1801, A. F. Marcus bought the castle and completely repaired it. His friend, the famous German writer E.T.A. Hoffmann, who was very impressed by the building, lived there for a while. The next owner, Anton von Greifenstein, in 1818 founded an association to save the castle. This society still maintains the whole property today. The Altenburg today houses a restaurant.
Other churches are the ', an 11th-century Romanesque basilica; the '; the ' or ' (1320–1387), which has now been restored to its original pure Gothic style. The ', 12th century Romanesque (restored), on the Michaelsberg, was formerly the church of the Benedictine Michaelsberg Abbey secularized in 1803 and now contains the ', or almshouse, and the museum and municipal art collections.
Of the bridges connecting the sections of the lower town the ' was completed in 1455. Halfway across this, on an island, is the ' or town hall (rebuilt 1744–1756). The lyceum, formerly a Jesuit college, contains a natural history museum. The old palace ("") was built in 1591 on the site of an old residence of the counts of Babenberg. Monuments include the Maximilian fountain (1880), with statues of King Maximilian I of Bavaria, the emperor Henry II and his wife, Conrad III and Saint Otto, bishop of Bamberg.
There are also tunnels beneath the town. These were originally constructed as mines which supplied sandstone which could be used for construction or as an abrasive cleaner. Mining came to an end in 1920 but a tunnel network remained. The tunnels were used as an air raid shelter during World War II. A part of the network can be visited on a guided tour.
Bamberg is known for its smoked Rauchbier and is home to nine breweries, , , Brauerei Heller-Trum (Schlenkerla), Brauerei Kaiserdom, , Klosterbräu, and , and one brewpub, Ambräusianum. Every August there is a five-day "", a kirmess celebrated with beers.
The University of Bamberg, named Otto-Friedrich University, offers higher education in the areas of social science, business studies and the humanities, and is attended by more than 13,000 students. The University of Applied Sciences Bamberg offers higher education in the areas of public health. Bamberg is also home to eight secondary schools (gymnasiums):
There are also numerous other institutes for primary, secondary, technical, vocational and adult education.
The InterCityExpress main line No. 28 (Munich – Nuremberg – Leipzig – Berlin / Hamburg) runs through Bamberg station on the Nuremberg–Bamberg and the Bamberg–Hof lines. It takes less than two hours to Munich on the train and with the Nuremberg–Erfurt high-speed railway through the Thuringian mountains finished in 2017 less than three hours to Berlin.
East-west connections are poorer. Bamberg is connected to other towns in eastern Upper Franconia such as Bayreuth, Coburg, and Kronach via the Bamberg–Hof line with trains usually running at least every hour. Connections on the Würzburg–Bamberg line to the west are hourly regional trains to Würzburg, which is fully connected to the ICE network. Tourists arriving at Frankfurt International Airport can take advantage of the new direct connection from Frankfurt main station.
Bamberg is not near any of the major (i.e. single-digit) autobahns. But it is nevertheless well connected to the network in all directions: the A70 from Schweinfurt (connecting to the A7 there) to Bayreuth (connecting to the A9) runs along the northern edge of the town. The A73 on the eastern side of town connects Bamberg to Nuremberg (connecting to the A9) and Thuringia, ending at Suhl.
Bamberg is served by . Mostly public aircraft operate there. It used to be a military airport. (IATA-Code: ZCD, ICAO-Code: EDQA) It is also possible to charter public flights to and from this airport.
Most international tourists who travel by plane arrive at Frankfurt International Airport or Munich Airport. The nearest major airport is Nuremberg Airport which can be reached within half an hour by car or one hour by train and subway.
Both the Rhine-Main-Danube Canal and its predecessor, the Ludwig Canal, begin near Bamberg. The Ludwig Canal was opened in 1846 but closed in 1950 after damage during the second world war. With the completion of the Rhine-Main-Danube Canal in 1992, uninterrupted water transport was again made possible between the North Sea and the Black Sea.
Local public transport within Bamberg relies exclusively on buses. More than 20 routes connect the outlying quarters and some villages in the vicinity to the central bus station. In addition, there are several "Night Lines" (the last of these, though, tend to run around midnight) and some park-and-ride lines from parking lots on the periphery to the town centre. A short-lived tram system existed in the 1920s.
Bamberg was an important base for the Bavarian, German and then American military stationed at Warner Barracks. Warner Barracks was closed in the fall of 2014, with the last battalion leaving being the 54th Engineer Battalion and returned to the German government. In 2016, a large part of the facility was taken over by the German Federal Police for training purposes.
Muna Kasserne was a small base occupied by the 504th Maintenance Company, 71st Maintenance Bn. It was part of Warner Barracks although located separately.
Bamberg is an urban district, or "kreisfreie Stadt". Its town council ("Stadtrat") and its mayor ("Oberbürgermeister") are elected every six years, though not in the same year. Thus, the last municipal election for the town council was in 2014, for the mayor in 2012. As an exception to the six-year term, the term starting in 2012 will take eight years to synchronize the elections with those in the rest of Bavaria.
As of the elections of 16 March 2014, the 44 member strong town council comprises 12 CSU councillors, 10 SPD councillors, 8 Green councillors, 4 councillors of the "Bamberger Bürger-Block" and 4 of the "Freie Wähler" (Free Voters), both local political movements. These five parties achieved the number of councillors necessary to form a parliamentary group. In addition, there are 3 councillors of the "Bamberger Unabhängige Bürger" and the 1 councillor each of the "Bamberger Realisten", the FDP and the "Bamberger Linke Liste".
The previous council, elected on 2 March 2008, was composed of 15 CSU councillors, 10 SPD councillors, 7 Green councillors, 5 councillors of the Bamberger Bürger-Block and 3 of the Freie Wähler (Free Voters), both local political movements. These five parties achieved the number of councillors necessary to form a parliamentary group. In addition, there were 2 councillors of the "Bamberger Realisten" and one of the FDP and the Republikaner, making them ineligible for caucus status.
Bamberg is twinned with:

</doc>
<doc id="4898" url="https://en.wikipedia.org/wiki?curid=4898" title="Black cow">
Black cow

Black Cow can refer to either:

</doc>
<doc id="4900" url="https://en.wikipedia.org/wiki?curid=4900" title="Bloody Mary">
Bloody Mary

Bloody Mary originally referred to:
"Bloody Mary" may also refer to:

</doc>
