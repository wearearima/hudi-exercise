<doc id="1911" url="https://en.wikipedia.org/wiki?curid=1911" title="Allele">
Allele

An allele (, ; ; modern formation from Greek ἄλλος "állos", "other") is a variant form of a given gene, meaning it is one of two or more versions of a known mutation at the same place on a chromosome. It can also refer to different sequence variations for a several-hundred base-pair or more region of the genome that codes for a protein. Alleles can come in different extremes of size. At the lowest possible end one can be the single base choice of a single nucleotide polymorphism (SNP). At the higher end, it can be the sequence variations for the regions of the genome that code for the same protein which can be up to several thousand base-pairs long.
Sometimes, different alleles can result in different observable phenotypic traits, such as different pigmentation. A notable example of this trait of color variation is Gregor Mendel's discovery that the white and purple flower colors in pea plants were the result of "pure line" traits which could be used as a control for future experiments. However, most alleles result in little or no observable phenotypic variation.
Most multicellular organisms have two sets of chromosomes; that is, they are diploid. In this case, the chromosomes can be paired: each pair is a set of homologous chromosomes. If both alleles of a gene at the locus on the homologous chromosomes are the same, they and the organism are homozygous with respect to that gene. If the alleles are different, they and the organism are heterozygous with respect to that gene.
The word "allele" is a short form of allelomorph ("other form", a word coined by British geneticists William Bateson and Edith Rebecca Saunders), which was used in the early days of genetics to describe variant forms of a gene detected as different phenotypes. It derives from the Greek prefix ἀλληλο-, "allelo-", meaning "mutual", "reciprocal", or "each other", which itself is related to the Greek adjective ἄλλος, "allos" (cognate with Latin "alius"), meaning "other".
In many cases, genotypic interactions between the two alleles at a locus can be described as dominant or recessive, according to which of the two homozygous phenotypes the heterozygote most resembles. Where the heterozygote is indistinguishable from one of the homozygotes, the allele expressed is the one that leads to the "dominant" phenotype, and the other allele is said to be "recessive". The degree and pattern of dominance varies among loci. This type of interaction was first formally described by Gregor Mendel. However, many traits defy this simple categorization and the phenotypes are modeled by co-dominance and polygenic inheritance.
The term "wild type" allele is sometimes used to describe an allele that is thought to contribute to the typical phenotypic character as seen in "wild" populations of organisms, such as fruit flies ("Drosophila melanogaster"). Such a "wild type" allele was historically regarded as leading to a dominant (overpowering - always expressed), common, and normal phenotype, in contrast to "mutant" alleles that lead to recessive, rare, and frequently deleterious phenotypes. It was formerly thought that most individuals were homozygous for the "wild type" allele at most gene loci, and that any alternative "mutant" allele was found in homozygous form in a small minority of "affected" individuals, often as genetic diseases, and more frequently in heterozygous form in "carriers" for the mutant allele. It is now appreciated that most or all gene loci are highly polymorphic, with multiple alleles, whose frequencies vary from population to population, and that a great deal of genetic variation is hidden in the form of alleles that do not produce obvious phenotypic differences.
A population or species of organisms typically includes multiple alleles at each locus among various individuals. Allelic variation at a locus is measurable as the number of alleles (polymorphism) present, or the proportion of heterozygotes in the population. A null allele is a gene variant that lacks the gene's normal function because it either is not expressed, or the expressed protein is inactive.
For example, at the gene locus for the ABO blood type carbohydrate antigens in humans, classical genetics recognizes three alleles, I, I, and i, which determine compatibility of blood transfusions. Any individual has one of six possible genotypes (II, Ii, II, Ii, II, and ii) which produce one of four possible phenotypes: "Type A" (produced by II homozygous and Ii heterozygous genotypes), "Type B" (produced by II homozygous and Ii heterozygous genotypes), "Type AB" produced by II heterozygous genotype, and "Type O" produced by ii homozygous genotype. (It is now known that each of the A, B, and O alleles is actually a class of multiple alleles with different DNA sequences that produce proteins with identical properties: more than 70 alleles are known at the ABO locus. Hence an individual with "Type A" blood may be an AO heterozygote, an AA homozygote, or an AA heterozygote with two different "A" alleles.)
The frequency of alleles in a diploid population can be used to predict the frequencies of the corresponding genotypes (see Hardy–Weinberg principle). For a simple model, with two alleles;
where "p" is the frequency of one allele and "q" is the frequency of the alternative allele, which necessarily sum to unity. Then, "p" is the fraction of the population homozygous for the first allele, 2"pq" is the fraction of heterozygotes, and "q" is the fraction homozygous for the alternative allele. If the first allele is dominant to the second then the fraction of the population that will show the dominant phenotype is "p" + 2"pq", and the fraction with the recessive phenotype is "q".
With three alleles:
In the case of multiple alleles at a diploid locus, the number of possible genotypes (G) with a number of alleles (a) is given by the expression:
A number of genetic disorders are caused when an individual inherits two recessive alleles for a single-gene trait. Recessive genetic disorders include albinism, cystic fibrosis, galactosemia, phenylketonuria (PKU), and Tay–Sachs disease. Other disorders are also due to recessive alleles, but because the gene locus is located on the X chromosome, so that males have only one copy (that is, they are hemizygous), they are more frequent in males than in females. Examples include red-green color blindness and fragile X syndrome.
Other disorders, such as Huntington's disease, occur when an individual inherits only one dominant allele.
While heritable traits are typically studied in terms of genetic alleles, epigenetic marks such as DNA methylation can be inherited at specific genomic regions in certain species, a process termed transgenerational epigenetic inheritance. The term "epiallele" is used to distinguish these heritable marks from traditional alleles, which are defined by nucleotide sequence. A specific class of epiallele, the metastable epialleles, has been discovered in mice and in humans which is characterized by stochastic (probabilistic) establishment of epigenetic state that can be mitotically inherited.

</doc>
<doc id="1912" url="https://en.wikipedia.org/wiki?curid=1912" title="Ampicillin">
Ampicillin

Ampicillin is an antibiotic used to prevent and treat a number of bacterial infections, such as respiratory tract infections, urinary tract infections, meningitis, salmonellosis, and endocarditis. It may also be used to prevent group B streptococcal infection in newborns. It is used by mouth, by injection into a muscle, or intravenously.
Common side effects include rash, nausea, and diarrhea. It should not be used in people who are allergic to penicillin. Serious side effects may include "Clostridium difficile" colitis or anaphylaxis. While usable in those with kidney problems, the dose may need to be decreased. Its use during pregnancy and breastfeeding appears to be generally safe.
Ampicillin was discovered in 1958 and came into commercial use in 1961. It is on the World Health Organization's List of Essential Medicines. The World Health Organization classifies ampicillin as critically important for human medicine. It is available as a generic medication.
Ampicillin used to also be used to treat gonorrhea, but there are now too many strains resistant to penicillins.
Ampicillin is used to treat infections by many Gram-positive and Gram-negative bacteria. It was the first "broad spectrum" penicillin with activity against Gram-positive bacteria, including "Streptococcus pneumoniae", "Streptococcus pyogenes", some isolates of "Staphylococcus aureus" (but not penicillin-resistant or methicillin-resistant strains), "Trueperella", and some "Enterococcus". It is one of the few antibiotics that works against multidrug resistant "Enterococcus faecalis" and "E. faecium". Activity against Gram-negative bacteria includes "Neisseria meningitidis", some "Haemophilus influenzae", and some of the Enterobacteriaceae (though most Enterobacteriaceae and "Pseudomonas" are resistant). Its spectrum of activity is enhanced by co-administration of sulbactam, a drug that inhibits beta lactamase, an enzyme produced by bacteria to inactivate ampicillin and related antibiotics. It is sometimes used in combination with other antibiotics that have different mechanisms of action, like vancomycin, linezolid, daptomycin, and tigecycline.
Ampicillin can be administered by mouth, an intramuscular injection (shot) or by intravenous infusion. The oral form, available as capsules or oral suspensions, is not given as an initial treatment for severe infections, but rather as a follow-up to an IM or IV injection. For IV and IM injections, ampicillin is kept as a powder that must be reconstituted.
IV injections must be given slowly, as rapid IV injections can lead to convulsive seizures.
Ampicillin is one of the most used drugs in pregnancy, and has been found to be generally harmless both by the Food and Drug Administration in the U.S. (which classified it as category B) and the Therapeutic Goods Administration in Australia (which classified it as category A). It is the drug of choice for treating "Listeria monocytogenes" in pregnant women, either alone or combined with an aminoglycoside. Pregnancy increases the clearance of ampicillin by up to 50%, and a higher dose is thus needed to reach therapeutic levels.
Ampicillin crosses the placenta and remains in the amniotic fluid at 50–100% of the concentration in maternal plasma; this can lead to high concentrations of ampicillin in the newborn.
While lactating mothers secrete some ampicillin into their breast milk, the amount is minimal.
In newborns, ampicillin has a longer half-life and lower plasma protein binding. The clearance by the kidneys is lower, as kidney function has not fully developed.
Ampicillin is contraindicated in those with a hypersensitivity to penicillins, as they can cause fatal anaphylactic reactions. Hypersensitivity reactions can include frequent skin rashes and hives, exfoliative dermatitis, erythema multiforme, and a temporary decrease in both red and white blood cells.
Ampicillin is not recommended in people with concurrent mononucleosis, as over 40% of patients develop a skin rash.
Ampicillin is comparatively less toxic than other antibiotics, and side effects are more likely in those who are sensitive to penicillins and those with a history of asthma or allergies. In very rare cases, it causes severe side effects such as angioedema, anaphylaxis, and "C. difficile" infection (that can range from mild diarrhea to serious pseudomembranous colitis). Some develop black "furry" tongue. Serious adverse effects also include seizures and serum sickness. The most common side effects, experienced by about 10% of users are diarrhea and rash. Less common side effects can be nausea, vomiting, itching, and blood dyscrasias. The gastrointestinal effects, such as hairy tongue, nausea, vomiting, diarrhea, and colitis, are more common with the oral form of penicillin. Other conditions may develop up several weeks after treatment.
Ampicillin overdose can cause behavioral changes, confusion, blackouts, and convulsions, as well as neuromuscular hypersensitivity, electrolyte imbalance, and kidney failure.
Ampicillin reacts with probenecid and methotrexate to decrease renal excretion. Large doses of ampicillin can increase the risk of bleeding with concurrent use of warfarin and other oral anticoagulants, possibly by inhibiting platelet aggregation. Ampicillin has been said to make oral contraceptives less effective, but this has been disputed. It can be made less effective by other antibiotic, such as chloramphenicol, erythromycin, cephalosporins, and tetracyclines. For example, tetracyclines inhibit protein synthesis in bacteria, reducing the target against which ampicillin acts. If given at the same time as aminoglycosides, it can bind to it and inactivate it. When administered separately, aminoglycosides and ampicillin can potentiate each other instead.
Ampicillin causes skin rashes more often when given with allopurinol.
Both the live cholera vaccine and live typhoid vaccine can be made ineffective if given with ampicillin. Ampicillin is normally used to treat cholera and typhoid fever, lowering the immunological response that the body has to mount.
Ampicillin is in the penicillin group of beta-lactam antibiotics and is part of the aminopenicillin family. It is roughly equivalent to amoxicillin in terms of activity. Ampicillin is able to penetrate Gram-positive and some Gram-negative bacteria. It differs from penicillin G, or benzylpenicillin, only by the presence of an amino group. This amino group, present on both ampicillin and amoxicillin, helps these antibiotics pass through the pores of the outer membrane of Gram-negative bacteria, such as "E. coli", "Proteus mirabilis", "Salmonella enterica", and "Shigella".
Ampicillin acts as an irreversible inhibitor of the enzyme transpeptidase, which is needed by bacteria to make the cell wall. It inhibits the third and final stage of bacterial cell wall synthesis in binary fission, which ultimately leads to cell lysis; therefore, ampicillin is usually bacteriolytic.
Ampicillin is well-absorbed from the GI tract (though food reduces its absorption), and reaches peak concentrations in one to two hours. The bioavailability is around 62% for parenteral routes. Unlike other penicillins, which usually have bind 60–90% to plasma proteins, ampicillin binds to only 15–20%.
Ampicillin is distributed through most tissues, though it is concentrated in the liver and kidneys. It can also be found in the cerebrospinal fluid when the meninges become inflamed (such as, for example, meningitis). Some ampicillin is metabolized by hydrolyzing the beta-lactam ring to penicilloic acid, though most of it is excreted unchanged. In the kidneys, it is filtered out mostly by tubular secretion; some also undergoes glomerular filtration, and the rest is excreted in the feces and bile.
Hetacillin and pivampicillin are ampicillin esters that have been developed to increase bioavailability.
Ampicillin has been used extensively to treat bacterial infections since 1961. Until the introduction of ampicillin by the British company Beecham, penicillin therapies had only been effective against Gram-positive organisms such as staphylococci and streptococci. Ampicillin (originally branded as "Penbritin") also demonstrated activity against Gram-negative organisms such as "H. influenzae", coliforms, and "Proteus" spp.
Ampicillin is relatively inexpensive. , ampicillin's wholesale cost is between US$0.13 and 1.20 for a vial of the intravenous solution.
In the United States, it is available as a generic medication.
In veterinary medicine, ampicillin is used in cats, dogs, and farm animals to treat:
Horses are generally not treated with ampicillin, as they have low bioavailability of beta-lactams.
The half-life in animals is around that same of that in humans (just over an hour). Oral absorption is less than 50% in cats and dogs, and less than 4% in horses.

</doc>
<doc id="1913" url="https://en.wikipedia.org/wiki?curid=1913" title="Annealing">
Annealing

Annealing may refer to:

</doc>
<doc id="1914" url="https://en.wikipedia.org/wiki?curid=1914" title="Antimicrobial resistance">
Antimicrobial resistance

Antimicrobial resistance (AMR or AR) is the ability of a pathogenic microbe to develop a resistance to the effects of an antimicrobial medication. The term antibiotic resistance (AR or ABR) is a subset of AMR, as it applies to bacteria that become resistant to antibiotics. Resistant microbes are more difficult to treat, requiring higher doses, or alternative medications which may prove more toxic. These approaches may also be more expensive. Microbes resistant to multiple antimicrobials are called multidrug resistant (MDR).
All classes of microbes can develop resistance. Fungi develop antifungal resistance. Viruses develop antiviral resistance. Protozoa develop antiprotozoal resistance, and bacteria develop antibiotic resistance. Those bacteria that are considered extensively drug resistant (XDR) or totally drug-resistant (TDR) are sometimes called "superbugs". Resistance in bacteria can arise naturally, by genetic mutation, or by one species acquiring resistance from another. Resistance can appear spontaneously because of random mutations. However, extended use of antimicrobials appears to encourage selection for mutations which can render antimicrobials ineffective.
The prevention of antibiotic misuse which can lead to antibiotic resistance, includes prescribing or using antibiotics only when they are needed. Narrow-spectrum antibiotics are preferred over broad-spectrum antibiotics when possible, as effectively and accurately targeting specific organisms is less likely to cause resistance, as well as side effects. For people who take these medications at home, education about proper use is essential. Health care providers can minimize spread of resistant infections by use of proper sanitation and hygiene, including handwashing and disinfecting between patients, and should encourage the same of the patient, visitors, and family members.
Rising drug resistance is caused mainly by use of antimicrobials in humans and other animals, and spread of resistant strains between the two. Growing resistance has also been linked to dumping of inadequately treated effluents from the pharmaceutical industry, especially in countries where bulk drugs are manufactured. Antibiotics increase selective pressure in bacterial populations, causing vulnerable bacteria to die; this increases the percentage of resistant bacteria which continue growing. Even at very low levels of antibiotic, resistant bacteria can have a growth advantage and grow faster than vulnerable bacteria. With resistance to antibiotics becoming more common there is greater need for alternative treatments. Calls for new antibiotic therapies have been issued, but new drug development is becoming rarer.
Antimicrobial resistance is increasing globally because of greater access to antibiotic drugs in developing countries. Estimates are that 700,000 to several million deaths result per year. Each year in the United States, at least 2.8 million people become infected with bacteria that are resistant to antibiotics and at least 35,000 people die as a result. According to World Health Organization (WHO) estimates, three hundred and fifty million deaths could be caused by AMR by 2050.
There are public calls for global collective action to address the threat that include proposals for international treaties on antimicrobial resistance. Worldwide antibiotic resistance is not completely identified, but poorer countries with weaker healthcare systems are more affected.
The WHO defines antimicrobial resistance as a microorganism's resistance to an antimicrobial drug that was once able to treat an infection by that microorganism. A person cannot become resistant to antibiotics. Resistance is a property of the microbe, not a person or other organism infected by a microbe.
Antibiotic resistance is a subset of antimicrobial resistance. This more specified resistance is linked to pathogenic bacteria and thus broken down into two further subsets, microbiological and clinical. Resistance linked microbiologically is the most common and occurs from genes, mutated or inherited, that allow the bacteria to resist the mechanism associated with certain antibiotics. Clinical resistance is shown through the failure of many therapeutic techniques where the bacteria that are normally susceptible to a treatment become resistant after surviving the outcome of the treatment. In both cases of acquired resistance, the bacteria can pass the genetic catalyst for resistance through conjugation, transduction, or transformation. This allows the resistance to spread across the same pathogen or even similar bacterial pathogens.
WHO report released April 2014 stated, "this serious threat is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country. Antibiotic resistance—when bacteria change so antibiotics no longer work in people who need them to treat infections—is now a major threat to public health." In 2018, WHO considered antibiotic resistance to be one of the biggest threats to global health, food security and development. The European Centre for Disease Prevention and Control calculated that in 2015 there were 671,689 infections in the EU and European Economic Area caused by antibiotic-resistant bacteria, resulting in 33,110 deaths. Most were acquired in healthcare settings.
Antimicrobial resistance is mainly caused by the overuse of antimicrobials. This leads to microbes either developing a defense against drugs used to treat them, or certain strains of microbes that have a natural resistance to antimicrobials becoming much more prevalent than the ones that are easily defeated with medication.  While antimicrobial resistance does occur naturally over time, the use of antimicrobial agents in a variety of settings both within the healthcare industry and outside of has led to antimicrobial resistance becoming increasingly more prevalent.
Antimicrobial resistance can develop naturally as the evolutionary response of continued exposure to antimicrobials. Natural selection means that organisms that are able to adapt to their environment survive and continue to produce offspring. As a result, the types of microorganisms that are able to survive over time with continued attack by certain antimicrobial agents will naturally become more prevalent in the environment, and those without this resistance will become obsolete. Over time most of the strains of bacteria and infections present will be the type resistant to the antimicrobial agent being used to treat them, making this agent now ineffective to defeat most microbes. With the increased use of antimicrobial agents, there is a speeding up of this natural process.
Self medication by consumers is defined as "the taking of medicines on one's own initiative or on another person's suggestion, who is not a certified medical professional", and it has been identified as one of the primary reasons for the development of antimicrobial resistance. In an effort to manage their own illness, patients take the advice of false media sources, friends, and family causing them to take antimicrobials unnecessarily or in excess. Many people resort to this out of necessity, when they have a limited amount of money to see a doctor, or in many developing countries a poorly developed economy and lack of doctors are the cause of self-medication. In these developing countries, governments resort to allowing the sale of antimicrobials as over the counter medications so people could have access to them without having to find or pay to see a medical professional. This increased access makes it extremely easy to obtain antimicrobials without the advice of a physician, and as a result many antimicrobials are taken incorrectly leading to resistant microbial strains. One major example of a place that faces these challenges is India, where in the state of Punjab 73% of the population resorted to treating their minor health issues and chronic illnesses through self-medication.
The major issue with self-medication is the lack of knowledge of the public on the dangerous effects of antimicrobial resistance, and how they can contribute to it through mistreating or misdiagnosing themselves.  In order to determine the public's knowledge and preconceived notions on antibiotic resistance, a major type of antimicrobial resistance, a screening of 3537 articles published in Europe, Asia, and North America was done.  Of the 55,225 total people surveyed, 70% had heard of antibiotic resistance previously, but 88% of those people thought it referred to some type of physical change in the body.  With so many people around the world with the ability to self-medicate using antibiotics, and a vast majority unaware of what antimicrobial resistance is, it makes the increase of antimicrobial resistance much more likely.
Clinical misuse by healthcare professionals is another cause leading to increased antimicrobial resistance. Studies done by the CDC show that the indication for treatment of antibiotics, choice of the agent used, and the duration of therapy was incorrect in up to 50% of the cases studied.  In another study done in an intensive care unit in a major hospital in France, it was shown that 30% to 60% of prescribed antibiotics were unnecessary. These inappropriate uses of antimicrobial agents promote the development of antimicrobial resistance by supporting the bacteria in developing genetic alterations that lead to resistance. In a study done by the American Journal of Infection Control aimed to evaluate physicians’ attitudes and knowledge on antimicrobial resistance in ambulatory settings, only 63% of those surveyed reported antibiotic resistance as a problem in their local practices, while 23% reported the aggressive prescription of antibiotics as necessary to avoid failing to provide adequate care.  This demonstrates how a majority of doctors underestimate the impact that their own prescribing habits have on antimicrobial resistance as a whole. It also confirms that some physicians may be overly cautious when it comes to prescribing antibiotics for both medical or legal reasons, even when indication for use for these medications is not always confirmed. This can lead to unnecessary antimicrobial use.
The antimicrobial resistance crisis also extends to the food industry, specifically with food producing animals.  Antibiotics are fed to livestock to act as growth supplements, and a preventative measure to decrease the likelihood of infections.  This results in the transfer of resistant bacterial strains into the food that humans eat, causing potentially fatal transfer of disease.  While this practice does result in better yields and meat products, it is a major issue in terms of preventing antimicrobial resistance. Though the evidence linking antimicrobial usage in livestock to antimicrobrial resistance is limited, the World Health Organization Advisory Group on Integrated Surveillance of Antimicrobial Resistance strongly recommended the reduction of use of medically important antimicrobials in livestock. Additionally, the Advisory Group stated that such antimicrobials should be expressly prohibited for both growth promotion and disease prevention.
In a study published by the National Academy of Sciences mapping antimicrobial consumption in livestock globally, it was predicted that in the 228 countries studied, there would be a total 67% increase in consumption of antibiotics by livestock by 2030. In some countries such as Brazil, Russia, India, China, and South Africa it is predicted that a 99% increase will occur. Several countries have restricted the use of antibiotics in livestock, including Canada, China, Japan, and the US. These restrictions are sometimes associated with a reduction of the prevalence of antimicrobial resistance in humans.
Most pesticides protect crops against insects and plants, but in some cases antimicrobial pesticides are used to protect against various microorganisms such as bacteria, viruses, fungi, algae, and protozoa. The overuse of many pesticides in an effort to have a higher yield of crops has resulted in many of these microbes developing a tolerance against these antimicrobial agents. Currently there are over 4000 antimicrobial pesticides registered with the EPA and sold to market, showing the widespread use of these agents. It is estimated that for every single meal a person consumes, 0.3  g of pesticides is used, as 90% of all pesticide use is used on agriculture. A majority of these products are used to help defend against the spread of infectious diseases, and hopefully protect public health. But out of the large amount of pesticides used, it is also estimated that less than 0.1% of those antimicrobial agents, actually reach their targets. That leaves over 99% of all pesticides used available to contaminate other resources. In soil, air, and water these antimicrobial agents are able to spread, coming in contact with more microorganisms and leading to these microbes developing mechanisms to tolerate and further resist pesticides.
There have been increasing public calls for global collective action to address the threat, including a proposal for international treaty on antimicrobial resistance. Further detail and attention is still needed in order to recognize and measure trends in resistance on the international level; the idea of a global tracking system has been suggested but implementation has yet to occur. A system of this nature would provide insight to areas of high resistance as well as information necessary for evaluating programs and other changes made to fight or reverse antibiotic resistance.
Antibiotic treatment duration should be based on the infection and other health problems a person may have. For many infections once a person has improved there is little evidence that stopping treatment causes more resistance. Some therefore feel that stopping early may be reasonable in some cases. Other infections, however, do require long courses regardless of whether a person feels better.
There are multiple national and international monitoring programs for drug-resistant threats, including methicillin-resistant "Staphylococcus aureus" (MRSA), vancomycin-resistant "S. aureus" (VRSA), extended spectrum beta-lactamase (ESBL), vancomycin-resistant "Enterococcus" (VRE), multidrug-resistant "Acinetobacter baumannii" (MRAB).
ResistanceOpen is an online global map of antimicrobial resistance developed by HealthMap which displays aggregated data on antimicrobial resistance from publicly available and user submitted data. The website can display data for a 25-mile radius from a location. Users may submit data from antibiograms for individual hospitals or laboratories. European data is from the EARS-Net (European Antimicrobial Resistance Surveillance Network), part of the ECDC.
ResistanceMap is a website by the Center for Disease Dynamics, Economics & Policy and provides data on antimicrobial resistance on a global level.
Antibiotic stewardship programmes appear useful in reducing rates of antibiotic resistance. The antibiotic stewardship program will also provide pharmacists with the knowledge to educate patients that antibiotics will not work for a virus.
Excessive antibiotic use has become one of the top contributors to the development of antibiotic resistance. Since the beginning of the antibiotic era, antibiotics have been used to treat a wide range of disease. Overuse of antibiotics has become the primary cause of rising levels of antibiotic resistance. The main problem is that doctors are willing to prescribe antibiotics to ill-informed individuals who believe that antibiotics can cure nearly all illnesses, including viral infections like the common cold. In an analysis of drug prescriptions, 36% of individuals with a cold or an upper respiratory infection (both viral in origin) were given prescriptions for antibiotics. These prescriptions accomplished nothing other than increasing the risk of further evolution of antibiotic resistant bacteria.
Antimicrobial stewardship teams in hospitals are encouraging optimal use of antimicrobials. The goals of antimicrobial stewardship are to help practitioners pick the right drug at the right dose and duration of therapy while preventing misuse and minimizing the development of resistance. Stewardship may reduce the length of stay by an average of slightly over 1 day while not increasing the risk of death.
It is established that the use of antibiotics in animal husbandry can give rise to AMR resistances in bacteria found in food animals to the antibiotics being administered (through injections or medicated feeds). For this reason only antimicrobials that are deemed "not-clinically relevant" are used in these practices.
Recent studies have shown that the prophylactic use of "non-priority" or "non-clinically relevant" antimicrobials in feeds can potentially, under certain conditions, lead to co-selection of environmental AMR bacteria with resistance to medically important antibiotics. The possibility for co-selection of AMR resistances in the food chain pipeline may have far-reaching implications for human health.
Given the volume of care provided in primary care (General Practice), recent strategies have focused on reducing unnecessary antibiotic prescribing in this setting. Simple interventions, such as written information explaining the futility of antibiotics for common infections such as upper respiratory tract infections, have been shown to reduce antibiotic prescribing.
The prescriber should closely adhere to the five rights of drug administration: the right patient, the right drug, the right dose, the right route, and the right time.
Cultures should be taken before treatment when indicated and treatment potentially changed based on the susceptibility report.
About a third of antibiotic prescriptions written in outpatient settings in the United States were not appropriate in 2010 and 2011. Doctors in the U.S. wrote 506 annual antibiotic scripts for every 1,000 people, with 353 being medically necessary.
Health workers and pharmacists can help tackle resistance by: enhancing infection prevention and control; only prescribing and dispensing antibiotics when they are truly needed; prescribing and dispensing the right antibiotic(s) to treat the illness.
People can help tackle resistance by using antibiotics only when prescribed by a doctor; completing the full prescription, even if they feel better; never sharing antibiotics with others or using leftover prescriptions.
Infectious disease control through improved water, sanitation and hygiene (WASH) infrastructure needs to be included in the antimicrobial resistance (AMR) agenda. The "Interagency Coordination Group on Antimicrobial Resistance" stated in 2018 that "the spread of pathogens through unsafe water results in a high burden of gastrointestinal disease, increasing even further the need for antibiotic treatment." This is particularly a problem in developing countries where the spread of infectious diseases caused by inadequate WASH standards is a major driver of antibiotic demand. Growing usage of antibiotics together with persistent infectious disease levels have led to a dangerous cycle in which reliance on antimicrobials increases while the efficacy of drugs diminishes. The proper use of infrastructure for water, sanitation and hygiene (WASH) can result in a 47–72 percent decrease of diarrhea cases treated with antibiotics depending on the type of intervention and its effectiveness. A reduction of the diarrhea disease burden through improved infrastructure would result in large decreases in the number of diarrhea cases treated with antibiotics. This was estimated as ranging from 5 million in Brazil to up to 590 million in India by the year 2030. The strong link between increased consumption and resistance indicates that this will directly mitigate the accelerating spread of AMR. Sanitation and water for all by 2030 is Goal Number 6 of the Sustainable Development Goals.
An increase in hand washing compliance by hospital staff results in decreased rates of resistant organisms.
Water supply and sanitation infrastructure in health facilities offer significant co-benefits for combatting AMR, and investment should be increased. There is much room for improvement: WHO and UNICEF estimated in 2015 that globally 38% of health facilities did not have a source of water, nearly 19% had no toilets and 35% had no water and soap or alcohol-based hand rub for handwashing.
Manufacturers of antimicrobials need to improve the treatment of their wastewater (by using industrial wastewater treatment processes) to reduce the release of residues into the environment.
In 1997, European Union health ministers voted to ban avoparcin and four additional antibiotics used to promote animal growth in 1999. In 2006 a ban on the use of antibiotics in European feed, with the exception of two antibiotics in poultry feeds, became effective. In Scandinavia, there is evidence that the ban has led to a lower prevalence of antibiotic resistance in (nonhazardous) animal bacterial populations. As of 2004, several European countries established a decline of antimicrobial resistance in humans through limiting the use of antimicrobials in agriculture and food industries without jeopardizing animal health or economic cost.
The United States Department of Agriculture (USDA) and the Food and Drug Administration (FDA) collect data on antibiotic use in humans and in a more limited fashion in animals. The FDA first determined in 1977 that there is evidence of emergence of antibiotic-resistant bacterial strains in livestock. The long-established practice of permitting OTC sales of antibiotics (including penicillin and other drugs) to lay animal owners for administration to their own animals nonetheless continued in all states.
In 2000, the FDA announced their intention to revoke approval of fluoroquinolone use in poultry production because of substantial evidence linking it to the emergence of fluoroquinolone-resistant "Campylobacter" infections in humans. Legal challenges from the food animal and pharmaceutical industries delayed the final decision to do so until 2006. Fluroquinolones have been banned from extra-label use in food animals in the USA since 2007. However, they remain widely used in companion and exotic animals.
The increasing interconnectedness of the world and the fact that new classes of antibiotics have not been developed and approved for more than 25 years highlight the extent to which antimicrobial resistance is a global health challenge. A global action plan to tackle the growing problem of resistance to antibiotics and other antimicrobial medicines was endorsed at the Sixty-eighth World Health Assembly in May 2015. One of the key objectives of the plan is to improve awareness and understanding of antimicrobial resistance through effective communication, education and training. This global action plan developed by the World Health Organization was created to combat the issue of antimicrobial resistance and was guided by the advice of countries and key stakeholders. The WHO's global action plan is composed of five key objectives that can be targeted through different means, and represents countries coming together to solve a major problem that can have future health consequences. These objectives are as follows:
Steps towards progress
The World Health Organization has promoted the first World Antibiotic Awareness Week running from 16–22 November 2015. The aim of the week is to increase global awareness of antibiotic resistance. It also wants to promote the correct usage of antibiotics across all fields in order to prevent further instances of antibiotic resistance.
World Antibiotic Awareness Week has been held every November since 2015. For 2017, the Food and Agriculture Organization of the United Nations (FAO), the World Health Organization (WHO) and the World Organisation for Animal Health (OIE) are together calling for responsible use of antibiotics in humans and animals to reduce the emergence of antibiotic resistance.
United Nations
In 2016 the Secretary-General of the United Nations convened the Interagency Coordination Group (IACG) on Antimicrobial Resistance. The IACG worked with international organizations and experts in human, animal, and plant health to create a plan to fight antimicrobial resistance. Their report released in April 2019 highlights the seriousness of antimicrobial resistance and the threat it poses to world health. It suggests five recommendations for member states to follow in order to tackle this increasing threat. The IACG recommendations are as follows:
The four main mechanisms by which bacteria exhibit resistance to antibiotics are:
In gram-negative bacteria, plasmid-mediated resistance genes produce proteins that can bind to DNA gyrase, protecting it from the action of quinolones. Finally, mutations at key sites in DNA gyrase or topoisomerase IV can decrease their binding affinity to quinolones, decreasing the drug's effectiveness.
Some bacteria are naturally resistant to certain antibiotics; for example, gram-negative bacteria are resistant to most β-lactam antibiotics due to the presence of β-lactamase. Antibiotic resistance can also be acquired as a result of either genetic mutation or horizontal gene transfer. Although mutations are rare, with spontaneous mutations in the pathogen genome occurring at a rate of about 1 in 10 to 1 in 10 per chromosomal replication, the fact that bacteria reproduce at a high rate allows for the effect to be significant. Given that lifespans and production of new generations can be on a timescale of mere hours, a new (de novo) mutation in a parent cell can quickly become an inherited mutation of widespread prevalence, resulting in the microevolution of a fully resistant colony. However, chromosomal mutations also confer a cost of fitness. For example, a ribosomal mutation may protect a bacterial cell by changing the binding site of an antibiotic but will also slow protein synthesis. manifesting, in slower growth rate. Moreover, some adaptive mutations can propagate not only through inheritance but also through horizontal gene transfer. The most common mechanism of horizontal gene transfer is the transferring of plasmids carrying antibiotic resistance genes between bacteria of the same or different species via conjugation. However, bacteria can also acquire resistance through transformation, as in "Streptococcus pneumoniae" uptaking of naked fragments of extracellular DNA that contain antibiotic resistance genes to streptomycin, through transduction, as in the bacteriophage-mediated transfer of tetracycline resistance genes between strains of "S. pyogenes", or through gene transfer agents, which are particles produced by the host cell that resemble bacteriophage structures and are capable of transferring DNA.
Antibiotic resistance can be introduced artificially into a microorganism through laboratory protocols, sometimes used as a selectable marker to examine the mechanisms of gene transfer or to identify individuals that absorbed a piece of DNA that included the resistance gene and another gene of interest.
Recent findings show no necessity of large populations of bacteria for the appearance of antibiotic resistance. Small populations of "Escherichia coli" in an antibiotic gradient can become resistant. Any heterogeneous environment with respect to nutrient and antibiotic gradients may facilitate antibiotic resistance in small bacterial populations. Researchers hypothesize that the mechanism of resistance development is based on four SNP mutations in the genome of "E. coli" produced by the gradient of antibiotic. 
In one study, which has implications for space microbiology, a non-pathogenic strain "E. coli" MG1655 was exposed to trace levels of the broad spectrum antibiotic chloramphenicol, under simulated microgravity (LSMMG, or, Low Shear Modeled Microgravity) over 1000 generations. The adapted strain acquired resistance to not only chloramphenicol, but also cross-resistance to other antibiotics; this was in contrast to the observation on the same strain, which was adapted to over 1000 generations under LSMMG, but without any antibiotic exposure; the strain in this case did not acquire any such resistance. Thus, irrespective of where they are used, the use of an antibiotic would likely result in persistent resistance to that antibiotic, as well as cross-resistance to other antimicrobials. 
In recent years, the emergence and spread of β-lactamases called carbapenemases has become a major health crisis. One such carbapenemase is New Delhi metallo-beta-lactamase 1 (NDM-1), an enzyme that makes bacteria resistant to a broad range of beta-lactam antibiotics. The most common bacteria that make this enzyme are gram-negative such as "E. coli" and "Klebsiella pneumoniae", but the gene for NDM-1 can spread from one strain of bacteria to another by horizontal gene transfer.
Specific antiviral drugs are used to treat some viral infections. These drugs prevent viruses from reproducing by inhibiting essential stages of the virus's replication cycle in infected cells. Antivirals are used to treat HIV, hepatitis B, hepatitis C, influenza, herpes viruses including varicella zoster virus, cytomegalovirus and Epstein-Barr virus. With each virus, some strains have become resistant to the administered drugs.
Antiviral drugs typically target key components of viral reproduction; for example, oseltamivir targets influenza neuraminidase, while guanosine analogs inhibit viral DNA polymerase. Resistance to antivirals is thus acquired through mutations in the genes that encode the protein targets of the drugs.
Resistance to HIV antivirals is problematic, and even multi-drug resistant strains have evolved. One source of resistance is that many current HIV drugs, including NRTIs and NNRTIs, target reverse transcriptase; however, HIV-1 reverse transcriptase is highly error prone and thus mutations conferring resistance arise rapidly. Resistant strains of the HIV virus emerge rapidly if only one antiviral drug is used. Using three or more drugs together, termed combination therapy, has helped to control this problem, but new drugs are needed because of the continuing emergence of drug-resistant HIV strains.
Infections by fungi are a cause of high morbidity and mortality in immunocompromised persons, such as those with HIV/AIDS, tuberculosis or receiving chemotherapy. The fungi candida, "Cryptococcus neoformans" and "Aspergillus fumigatus" cause most of these infections and antifungal resistance occurs in all of them. Multidrug resistance in fungi is increasing because of the widespread use of antifungal drugs to treat infections in immunocompromised individuals.
Of particular note, Fluconazole-resistant Candida species have been highlighted as a growing problem by the CDC. More than 20 species of Candida can cause Candidiasis infection, the most common of which is "Candida albicans". Candida yeasts normally inhabit the skin and mucous membranes without causing infection. However, overgrowth of Candida can lead to Candidiasis. Some Candida strains are becoming resistant to first-line and second-line antifungal agents such as azoles and echinocandins.
The protozoan parasites that cause the diseases malaria, trypanosomiasis, toxoplasmosis, cryptosporidiosis and leishmaniasis are important human pathogens.
Malarial parasites that are resistant to the drugs that are currently available to infections are common and this has led to increased efforts to develop new drugs. Resistance to recently developed drugs such as artemisinin has also been reported. The problem of drug resistance in malaria has driven efforts to develop vaccines.
Trypanosomes are parasitic protozoa that cause African trypanosomiasis and Chagas disease (American trypanosomiasis). There are no vaccines to prevent these infections so drugs such as pentamidine and suramin, benznidazole and nifurtimox are used to treat infections. These drugs are effective but infections caused by resistant parasites have been reported.
Leishmaniasis is caused by protozoa and is an important public health problem worldwide, especially in sub-tropical and tropical countries. Drug resistance has "become a major concern".
The discovery of penicillin in 1928 and other antibiotics in the 20th century proved to be a significant medical achievement, saving millions of lives and significantly reducing the burden of infectious diseases. The 1950s to 1970s represented the golden age of antibiotic discovery, where countless new classes of antibiotics were discovered to treat previously incurable diseases such as tuberculosis and syphilis. However, since that time the discovery of new classes of antibiotics has been almost nonexistent, and represents a situation that is especially problematic considering the resiliency of bacteria shown over time and the continued misuse and overuse of antibiotics in treatment.
The phenomenon of antimicrobial resistance caused by overuse of antibiotics was predicted as early as 1945 by Alexander Fleming who said "The time may come when penicillin can be bought by anyone in the shops. Then there is the danger that the ignorant man may easily under-dose himself and by exposing his microbes to nonlethal quantities of the drug make them resistant." Without the creation of new and stronger antibiotics an era where common infections and minor injuries can kill, and where complex procedures such as surgery and chemotherapy become too risky, is a very real possibility. Antimicrobial resistance threatens the world as we know it, and can lead to epidemics of enormous proportions if preventive actions are not taken. In this day and age current antimicrobial resistance leads to longer hospital stays, higher medical costs, and increased mortality.
Since the mid-1980s pharmaceutical companies have invested in medications for cancer or chronic disease that have greater potential to make money and have "de-emphasized or dropped development of antibiotics". On 20 January 2016 at the World Economic Forum in Davos, Switzerland, more than "80 pharmaceutical and diagnostic companies" from around the world called for "transformational commercial models" at a global level to spur research and development on antibiotics and on the "enhanced use of diagnostic tests that can rapidly identify the infecting organism".
Some global health scholars have argued that a global, legal framework is needed to prevent and control antimicrobial resistance. For instance, binding global policies could be used to create antimicrobial use standards, regulate antibiotic marketing, and strengthen global surveillance systems. Ensuring compliance of involved parties is a challenge. Global antimicrobial resistance policies could take lessons from the environmental sector by adopting strategies that have made international environmental agreements successful in the past such as: sanctions for non-compliance, assistance for implementation, majority vote decision-making rules, an independent scientific panel, and specific commitments.
For the United States 2016 budget, U.S. president Barack Obama proposed to nearly double the amount of federal funding to "combat and prevent" antibiotic resistance to more than $1.2 billion. Many international funding agencies like USAID, DFID, SIDA and Bill & Melinda Gates Foundation have pledged money for developing strategies to counter antimicrobial resistance.
On 27 March 2015, the White House released a comprehensive plan to address the increasing need for agencies to combat the rise of antibiotic-resistant bacteria. The Task Force for Combating Antibiotic-Resistant Bacteria developed "The National Action Plan for Combating Antibiotic-Resistant Bacteria" with the intent of providing a roadmap to guide the US in the antibiotic resistance challenge and with hopes of saving many lives. This plan outlines steps taken by the Federal government over the next five years needed in order to prevent and contain outbreaks of antibiotic-resistant infections; maintain the efficacy of antibiotics already on the market; and to help to develop future diagnostics, antibiotics, and vaccines.
The Action Plan was developed around five goals with focuses on strengthening health care, public health veterinary medicine, agriculture, food safety and research, and manufacturing. These goals, as listed by the White House, are as follows:
The following are goals set to meet by 2020:
Public Health England reported that the total number of antibiotic resistant infections in England rose by 9% from 55,812 in 2017 to 60,788 in 2018, but antibiotic consumption had fallen by 9% from 20.0 to 18.2 defined daily doses per 1,000 inhabitants per day between 2014 and 2018.
The Combating Antibiotic-Resistant Bacteria Biopharmaceutical Accelerator (CARB-X) was launched July 28, 2016 as a global initiative in response to the U.S. government's 2015 Combating Antibiotic-Resistant Bacteria task force, and the U.K. government’s call in 2016 for concerted global effort to address drug-resistance. CARB-X funds and supports the pre-clinical development of innovative vaccines, diagnostics, antibiotics and other therapeutics to address drug-resistant bacterial infections. 
According to World Health Organization, policymakers can help tackle resistance by strengthening resistance-tracking and laboratory capacity and by regulating and promoting the appropriate use of medicines. Policymakers and industry can help tackle resistance by: fostering innovation and research and development of new tools; and promoting cooperation and information sharing among all stakeholders.
It is unclear if rapid viral testing affects antibiotic use in children.
Microorganisms do not develop resistance to vaccines because a vaccine enhances the body's immune system, whereas an antibiotic operates separately from the body's normal defenses. Furthermore, if the use of vaccines increases, there is evidence that antibiotic resistant strains of pathogens will decrease; the need for antibiotics will naturally decrease as vaccines prevent infection before it occurs. However, new strains that escape immunity induced by vaccines may evolve; for example, an updated influenza vaccine is needed each year.
While theoretically promising, antistaphylococcal vaccines have shown limited efficacy, because of immunological variation between "Staphylococcus" species, and the limited duration of effectiveness of the antibodies produced. Development and testing of more effective vaccines is underway.
Alternating therapy is a proposed method in which two or three antibiotics are taken in a rotation versus taking just one antibiotic such that bacteria resistant to one antibiotic are killed when the next antibiotic is taken. Studies have found that this method reduces the rate at which antibiotic resistant bacteria emerge in vitro relative to a single drug for the entire duration.
Studies have found that bacteria that evolve antibiotic resistance towards one group of antibiotic may become more sensitive to others. This phenomenon can be used to select against resistant bacteria using an approach termed collateral sensitivity cycling, which has recently been found to be relevant in developing treatment strategies for chronic infections caused by "Pseudomonas aeruginosa".
Since the discovery of antibiotics, research and development (R&D) efforts have provided new drugs in time to treat bacteria that became resistant to older antibiotics, but in the 2000s there has been concern that development has slowed enough that seriously ill people may run out of treatment options. Another concern is that doctors may become reluctant to perform routine surgeries because of the increased risk of harmful infection. Backup treatments can have serious side-effects; for example, treatment of multi-drug-resistant tuberculosis can cause deafness or psychological disability. The potential crisis at hand is the result of a marked decrease in industry R&D. Poor financial investment in antibiotic research has exacerbated the situation. The pharmaceutical industry has little incentive to invest in antibiotics because of the high risk and because the potential financial returns are less likely to cover the cost of development than for other pharmaceuticals. In 2011, Pfizer, one of the last major pharmaceutical companies developing new antibiotics, shut down its primary research effort, citing poor shareholder returns relative to drugs for chronic illnesses. However, small and medium-sized pharmaceutical companies are still active in antibiotic drug research.
In the United States, drug companies and the administration of President Barack Obama had been proposing changing the standards by which the FDA approves antibiotics targeted at resistant organisms.
On 18 September 2014 Obama signed an executive order to implement the recommendations proposed in a report by the President's Council of Advisors on Science and Technology (PCAST) which outlines strategies to stream-line clinical trials and speed up the R&D of new antibiotics. Among the proposals:
Similar to the situation in malaria therapy, where successful treatments based on ancient recipes have been found, there has already been some success in finding and testing ancient drugs and other treatments that are effective against AMR bacteria.
Distinguishing infections requiring antibiotics from self-limiting ones is clinically challenging. In order to guide appropriate use of antibiotics and prevent the development and spread of antimicrobial resistance, diagnostic tests that provide clinicians with timely, actionable results are needed.
Acute febrile illness is a common reason for seeking medical care worldwide and a major cause of morbidity and mortality. In areas with decreasing malaria incidence, many febrile patients are inappropriately treated for malaria, and in the absence of a simple diagnostic test to identify alternative causes of fever, clinicians presume that a non-malarial febrile illness is most likely a bacterial infection, leading to inappropriate use of antibiotics. Multiple studies have shown that the use of malaria rapid diagnostic tests without reliable tools to distinguish other fever causes has resulted in increased antibiotic use.
Antimicrobial susceptibility testing (AST) can help practitioners avoid prescribing unnecessary antibiotics in the style of precision medicine, and help them prescribe effective antibiotics, but with the traditional approach it could take 12 to 48 hours. Rapid testing, possible from molecular diagnostics innovations, is defined as "being feasible within an 8-h working shift". Progress has been slow due to a range of reasons including cost and regulation.
Phage therapy is the therapeutic use of bacteriophages to treat pathogenic bacterial infections. Phage therapy has many potential applications in human medicine as well as dentistry, veterinary science, and agriculture.
Phage therapy relies on the use of naturally-occurring bacteriophages to infect and lyse bacteria at the site of infection in a host. Due to current advances in genetics and biotechnology these bacteriophages can possibly be manufactured to treat specific infections. Phages can be bioengineered to target multidrug-resistant bacterial infections, and their use involves the added benefit of preventing the elimination of beneficial bacteria in the human body. Phages destroy bacterial cell walls and membrane through the use of lytic proteins which kill bacteria by making many holes from the inside out. Bacteriophages can even possess the ability to digest the biofilm that many bacteria develop that protect them from antibiotics in order to effectively infect and kill bacteria. Bioengineering can play a role in creating successful bacteriophages.
Understanding the mutual interactions and evolutions of bacterial and phage populations in the environment of a human or animal body is essential for rational phage therapy.
Bacteriophagics are used against antibiotic resistant bacteria in Georgia (George Eliava Institute) and in one institute in Wrocław, Poland. Bacteriophage cocktails are common drugs sold over the counter in pharmacies in eastern countries.

</doc>
<doc id="1915" url="https://en.wikipedia.org/wiki?curid=1915" title="Antigen">
Antigen

In immunology, an antigen (Ag) is a molecule or molecular structure, such as may be present at the outside of a pathogen, that can be bound to by an antigen-specific antibody (Ab) or B cell antigen receptor (BCR). The presence of antigens in the body normally triggers an immune response. The term "antigen" originally described a structural molecule that binds specifically to an antibody only in the form of native antigen. It was expanded later to refer to any molecule or a linear molecular fragment after processing the native antigen that can be recognized by T-cell receptor (TCR). BCR and TCR are both highly variable antigen receptors diversified by somatic V(D)J recombination. Both T cells and B cells are cellular components of adaptive immunity. The Ag abbreviation stands for an "antibody generator".
Antigens are "targeted" by antibodies. Each antibody is specifically produced by the immune system to match an antigen after cells in the immune system come into "contact" with it; this allows a precise identification or matching of the antigen and the initiation of a tailored response. The antibody is said to "match" the antigen in the sense that it can bind to it due to an adaptation in a region of the antibody; because of this, many different antibodies are produced, each able to bind a different antigen while sharing the same basic structure. In most cases, an adapted antibody can only react to and bind one specific antigen; in some instances, however, antibodies may cross-react and bind more than one antigen.
Also, an antigen is a molecule that binds to Ag-specific receptors, but cannot necessarily induce an immune response in the body by itself. Antigens are usually proteins, peptides (amino acid chains) and polysaccharides (chains of monosaccharides/simple sugars) but lipids and nucleic acids become antigens only when combined with proteins and polysaccharides. In general, saccharides and lipids (as opposed to peptides) qualify as antigens but not as immunogens since they cannot elicit an immune response on their own. Furthermore, for a peptide to induce an immune response (activation of T-cells by antigen-presenting cells) it must be a large enough size, since peptides too small will also not elicit an immune response.
The antigen may originate from within the body ("self-antigen") or from the external environment ("non-self"). The immune system is supposed to identify and attack "non-self" invaders from the outside world or modified/harmful substances present in the body and usually does not react to self-antigens under normal homeostatic conditions due to negative selection of T cells in the thymus.
Vaccines are examples of antigens in an immunogenic form, which are intentionally administered to a recipient to induce the memory function of adaptive immune system toward the antigens of the pathogen invading that recipient.
Paul Ehrlich coined the term antibody (in German "Antikörper") in his side-chain theory at the end of the 19th century. In 1899, Ladislas Deutsch (Laszlo Detre) (1874–1939) named the hypothetical substances halfway between bacterial constituents and antibodies "substances immunogenes ou antigenes" (antigenic or immunogenic substances). He originally believed those substances to be precursors of antibodies, just as zymogen is a precursor of an enzyme. But, by 1903, he understood that an antigen induces the production of immune bodies (antibodies) and wrote that the word "antigen" is a contraction of antisomatogen ("Immunkörperbildner"). The "Oxford English Dictionary" indicates that the logical construction should be "anti(body)-gen".
Antigen-presenting cells present antigens in the form of peptides on histocompatibility molecules. The T cell selectively recognize the antigens; depending on the antigen and the type of the histocompatibility molecule, different types of T cells will be activated. For T Cell Receptor (TCR) recognition, the peptide must be processed into small fragments inside the cell and presented by a major histocompatibility complex (MHC). The antigen cannot elicit the immune response without the help of an immunologic adjuvant. Similarly, the adjuvant component of vaccines plays an essential role in the activation of the innate immune system.
An immunogen is an antigen substance (or adduct) that is able to trigger a humoral (innate) or cell-mediated immune response. It first initiates an innate immune response, which then causes the activation of the adaptive immune response. An antigen binds the highly variable immunoreceptor products (B cell receptor or T cell receptor) once these have been generated. Immunogens are those antigens, termed immunogenic, capable of inducing an immune response.
At the molecular level, an antigen can be characterized by its ability to bind to an antibody's variable Fab region. Different antibodies have the potential to discriminate among specific epitopes present on the antigen surface. A hapten is a small molecule that changes the structure of an antigenic epitope. In order to induce an immune response, it needs to be attached to a large carrier molecule such as a protein (a complex of peptides). Antigens are usually carried by proteins and polysaccharides, and less frequently, lipids. This includes parts (coats, capsules, cell walls, flagella, fimbriae, and toxins) of bacteria, viruses, and other microorganisms. Lipids and nucleic acids are antigenic only when combined with proteins and polysaccharides. Non-microbial non-self antigens can include pollen, egg white, and proteins from transplanted tissues and organs or on the surface of transfused blood cells.
Antigens can be classified according to their source.
Exogenous antigens are antigens that have entered the body from the outside, for example, by inhalation, ingestion or injection. The immune system's response to exogenous antigens is often subclinical. By endocytosis or phagocytosis, exogenous antigens are taken into the antigen-presenting cells (APCs) and processed into fragments. APCs then present the fragments to T helper cells (CD4) by the use of class II histocompatibility molecules on their surface. Some T cells are specific for the peptide:MHC complex. They become activated and start to secrete cytokines, substances that activate cytotoxic T lymphocytes (CTL), antibody-secreting B cells, macrophages and other particles.
Some antigens start out as exogenous and later become endogenous (for example, intracellular viruses). Intracellular antigens can be returned to circulation upon the destruction of the infected cell.
Endogenous antigens are generated within normal cells as a result of normal cell metabolism, or because of viral or intracellular bacterial infection. The fragments are then presented on the cell surface in the complex with MHC class I molecules. If activated cytotoxic CD8 T cells recognize them, the T cells secrete various toxins that cause the lysis or apoptosis of the infected cell. In order to keep the cytotoxic cells from killing cells just for presenting self-proteins, the cytotoxic cells (self-reactive T cells) are deleted as a result of tolerance (negative selection). Endogenous antigens include xenogenic (heterologous), autologous and idiotypic or allogenic (homologous) antigens. Sometimes antigens are part of the host itself in an autoimmune disease.
An autoantigen is usually a normal protein or protein complex (and sometimes DNA or RNA) that is recognized by the immune system of patients suffering from a specific autoimmune disease. Under normal conditions, these antigens should not be the target of the immune system, but in autoimmune diseases, their associated T cells are not deleted and instead attack.
Neoantigens are those that are entirely absent from the normal human genome. As compared with nonmutated self-antigens, neoantigens are of relevance to tumor control, as the quality of the T cell pool that is available for these antigens is not affected by central T cell tolerance. Technology to systematically analyze T cell reactivity against neoantigens became available only recently. Neoantigens can be directly detected and quantified through a method called MANA-SRM developed by a molecular diagnostics company, Complete Omics Inc., through collaborating with a team in Johns Hopkins University School of Medicine. 
For virus-associated tumors, such as cervical cancer and a subset of head and neck cancers, epitopes derived from viral open reading frames contribute to the pool of neoantigens.
"Tumor antigens" are those antigens that are presented by MHC class I or MHC class II molecules on the surface of tumor cells. Antigens found only on such cells are called tumor-specific antigens (TSAs) and generally result from a tumor-specific mutation. More common are antigens that are presented by tumor cells and normal cells, called tumor-associated antigens (TAAs). Cytotoxic T lymphocytes that recognize these antigens may be able to destroy tumor cells.
Tumor antigens can appear on the surface of the tumor in the form of, for example, a mutated receptor, in which case they are recognized by B cells.
For human tumors without a viral etiology, novel peptides (neo-epitopes) are created by tumor-specific DNA alterations.
A large fraction of human tumor mutations is effectively patient-specific. Therefore, neoantigens may also be based on individual tumor genomes. Deep-sequencing technologies can identify mutations within the protein-coding part of the genome (the exome) and predict potential neoantigens. In mice models, for all novel protein sequences, potential MHC-binding peptides were predicted. The resulting set of potential neoantigens was used to assess T cell reactivity. Exome–based analyses were exploited in a clinical setting, to assess reactivity in patients treated by either tumor-infiltrating lymphocyte (TIL) cell therapy or checkpoint blockade. Neoantigen identification was successful for multiple experimental model systems and human malignancies.
The false-negative rate of cancer exome sequencing is low—i.e.: the majority of neoantigens occur within exonic sequence with sufficient coverage. However, the vast majority of mutations within expressed genes do not produce neoantigens that are recognized by autologous T cells.
As of 2015 mass spectrometry resolution is insufficient to exclude many false positives from the pool of peptides that may be presented by MHC molecules. Instead, algorithms are used to identify the most likely candidates. These algorithms consider factors such as the likelihood of proteasomal processing, transport into the endoplasmic reticulum, affinity for the relevant MHC class I alleles and gene expression or protein translation levels.
The majority of human neoantigens identified in unbiased screens display a high predicted MHC binding affinity. Minor histocompatibility antigens, a conceptually similar antigen class are also correctly identified by MHC binding algorithms. Another potential filter examines whether the mutation is expected to improve MHC binding. The nature of the central TCR-exposed residues of MHC-bound peptides is associated with peptide immunogenicity.
A native antigen is an antigen that is not yet processed by an APC to smaller parts. T cells cannot bind native antigens, but require that they be processed by APCs, whereas B cells can be activated by native ones.
Antigenic specificity is the ability of the host cells to recognize an antigen specifically as a unique molecular entity and distinguish it from another with exquisite precision. Antigen specificity is due primarily to the side-chain conformations of the antigen. It is measurable and need not be linear or of a rate-limited step or equation.

</doc>
<doc id="1916" url="https://en.wikipedia.org/wiki?curid=1916" title="Autosome">
Autosome

An autosome is any chromosome that is not a sex chromosome (an allosome). The members of an autosome pair in a diploid cell have the same morphology, unlike those in allosome pairs which may have different structures. The DNA in autosomes is collectively known as atDNA or auDNA.
For example, humans have a diploid genome that usually contains 22 pairs of autosomes and one allosome pair (46 chromosomes total). The autosome pairs are labeled with numbers (1–22 in humans) roughly in order of their sizes in base pairs, while allosomes are labelled with their letters. By contrast, the allosome pair consists of two X chromosomes in females or one X and one Y chromosome in males. Unusual combinations of XYY, XXY, XXX, XXXX, XXXXX or XXYY, among other allosome combinations, are known to occur and usually cause developmental abnormalities.
Autosomes still contain sexual determination genes even though they are not sex chromosomes. For example, the SRY gene on the Y chromosome encodes the transcription factor TDF and is vital for male sex determination during development. TDF functions by activating the SOX9 gene on chromosome 17, so mutations of the SOX9 gene can cause humans with an ordinary Y chromosome to develop as females.
All human autosomes have been identified and mapped by extracting the chromosomes from a cell arrested in metaphase or prometaphase and then staining them with a type of dye (most commonly, Giemsa). These chromosomes are typically viewed as karyograms for easy comparison. Clinical geneticists can compare the karyogram of an individual to a reference karyogram to discover the cytogenetic basis of certain phenotypes. For example, the karyogram of someone with Patau Syndrome would show that they possess three copies of chromosome 13. Karyograms and staining techniques can only detect large-scale disruptions to chromosomes—chromosomal aberrations smaller than a few million base pairs generally cannot be seen on a karyogram.
Autosomal genetic disorders can arise due to a number of causes, some of the most common being nondisjunction in parental germ cells or Mendelian inheritance of deleterious alleles from parents. Autosomal genetic disorders which exhibit Mendelian inheritance can be inherited either in an autosomal dominant or recessive fashion. These disorders manifest in and are passed on by either sex with equal frequency. Autosomal dominant disorders are often present in both parent and child, as the child needs to inherit only one copy of the deleterious allele to manifest the disease. Autosomal recessive diseases, however, require two copies of the deleterious allele for the disease to manifest. Because it is possible to possess one copy of a deleterious allele without presenting a disease phenotype, two phenotypically normal parents can have a child with the disease if both parents are carriers (also known as heterozygotes) for the condition.
Autosomal aneuploidy can also result in disease conditions. Aneuploidy of autosomes is not well tolerated and usually results in miscarriage of the developing fetus. Fetuses with aneuploidy of gene-rich chromosomes—such as chromosome 1—never survive to term, and fetuses with aneuploidy of gene-poor chromosomes—such as chromosome 21— are still miscarried over 23% of the time. Possessing a single copy of an autosome (known as a monosomy) is nearly always incompatible with life, though very rarely some monosomies can survive past birth. Having three copies of an autosome (known as a trisomy) is far more compatible with life, however. A common example is Down syndrome, which is caused by possessing three copies of chromosome 21 instead of the usual two.
Partial aneuploidy can also occur as a result of unbalanced translocations during meiosis. Deletions of part of a chromosome cause partial monosomies, while duplications can cause partial trisomies. If the duplication or deletion is large enough, it can be discovered by analyzing a karyogram of the individual. Autosomal translocations can be responsible for a number of diseases, ranging from cancer to schizophrenia. Unlike single gene disorders, diseases caused by aneuploidy are the result of improper gene dosage, not nonfunctional gene product.

</doc>
<doc id="1919" url="https://en.wikipedia.org/wiki?curid=1919" title="Antwerp (disambiguation)">
Antwerp (disambiguation)

Antwerp is a city in Belgium and capital of the Antwerp province.
Antwerp may also refer to:

</doc>
<doc id="1920" url="https://en.wikipedia.org/wiki?curid=1920" title="Aquila">
Aquila

Aquila is the Latin and Romance languages word for "eagle". Specifically, it may refer to:

</doc>
<doc id="1921" url="https://en.wikipedia.org/wiki?curid=1921" title="Al-Qaeda">
Al-Qaeda

Al-Qaeda (; ', , translation: "The Base", "The Foundation", alternatively spelled al-Qaida and al-Qa'ida) is a transnational extremist Salafist militant organization founded in 1988 by Osama bin Laden, Ayman al-Zawahiri and Abdullah Azzam, and several other Arab volunteers during the Soviet–Afghan War.
Al-Qaeda operates as a network of Islamic extremists and Salafist jihadists. The organization has been designated as a terrorist group by the United Nations Security Council, the North Atlantic Treaty Organization (NATO), the European Union, the United States, the United Kingdom, Russia, India, and various other countries (see below). Al-Qaeda has mounted attacks on non-military and military targets in various countries, including the 1998 United States embassy bombings, the September 11 attacks, and the 2002 Bali bombings.
The United States government responded to the September 11 attacks by launching the "War on Terror", which sought to undermine al-Qaeda and its allies. The deaths of key leaders, including that of Osama bin Laden, have led al-Qaeda's operations to shift from top-down organization and planning of attacks, to the planning of attacks which are carried out by a loose network of associated groups and lone-wolf operators. Al-Qaeda characteristically organises attacks which include suicide attacks and the simultaneous bombing of several targets. Al-Qaeda ideologues envision the removal of all foreign influences in Muslim countries.
Al-Qaeda members believe that a Christian–Jewish alliance is conspiring to destroy Islam. As Salafist jihadists, members of al-Qaeda believe that the killing of non-combatants is religiously sanctioned. This belief ignores the aspects of Muslim scripture which forbid the murder of non-combatants and internecine fighting. Al-Qaeda also opposes what it regards as man-made laws, and wants to replace them with a strict form of sharia law.
Al-Qaeda has carried out many attacks on people whom it considers "kafir". It is also responsible for instigating sectarian violence among Muslims. Al-Qaeda regards liberal Muslims, Shias, Sufis and other sects as heretical and its members and sympathizers have attacked their mosques and gatherings. Examples of sectarian attacks include the Yazidi community bombings, the Sadr City bombings, the Ashoura massacre and the April 2007 Baghdad bombings.
Following the death of bin Laden in 2011, the group has been led by Egyptian Ayman al-Zawahiri.
Al-Qaeda's philosophy calls for the centralization of decision making, while allowing for the decentralization of execution. However, after the War on Terror, al-Qaeda's leadership has become isolated. As a result, the leadership has become decentralized, and the organization has become regionalized into several al-Qaeda groups.
Many terrorism experts do not believe that the global jihadist movement is driven at every level by al-Qaeda's leadership. However, bin Laden held considerable ideological sway over some Muslim extremists before his death. Experts argue that al-Qaeda has fragmented into a number of disparate regional movements, and that these groups bear little connection with one another.
This view mirrors the account given by Osama bin Laden in his October 2001 interview with Tayseer Allouni:
Bruce Hoffman, however, sees al-Qaeda as a cohesive network that is strongly led from the Pakistani tribal areas.
Al-Qaeda has the following direct affiliates:
The following are presently believed to be indirect affiliates of al-Qaeda:
Al-Qaeda's former affiliates include the following:
Osama bin Laden served as the emir of al-Qaeda from the organization's founding in 1988 until his assassination by US forces on May 1, 2011. Atiyah Abd al-Rahman was alleged to be second in command prior to his death on August 22, 2011.
Bin Laden was advised by a Shura Council, which consists of senior al-Qaeda members. The group was estimated to consist of 20–30 people.
Ayman al-Zawahiri had been al-Qaeda's deputy emir and assumed the role of emir following bin Laden's death. Al-Zawahiri replaced Saif al-Adel, who had served as interim commander.
On June 5, 2012, Pakistani intelligence officials announced that al-Rahman's alleged successor as second in command, Abu Yahya al-Libi, had been killed in Pakistan.
Nasir al-Wuhayshi was alleged to have become al-Qaeda's overall second in command and general manager in 2013. He was concurrently the leader of al-Qaeda in the Arabian Peninsula (AQAP) until he was killed by a US airstrike in Yemen in June 2015.
Abu Khayr al-Masri, Wuhayshi's alleged successor as the deputy to Ayman al-Zawahiri, was killed by a US airstrike in Syria in February 2017.
Al-Qaeda's network was built from scratch as a conspiratorial network which drew upon the leadership of a number of regional nodes. The organization divided itself into several committees, which include:
Al-Qaeda is not operationally managed by Ayman al-Zawahiri. Several operational groups exist, which consult with the leadership in situations where attacks are in preparation.
When asked in 2005 about the possibility of al-Qaeda's connection to the July 7, 2005 London bombings, Metropolitan Police Commissioner Sir Ian Blair said: "Al-Qaeda is not an organization. Al-Qaeda is a way of working... but this has the hallmark of that approach... al-Qaeda clearly has the ability to provide training... to provide expertise... and I think that is what has occurred here." On August 13, 2005, "The Independent" newspaper, reported that the July 7 bombers had acted independently of an al-Qaeda mastermind.
Nasser al-Bahri, who was Osama bin Laden's bodyguard for four years in the run-up to 9/11 wrote in his memoir a highly detailed description of how the group functioned at that time. Al-Bahri described al-Qaeda's formal administrative structure and vast arsenal. However, the author Adam Curtis argued that the idea of al-Qaeda as a formal organization is primarily an American invention. Curtis contended the name "al-Qaeda" was first brought to the attention of the public in the 2001 trial of bin Laden and the four men accused of the 1998 US embassy bombings in East Africa. Curtis wrote:
During the 2001 trial, the US Department of Justice needed to show that bin Laden was the leader of a criminal organization in order to charge him "in absentia" under the Racketeer Influenced and Corrupt Organizations Act. The name of the organization and details of its structure were provided in the testimony of Jamal al-Fadl, who said he was a founding member of the group and a former employee of bin Laden. Questions about the reliability of al-Fadl's testimony have been raised by a number of sources because of his history of dishonesty, and because he was delivering it as part of a plea bargain agreement after being convicted of conspiring to attack US military establishments. Sam Schmidt, a defense attorney who defended al-Fadl said:
The number of individuals in the group who have undergone proper military training, and are capable of commanding insurgent forces, is largely unknown. Documents captured in the raid on bin Laden's compound in 2011 show that the core al-Qaeda membership in 2002 was 170. In 2006, it was estimated that al-Qaeda had several thousand commanders embedded in 40 different countries. , it was believed that no more than 200–300 members were still active commanders.
According to the 2004 BBC documentary "The Power of Nightmares", al-Qaeda was so weakly linked together that it was hard to say it existed apart from bin Laden and a small clique of close associates. The lack of any significant numbers of convicted al-Qaeda members, despite a large number of arrests on terrorism charges, was cited by the documentary as a reason to doubt whether a widespread entity that met the description of al-Qaeda existed.
According to author Robert Cassidy, al-Qaeda maintains two separate forces which are deployed alongside insurgents in Iraq and Pakistan. The first, numbering in the tens of thousands, was "organized, trained, and equipped as insurgent combat forces" in the Soviet–Afghan war. The force was composed primarily of foreign "mujahideen" from Saudi Arabia and Yemen. Many of these fighters went on to fight in Bosnia and Somalia for global "jihad". Another group, which numbered 10,000 in 2006, live in the West and have received rudimentary combat training.
Other analysts have described al-Qaeda's rank and file as being "predominantly Arab" in its first years of operation, but that the organization also includes "other peoples" . It has been estimated that 62% of al-Qaeda members have a university education.
In the 1990s, financing for al-Qaeda came partly from the personal wealth of Osama bin Laden. Other sources of income included the heroin trade and donations from supporters in Kuwait, Saudi Arabia and other Islamic Gulf states. A WikiLeaks-released 2009 internal US government cable stated that "terrorist funding emanating from Saudi Arabia remains a serious concern".
Among the first pieces of evidence regarding Saudi Arabia's support for al-Qaeda was the so-called "Golden Chain", a list of early al-Qaeda funders seized during a 2002 raid in Sarajevo by Bosnian police. The hand-written list was validated by al-Qaeda defector Jamal al-Fadl, and included the names of both donors and beneficiaries. Osama bin-Laden's name appeared seven times among the beneficiaries, while 20 Saudi and Gulf-based businessmen and politicians were listed among the donors. Notable donors included Adel Batterjee, and Wael Hamza Julaidan. Batterjee was designated as a terror financier by the US Department of the Treasury in 2004, and Julaidan is recognized as one of al-Qaeda's founders.
Documents seized during the 2002 Bosnia raid showed that al-Qaeda widely exploited charities to channel financial and material support to its operatives across the globe. Notably, this activity exploited the International Islamic Relief Organization (IIRO) and the Muslim World League (MWL). The IIRO had ties with al-Qaeda associates worldwide, including al-Qaeda's deputy Ayman al Zawahiri. Zawahiri's brother worked for the IIRO in Albania and had actively recruited on behalf of al-Qaeda. The MWL was openly identified by al-Qaeda's leader as one of the three charities al-Qaeda primarily relied upon for funding sources.
Several Qatari citizens have been accused of funding al-Qaeda. This includes Abd Al-Rahman al-Nuaimi, a Qatari citizen and a human-rights activist who founded the Swiss-based non-governmental organization (NGO) Alkarama. On December 18, 2013, the US Treasury designated Nuaimi as a terrorist for his activities supporting al-Qaeda. The US Treasury has stated that Nuaimi "has facilitated significant financial support to al-Qaeda in Iraq, and served as an interlocutor between al-Qaeda in Iraq and Qatar-based donors".
Nuaimi was accused of overseeing a $2 million monthly transfer to al-Qaeda in Iraq as part of his role as mediator between Iraq-based al-Qaeda senior officers and Qatari citizens. Nuaimi allegedly entertained relationships with Abu-Khalid al-Suri, al-Qaeda's top envoy in Syria, who processed a $600,000 transfer to al-Qaeda in 2013. Nuaimi is also known to be associated with Abd al-Wahhab Muhammad 'Abd al-Rahman al-Humayqani, a Yemeni politician and founding member of Alkarama, who was listed as a Specially Designated Global Terrorist (SDGT) by the US Treasury in 2013. The US authorities claimed that Humayqani exploited his role in Alkarama to fundraise on behalf of al-Qaeda in the Arabian Peninsula (AQAP). A prominent figure in AQAP, Nuaimi was also reported to have facilitated the flow of funding to AQAP affiliates based in Yemen. Nuaimi was also accused of investing funds in the charity directed by Humayqani to ultimately fund AQAP. About ten months after being sanctioned by the US Treasury, Nuaimi was also restrained from doing business in the UK.
Another Qatari citizen, Kalifa Mohammed Turki Subayi, was sanctioned by the US Treasury on June 5, 2008, for his activities as a "Gulf-based al-Qaeda financier". Subayi's name was added to the UN Security Council's Sanctions List in 2008 on charges of providing financial and material support to al-Qaeda senior leadership. Subayi allegedly moved al-Qaeda recruits to South Asia-based training camps. He also financially supported Khalid Sheikh Mohammed, a Pakistani national and senior al-Qaeda officer who is believed to be the mastermind behind the September 11 attack according to the September 11 Commission report.
Qataris provided support to al-Qaeda through the country's largest NGO, the Qatar Charity. Al-Qaeda defector al-Fadl, who was a former member of Qatar Charity, testified in court that Abdullah Mohammed Yusef, who served as Qatar Charity's director, was affiliated to al-Qaeda and simultaneously to the National Islamic Front, a political group that gave al-Qaeda leader Osama Bin Laden harbor in Sudan in the early 1990s.
Legal proceedings from the trial "United States vs. Enaam M. Arnaout" revealed that Qatar Charity was cited by Bin Laden in 1993 as one of the charities used to channel financial support to al-Qaeda operatives overseas. The same documents also report Bin Laden's complaint that the failed assassination attempt of Egyptian President Hosni Mubarak had compromised the ability of al-Qaeda to exploit charities to support its operatives to the extent that it was capable of before 1995.
It is alleged that the Qatar Charity gave financial support to members of al-Qaeda in Chechnya. This accusation was publicly denied by Hamad bin Nasser al-Thani. Qatar Charity is among the NGOs allegedly channelling funds to Ansar Dine in North Mali, according to French military intelligence reports from France's intervention in the country in early 2013.
Qatar financed al-Qaeda's enterprises through al-Qaeda's former affiliate in Syria, Jabhat al-Nusra. The funding was primarily channeled through kidnapping for ransom. The Consortium Against Terrorist Finance (CATF) reported that the Gulf country has funded al-Nusra since 2013. In 2017, "Asharq Al-Awsat" estimated that Qatar had disbursed $25 million in support of al-Nusra through kidnapping for ransom. In addition, Qatar has launched fundraising campaigns on behalf of al-Nusra. Al-Nusra acknowledged a Qatar-sponsored campaign "as one of the preferred conduits for donations intended for the group".
In the disagreement over whether Al-Qaeda's objectives are religious or political, Mark Sedgwick describes Al-Qaeda's strategy as political in the immediate term but with ultimate aims that are religious.
On March 11, 2005, "Al-Quds Al-Arabi" published extracts from Saif al-Adel's document "Al Qaeda's Strategy to the Year 2020". Abdel Bari Atwan summarizes this strategy as comprising five stages to rid the Ummah from all forms of oppression:
Atwan noted that, while the plan is unrealistic, "it is sobering to consider that this virtually describes the downfall of the Soviet Union."
According to Fouad Hussein, a Jordanian journalist and author who has spent time in prison with Al-Zarqawi, Al Qaeda's strategy consists of seven phases and is similar to the plan described in Al Qaeda's Strategy to the year 2020. These phases include:
According to the seven-phase strategy, the war is projected to last less than two years.
According to Charles Lister of the Middle East Institute and Katherine Zimmerman of the American Enterprise Institute, the new model of al-Qaeda is to "socialize communities" and build a broad territorial base of operations with the support of local communities, also gaining income independent of the funding of sheiks.
The English name of the organization is a simplified transliteration of the Arabic noun "" (), which means "the foundation" or "the base". The initial "al-" is the Arabic definite article "the", hence "the base".
In Arabic, "al-Qaeda" has four syllables (). However, since two of the Arabic consonants in the name are not phones found in the English language, the common naturalized English pronunciations include , and . Al-Qaeda's name can also be transliterated as "al-Qaida", "al-Qa'ida", or "el-Qaida".
Bin Laden explained the origin of the term in a videotaped interview with Al Jazeera journalist Tayseer Alouni in October 2001:
It has been argued that two documents seized from the Sarajevo office of the Benevolence International Foundation prove that the name was not simply adopted by the "mujahideen" movement and that a group called al-Qaeda was established in August 1988. Both of these documents contain minutes of meetings held to establish a new military group, and contain the term "al-Qaeda".
Former British Foreign Secretary Robin Cook wrote that the word al-Qaeda should be translated as "the database", because it originally referred to the computer file of the thousands of "mujahideen" militants who were recruited and trained with CIA help to defeat the Russians. In April 2002, the group assumed the name "Qa'idat al-Jihad" ( ""), which means "the base of Jihad". According to Diaa Rashwan, this was "apparently as a result of the merger of the overseas branch of Egypt's al-Jihad, which was led by Ayman al-Zawahiri, with the groups Bin Laden brought under his control after his return to Afghanistan in the mid-1990s."
The radical Islamist movement developed during the Islamic revival and the rise of the Islamist movement after the Iranian Revolution (1978-1979).
Some have argued that the writings of Islamic author and thinker Sayyid Qutb, inspired the al-Qaeda organization. In the 1950s and 1960s, Qutb preached that because of the lack of "sharia" law, the Muslim world was no longer Muslim, and had reverted to the pre-Islamic ignorance known as "jahiliyyah". To restore Islam, Qutb argued that a vanguard of righteous Muslims was needed in order to establish "true Islamic states", implement "sharia", and rid the Muslim world of any non-Muslim influences. In Qutb's view, the enemies of Islam included "world Jewry", which "plotted conspiracies" and opposed Islam.
In the words of Mohammed Jamal Khalifa, a close college friend of bin Laden: 
Qutb also influenced bin Laden's mentor, Ayman al-Zawahiri. Zawahiri's uncle and maternal family patriarch, Mafouz Azzam, was Qutb's student, protégé, personal lawyer, and an executor of his estate. Azzam was one of the last people to see Qutb alive before his execution. Zawahiri paid homage to Qutb in his work "Knights under the Prophet's Banner".
Qutb's argued that many Muslims were not true Muslims. Some Muslims, Qutb argued, were apostates. These alleged apostates included leaders of Muslim countries, since they failed to enforce "sharia" law.
The Afghan jihad against the pro-Soviet government further developed the Salafist Jihadist movement which inspired Al-Qaeda.
Abdel Bari Atwan wrote that:
Following its 9/11 attack and in response to its condemnation by Islamic scholars, Al-Qaeda provided a justification for the killing of non-combatants/civilians, entitled, "A Statement from Qaidat al-Jihad Regarding the Mandates of the Heroes and the Legality of the Operations in New York and Washington". According to a couple of critics, Quintan Wiktorowicz and John Kaltner, it provides "ample theological justification for killing civilians in almost any imaginable situation."
Among these justifications are that America is leading the west in waging a War on Islam so that attacks on America are a defense of Islam and any treaties and agreements between Muslim majority states and Western countries that would be violated by attacks are null and void. According to the tract, several conditions allow for the killing of civilians including: 
"The Guardian" in 2009 described five distinct phases in the development of al-Qaeda: its beginnings in the late 1980s, a "wilderness" period in 1990–1996, its "heyday" in 1996–2001, a network period from 2001 to 2005, and a period of fragmentation from 2005 to 2009.
The origins of al-Qaeda can be traced to the Soviet War in Afghanistan (December 1979 – February 1989). The United States viewed the conflict in Afghanistan in terms of the Cold War, with Marxists on one side and the native Afghan "mujahideen" on the other. This view led to a CIA program called Operation Cyclone, which channeled funds through Pakistan's Inter-Services Intelligence agency to the Afghan Mujahideen. The US government provided substantial financial support to the Afghan Islamic militants. Aid to Gulbuddin Hekmatyar, an Afghan "mujahideen" leader and founder of the Hezb-e Islami, amounted to more than $600 million. In addition to American aid, Hekmatyar was the recipient of Saudi aid. In the early 1990s, after the US had withdrawn support, Hekmatyar "worked closely" with bin Laden.
At the same time, a growing number of Arab "mujahideen" joined the "jihad" against the Afghan Marxist regime, which was facilitated by international Muslim organizations, particularly the Maktab al-Khidamat (MAK). In 1984, MAK was established in Peshawar, Pakistan, by bin Laden and Abdullah Yusuf Azzam, a Palestinian Islamic scholar and member of the Muslim Brotherhood. MAK organized guest houses in Peshawar, near the Afghan border, and gathered supplies for the construction of paramilitary training camps to prepare foreign recruits for the Afghan war front. MAK was funded by the Saudi government as well as by individual Muslims including Saudi businessmen. Bin Laden also became a major financier of the "mujahideen", spending his own money and using his connections to influence public opinion about the war.
From 1986, MAK began to set up a network of recruiting offices in the US, the hub of which was the Al Kifah Refugee Center at the Farouq Mosque on Brooklyn's Atlantic Avenue. Among notable figures at the Brooklyn center were "double agent" Ali Mohamed, whom FBI special agent Jack Cloonan called "bin Laden's first trainer", and "Blind Sheikh" Omar Abdel-Rahman, a leading recruiter of "mujahideen" for Afghanistan. Azzam and bin Laden began to establish camps in Afghanistan in 1987.
MAK and foreign "mujahideen" volunteers, or "Afghan Arabs", did not play a major role in the war. While over 250,000 Afghan "mujahideen" fought the Soviets and the communist Afghan government, it is estimated that were never more than 2,000 foreign "mujahideen" on the field at any one time. Nonetheless, foreign "mujahideen" volunteers came from 43 countries, and the total number that participated in the Afghan movement between 1982 and 1992 is reported to have been 35,000. Bin Laden played a central role in organizing training camps for the foreign Muslim volunteers.
The Soviet Union withdrew from Afghanistan in 1989. Mohammad Najibullah's Communist Afghan government lasted for three more years, before it was overrun by elements of the "mujahideen".
Toward the end of the Soviet military mission in Afghanistan, some foreign "mujahideen" wanted to expand their operations to include Islamist struggles in other parts of the world, such as Palestine and Kashmir. A number of overlapping and interrelated organizations were formed, to further those aspirations. One of these was the organization that would eventually be called al-Qaeda.
Research suggests that al-Qaeda was formed on August 11, 1988, when a meeting in Afghanistan between leaders of Egyptian Islamic Jihad, Abdullah Azzam, and bin Laden took place. An agreement was reached to link bin Laden's money with the expertise of the Islamic Jihad organization and take up the jihadist cause elsewhere after the Soviets withdrew from Afghanistan.
Notes indicate al-Qaeda was a formal group by August 20, 1988. A list of requirements for membership itemized the following: listening ability, good manners, obedience, and making a pledge ("bayat" ) to follow one's superiors. In his memoir, bin Laden's former bodyguard, Nasser al-Bahri, gives the only publicly available description of the ritual of giving "bayat" when he swore his allegiance to the al-Qaeda chief. According to Wright, the group's real name was not used in public pronouncements because "its existence was still a closely held secret."
After Azzam was assassinated in 1989 and MAK broke up, significant numbers of MAK followers joined bin Laden's new organization.
In November 1989, Ali Mohamed, a former special forces sergeant stationed at Fort Bragg, North Carolina, left military service and moved to California. He traveled to Afghanistan and Pakistan and became "deeply involved with bin Laden's plans." In 1991, Ali Mohammed is said to have helped orchestrate bin Laden's relocation to Sudan.
Following the Soviet Union's withdrawal from Afghanistan in February 1989, bin Laden returned to Saudi Arabia. The Iraqi invasion of Kuwait in August 1990 had put the Kingdom and its ruling House of Saud at risk. The world's most valuable oil fields were within striking distance of Iraqi forces in Kuwait, and Saddam's call to pan-Arab/Islamism could potentially rally internal dissent.
In the face of a seemingly massive Iraqi military presence, Saudi Arabia's own forces were outnumbered. Bin Laden offered the services of his "mujahideen" to King Fahd to protect Saudi Arabia from the Iraqi army. The Saudi monarch refused bin Laden's offer, opting instead to allow US and allied forces to deploy troops into Saudi territory.
The deployment angered bin Laden, as he believed the presence of foreign troops in the "land of the two mosques" (Mecca and Medina) profaned sacred soil. After speaking publicly against the Saudi government for harboring American troops, he was banished and forced to live in exile in Sudan.
From around 1992 to 1996, al-Qaeda and bin Laden based themselves in Sudan at the invitation of Islamist theoretician Hassan al-Turabi. The move followed an Islamist coup d'état in Sudan, led by Colonel Omar al-Bashir, who professed a commitment to reordering Muslim political values. During this time, bin Laden assisted the Sudanese government, bought or set up various business enterprises, and established training camps.
A key turning point for bin Laden occurred in 1993 when Saudi Arabia gave support for the Oslo Accords, which set a path for peace between Israel and Palestinians. Due to bin Laden's continuous verbal assault on King Fahd of Saudi Arabia, Fahd sent an emissary to Sudan on March 5, 1994 demanding bin Laden's passport. Bin Laden's Saudi citizenship was also revoked. His family was persuaded to cut off his stipend, $7 million a year, and his Saudi assets were frozen. His family publicly disowned him. There is controversy as to what extent bin Laden continued to garner support from members afterwards.
In 1993, a young schoolgirl was killed in an unsuccessful attempt on the life of the Egyptian prime minister, Atef Sedki. Egyptian public opinion turned against Islamist bombings, and the police arrested 280 of al-Jihad's members and executed 6. In June 1995, an attempt to assassinate Egyptian president Mubarak led to the expulsion of Egyptian Islamic Jihad (EIJ), and in May 1996, of bin Laden from Sudan.
According to Pakistani-American businessman Mansoor Ijaz, the Sudanese government offered the Clinton Administration numerous opportunities to arrest bin Laden. Ijaz's claims appeared in numerous op-ed pieces, including one in the "Los Angeles Times" and one in "The Washington Post" co-written with former Ambassador to Sudan Timothy M. Carney. Similar allegations have been made by "Vanity Fair" contributing editor David Rose, and Richard Miniter, author of "Losing bin Laden", in a November 2003 interview with "World".
Several sources dispute Ijaz's claim, including the 9/11 Commission, which concluded in part: 
After the fall of the Afghan communist regime in 1992, Afghanistan was effectively ungoverned for four years and plagued by constant infighting between various "mujahideen" groups. This situation allowed the Taliban to organize. The Taliban also garnered support from graduates of Islamic schools, which are called "madrassa". According to Ahmed Rashid, five leaders of the Taliban were graduates of Darul Uloom Haqqania, a madrassa in the small town of Akora Khattak. The town is situated near Peshawar in Pakistan, but the school is largely attended by Afghan refugees. This institution reflected Salafi beliefs in its teachings, and much of its funding came from private donations from wealthy Arabs. Four of the Taliban's leaders attended a similarly funded and influenced madrassa in Kandahar. Bin Laden's contacts were laundering donations to these schools, and Islamic banks were used to transfer money to an "array" of charities which served as front groups for al-Qaeda.
Many of the "mujahideen" who later joined the Taliban fought alongside Afghan warlord Mohammad Nabi Mohammadi's Harkat i Inqilabi group at the time of the Russian invasion. This group also enjoyed the loyalty of most Afghan Arab fighters.
The continuing lawlessness enabled the growing and well-disciplined Taliban to expand their control over territory in Afghanistan, and it came to establish an enclave which it called the Islamic Emirate of Afghanistan. In 1994, it captured the regional center of Kandahar, and after making rapid territorial gains thereafter, the Taliban captured the capital city Kabul in September 1996.
In 1996, Taliban-controlled Afghanistan provided a perfect staging ground for al-Qaeda. While not officially working together, Al-Qaeda enjoyed the Taliban's protection and supported the regime in such a strong symbiotic relationship that many Western observers dubbed the Taliban's Islamic Emirate of Afghanistan as, "the world's first terrorist-sponsored state." However, at this time, only Pakistan, Saudi Arabia, and the United Arab Emirates recognized the Taliban as the legitimate government of Afghanistan.
While in Afghanistan, the Taliban government tasked al-Qaeda with the training of Brigade 055, an elite element of the Taliban's army. The Brigade mostly consisted of foreign fighters, veterans from the Soviet Invasion, and adherents to the ideology of the mujahideen. In November 2001, as Operation Enduring Freedom had toppled the Taliban government, many Brigade 055 fighters were captured or killed, and those that survived were thought to have escaped into Pakistan along with bin Laden.
By the end of 2008, some sources reported that the Taliban had severed any remaining ties with al-Qaeda, however, there is reason to doubt this. According to senior US military intelligence officials, there were fewer than 100 members of al-Qaeda remaining in Afghanistan in 2009.
Al Qaeda chief, Asim Omar was killed in Afghanistan's Musa Qala district after a joint U.S.-Afghanistan commando airstrike on September 23, Afghan's National Directorate of Security (NDS) confirmed in October 2019.
On June 14, 2020, the United Nations reported that the Taliban-Al Qaeda relations remains strong to this day and additionally, Al Qaeda itself has admitted that it operates inside Afghanistan.
On July 26, 2020, a United Nations report stated that the Al Qaeda group is still active in twelve provinces in Afghanistan and its leader al-Zawahiri is still based in the country.<ref name="https://www.daijiworld.com"></ref> and that the UN Monitoring Team has estimated that the total number of Al Qaeda fighters in Afghanistan were "between 400 and 600".
In 1994, the Salafi groups waging Salafi jihadism in Bosnia entered into decline, and groups such as the Egyptian Islamic Jihad began to drift away from the Salafi cause in Europe. Al-Qaeda stepped in and assumed control of around 80% of non-state armed cells in Bosnia in late 1995. At the same time, al-Qaeda ideologues instructed the network's recruiters to look for "Jihadi international" Muslims who believed that extremist-"jihad" must be fought on a global level. Al-Qaeda also sought to open the "offensive phase" of the global Salafi "jihad". Bosnian Islamists in 2006 called for "solidarity with Islamic causes around the world", supporting the insurgents in Kashmir and Iraq as well as the groups fighting for a Palestinian state.
In 1996, al-Qaeda announced its "jihad" to expel foreign troops and interests from what they considered Islamic lands. Bin Laden issued a "fatwa", which amounted to a public declaration of war against the US and its allies, and began to refocus al-Qaeda's resources on large-scale, propagandist strikes.
On February 23, 1998, bin Laden and Ayman al-Zawahiri, a leader of Egyptian Islamic Jihad, along with three other Islamist leaders, co-signed and issued a "fatwa" calling on Muslims to kill Americans and their allies. Under the banner of the World Islamic Front for Combat Against the Jews and Crusaders, they declared:
Neither bin Laden nor al-Zawahiri possessed the traditional Islamic scholarly qualifications to issue a "fatwa". However, they rejected the authority of the contemporary "ulema" (which they saw as the paid servants of "jahiliyya" rulers), and took it upon themselves.
Al-Qaeda has launched attacks against the Iraqi Shia majority in an attempt to incite sectarian violence. Al-Zarqawi purportedly declared an all-out war on Shiites while claiming responsibility for Shiite mosque bombings. The same month, a statement claiming to be from Al-Qaeda in Iraq was rejected as a "fake". In a December 2007 video, al-Zawahiri defended the Islamic State in Iraq, but distanced himself from the attacks against civilians, which he deemed to be perpetrated by "hypocrites and traitors existing among the ranks".
US and Iraqi officials accused Al-Qaeda in Iraq of trying to slide Iraq into a full-scale civil war between Iraq's Shiite population and Sunni Arabs. This was done through an orchestrated campaign of civilian massacres and a number of provocative attacks against high-profile religious targets. With attacks including the 2003 Imam Ali Mosque bombing, the 2004 Day of Ashura and Karbala and Najaf bombings, the 2006 first al-Askari Mosque bombing in Samarra, the deadly single-day series of bombings in which at least 215 people were killed in Baghdad's Shiite district of Sadr City, and the second al-Askari bombing in 2007, Al-Qaeda in Iraq provoked Shiite militias to unleash a wave of retaliatory attacks, resulting in death squad-style killings and further sectarian violence which escalated in 2006. In 2008, sectarian bombings blamed on al-Qaeda in Iraq killed at least 42 people at the Imam Husayn Shrine in Karbala in March, and at least 51 people at a bus stop in Baghdad in June.
In February 2014, after a prolonged dispute with al-Qaeda in Iraq's successor organisation, the Islamic State of Iraq and the Levant (ISIS), al-Qaeda publicly announced it was cutting all ties with the group, reportedly for its brutality and "notorious intractability".
In Somalia, al-Qaeda agents had been collaborating closely with its Somali wing, which was created from the al-Shabaab group. In February 2012, al-Shabaab officially joined al-Qaeda, declaring loyalty in a video. Somalian al-Qaeda recruited children for suicide-bomber training, recruited young people to participate in militant actions against Americans.
The percentage of attacks in the First World originating from the Afghanistan–Pakistan (AfPak) border declined starting in 2007, as al-Qaeda shifted to Somalia and Yemen. While al-Qaeda leaders were hiding in the tribal areas along the AfPak border, middle-tier leaders heightened activity in Somalia and Yemen.
In January 2009, al-Qaeda's division in Saudi Arabia merged with its Yemeni wing to form al-Qaeda in the Arabian Peninsula (AQAP). Centered in Yemen, the group takes advantage of the country's poor economy, demography and domestic security. In August 2009, the group made an assassination attempt against a member of the Saudi royal family. President Obama asked Ali Abdullah Saleh to ensure closer cooperation with the US in the struggle against the growing activity of al-Qaeda in Yemen, and promised to send additional aid. The wars in Iraq and Afghanistan, drew US attention from Somalia and Yemen. In December 2011, US Secretary of Defense Leon Panetta said that the US operations against al-Qaeda "are now concentrating on key groups in Yemen, Somalia and North Africa." Al-Qaeda in the Arabian Peninsula claimed responsibility for the 2009 bombing attack on Northwest Airlines Flight 253 by Umar Farouk Abdulmutallab. The AQAP declared the Al-Qaeda Emirate in Yemen on March 31, 2011, after capturing the most of the Abyan Governorate.
As the Saudi-led military intervention in Yemen escalated in July 2015, 50 civilians were killed, and 20 million were in need of aid. In February 2016, al-Qaeda forces and Saudi Arabian-led coalition forces were both seen fighting Houthi rebels in the same battle. In August 2018, "Al Jazeera" reported that "A military coalition battling Houthi rebels secured secret deals with al-Qaeda in Yemen and recruited hundreds of the group's fighters. ... Key figures in the deal-making said the United States was aware of the arrangements and held off on drone attacks against the armed group, which was created by Osama bin Laden in 1988."
In December 1998, the Director of the CIA Counterterrorism Center reported to President Bill Clinton that al-Qaeda was preparing to launch attacks in the United States, and that the group was training personnel to hijack aircraft. On September 11, 2001, al-Qaeda attacked the United States, hijacking four airliners within the country and deliberately crashing two into the twin towers of the World Trade Center in New York City. The third plane crashed into the western side of the Pentagon in Arlington County, Virginia. The fourth plane was crashed into a field in Shanksville, Pennsylvania. In total, the attackers killed 2,977 victims and injured more than 6,000 others.
US officials noted that Anwar al-Awlaki had considerable reach within the US. A former FBI agent identified Awlaki as a known "senior recruiter for al-Qaeda", and a spiritual motivator. Awlaki's sermons in the US were attended by three of the 9/11 hijackers, and accused Fort Hood shooter Nidal Malik Hasan. US intelligence intercepted emails from Hasan to Awlaki between December 2008 and early 2009. On his website, Awlaki has praised Hasan's actions in the Fort Hood shooting.
An unnamed official claimed there was good reason to believe Awlaki "has been involved in very serious terrorist activities since leaving the US [in 2002], including plotting attacks against America and our allies." US President Barack Obama approved the targeted killing of al-Awlaki by April 2010, making al-Awlaki the first US citizen ever placed on the CIA target list. That required the consent of the US National Security Council, and officials argued that the attack was appropriate because the individual posed an imminent danger to national security. In May 2010, Faisal Shahzad, who pleaded guilty to the 2010 Times Square car bombing attempt, told interrogators he was "inspired by" al-Awlaki, and sources said Shahzad had made contact with al-Awlaki over the Internet. Representative Jane Harman called him "terrorist number one", and "Investor's Business Daily" called him "the world's most dangerous man". In July 2010, the US Treasury Department added him to its list of Specially Designated Global Terrorists, and the UN added him to its list of individuals associated with al-Qaeda. In August 2010, al-Awlaki's father initiated a lawsuit against the US government with the American Civil Liberties Union, challenging its order to kill al-Awlaki. In October 2010, US and UK officials linked al-Awlaki to the 2010 cargo plane bomb plot. In September 2011, al-Awlaki was killed in a targeted killing drone attack in Yemen. On March 16, 2012, it was reported that Osama bin Laden plotted to kill US President Barack Obama.
On May 1, 2011, US President Barack Obama announced that Osama bin Laden had been killed by "a small team of Americans" acting under direct orders, in a covert operation in Abbottabad, Pakistan. The action took place north of Islamabad. According to US officials, a team of 20–25 US Navy SEALs under the command of the Joint Special Operations Command stormed bin Laden's compound with two helicopters. Bin Laden and those with him were killed during a firefight in which US forces experienced no casualties. According to one US official the attack was carried out without the knowledge or consent of the Pakistani authorities. In Pakistan some people were reported to be shocked at the unauthorized incursion by US armed forces. The site is a few miles from the Pakistan Military Academy in Kakul. In his broadcast announcement President Obama said that US forces "took care to avoid civilian casualties."
Details soon emerged that three men and a woman were killed along with bin Laden, the woman being killed when she was "used as a shield by a male combatant". DNA from bin Laden's body, compared with DNA samples on record from his dead sister, confirmed bin Laden's identity. The body was recovered by the US military and was in its custody until, according to one US official, his body was buried at sea according to Islamic traditions. One US official stated that "finding a country willing to accept the remains of the world's most wanted terrorist would have been difficult." US State Department issued a "Worldwide caution" for Americans following bin Laden's death and US diplomatic facilities everywhere were placed on high alert, a senior US official said. Crowds gathered outside the White House and in New York City's Times Square to celebrate bin Laden's death.
In 2003, President Bashar al-Assad revealed in an interview with a Kuwaiti newspaper that he doubted that al-Qaeda even existed. He was quoted as saying, "Is there really an entity called al-Qaeda? Was it in Afghanistan? Does it exist now?" He went on further to remark about bin Laden, commenting "[he] cannot talk on the phone or use the Internet, but he can direct communications to the four corners of the world? This is illogical."
Following the mass protests that took place in 2011, which demanded the resignation of al-Assad, al-Qaeda affiliated groups and Sunni sympathizers soon began to constitute an effective fighting force against al-Assad. Before the Syrian Civil War, al-Qaeda's presence in Syria was negligible, but its growth thereafter was rapid. Groups such as the al-Nusra Front and the Islamic State of Iraq and the Levant have recruited many foreign Mujahideen to train and fight in what has gradually become a highly sectarian war. Ideologically, the Syrian Civil War has served the interests of al-Qaeda as it pits a mainly Sunni opposition against a Shia government. Al-Qaeda and other fundamentalist Sunni militant groups have invested heavily in the civil conflict, at times actively backing and supporting the mainstream Syrian Opposition.
On February 2, 2014, al-Qaeda distanced itself from ISIS and its actions in Syria; however, during 2014–15, ISIS and the al-Qaeda-linked al-Nusra Front were still able to occasionally cooperate in their fight against the Syrian government. Al-Nusra (backed by Saudi Arabia and Turkey as part of the Army of Conquest during 2015–2017) launched many attacks and bombings, mostly against targets affiliated with or supportive of the Syrian government. From October 2015, Russian air strikes targeted positions held by al-Nusra Front, as well as other Islamist and non-Islamist rebels, while the US also targeted al-Nusra with airstrikes. In early 2016, a leading ISIL ideologue described al-Qaeda as the "Jews of jihad".
In September 2014 al-Zawahiri announced al-Qaeda was establishing a front in India to "wage jihad against its enemies, to liberate its land, to restore its sovereignty, and to revive its Caliphate." Al-Zawahiri nominated India as a beachhead for regional jihad taking in neighboring countries such as Myanmar and Bangladesh. The motivation for the video was questioned, as it appeared the militant group was struggling to remain relevant in light of the emerging prominence of ISIS. The new wing was to be known as "Qaedat al-Jihad fi'shibhi al-qarrat al-Hindiya" or al-Qaida in the Indian Subcontinent (AQIS). Leaders of several Indian Muslim organizations rejected al-Zawahiri's pronouncement, saying they could see no good coming from it, and viewed it as a threat to Muslim youth in the country.
In 2014 "Zee News" reported that Bruce Riedel, a former CIA analyst and National Security Council official for South Asia, accused the Pakistan military intelligence and Inter-Services Intelligence (ISI) of organising and assisting Al-Qaeda to organise in India, that Pakistan ought to be warned that it will be placed on the list of State Sponsors of Terrorism, and wrote that "Zawahiri made the tape in his hideout in Pakistan, no doubt, and many Indians suspect the ISI is helping to protect him".
Al-Qaeda has carried out a total of six major attacks, four of them in its jihad against America. In each case the leadership planned the attack years in advance, arranging for the shipment of weapons and explosives and using its businesses to provide operatives with safehouses and false identities.
Al-Qaeda usually does not disburse funds for attacks, and very rarely makes wire transfers.
On December 29, 1992, al-Qaeda's launched its first attack, the 1992 Yemen hotel bombings. Two bombs were detonated in Aden, Yemen. The first target was the Movenpick Hotel and the second was the parking lot of the Goldmohur Hotel.
The bombings were an attempt to eliminate American soldiers on their way to Somalia to take part in the international famine relief effort, Operation Restore Hope. Internally, al-Qaeda considered the bombing a victory that frightened the Americans away, but in the US, the attack was barely noticed. No American soldiers were killed because no soldiers were staying in the hotel which was bombed. However, an Australian tourist and a Yemeni hotel worker were killed in the bombing. Seven others, mostly Yemenis, were severely injured. Two fatwas are said to have been appointed by al-Qaeda's members, Mamdouh Mahmud Salim, to justify the killings according to Islamic law. Salim referred to a famous fatwa appointed by Ibn Taymiyyah, a 13th-century scholar much admired by Wahhabis, which sanctioned resistance by any means during the Mongol invasions.
In 1996, bin Laden personally engineered a plot to assassinate United States President Bill Clinton while the president was in Manila for the Asia-Pacific Economic Cooperation. However, intelligence agents intercepted a message before the motorcade was to leave, and alerted the US Secret Service. Agents later discovered a bomb planted under a bridge.
On August 7, 1998, al-Qaeda bombed the US embassies in East Africa, killing 224 people, including 12 Americans. In retaliation, a barrage of cruise missiles launched by the US military devastated an al-Qaeda base in Khost, Afghanistan. The network's capacity was unharmed. In late 1999 and 2000, Al-Qaeda planned attacks to coincide with the millennium, masterminded by Abu Zubaydah and involving Abu Qatada, which would include the bombing of Christian holy sites in Jordan, the bombing of Los Angeles International Airport by Ahmed Ressam, and the bombing of the .
On October 12, 2000, al-Qaeda militants in Yemen bombed the missile destroyer "USS Cole" in a suicide attack, killing 17 US servicemen and damaging the vessel while it lay offshore. Inspired by the success of such a brazen attack, al-Qaeda's command core began to prepare for an attack on the US itself.
The September 11 attacks on America by al-Qaeda killed 2,977 people — 2,507 civilians, 343 firefighters, 72 law enforcement officers, and 55 military personnel. Two commercial airliners were deliberately flown into the twin towers of the World Trade Center, a third into the Pentagon, and a fourth, originally intended to target either the United States Capitol or the White House, crashed in a field in Stonycreek Township near Shanksville, Pennsylvania. It was also the deadliest foreign attack on American soil since the Japanese attack on Pearl Harbor on December 7, 1941.
The attacks were conducted by al-Qaeda, acting in accord with the 1998 "fatwa" issued against the US and its allies by persons under the command of bin Laden, al-Zawahiri, and others. Evidence points to suicide squads led by al-Qaeda military commander Mohamed Atta as the culprits of the attacks, with bin Laden, Ayman al-Zawahiri, Khalid Sheikh Mohammed, and Hambali as the key planners and part of the political and military command.
Messages issued by bin Laden after September 11, 2001, praised the attacks, and explained their motivation while denying any involvement. Bin Laden legitimized the attacks by identifying grievances felt by both mainstream and Islamist Muslims, such as the general perception that the US was actively oppressing Muslims.
Bin Laden asserted that America was massacring Muslims in "Palestine, Chechnya, Kashmir and Iraq" and that Muslims should retain the "right to attack in reprisal." He also claimed the 9/11 attacks were not targeted at people, but "America's icons of military and economic power," despite the fact he planned to attack in the morning when most of the people in the intended targets were present and thus generating the maximum number of human casualties.
Evidence has since come to light that the original targets for the attack may have been nuclear power stations on the US East Coast. The targets were later altered by al-Qaeda, as it was feared that such an attack "might get out of hand".
Al-Qaeda is deemed a designated terrorist group by the following countries and international organizations:
 
In the immediate aftermath of the 9/11 attacks, the US government responded, and began to prepare its armed forces to overthrow the Taliban, which it believed was harboring al-Qaeda. The US offered Taliban leader Mullah Omar a chance to surrender bin Laden and his top associates. The first forces to be inserted into Afghanistan were paramilitary officers from the CIA's elite Special Activities Division (SAD).
The Taliban offered to turn over bin Laden to a neutral country for trial if the US would provide evidence of bin Laden's complicity in the attacks. US President George W. Bush responded by saying: "We know he's guilty. Turn him over", and British Prime Minister Tony Blair warned the Taliban regime: "Surrender bin Laden, or surrender power".
Soon thereafter the US and its allies invaded Afghanistan, and together with the Afghan Northern Alliance removed the Taliban government as part of the war in Afghanistan. As a result of the US special forces and air support for the Northern Alliance ground forces, a number of Taliban and al-Qaeda training camps were destroyed, and much of the operating structure of al-Qaeda is believed to have been disrupted. After being driven from their key positions in the Tora Bora area of Afghanistan, many al-Qaeda fighters tried to regroup in the rugged Gardez region of the nation.
By early 2002, al-Qaeda had been dealt a serious blow to its operational capacity, and the Afghan invasion appeared to be a success. Nevertheless, a significant Taliban insurgency remained in Afghanistan.
Debate continued regarding the nature of al-Qaeda's role in the 9/11 attacks. The US State Department released a videotape showing bin Laden speaking with a small group of associates somewhere in Afghanistan shortly before the Taliban was removed from power. Although its authenticity has been questioned by a couple of people, the tape definitively implicates bin Laden and al-Qaeda in the September 11 attacks. The tape was aired on many television channels, with an accompanying English translation provided by the US Defense Department.
In September 2004, the 9/11 Commission officially concluded that the attacks were conceived and implemented by al-Qaeda operatives. In October 2004, bin Laden appeared to claim responsibility for the attacks in a videotape released through Al Jazeera, saying he was inspired by Israeli attacks on high-rises in the 1982 invasion of Lebanon: "As I looked at those demolished towers in Lebanon, it entered my mind that we should punish the oppressor in kind and that we should destroy towers in America in order that they taste some of what we tasted and so that they be deterred from killing our women and children."
By the end of 2004, the US government proclaimed that two-thirds of the most senior al-Qaeda figures from 2001 had been captured and interrogated by the CIA: Abu Zubaydah, Ramzi bin al-Shibh and Abd al-Rahim al-Nashiri in 2002; Khalid Sheikh Mohammed in 2003; and Saif al Islam el Masry in 2004. Mohammed Atef and several others were killed. The West was criticized for not being able to handle Al-Qaida despite a decade of the war.
Al-Qaeda involvement in Africa has included a number of bombing attacks in North Africa, while supporting parties in civil wars in Eritrea and Somalia. From 1991 to 1996, bin Laden and other al-Qaeda leaders were based in Sudan.
Islamist rebels in the Sahara calling themselves al-Qaeda in the Islamic Maghreb have stepped up their violence in recent years. French officials say the rebels have no real links to the al-Qaeda leadership, but this has been disputed. It seems likely that bin Laden approved the group's name in late 2006, and the rebels "took on the al Qaeda franchise label", almost a year before the violence began to escalate.
In Mali, the Ansar Dine faction was also reported as an ally of al-Qaeda in 2013. The Ansar al Dine faction aligned themselves with the AQIM.
Following the Libyan Civil War, the removal of Gaddafi and the ensuing period of post-civil war violence in Libya, various Islamist militant groups affiliated with al-Qaeda were able to expand their operations in the region. The 2012 Benghazi attack, which resulted in the death of US Ambassador J. Christopher Stevens and three other Americans, is suspected of having been carried out by various Jihadist networks, such as Al-Qaeda in the Islamic Maghreb, Ansar al-Sharia and several other Al-Qaeda affiliated groups. The capture of Nazih Abdul-Hamed al-Ruqai, a senior al-Qaeda operative wanted by the United States for his involvement in the 1998 United States embassy bombings, on October 5, 2013, by US Navy Seals, FBI and CIA agents illustrates the importance the US and other Western allies have placed on North Africa.
Prior to the September 11 attacks, al-Qaeda was present in Bosnia and Herzegovina, and its members were mostly veterans of the El Mudžahid detachment of the Bosnian Muslim Army of the Republic of Bosnia and Herzegovina. Three al-Qaeda operatives carried out the Mostar car bombing in 1997. The operatives were closely linked to and financed by the Saudi High Commission for Relief of Bosnia and Herzegovina founded by then-prince King Salman of Saudi Arabia.
Before the 9/11 attacks and the US invasion of Afghanistan, westerners who had been recruits at al-Qaeda training camps were sought after by al-Qaeda's military wing. Language skills and knowledge of Western culture were generally found among recruits from Europe, such was the case with Mohamed Atta, an Egyptian national studying in Germany at the time of his training, and other members of the Hamburg Cell. Osama bin Laden and Mohammed Atef would later designate Atta as the ringleader of the 9/11 hijackers. Following the attacks, Western intelligence agencies determined that al-Qaeda cells operating in Europe had aided the hijackers with financing and communications with the central leadership based in Afghanistan.
In 2003, Islamists carried out a series of bombings in Istanbul killing fifty-seven people and injuring seven hundred. Seventy-four people were charged by the Turkish authorities. Some had previously met bin Laden, and though they specifically declined to pledge allegiance to al-Qaeda they asked for its blessing and help.
In 2009, three Londoners, Tanvir Hussain, Assad Sarwar and Ahmed Abdullah Ali, were convicted of conspiring to detonate bombs disguised as soft drinks on seven airplanes bound for Canada and the US The MI5 investigation regarding the plot involved more than a year of surveillance work conducted by over two hundred officers. British and US officials said the plot – unlike many similar homegrown European Islamic militant plots – was directly linked to al-Qaeda and guided by senior al-Qaeda members in Pakistan.
In 2012, Russian Intelligence indicated that al-Qaeda had given a call for "forest jihad" and has been starting massive forest fires as part of a strategy of "thousand cuts".
Following Yemeni unification in 1990, Wahhabi networks began moving missionaries into the country. Although it is unlikely that bin Laden or Saudi al-Qaeda were directly involved, the personal connections they made would be established over the next decade and used in the USS "Cole" bombing. Concerns grew over Al Qaeda's group in Yemen.
In Iraq, al-Qaeda forces loosely associated with the leadership were embedded in the Jama'at al-Tawhid wal-Jihad group commanded by Abu Musab al-Zarqawi. Specializing in suicide operations, they have been a "key driver" of the Sunni insurgency. Although they played a small part in the overall insurgency, between 30% and 42% of all suicide bombings which took place in the early years were claimed by Zarqawi's group. Reports have indicated that oversights such as the failure to control access to the Qa'qaa munitions factory in Yusufiyah have allowed large quantities of munitions to fall into the hands of al-Qaida. In November 2010, the militant group Islamic State of Iraq, which is linked to al-Qaeda in Iraq, threatened to "exterminate all Iraqi Christians".
Al-Qaeda did not begin training Palestinians until the late 1990s. Large groups such as Hamas and Palestinian Islamic Jihad have rejected an alliance with al-Qaeda, fearing that al-Qaeda will co-opt their cells. This may have changed recently. The Israeli security and intelligence services believe that al-Qaeda has managed to infiltrate operatives from the Occupied Territories into Israel, and is waiting for an opportunity to attack.
, Saudi Arabia, Qatar and Turkey are openly supporting the Army of Conquest, an umbrella rebel group fighting in the Syrian Civil War against the Syrian government that reportedly includes an al-Qaeda linked al-Nusra Front and another Salafi coalition known as Ahrar al-Sham.
Bin Laden and Ayman al-Zawahiri consider India to be a part of an alleged Crusader-Zionist-Hindu conspiracy against the Islamic world. According to a 2005 report by the Congressional Research Service, bin Laden was involved in training militants for Jihad in Kashmir while living in Sudan in the early 1990s. By 2001, Kashmiri militant group Harkat-ul-Mujahideen had become a part of the al-Qaeda coalition. According to the United Nations High Commissioner for Refugees (UNHCR), al-Qaeda was thought to have established bases in Pakistan administered Kashmir (in Azad Kashmir, and to some extent in Gilgit–Baltistan) during the 1999 Kargil War and continued to operate there with tacit approval of Pakistan's Intelligence services.
Many of the militants active in Kashmir were trained in the same madrasahs as Taliban and al-Qaeda. Fazlur Rehman Khalil of Kashmiri militant group Harkat-ul-Mujahideen was a signatory of al-Qaeda's 1998 declaration of Jihad against America and its allies. In a 'Letter to American People' (2002), bin Laden wrote that one of the reasons he was fighting America was because of its support to India on the Kashmir issue. In November 2001, Kathmandu airport went on high alert after threats that bin Laden planned to hijack a plane and crash it into a target in New Delhi. In 2002, US Secretary of Defense Donald Rumsfeld, on a trip to Delhi, suggested that al-Qaeda was active in Kashmir though he did not have any evidence. Rumsfeld proposed hi-tech ground sensors along the Line of Control to prevent militants from infiltrating into Indian-administered Kashmir.
An investigation in 2002 found evidence that al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's Inter-Services Intelligence. In 2002, a special team of Special Air Service and Delta Force was sent into Indian-Administered Kashmir to hunt for bin Laden after receiving reports that he was being sheltered by Kashmiri militant group Harkat-ul-Mujahideen, which had been responsible for kidnapping western tourists in Kashmir in 1995. Britain's highest-ranking al-Qaeda operative Rangzieb Ahmed had previously fought in Kashmir with the group Harkat-ul-Mujahideen and spent time in Indian prison after being captured in Kashmir.
US officials believe that al-Qaeda was helping organize attacks in Kashmir in order to provoke conflict between India and Pakistan. Their strategy was to force Pakistan to move its troops to the border with India, thereby relieving pressure on al-Qaeda elements hiding in northwestern Pakistan. In 2006 al-Qaeda claimed they had established a wing in Kashmir. However Indian Army General H. S. Panag argued that the army had ruled out the presence of al-Qaeda in Indian-administered Jammu and Kashmir. Panag also stated that al-Qaeda had strong ties with Kashmiri militant groups Lashkar-e-Taiba and Jaish-e-Mohammed based in Pakistan. It has been noted that Waziristan has become a battlefield for Kashmiri militants fighting NATO in support of al-Qaeda and Taliban. Dhiren Barot, who wrote the "Army of Madinah in Kashmir" and was an al-Qaeda operative convicted for involvement in the 2004 financial buildings plot, had received training in weapons and explosives at a militant training camp in Kashmir.
Maulana Masood Azhar, the founder of Kashmiri group Jaish-e-Mohammed, is believed to have met bin Laden several times and received funding from him. In 2002, Jaish-e-Mohammed organized the kidnapping and murder of Daniel Pearl in an operation run in conjunction with al-Qaeda and funded by bin Laden. According to American counter-terrorism expert Bruce Riedel, al-Qaeda and Taliban were closely involved in the 1999 hijacking of Indian Airlines Flight 814 to Kandahar which led to the release of Maulana Masood Azhar and Ahmed Omar Saeed Sheikh from an Indian prison. This hijacking, Riedel stated, was rightly described by then Indian Foreign Minister Jaswant Singh as a 'dress rehearsal' for September 11 attacks. Bin Laden personally welcomed Azhar and threw a lavish party in his honor after his release. Ahmed Omar Saeed Sheikh, who had been in prison for his role in the 1994 kidnappings of Western tourists in India, went on to murder Daniel Pearl and was sentenced to death in Pakistan. Al-Qaeda operative Rashid Rauf, who was one of the accused in 2006 transatlantic aircraft plot, was related to Maulana Masood Azhar by marriage.
Lashkar-e-Taiba, a Kashmiri militant group which is thought to be behind 2008 Mumbai attacks, is also known to have strong ties to senior al-Qaeda leaders living in Pakistan. In late 2002, top al-Qaeda operative Abu Zubaydah was arrested while being sheltered by Lashkar-e-Taiba in a safe house in Faisalabad. The FBI believes that al-Qaeda and Lashkar have been 'intertwined' for a long time while the CIA has said that al-Qaeda funds Lashkar-e-Taiba. Jean-Louis Bruguière told Reuters in 2009 that "Lashkar-e-Taiba is no longer a Pakistani movement with only a Kashmir political or military agenda. Lashkar-e-Taiba is a member of al-Qaeda."
In a video released in 2008, American-born senior al-Qaeda operative Adam Yahiye Gadahn stated that "victory in Kashmir has been delayed for years; it is the liberation of the jihad there from this interference which, Allah willing, will be the first step towards victory over the Hindu occupiers of that Islam land."
In September 2009, a US drone strike reportedly killed Ilyas Kashmiri who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' al-Qaeda member while others have described him as head of military operations for al-Qaeda. Kashmiri was also charged by the US in a plot against Jyllands-Posten, the Danish newspaper which was at the center of Jyllands-Posten Muhammad cartoons controversy. US officials also believe that Kashmiri was involved in the Camp Chapman attack against the CIA. In January 2010, Indian authorities notified Britain of an al-Qaeda plot to hijack an Indian airlines or Air India plane and crash it into a British city. This information was uncovered from interrogation of Amjad Khwaja, an operative of Harkat-ul-Jihad al-Islami, who had been arrested in India.
In January 2010, US Defense secretary Robert Gates, while on a visit to Pakistan, stated that al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan.
Al-Qaeda and its successors have migrated online to escape detection in an atmosphere of increased international vigilance. The group's use of the Internet has grown more sophisticated, with online activities that include financing, recruitment, networking, mobilization, publicity, and information dissemination, gathering and sharing.
Abu Ayyub al-Masri's al-Qaeda movement in Iraq regularly releases short videos glorifying the activity of jihadist suicide bombers. In addition, both before and after the death of Abu Musab al-Zarqawi (the former leader of al-Qaeda in Iraq), the umbrella organization to which al-Qaeda in Iraq belongs, the Mujahideen Shura Council, has a regular presence on the Web.
The range of multimedia content includes guerrilla training clips, stills of victims about to be murdered, testimonials of suicide bombers, and videos that show participation in jihad through stylized portraits of mosques and musical scores. A website associated with al-Qaeda posted a video of captured American entrepreneur Nick Berg being decapitated in Iraq. Other decapitation videos and pictures, including those of Paul Johnson, Kim Sun-il, and Daniel Pearl, were first posted on jihadist websites.
In December 2004 an audio message claiming to be from bin Laden was posted directly to a website, rather than sending a copy to al Jazeera as he had done in the past. Al-Qaeda turned to the Internet for release of its videos in order to be certain they would be available unedited, rather than risk the possibility of al Jazeera editing out anything critical of the Saudi royal family.
Alneda.com and Jehad.net were perhaps the most significant al-Qaeda websites. Alneda was initially taken down by American Jon Messner, but the operators resisted by shifting the site to various servers and strategically shifting content.
The US government charged a British information technology specialist, Babar Ahmad, with terrorist offences related to his operating a network of English-language al-Qaeda websites, such as Azzam.com. He was convicted and sentenced to 12-and-a-half years in prison.
In 2007, al-Qaeda released "Mujahedeen Secrets", encryption software used for online and cellular communications. A later version, "Mujahideen Secrets 2", was released in 2008.
Al-Qaeda is believed to be operating a clandestine aviation network including "several Boeing 727 aircraft", turboprops and executive jets, according to a 2010 Reuters story. Based on a US Department of Homeland Security report, the story said that al-Qaeda is possibly using aircraft to transport drugs and weapons from South America to various unstable countries in West Africa. A Boeing 727 can carry up to 10 tons of cargo. The drugs eventually are smuggled to Europe for distribution and sale, and the weapons are used in conflicts in Africa and possibly elsewhere. Gunmen with links to al-Qaeda have been increasingly kidnapping Europeans for ransom. The profits from the drug and weapon sales, and kidnappings can, in turn, fund more militant activities.
The following is a list of military conflicts in which Al-Qaeda and its direct affiliates have taken part militarily.
 Experts debate the notion al-Qaeda attacks were an indirect result from the American CIA's Operation Cyclone program to help the Afghan mujahideen. Robin Cook, British Foreign Secretary from 1997 to 2001, has written that al-Qaeda and bin Laden were "a product of a monumental miscalculation by western security agencies", and that "Al-Qaida, literally 'the database', was originally the computer file of the thousands of mujahideen who were recruited and trained with help from the CIA to defeat the Russians."
Munir Akram, Permanent Representative of Pakistan to the United Nations from 2002 to 2008, wrote in a letter published in "The New York Times" on January 19, 2008:
A variety of sources, including CNN journalist Peter Bergen, Pakistani ISI Brigadier Mohammad Yousaf, and CIA operatives involved in the Afghan program, such as Vincent Cannistraro, deny that the CIA or other American officials had contact with the foreign mujahideen or bin Laden, let alone armed, trained, coached or indoctrinated them.
Bergen and others argue that there was no need to recruit foreigners unfamiliar with the local language, customs or lay of the land since there were a quarter of a million local Afghans willing to fight. Bergen further argues that foreign mujahideen had no need for American funds since they received several million dollars per year from internal sources. Lastly, he argues that Americans could not have trained the mujahideen because Pakistani officials would not allow more than a handful of them to operate in Pakistan and none in Afghanistan, and that the Afghan Arabs were almost invariably militant Islamists reflexively hostile to Westerners whether or not the Westerners were helping the Muslim Afghans.
According to Bergen, who conducted the first television interview with bin Laden in 1997: the idea that "the CIA funded bin Laden or trained bin Laden... [is] a folk myth. There's no evidence of this... Bin Laden had his own money, he was anti-American and he was operating secretly and independently... The real story here is the CIA didn't really have a clue about who this guy was until 1996 when they set up a unit to really start tracking him."
Jason Burke also wrote:
CNN report has revealed that Saudi Arabia and the United Arab Emirates (UAE) have been handing out sophisticated American-made weapons to al-Qaeda-linked fighters in Yemen.
In October 2014, US Vice President Joe Biden stated that Saudi Arabia and the United Arab Emirates had "poured hundreds of millions of dollars and tens of thousands of tons of weapons into anyone who would fight against Al-Assad, except that the people who were being supplied were al-Nusra, and al Qaeda, and the extremist elements of jihadis coming from other parts of the world."
Anders Behring Breivik, the perpetrator of the 2011 Norway attacks, was inspired by Al-Qaeda, calling it "the most successful revolutionary movement in the world." While admitting different aims, he sought to "create a European version of Al-Qaida."
The appropriate response to offshoots is a subject of debate. A journalist reported in 2012 that a senior U.S. military planner had asked: "Should we resort to drones and Special Operations raids every time some group raises the black banner of al Qaeda? How long can we continue to chase offshoots of offshoots around the world?"
Islamic extremism dates back to the Kharijites of the 7th century. From their essentially political position, the Kharijites developed extreme doctrines that set them apart from both mainstream Sunni and Shiʿa Muslims. The Kharijites were particularly noted for adopting a radical approach to Takfir, whereby they declared other Muslims to be unbelievers and therefore deemed them worthy of death.
According to a number of sources, a "wave of revulsion" has been expressed against al-Qaeda and its affiliates by "religious scholars, former fighters and militants" who are alarmed by al-Qaeda's takfir and its killing of Muslims in Muslim countries, especially in Iraq.
Noman Benotman, a former Afghan Arab and a militant member of the Libyan Islamic Fighting Group (LIFG), went public with an open letter of criticism to Ayman al-Zawahiri in November 2007, after persuading the imprisoned senior leaders of his former group to enter into peace negotiations with the Libyan regime. While Ayman al-Zawahiri announced the affiliation of the group with al-Qaeda in November 2007, the Libyan government released 90 members of the group from prison several months after "they were said to have renounced violence."
In 2007, on the anniversary of the September 11 attacks, the Saudi sheikh Salman al-Ouda delivered a personal rebuke to bin Laden. Al-Ouda, a religious scholar and one of the fathers of the Sahwa, the fundamentalist awakening movement that swept through Saudi Arabia in the 1980s, is a widely respected critic of jihadism. Al-Ouda addressed al-Qaeda's leader on television asking him:
According to Pew polls, support for al-Qaeda had dropped in the Muslim world in the years before 2008. Support of suicide bombings in Indonesia, Lebanon, and Bangladesh, dropped by half or more in the last five years. In Saudi Arabia, only 10 percent had a favorable view of al-Qaeda, according to a December 2017 poll by Terror Free Tomorrow, a Washington-based think tank.
In 2007, the imprisoned Sayyed Imam Al-Sharif, an influential Afghan Arab, "ideological godfather of al-Qaeda", and former supporter of takfir, withdrew his support from al-Qaeda with a book "Wathiqat Tarshid Al-'Aml Al-Jihadi fi Misr w'Al-'Alam" ().
Although once associated with al-Qaeda, in September 2009 LIFG completed a new "code" for jihad, a 417-page religious document entitled "Corrective Studies". Given its credibility and the fact that several other prominent Jihadists in the Middle East have turned against al-Qaeda, the LIFG's reversal may be an important step toward staunching al-Qaeda's recruitment.
Bilal Abdul Kareem, an American journalist based in Syria created a documentary about al-Shabab, al-Qaeda's affiliate in Somalia. The documentary included interviews with former members of the group who stated their reasons for leaving al-Shabab. The members made accusations of segregation, lack of religious awareness and internal corruption and favoritism. In response to Kareem, the Global Islamic Media Front condemned Kareem, called him a liar, and denied the accusations from the former fighters.
In mid-2014 after the Islamic State of Iraq and the Levant declared that they had restored the Caliphate, an audio statement was released by the then-spokesman of the group Abu Muhammad al-Adnani claiming that "the legality of all emirates, groups, states, and organizations, becomes null by the expansion of the Caliphate's authority". The speech included a religious refutation of Al-Qaeda for being too lenient regarding Shiites and their refusal to recognize the authority Abu Bakr al-Baghdadi, al-Adnani specifically noting: "It is not suitable for a state to give allegiance to an organization". He also recalled a past instance in which Osama bin Laden called on al-Qaeda members and supporters to give allegiance to Abu Omar al-Baghdadi when the group was still solely operating in Iraq, as the Islamic State of Iraq, and condemned Ayman al-Zawahiri for not making this same claim for Abu Bakr al-Baghdadi, and that Zawahiri was encouraging factionalism and division between former allies of ISIL such as the al-Nusra Front.

</doc>
<doc id="1923" url="https://en.wikipedia.org/wiki?curid=1923" title="Alessandro Volta">
Alessandro Volta

Alessandro Giuseppe Antonio Anastasio Volta (; 18 February 1745 – 5 March 1827) was an Italian physicist, chemist, and pioneer of electricity and power who is credited as the inventor of the electric battery and the discoverer of methane. He invented the Voltaic pile in 1799, and reported the results of his experiments in 1800 in a two-part letter to the President of the Royal Society. With this invention Volta proved that electricity could be generated chemically and debunked the prevalent theory that electricity was generated solely by living beings. Volta's invention sparked a great amount of scientific excitement and led others to conduct similar experiments which eventually led to the development of the field of electrochemistry.
Volta also drew admiration from Napoleon Bonaparte for his invention, and was invited to the Institute of France to demonstrate his invention to the members of the Institute. Volta enjoyed a certain amount of closeness with the emperor throughout his life and he was conferred numerous honours by him. Volta held the chair of experimental physics at the University of Pavia for nearly 40 years and was widely idolised by his students.
Despite his professional success, Volta tended to be a person inclined towards domestic life and this was more apparent in his later years. At this time he tended to live secluded from public life and more for the sake of his family until his eventual death in 1827 from a series of illnesses which began in 1823. The SI unit of electric potential is named in his honour as the volt.
Volta was born in Como, a town in present-day northern Italy, on 18 February 1745. In 1794, Volta married an aristocratic lady also from Como, Teresa Peregrini, with whom he raised three sons: Zanino, Flaminio, and Luigi. His father, Filippo Volta, was of noble lineage. His mother, Donna Maddalena, came from the family of the Inzaghis.
In 1774, he became a professor of physics at the Royal School in Como. A year later, he improved and popularised the electrophorus, a device that produced static electricity. His promotion of it was so extensive that he is often credited with its invention, even though a machine operating on the same principle was described in 1762 by the Swedish experimenter Johan Wilcke. In 1777, he travelled through Switzerland. There he befriended H. B. de Saussure.
In the years between 1776 and 1778, Volta studied the chemistry of gases. He researched and discovered methane after reading a paper by Benjamin Franklin of the United States on "flammable air". In November 1776, he found methane at Lake Maggiore, and by 1778 he managed to isolate methane. He devised experiments such as the ignition of methane by an electric spark in a closed vessel.
Volta also studied what we now call electrical capacitance, developing separate means to study both electrical potential ("V") and charge ("Q"), and discovering that for a given object, they are proportional. This is called Volta's Law of Capacitance, and for this work the unit of electrical potential has been named the volt.
In 1779 he became a professor of experimental physics at the University of Pavia, a chair that he occupied for almost 40 years.
Luigi Galvani, an Italian physicist, discovered something he named, "animal electricity" when two different metals were connected in series with a frog's leg and to one another. Volta realised that the frog's leg served as both a conductor of electricity (what we would now call an electrolyte) and as a detector of electricity. He also understood that the frog's legs were irrelevant to the electric current, which was caused by the two differing metals. He replaced the frog's leg with brine-soaked paper, and detected the flow of electricity by other means familiar to him from his previous studies.
In this way he discovered the electrochemical series, and the law that the electromotive force (emf) of a galvanic cell, consisting of a pair of metal electrodes separated by electrolyte, is the difference between their two electrode potentials (thus, two identical electrodes and a common electrolyte give zero net emf). This may be called Volta's Law of the electrochemical series.
In 1800, as the result of a professional disagreement over the galvanic response advocated by Galvani, Volta invented the voltaic pile, an early electric battery, which produced a steady electric current. Volta had determined that the most effective pair of dissimilar metals to produce electricity was zinc and copper. Initially he experimented with individual cells in series, each cell being a wine goblet filled with brine into which the two dissimilar electrodes were dipped. The voltaic pile replaced the goblets with cardboard soaked in brine.
In announcing his discovery of the voltaic pile, Volta paid tribute to the influences of William Nicholson, Tiberius Cavallo, and Abraham Bennet.
The battery made by Volta is credited as one of the first electrochemical cells. It consists of two electrodes: one made of zinc, the other of copper. The electrolyte is either sulfuric acid mixed with water or a form of saltwater brine. The electrolyte exists in the form 2H and SO. Zinc metal, which is higher in the electrochemical series than both copper and hydrogen, is oxidized to zinc cations (Zn) and creates electrons that move to the copper electrode. The positively charged hydrogen ions (protons) capture electrons from the copper electrode, forming bubbles of hydrogen gas, H. This makes the zinc rod the negative electrode and the copper rod the positive electrode.
Thus, there are two terminals, and an electric current will flow if they are connected. The chemical reactions in this voltaic cell are as follows:
Copper metal does not react, but rather it functions as an electrode for the electric current. Sulfate anion (SO) does not undergo any chemical reaction either, but migrates to the zinc anode to compensate for the charge of the zinc cations formed there.
However, this cell also has some disadvantages. It is unsafe to handle, since sulfuric acid, even if diluted, can be hazardous. Also, the power of the cell diminishes over time because the hydrogen gas is not released. Instead, it accumulates on the surface of the copper electrode and forms a barrier between the metal and the electrolyte solution.
In 1809 Volta became associated member of the Royal Institute of the Netherlands. In honour of his work, Volta was made a count by Napoleon Bonaparte in 1810.
Volta retired in 1819 to his estate in Camnago, a frazione of Como, Italy, now named "Camnago Volta" in his honour. He died there on 5 March 1827, just after his 82nd birthday. Volta's remains were buried in Camnago Volta.
Volta's legacy is celebrated by the Tempio Voltiano memorial located in the public gardens by the lake. There is also a museum which has been built in his honour, which exhibits some of the equipment that Volta used to conduct experiments. Nearby stands the Villa Olmo, which houses the Voltian Foundation, an organization promoting scientific activities. Volta carried out his experimental studies and produced his first inventions near Como.
His image was depicted on the Italian 10,000 lire note (1990–1997) along with a sketch of his voltaic pile.
In late 2017, Nvidia announced a new workstation-focused microarchitecture called Volta, succeeding Pascal and preceding Turing. The first graphics cards featuring Volta were released in December 2017, with two more cards releasing over the course of 2018.
Volta was raised as a Catholic and for all of his life continued to maintain his belief. Because he was not ordained a clergyman as his family expected, he was sometimes accused of being irreligious and some people have speculated about his possible unbelief, stressing that "he did not join the Church", or that he virtually "ignored the church's call". Nevertheless, he cast out doubts in a declaration of faith in which he said:
I do not understand how anyone can doubt the sincerity and constancy of my attachment to the religion which I profess, the Roman, Catholic and Apostolic religion in which I was born and brought up, and of which I have always made confession, externally and internally. I have, indeed, and only too often, failed in the performance of those good works which are the mark of a Catholic Christian, and I have been guilty of many sins: but through the special mercy of God I have never, as far as I know, wavered in my faith... In this faith I recognise a pure gift of God, a supernatural grace; but I have not neglected those human means which confirm belief, and overthrow the doubts which at times arise. I studied attentively the grounds and basis of religion, the works of apologists and assailants, the reasons for and against, and I can say that the result of such study is to clothe religion with such a degree of probability, even for the merely natural reason, that every spirit unperverted by sin and passion, every naturally noble spirit must love and accept it. May this confession which has been asked from me and which I willingly give, written and subscribed by my own hand, with authority to show it to whomsoever you will, for I am not ashamed of the Gospel, may it produce some good fruit!

</doc>
<doc id="1924" url="https://en.wikipedia.org/wiki?curid=1924" title="Argo Navis">
Argo Navis

Argo Navis (the Ship Argo), or simply Argo, was a large constellation in the southern sky. The genitive was "Argus Navis", abbreviated "Arg". Flamsteed and other early modern astronomers called it Navis (the Ship), genitive "Navis", abbreviated "Nav".
The constellation proved to be of unwieldy size, as it was 28% larger than the next largest constellation and had more than 160 easily visible stars. The 1755 catalogue of Nicolas Louis de Lacaille divided it into the three modern constellations that occupy much of the same area: Carina (the hull), Puppis (the poop deck) and Vela (the sails).
Argo derived from the ship "Argo" in Greek mythology, sailed by Jason and the Argonauts to Colchis in search of the Golden Fleece. Some stars of Puppis and Vela can be seen from Mediterranean latitudes in winter and spring, the ship appearing to skim along the "river of the Milky Way." Due to precession of the equinoxes, the position of the stars from Earth's viewpoint has shifted southward, and though most of the constellation was visible in Classical times, the constellation is now not easily visible from most of the northern hemisphere. All the stars of Argo Navis are easily visible from the tropics southward, and pass near zenith from southern temperate latitudes. The brightest of these is Canopus (α Carinae), the second-brightest night-time star, now assigned to Carina.
Argo Navis was long-known to Greek observers, who are theorised to have derived it from Egypt around 1000 BCE. Plutarch attributed it to the Egyptian "Boat of Osiris." Some academics theorized a Sumerian origin related to the Epic of Gilgamesh, a hypothesis rejected for lack of evidence that the Sumerians or other Mesopotamian culture considered these stars, or any portion of them, to form a boat.
Over time, Argo became identified exclusively with ancient Greek myth of Jason and the Argonauts. In his Almagest, Claudius Ptolemy described Argo Navis as occupying the portion of the Milky Way between Canis Major and Centaurus, and identified stars comprising such details as the "little shield", the "steering-oar", the "mast-holder", and the "stern-ornament", which continued to be reflected in cartographic representations in celestial atlases into the nineteenth century (see below). The ship appeared to rotate about the pole sternwards, so nautically in reverse. Aratus, the Greek poet / historian living in the third century BCE, noted this backward progression writing, "Argo by the "Great Dog's" [Canis Major's] tail is drawn; for hers is not a usual course, but backward turned she comes ...".
In modern times, Argo Navis was considered unwieldy due to its enormous size (28% larger than Hydra, the largest modern constellation). In his 1763 star atlas, Nicolas Louis de Lacaille explained that there were more than a hundred and sixty stars clearly visible to the naked eye in Navis, and so he used the set of lowercase and uppercase Latin letters three times on portions of the constellation referred to as ""Argûs in carina" (Carina, the keel or hull), "Argûs in puppi" (Puppis, the poop deck or stern), and "Argûs in velis"" (Vela, the sails). Lacaille replaced Bayer's designations with new ones that followed stellar magnitudes more closely, but used only a single Greek-letter sequence and described the constellation for those stars as "Argûs". Similarly, faint unlettered stars were listed only as in "Argûs".
The final breakup and abolition of Argo Navis was proposed by Sir John Herschel in 1841 and again in 1844. Despite this, the constellation remained in use in parallel with its constituent parts into the 20th century. In 1922, along with the other constellations, it received a three-letter abbreviation: "Arg". The breakup and relegation to a former constellation occurred in 1930 when the IAU defined the 88 modern constellations, formally instituting "Carina", "Puppis", and "Vela", and declaring "Argo" obsolete. Lacaille's designations were kept in the offspring, so "Carina" has α, β, and ε; "Vela" has γ and δ; "Puppis" has ζ; and so on. As a result of this breakup, Argo Navis is the only one of the 48 listed by Ptolemy in his "Almagest" no longer officially recognized as a single constellation.
In addition, the constellation "Pyxis" (the mariner's compass) occupies an area near that which in antiquity was considered part of Argo's mast. Some recent authors state that modern Pyxis was part of the ancient Greek conception of Argo Navis, but magnetic compasses were unknown in ancient Greek times, nor does it appear that the stars now in Pyxis were included in the original conception of Argo Navis. Lacaille considered it a separate constellation representing a modern scientific instrument (like "Microscopium" and "Telescopium"), that he created for maps of the stars of the southern hemisphere. Pyxis was listed among his 14 new constellations, separate from Argo. In 1844, John Herschel suggested formalizing the mast as a new constellation, "Malus", to replace Lacaille's "Pyxis", but the idea did not catch on. Similarly, an effort by Edmond Halley to detach the "cloud of mist" at the prow of Argo Navis to form a new constellation named "Robur Carolinum" (Charles' Oak) in honor of King Charles II, his patron, was unsuccessful.
In Vedic astronomy, Indian observers also saw the asterism as "the Boat".
The Māori had several names for what was the constellation, including "Te Waka-o-Tamarereti" (the canoe of Tamarereti), "Te Kohi-a-Autahi" (an expression meaning "cold of autumn settling down on land and water"), and "Te Kohi".

</doc>
<doc id="1925" url="https://en.wikipedia.org/wiki?curid=1925" title="Andromeda (mythology)">
Andromeda (mythology)

In Greek mythology, Andromeda (; Greek: Ἀνδρομέδα, "Androméda" or Ἀνδρομέδη, "Andromédē") is the daughter of the king of Aethiopia, Cepheus, and his wife Cassiopeia. When Cassiopeia boasts that she is more beautiful than the Nereids, Poseidon sends the sea monster Cetus to ravage the coast of Aethiopia as divine punishment. Andromeda is chained to a rock as a sacrifice to sate the monster, but is saved from death by Perseus, who marries her and takes her to Greece to reign as his queen.
Her name is the Latinized form of the Greek ("Androméda") or ("Andromédē"): "ruler of men", from ("anēr, andrós") meaning "man", "husband", or "human being", and ("medō") "I protect, rule over".
As a subject, Andromeda has been popular in art since classical times; it is one of several Greek myths of a Greek hero's rescue of the intended victim of an archaic "hieros gamos" (sacred marriage), giving rise to the "princess and dragon" motif. From the Renaissance, interest revived in the original story, typically as derived from Ovid's "Metamorphoses" (4.663ff).
In Greek mythology Andromeda is the daughter of Cepheus and Cassiopeia, king and queen of ancient Ethiopia. Her mother Cassiopeia foolishly boasts that she is more beautiful than the Nereids, a display of hubris by a human that is unacceptable to the gods. To punish the queen for her arrogance, Poseidon floods the Ethiopian coast and sends a sea monster named Cetus to ravage the kingdom's inhabitants. In desperation, King Cepheus consults the oracle of Ammon, who announces that no respite can be found until the king sacrifices his daughter, Andromeda, to the monster. She is thus chained to a rock by the sea to await her death.
Perseus is just then flying near the coast of Ethiopia on his winged sandals, having slain the Gorgon Medusa and carrying her severed head, which instantly turns to stone any who look at it. Upon seeing Andromeda bound to the rock, Perseus falls in love with her, and he secures Cepheus' promise of her hand in marriage if he can save her. Perseus kills the monster with the magical sword he had used against Medusa, saving Andromeda. Preparations are then made for their marriage, in spite of her having been previously promised to her uncle, Phineus. Andromeda was never asked for her opinion. At the wedding a quarrel takes place between the rivals, and Perseus is forced to show Medusa's head to Phineus and his allies, turning them to stone.
Andromeda follows her husband to his native island of Serifos, where he rescues his mother Danaë. They next go to Argos, where Perseus is the rightful heir to the throne. After accidentally killing Argos' king, his grandfather Acrisius, however, Perseus chooses to become king of neighboring Tiryns instead. Perseus and Andromeda have seven sons: Perses (who, according to folk etymology, is the ancestor of the Persians), Alcaeus, Heleus, Mestor, Sthenelus, Electryon, and Cynurus as well as two daughters, Autochthe and Gorgophone. Their descendants rule Mycenae from Electryon down to Eurystheus, after whom Atreus attains the kingdom. The great hero Heracles (Hercules in Roman mythology) is also a descendant, his mother Alcmene being Electryon's daughter, while (like his grandfather Perseus) his father is the god Zeus. 
The goddess Athena (or her Roman version Minerva) places Andromeda in the northern sky at her death as the constellation Andromeda, along with Perseus and her parents Cepheus and Cassiopeia, in commemoration of Perseus' bravery in fighting the sea monster Cetus.
Variants of this story include:
Andromeda was the daughter of the king and queen of Ethiopia (Aithiopia/Aethiopia), which ancient Greeks located at the edge of the world. The term "Aithiops" was generally applied to peoples who dwelt above the equator, between the Atlantic Ocean and the Indian Ocean, being derived from the Greek words and ("aitho" “I burn” + "ops" “face”), translating as "burnt-face" in noun form and "red-brown" in adjectival form, as a reference to the Black African natives of the Kingdom of Kush. Homer says the Ethiopians live "at the world's end, and lie in two halves, the one looking West and the other East," an idea echoed by Ovid, who located Ethiopia next to India, close to where the sun rises each day. The 5th century BC historian Herodotus writes that "Where south inclines westwards, the part of the world stretching farthest towards the sunset is Ethiopia", while also claiming that there were Ethiopians who lived in Asia. 
By the 1st century BC a rival location for Andromeda's story had been established, however: an outcrop of rocks near the harbor of the ancient port city of Joppa (Iope or Jaffa, today part of Tel Aviv, Israel) had become associated with the place of Andromeda's chaining and rescue, as reported by Pliny the Elder, the traveler Pausanias, the geographer Strabo, and the historian Josephus. A case has been made that this new version of the myth was exploited to enhance the fame and serve the local tourist trade of Joppa, which also became connected with the biblical story of Jonah featuring yet another huge sea creature. This was, of course, at odds with Andromeda's Ethiopian origins, adding to the confusion already surrounding her ethnicity, as reflected in 5th century Greek vase images showing Andromeda attended by dark-skinned African servants and wearing clothing that would have looked foreign to Greeks, yet with light skin
Elizabeth McGrath, in her article "The Black Andromeda", discusses the tradition, as promoted by the influential Roman poet Ovid, of Andromeda being a dark-skinned woman of either Ethiopian or Indian origin. In his "Heroides" Ovid has Sappho explain to Phaon: "though I'm not pure white, Cepheus's dark Andromeda/charmed Perseus with her native color./White doves often choose mates of different hue/and the parrot loves the black turtle dove"; the Latin word "fuscae" Ovid uses here for "dark Andromeda" refers to the color black or brown. Elsewhere he says that Perseus brought Andromeda from "darkest" India and declares “Nor was Andromeda’s color any problem/to her wing-footed aerial lover” adding that “White suits dark girls; you looked so attractive in white, Andromeda”. Ovid's account of Andromeda's story follows Euripides' play "Andromeda" in having Perseus initially mistake the chained Andromeda for a statue of marble, which has been taken to mean she was light-skinned; but since statues in Ovid's time were commonly painted to look like living people, her skin tone could have been of any color. 
The "Aethiopica", a Greek romance attributed to the 3rd century AD writer Heliodorus of Emesa, reflects the ambiguity between dark-skinned and light-skinned Andromedas in Late Antiquity. In the kingdom of Meroë (modern Sudan), Queen Persinna gives birth to her daughter, Chariclea who, despite having black parents, is born with white skin. The mother's explanation is that, during the moment of conception, she was gazing at a picture of a white-skinned Andromeda "brought down by Perseus naked from the rock, and so by mishap engendered presently a thing like to her." After being long separated from her parents, living in Egypt and Greece, Princess Chariclea returns home with her lover Theagnes and proves both her heritage and her mother's story as true by showing her parents a single black spot upon her elbow. Like the mythical Andromeda, Chariclea thus 'passes' as a member of the Greek/Roman world as well as of her African birthplace.
This ambiguity is also reflected in a description by the 2nd century AD sophist Philostratus of a painting depicting Perseus and Andromeda. He emphasizes the painting's Ethiopian setting, and notes that Andromeda "is charming in that she is fair of skin "though in Ethiopia"," in clear contrast to the other "charming Ethiopians with their strange coloring and their grim smiles" who have assembled to cheer Perseus in this picture.
Through the centuries, Ovid's descriptions of Andromeda and/or other authors' references to her Ethiopian/Indian origins have influenced some Western artists, but not the majority. The alternative tradition of Andromeda's story taking place at Joppa (on the coast of modern Israel) suggested that she was of light complexion to some artists, while others simply followed the tendency of artists everywhere to make the main subjects of their works look like themselves and the people around them. Roman frescoes from Pompeii show light-skinned Andromedas, for instance, but a 2nd-3rd century AD Roman mosaic found at Zeugma in modern Turkey shows her with darker skin tones, which would be more common in the Middle East (see illustrations). A few Renaissance and Baroque artists, such as Piero di Cosimo, Titian, Giorgio Vasari, and Abraham van Diepenbeeck, painted Andromedas with darker or dusky-colored skin tones (see Gallery), but Ovid's tradition was not continued by their contemporaries or later artists.
Andromeda is represented in the Northern sky by the constellation Andromeda, which contains the Andromeda Galaxy.
Several constellations are associated with the myth. Viewing the fainter stars visible to the naked eye, the constellations are rendered as:
Andromeda appears as Andy in 2020's The Old Guard where she leads a team of immortals. Played by Charlize Theron
Andromeda, and her role in the popular myth of Perseus, has been the subject of numerous ancient and modern works of art, where she is represented as a bound and helpless, typically beautiful, young woman placed in terrible danger, who must be saved through the unswerving courage of a hero who loves her: (see Gallery)
Andromeda was a popular subject for artists especially in the Renaissance and Baroque eras, followed by a resurgence of interest in her myth in the 19th century, but since then artists have shown much less interest in this subject.
Other Art Traditions Inspired by the Andomeda Myth:
For Full Text and Translations of Source Materials and Articles on Mythology:
Primary Greek and Roman sources:
Comprehensive Studies of the Perseus myth:

</doc>
<doc id="1926" url="https://en.wikipedia.org/wiki?curid=1926" title="Antlia">
Antlia

Antlia (; from Ancient Greek "ἀντλία") is a constellation in the Southern Celestial Hemisphere. Its name means "pump" in Latin and Greek; it represents an air pump. Originally Antlia Pneumatica, the constellation was established by Nicolas-Louis de Lacaille in the 18th century, though its name was later abbreviated by John Herschel. Located close to the stars forming the old constellation of the ship Argo Navis, Antlia is completely visible from latitudes south of 49 degrees north.
Antlia is a faint constellation; its brightest star is Alpha Antliae, an orange giant that is a suspected variable star, ranging between apparent magnitudes 4.22 and 4.29. S Antliae is an eclipsing binary star system, changing in brightness as one star passes in front of the other. Sharing a common envelope, the stars are so close they will one day merge to form a single star. Two star systems with known exoplanets, HD 93083 and WASP-66, lie within Antlia, as do NGC 2997, a spiral galaxy, and the Antlia Dwarf Galaxy.
The French astronomer Nicolas-Louis de Lacaille first described the constellation in French as "la Machine Pneumatique" (the Pneumatic Machine) in 1751–52, commemorating the air pump invented by the French physicist Denis Papin. De Lacaille had observed and catalogued almost 10,000 southern stars during a two-year stay at the Cape of Good Hope, devising fourteen new constellations in uncharted regions of the Southern Celestial Hemisphere not visible from Europe. He named all but one in honour of instruments that symbolised the Age of Enlightenment. Lacaille depicted Antlia as a single-cylinder vacuum pump used in Papin's initial experiments, while German astronomer Johann Bode chose the more advanced double-cylinder version. Lacaille Latinised the name to "Antlia pneumatica" on his 1763 chart. English astronomer John Herschel proposed shrinking the name to one word in 1844, noting that Lacaille himself had abbreviated his constellations thus on occasion. This was universally adopted. The International Astronomical Union adopted it as one of the 88 modern constellations in 1922.
Although visible to the Ancient Greeks, Antlia's stars were too faint to have been included in any ancient constellations. The stars that now comprise Antlia lay within an area of the sky covered by the ancient constellation Argo Navis, the Ship of the Argonauts, which due to its immense size was split into several smaller constellations by Lacaille in 1763. Ridpath reports that due to their faintness, the stars of Antlia did not make up part of the classical depiction of Argo Navis.
Chinese astronomers were able to view what is modern Antlia from their latitudes, and incorporated its stars into two different constellations. Several stars in the southern part of Antlia were a portion of "Dong'ou", which represented an area in southern China. Furthermore, Epsilon, Eta, and Theta Antliae were incorporated into the celestial temple, which also contained stars from modern Pyxis.
Covering 238.9 square degrees and hence 0.579% of the sky, Antlia ranks 62nd of the 88 modern constellations by area. Its position in the Southern Celestial Hemisphere means that the whole constellation is visible to observers south of 49°N. Hydra the sea snake runs along the length of its northern border, while Pyxis the compass, Vela the sails, and Centaurus the centaur line it to the west, south and east respectively. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union, is "Ant". The official constellation boundaries, as set by Belgian astronomer Eugène Delporte in 1930, are defined by a polygon with an east side, south side and ten other sides (facing the two other cardinal compass points) ("illustrated in infobox at top-right"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −24.54° and −40.42°.
Lacaille gave nine stars Bayer designations, labelling them Alpha through to Theta, combining two stars next to each other as Zeta. Gould later added a tenth, Iota Antliae. Beta and Gamma Antliae (now HR 4339 and HD 90156) ended up in the neighbouring constellation Hydra once the constellation boundaries were delineated in 1930. Within the constellation's borders, there are 42 stars brighter than or equal to apparent magnitude 6.5.
The constellation's two brightest stars—Alpha and Epsilon Antliae—shine with a reddish tinge. Alpha is an orange giant of spectral type K4III that is a suspected variable star, ranging between apparent magnitudes 4.22 and 4.29. It is located 320 ± 10 light-years away from Earth. Estimated to be shining with around 480 to 555 times the luminosity of the Sun, it is most likely an ageing star that is brightening and on its way to becoming a Mira variable star, having converted all its core fuel into carbon. Located 590 ± 30 light-years from Earth, Epsilon Antliae is an evolved orange giant star of spectral type K3 IIIa, that has swollen to have a diameter about 69 times that of the Sun, and a luminosity of around 1279 Suns. It is slightly variable. At the other end of Antlia, Iota Antliae is likewise an orange giant of spectral type K1 III. It is 202 ± 2 light-years distant.
Located near Alpha is Delta Antliae, a binary star, 450 ± 10 light-years distant from Earth. The primary is a blue-white main sequence star of spectral type B9.5V and magnitude 5.6, and the secondary is a yellow-white main sequence star of spectral type F9Ve and magnitude 9.6. Zeta Antliae is a wide optical double star. The brighter star—Zeta Antliae—is 410 ± 40 light-years distant and has a magnitude of 5.74, though it is a true binary star system composed of two white main sequence stars of magnitudes 6.20 and 7.01 that are separated by 8.042 arcseconds. The fainter star—Zeta Antliae—is 386 ± 5 light-years distant and of magnitude 5.9. Eta Antliae is another double composed of a yellow white star of spectral type F1V and magnitude 5.31, with a companion of magnitude 11.3. Theta Antliae is likewise double, most likely composed of an A-type main sequence star and a yellow giant. S Antliae is an eclipsing binary star system that varies in apparent magnitude from 6.27 to 6.83 over a period of 15.6 hours. The system is classed as a W Ursae Majoris variable—the primary is hotter than the secondary and the drop in magnitude is caused by the latter passing in front of the former. Calculating the properties of the component stars from the orbital period indicates that the primary star has a mass 1.94 times and a diameter 2.026 times that of the Sun, and the secondary has a mass 0.76 times and a diameter 1.322 times that of the Sun. The two stars have similar luminosity and spectral type as they have a common envelope and share stellar material. The system is thought to be around 5–6 billion years old. The two stars will eventually merge to form a single fast-spinning star.
T Antliae is a yellow-white supergiant of spectral type F6Iab and Classical Cepheid variable ranging between magnitude 8.88 and 9.82 over 5.9 days. U Antliae is a red C-type carbon star and is an irregular variable that ranges between magnitudes 5.27 and 6.04. At 910 ± 50 light-years distant, it is around 5819 times as luminous as the Sun. BF Antliae is a Delta Scuti variable that varies by 0.01 of a magnitude. HR 4049, also known as AG Antliae, is an unusual hot variable ageing star of spectral type B9.5Ib-II. It is undergoing intense loss of mass and is a unique variable that does not belong to any class of known variable star, ranging between magnitudes 5.29 and 5.83 with a period of 429 days. It is around 6000 light-years away from Earth. UX Antliae is an R Coronae Borealis variable with a baseline apparent magnitude of around 11.85, with irregular dimmings down to below magnitude 18.0. A luminous and remote star, it is a supergiant with a spectrum resembling that of a yellow-white F-type star but it has almost no hydrogen.
HD 93083 is an orange dwarf star of spectral type K3V that is smaller and cooler than the Sun. It has a planet that was discovered by the radial velocity method with the HARPS spectrograph in 2005. About as massive as Saturn, the planet orbits its star with a period of 143 days at a mean distance of 0.477 AU. WASP-66 is a sunlike star of spectral type F4V. A planet with 2.3 times the mass of Jupiter orbits it every 4 days, discovered by the transit method in 2012. DEN 1048-3956 is a brown dwarf of spectral type M8 located around 13 light-years distant from Earth. At magnitude 17 it is much too faint to be seen with the unaided eye. It has a surface temperature of about 2500 K. Two powerful flares lasting 4–5 minutes each were detected in 2002. 2MASS 0939-2448 is a system of two cool and faint brown dwarfs, probably with effective temperatures of about 500 and 700 K and masses of about 25 and 40 times that of Jupiter, though it is also possible that both objects have temperatures of 600 K and 30 Jupiter masses.
Antlia contains many faint galaxies, the brightest of which is NGC 2997 at magnitude 10.6. It is a loosely wound face-on spiral galaxy of type Sc. Though nondescript in most amateur telescopes, it presents bright clusters of young stars and many dark dust lanes in photographs. Discovered in 1997, the Antlia Dwarf is a 14.8 dwarf spheroidal galaxy that belongs to the Local Group of galaxies. In 2018 the discovery was announced of a very low surface brightness galaxy near Epsilon Antliae, Antlia 2, which is a satellite galaxy of the Milky Way.
The Antlia Cluster, also known as Abell S0636, is a cluster of galaxies located in the Hydra-Centaurus Supercluster. It is the third nearest to the Local Group after the Virgo Cluster and the Fornax Cluster. The cluster's distance from earth is to Located in the southeastern corner of the constellation, it boasts the giant elliptical galaxies NGC 3268 and NGC 3258 as the main members of a southern and northern subgroup respectively, and contains around 234 galaxies in total.

</doc>
<doc id="1927" url="https://en.wikipedia.org/wiki?curid=1927" title="Ara (constellation)">
Ara (constellation)

Ara (Latin: "the Altar"), the southern constellation between Scorpius, Telescopium, Triangulum Australe and Norma, was (as Βωμός (Bōmǒs)) one of the Greek bulk (namely 48) described by the 2nd-century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union.
The orange supergiant Beta Arae, to us its brightest star measured with near-constant apparent magnitude of 2.85, is marginally brighter than blue-white Alpha Arae. Seven star systems are known to host planets. Sunlike Mu Arae hosts four known planets. Gliese 676 is a (gravity-paired) binary red-dwarf system with four known planets. 
The Milky Way crosses the northwestern part of Ara. Within the constellation is Westerlund 1, a super star cluster that contains the red hypergiant Westerlund 1-26, possibly the largest star known.
In ancient Greek mythology, Ara was identified as the altar where the gods first made offerings and formed an alliance before defeating the Titans. One of the southernmost constellations depicted by Ptolemy, it had been recorded by Aratus in 270 BC as lying close to the horizon, and the Almagest portrays stars as far south as Gamma Arae. Professor Bradley Schaefer proposes such Ancients must have been able to see as far south as Zeta Arae, for a pattern that looked like an altar.
In illustrations, Ara is usually depicted as compact classical altar with its smoke 'rising' southward. However, depictions often vary. In the early days of printing, a 1482 woodcut of Gaius Julius Hyginus's classic "Poeticon Astronomicon" depicts the altar as surrounded by demons. Johann Bayer in 1603 depicted Ara as an altar with burning incense. Hyginus depicted the same though his featured devils on either side of the flames. Willem Blaeu, a Dutch uranographer of the 16th and 17th centuries, drew Ara as an altar for sacrifices, with a burning animal offering unusually whose smoke rises northward, represented by Alpha Arae. 
"The Castle of Knowledge" by Robert Record of 1556 lists the constellation stating that "Under the Scorpions tayle, standeth the Altar."; a decade later a translation of a fairly recent mainly astrological work by Marcellus Palingenius of 1565, by Barnabe Googe states "Here mayst thou both the Altar, and the myghty Cup beholde."
In Chinese astronomy, the stars of the constellation Ara lie within "The Azure Dragon of the East" (東方青龍, "Dōng Fāng Qīng Lóng"). Five stars of Ara formed Guī (龜), a tortoise, while another three formed Chǔ (杵), a pestle.
The Wardaman people of the Northern Territory in Australia saw the stars of Ara and the neighbouring constellation Pavo as flying foxes.
Covering 237.1 square degrees and hence 0.575% of the sky, Ara ranks 63rd of the 88 modern constellations by area. Its position in the Southern Celestial Hemisphere means that the whole constellation is visible to observers south of 22°N. Scorpius runs along the length of its northern border, while Norma and Triangulum Australe border it to the west, Apus to the south, and Pavo and Telescopium to the east respectively. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union, is "Ara". The official constellation boundaries, as set by Belgian astronomer Eugène Delporte in 1930, are defined by a polygon of twelve segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −45.49° and −67.69°.
Bayer gave eight stars Bayer designations, labelling them Alpha through to Theta, though he had never seen the constellation directly as it never rises above the horizon in Germany. After charting the southern constellations, French astronomer Nicolas-Louis de Lacaille recharted the stars of Ara from Alpha though to Sigma, including three pairs of stars next to each other as Epsilon, Kappa and Nu.
Ara contains part of the Milky Way to the south of Scorpius and thus has rich star fields. Within the constellation's borders, there are 71 stars brighter than or equal to apparent magnitude 6.5.
Just shading Alpha Arae, Beta Arae is the brightest star in the constellation. It is an orange-hued star of spectral type K3Ib-IIa that has been classified as a supergiant or bright giant, that is around 650 light-years from Earth. It is around 8.21 times as massive and 5,636 times as luminous as the Sun. At apparent magnitude 2.85, this difference in brightness between the two is undetectable by the unaided eye. Close to Beta Arae is Gamma Arae, a blue-hued supergiant of spectral type B1Ib. Of apparent magnitude 3.3, it is 1110 ± 60 light-years from Earth. It has been estimated to be between 12.5 and 25 times as massive as the Sun, and have around 120,000 times its luminosity.
Alpha Arae is a blue-white main sequence star of magnitude 2.95, that is 270 ± 20 light-years from Earth. This star is around 9.6 times as massive as the Sun, and has an average of 4.5 times its radius. It is 5,800 times as luminous as the Sun, its energy emitted from its outer envelope at an effective temperature of 18,044 K. A Be star, Alpha Arae is surrounded by a dense equatorial disk of material in Keplerian (rather than uniform) rotation. The star is losing mass by a polar stellar wind with a terminal velocity of approximately 1,000 km/s.
The third brightest star in Ara at magnitude 3.13 is Zeta Arae, an orange giant of spectral type K3III that is located 490 ± 10 light-years from Earth. Around 7–8 times as massive as the Sun, it has swollen to a diameter around 114 times that of the Sun and is 3800 times as luminous. Were it not dimmer by intervening interstellar dust, it would be significantly brighter at magnitude 2.11.
Delta Arae is a blue-white main sequence star of spectral type B8Vn and magnitude 3.6, 198 ± 4 light-years from Earth. It is around 3.56 times as massive as the Sun.
Epsilon Arae is an orange giant of apparent magnitude 4.1, 360 ± 10 light-years distant from Earth. It is around 74% more massive than the Sun. At an age of about 1.7 billion years, the outer envelope of the star has expanded to almost 34 times the Sun's radius.
Eta Arae is an orange giant of apparent magnitude 3.76, located 299 ± 5 light-years distant from Earth. Estimated to be around five billion years old, it has reached the giant star stage of its evolution. With 1.12 times the mass of the Sun, it has an outer envelope that has expanded to 40 times the Sun's radius. The star is now spinning so slowly that it takes more than eleven years to complete a single rotation.
GX 339-4 (V821 Arae) is a moderately strong variable galactic low-mass X-ray binary (LMXB) source and black-hole candidate that flares from time to time. From spectroscopic measurements, the mass of the black-hole was found to be at least of 5.8 solar masses.
Exoplanets have been discovered in seven star systems in the constellation. Mu Arae (Cervantes) is a sunlike star that hosts four planets. HD 152079 is a sunlike star with a planet. HD 154672 is an ageing sunlike star with a Hot Jupiter. HD 154857 is a sunlike star with one confirmed and one suspected planet. HD 156411 is a star hotter and larger than the sun with a gas giant planet in orbit. Gliese 674 is a nearby red dwarf star with a planet. Gliese 676 is a binary star system composed of two red dwarves with four planets.
The northwest corner of Ara is crossed by the galactic plane of the Milky Way and contains several open clusters (notably NGC 6200) and diffuse nebulae (including the bright cluster/nebula pair NGC 6188 and NGC 6193). The brightest of the globular clusters, sixth magnitude NGC 6397, lies at a distance of just , making it one of the closest globular clusters to the Solar System.
Ara also contains Westerlund 1, a super star cluster containing itself the red supergiant Westerlund 1-237 and the red hypergiant Westerlund 1-26. The latter is possibly the largest star known with an estimate varying between and .
Although Ara lies close to the heart of the Milky Way, two spiral galaxies (NGC 6215 and NGC 6221) are visible near star Eta Arae.
Online sources

</doc>
<doc id="1928" url="https://en.wikipedia.org/wiki?curid=1928" title="Auriga">
Auriga

Auriga or AURIGA can refer to:

</doc>
<doc id="1930" url="https://en.wikipedia.org/wiki?curid=1930" title="Arkansas">
Arkansas

Arkansas () is a state in the south central region of the United States, home to more than three million people as of 2018. Its name is from the Osage language, of Siouan derivation; it denoted their related kin, the Quapaw people. The state's diverse geography ranges from the mountainous regions of the Ozark and the Ouachita Mountains, which make up the U.S. Interior Highlands, to the densely forested land in the south known as the Arkansas Timberlands, to the eastern lowlands along the Mississippi River and the Arkansas Delta.
Arkansas is the 29th largest by area and the 33rd most populous of the 50 United States. The capital and most populous city is Little Rock, located in the central portion of the state, a hub for transportation, business, culture, and government. The northwestern corner of the state, such as the Fayetteville–Springdale–Rogers Metropolitan Area and Fort Smith metropolitan area, is a population, education, and economic center. The largest city in the state's eastern part is Jonesboro. The largest city in the state's southeastern part is Pine Bluff.
The Territory of Arkansas was admitted to the Union as the 25th state on June 15, 1836. Much of the Delta had been developed for cotton plantations, and the state landowners there largely depended on enslaved African Americans as workers. In 1861, Arkansas seceded from the United States and joined the Confederate States of America during the Civil War.
On returning to the Union in 1868, the state continued to suffer due to its reliance on the large-scale plantation economy. Cotton continued as the leading commodity crop, although the cotton market declined. Because farmers and businessmen did not diversify and there was little industrial investment, the state fell behind in terms of its economy and opportunities for residents.
White rural interests dominated the state's politics by disenfranchisement of African Americans and by refusal to reapportion the legislature. It was not until after the civil rights movement and federal intervention that more African Americans were able to vote. The Supreme Court overturned rural domination in the South and other states that had refused to reapportion their state legislatures, or retained rules based on geographic districts. In the landmark ruling of "one man, one vote", it ruled that states had to organize both houses of their legislatures by districts that held approximately equal populations, and that these had to be redefined as necessary after each decade's census.
Following World War II, Arkansas began to diversify its economy. In the 21st century, its economy is based on service industries, aircraft, poultry, steel, and tourism; along with important commodity crops of cotton, soybeans and rice.
The culture of Arkansas is observable in museums, theaters, novels, television shows, restaurants, and athletic venues across the state. Notable people from the state include politician and educational advocate William Fulbright; former president Bill Clinton, who also served as the 40th and 42nd governor of Arkansas; general Wesley Clark, former NATO Supreme Allied Commander; Walmart founder and magnate Sam Walton; singer-songwriters Johnny Cash, Charlie Rich, Jimmy Driftwood, and Glen Campbell; actor-filmmaker Billy Bob Thornton; poet C. D. Wright; and physicist William L. McMillan, who was a pioneer in superconductor research.
The name "Arkansas" was initially applied to the Arkansas River. It derives from a French term, "Arcansas", their plural term for their transliteration of "akansa", an Algonquian term for the Quapaw people. These were a Dhegiha Siouan-speaking people who settled in Arkansas around the 13th century. "Akansa" is likely also the root term for Kansas.
The name has been pronounced and spelled in a variety of fashions. In 1881, the state legislature defined the official pronunciation of Arkansas as having the final "s" be silent (as it would be in French). A dispute had arisen between the state's two senators over the pronunciation issue. One favored pronunciation as while the other favored .
In 2007, the state legislature passed a non-binding resolution declaring that the possessive form of the state's name is "Arkansas's", which has been followed increasingly by the state government.
Arkansas borders Louisiana to the south, Texas to the southwest, Oklahoma to the west, Missouri to the north, and Tennessee and Mississippi to the east. The United States Census Bureau classifies Arkansas as a southern state, sub-categorized among the West South Central States. The Mississippi River forms most of Arkansas's eastern border, except in Clay and Greene counties, where the St. Francis River forms the western boundary of the Missouri Bootheel, and in many places where the channel of the Mississippi has meandered (or been straightened by man) from its original 1836 course.
Arkansas can generally be split into two halves, the highlands in the northwest half and the lowlands of the southeastern half. The highlands are part of the Southern Interior Highlands, including The Ozarks and the Ouachita Mountains. The southern lowlands include the Gulf Coastal Plain and the Arkansas Delta. This dual split can yield to general regions named northwest, southwest, northeast, southeast, or central Arkansas. These directionally named regions are broad and not defined along county lines. Arkansas has seven distinct natural regions: the Ozark Mountains, Ouachita Mountains, Arkansas River Valley, Gulf Coastal Plain, Crowley's Ridge, and the Arkansas Delta, with Central Arkansas sometimes included as a blend of multiple regions.
The southeastern part of Arkansas along the Mississippi Alluvial Plain is sometimes called the Arkansas Delta. This region is a flat landscape of rich alluvial soils formed by repeated flooding of the adjacent Mississippi. Farther away from the river, in the southeast portion of the state, the Grand Prairie consists of a more undulating landscape. Both are fertile agricultural areas. The Delta region is bisected by a geological formation known as Crowley's Ridge. A narrow band of rolling hills, Crowley's Ridge rises from above the surrounding alluvial plain and underlies many of the major towns of eastern Arkansas.
Northwest Arkansas is part of the Ozark Plateau including the Ozark Mountains, to the south are the Ouachita Mountains, and these regions are divided by the Arkansas River; the southern and eastern parts of Arkansas are called the Lowlands. These mountain ranges are part of the U.S. Interior Highlands region, the only major mountainous region between the Rocky Mountains and the Appalachian Mountains. The highest point in the state is Mount Magazine in the Ouachita Mountains, which rises to above sea level.
Arkansas is home to many caves, such as Blanchard Springs Caverns. More than 43,000 Native American living, hunting and tool making sites, many of them Pre-Columbian burial mounds and rock shelters, have been cataloged by the State Archeologist. Crater of Diamonds State Park near Murfreesboro is the world's only diamond-bearing site accessible to the public for digging. Arkansas is home to a dozen Wilderness Areas totaling . These areas are set aside for outdoor recreation and are open to hunting, fishing, hiking, and primitive camping. No mechanized vehicles nor developed campgrounds are allowed in these areas.
Arkansas has many rivers, lakes, and reservoirs within or along its borders. Major tributaries of the Mississippi River include the Arkansas River, the White River, and the St. Francis River. The Arkansas is fed by the Mulberry River and the Fourche LaFave River in the Arkansas River Valley, which is also home to Lake Dardanelle. The Buffalo River, Little Red River, Black River and Cache River all serve as tributaries to the White River, which also empties into the Mississippi. The Saline River, Little Missouri River, Bayou Bartholomew, and the Caddo River all serve as tributaries to the Ouachita River in south Arkansas, which eventually empties into the Mississippi in Louisiana. The Red River briefly serves as the state's boundary with Texas. Arkansas has few natural lakes and many reservoirs, such as Bull Shoals Lake, Lake Ouachita, Greers Ferry Lake, Millwood Lake, Beaver Lake, Norfork Lake, DeGray Lake, and Lake Conway.
Arkansas's temperate deciduous forest is divided into three broad ecoregions; the "Ozark, Ouachita-Appalachian Forests", the "Mississippi Alluvial and Southeast USA Coastal Plains", and the "Southeastern USA Plains". The state is further divided into seven subregions: the Arkansas Valley, Boston Mountains, Mississippi Alluvial Plain, Mississippi Valley Loess Plain, Ozark Highlands, Ouachita Mountains, and the South Central Plains. A 2010 United States Forest Service survey determined of Arkansas's land is forestland, or 56% of the state's total area. Dominant species in Arkansas's forests include "Quercus" (oak), "Carya" (hickory), "Pinus echinata" (shortleaf pine) and "Pinus taeda" (loblolly pine).
Arkansas's plant life varies with its climate and elevation. The pine belt stretching from the Arkansas delta to Texas consists of dense oak-hickory-pine growth. Lumbering and paper milling activity is active throughout the region. In eastern Arkansas, one can find "Taxodium " (cypress), "Quercus nigra" (water oaks), and hickories with their roots submerged in the Mississippi Valley bayous indicative of the deep south. Nearby Crowley's Ridge is the only home of the tulip tree in the state, and generally hosts more northeastern plant life such as the beech tree. The northwestern highlands are covered in an oak-hickory mixture, with Ozark white cedars, "cornus" (dogwoods), and "Cercis canadensis" (redbuds) also present. The higher peaks in the Arkansas River Valley play host to scores of ferns, including the "Woodsia scopulina" and "Adiantum" (maidenhair fern) on Mount Magazine.
Arkansas generally has a humid subtropical climate. While not bordering the Gulf of Mexico, Arkansas is still close enough to this warm, large body of water for it to influence the weather in the state. Generally, Arkansas has hot, humid summers and slightly drier, mild to cool winters. In Little Rock, the daily high temperatures average around with lows around in July. In January highs average around and lows around . In Siloam Springs in the northwest part of the state, the average high and low temperatures in July are and in January the average high and low are . Annual precipitation throughout the state averages between about ; somewhat wetter in the south and drier in the northern part of the state. Snowfall is infrequent but most common in the northern half of the state. The half of the state south of Little Rock is more apt to see ice storms. Arkansas's all-time record high is at Ozark on August 10, 1936; the all-time record low is at Gravette, on February 13, 1905.
Arkansas is known for extreme weather and frequent storms. A typical year brings thunderstorms, tornadoes, hail, snow and ice storms. Between both the Great Plains and the Gulf States, Arkansas receives around 60 days of thunderstorms. Arkansas is located in Tornado Alley, and as a result, a few of the most destructive tornadoes in U.S. history have struck the state. While sufficiently far from the coast to avoid a direct hit from a hurricane, Arkansas can often get the remnants of a tropical system, which dumps tremendous amounts of rain in a short time and often spawns smaller tornadoes.
Before European settlement of North America, Arkansas was inhabited by indigenous peoples for thousands of years. The Caddo, Osage, and Quapaw peoples encountered European explorers. The first of these Europeans was Spanish explorer Hernando de Soto in 1541, who crossed the Mississippi and marched across central Arkansas and the Ozark Mountains. After finding nothing he considered of value and encountering native resistance the entire way, he and his men returned to the Mississippi River where de Soto fell ill. From his deathbed he ordered his men to massacre all the men of the nearby village of Anilco, who he feared had been plotting with a powerful polity down the Mississippi River, "Quigualtam". His men obeyed and did not stop with the men, but were said to have massacred women and children as well. He died the following day in what is believed to be the vicinity of modern-day McArthur, Arkansas, in May 1542. His body was weighted down with sand and he was consigned to a watery grave in the Mississippi River under cover of darkness by his men. De Soto had attempted to deceive the native population into thinking he was an immortal deity, sun of the sun, in order to forestall attack by outraged Native Americans on his by then weakened and bedraggled army. In order to keep the ruse up, his men informed the locals that de Soto had ascended into the sky. His will at the time of his death listed "four Indian slaves, three horses and 700 hogs" which were auctioned off. The starving men, who had been living off maize stolen from natives, immediately started butchering the hogs and later, commanded by former aide-de-camp Moscoso, attempted an overland return to Mexico. They made it as far as Texas before running into territory too dry for maize farming and too thinly populated to sustain themselves by stealing food from the locals. The expedition promptly backtracked to Arkansas. After building a small fleet of boats they then headed down the Mississippi River and eventually on to Mexico by water.
Later explorers included the French Jacques Marquette and Louis Jolliet in 1673, and Frenchmen Robert La Salle and Henri de Tonti in 1681. Tonti established Arkansas Post at a Quapaw village in 1686, making it the first European settlement in the territory. The early Spanish or French explorers of the state gave it its name, which is probably a phonetic spelling of the Illinois tribe's name for the Quapaw people, who lived downriver from them. The name Arkansas has been pronounced and spelled in a variety of fashions. The region was organized as the Territory of Arkansaw on July 4, 1819, with the territory admitted to the United States as the state of Arkansas on June 15, 1836. The name was historically , , and several other variants. Historically and modernly, the people of Arkansas call themselves either "Arkansans" or "Arkansawyers". In 1881, the Arkansas General Assembly passed Arkansas Code 1-4-105 (official text):
Whereas, confusion of practice has arisen in the pronunciation of the name of our state and it is deemed important that the true pronunciation should be determined for use in oral official proceedings.
And, whereas, the matter has been thoroughly investigated by the State Historical Society and the Eclectic Society of Little Rock, which have agreed upon the correct pronunciation as derived from history, and the early usage of the American immigrants.
Be it therefore resolved by both houses of the General Assembly, that the only true pronunciation of the name of the state, in the opinion of this body, is that received by the French from the native Indians and committed to writing in the French word representing the sound. It should be pronounced in three (3) syllables, with the final "s" silent, the "a" in each syllable with the Italian sound, and the accent on the first and last syllables. The pronunciation with the accent on the second syllable with the sound of "a" in "man" and the sounding of the terminal "s" is an innovation to be discouraged.
Citizens of the state of Kansas often pronounce the Arkansas River as , in a manner similar to the common pronunciation of the name of their state.
Settlers, such as fur trappers, moved to Arkansas in the early 18th century. These people used Arkansas Post as a home base and entrepôt. During the colonial period, Arkansas changed hands between France and Spain following the Seven Years' War, although neither showed interest in the remote settlement of Arkansas Post. In April 1783, Arkansas saw its only battle of the American Revolutionary War, a brief siege of the post by British Captain James Colbert with the assistance of the Choctaw and Chickasaw.
Napoleon Bonaparte sold French Louisiana to the United States in 1803, including all of Arkansas, in a transaction known today as the Louisiana Purchase. French soldiers remained as a garrison at Arkansas Post. Following the purchase, the balanced give-and-take relationship between settlers and Native Americans began to change all along the frontier, including in Arkansas. Following a controversy over allowing slavery in the territory, the Territory of Arkansas was organized on July 4, 1819. Gradual emancipation in Arkansas was struck down by one vote, the Speaker of the House Henry Clay, allowing Arkansas to organize as a slave territory.
Slavery became a wedge issue in Arkansas, forming a geographic divide that remained for decades. Owners and operators of the cotton plantation economy in southeast Arkansas firmly supported slavery, as they perceived slave labor as the best or "only" economically viable method of harvesting their commodity crops. The "hill country" of northwest Arkansas was unable to grow cotton and relied on a cash-scarce, subsistence farming economy.
As European Americans settled throughout the East Coast and into the Midwest, in the 1830s the United States government forced the removal of many Native American tribes to Arkansas and Indian Territory west of the Mississippi River.
Additional Native American removals began in earnest during the territorial period, with final Quapaw removal complete by 1833 as they were pushed into Indian Territory. The capital was relocated from Arkansas Post to Little Rock in 1821, during the territorial period.
When Arkansas applied for statehood, the slavery issue was again raised in Washington, D.C.. Congress eventually approved the Arkansas Constitution after a 25-hour session, admitting Arkansas on June 15, 1836, as the 25th state and the 13th slave state, having a population of about 60,000. Arkansas struggled with taxation to support its new state government, a problem made worse by a state banking scandal and worse yet by the Panic of 1837.
In early antebellum Arkansas, the southeast Arkansas slave-based economy developed rapidly. On the eve of the Civil War in 1860, enslaved African Americans numbered 111,115 people, just over 25% of the state's population. Plantation agriculture set the state and region behind the nation for decades. The wealth developed among planters of southeast Arkansas caused a political rift to form between the northwest and southeast.
Many politicians were elected to office from the Family, the Southern rights political force in antebellum Arkansas. Residents generally wanted to avoid a civil war. When the Gulf states seceded in early 1861, Arkansas voted to remain in the Union. Arkansas did not secede until Abraham Lincoln demanded Arkansas troops be sent to Fort Sumter to quell the rebellion there. On May 6, a state convention voted to terminate Arkansas's membership in the Union and join the Confederate States of America.
Arkansas held a very important position for the Rebels, maintaining control of the Mississippi River and surrounding Southern states. The bloody Battle of Wilson's Creek just across the border in Missouri shocked many Arkansans who thought the war would be a quick and decisive Southern victory. Battles early in the war took place in northwest Arkansas, including the Battle of Cane Hill, Battle of Pea Ridge, and Battle of Prairie Grove. Union general Samuel Curtis swept across the state to Helena in the Delta in 1862. Little Rock was captured the following year. The government shifted the state Confederate capital to Hot Springs, and then again to Washington from 1863 to 1865, for the remainder of the war. Throughout the state, guerrilla warfare ravaged the countryside and destroyed cities. Passion for the Confederate cause waned after implementation of programs such as the draft, high taxes, and martial law.
Under the Military Reconstruction Act, Congress declared Arkansas restored to the Union in June 1868, after the Legislature accepted the 14th Amendment. The Republican-controlled reconstruction legislature established universal male suffrage (though temporarily disfranchising former Confederate Army officers, who were all Democrats), a public education system for blacks and whites, and passed general issues to improve the state and help more of the population. The State soon came under control of the Radical Republicans and Unionists, and led by Governor Powell Clayton, they presided over a time of great upheaval as Confederate sympathizers and the Ku Klux Klan fought the new developments, particularly voting rights for African Americans.
In 1874, the Brooks-Baxter War, a political struggle between factions of the Republican Party shook Little Rock and the state governorship. It was settled only when President Ulysses S. Grant ordered Joseph Brooks to disperse his militant supporters.
Following the Brooks-Baxter War, a new state constitution was ratified, re-enfranchising former Confederates.
In 1881, the Arkansas state legislature enacted a bill that adopted an official pronunciation of the state's name, to combat a controversy then simmering. (See Law and Government below.)
After Reconstruction, the state began to receive more immigrants and migrants. Chinese, Italian, and Syrian men were recruited for farm labor in the developing Delta region. None of these nationalities stayed long at farm labor; the Chinese especially quickly became small merchants in towns around the Delta. Many Chinese became such successful merchants in small towns that they were able to educate their children at college.
Some early 20th-century immigration included people from eastern Europe. Together, these immigrants made the Delta more diverse than the rest of the state. In the same years, some black migrants moved into the area because of opportunities to develop the bottomlands and own their own property.
Construction of railroads enabled more farmers to get their products to market. It also brought new development into different parts of the state, including the Ozarks, where some areas were developed as resorts. In a few years at the end of the 19th century, for instance, Eureka Springs in Carroll County grew to 10,000 people, rapidly becoming a tourist destination and the fourth-largest city of the state. It featured newly constructed, elegant resort hotels and spas planned around its natural springs, considered to have healthful properties. The town's attractions included horse racing and other entertainment. It appealed to a wide variety of classes, becoming almost as popular as Hot Springs.
In the late 1880s, the worsening agricultural depression catalyzed Populist and third party movements, leading to interracial coalitions. Struggling to stay in power, in the 1890s the Democrats in Arkansas followed other Southern states in passing legislation and constitutional amendments that disfranchised blacks and poor whites. Democrats wanted to prevent their alliance. In 1891 state legislators passed a requirement for a literacy test, knowing it would exclude many blacks and whites. At the time, more than 25% of the population could neither read nor write. In 1892, they amended the state constitution to require a poll tax and more complex residency requirements, both of which adversely affected poor people and sharecroppers, forcing most blacks and many poor whites from voter rolls.
By 1900 the Democratic Party expanded use of the white primary in county and state elections, further denying blacks a part in the political process. Only in the primary was there any competition among candidates, as Democrats held all the power. The state was a Democratic one-party state for decades, until after passage of the federal Civil Rights Act of 1964 and Voting Rights Act of 1965 to enforce constitutional rights.
Between 1905 and 1911, Arkansas began to receive a small immigration of German, Slovak, and Scots-Irish from Europe. The German and Slovak peoples settled in the eastern part of the state known as the Prairie, and the Irish founded small communities in the southeast part of the state. The Germans were mostly Lutheran and the Slovaks were primarily Catholic. The Irish were mostly Protestant from Ulster, of Scots and Northern Borders descent.
Black sharecroppers began to try to organize a farmers' union after World War I. They were seeking better conditions of payment and accounting from white landowners of the area cotton plantations. Whites resisted any change and often tried to break up their meetings.
On September 30, 1919, two white men, including a local deputy, tried to break up a meeting of black sharecroppers who were trying to organize a farmers' union. After a white deputy was killed in a confrontation with guards at the meeting, word spread to town and around the area. Hundreds of whites from Phillips and neighboring areas rushed to suppress the blacks, and started attacking blacks at large. Governor Charles Hillman Brough requested federal troops to stop what was called the Elaine massacre. White mobs spread throughout the county, killing an estimated 237 blacks before most of the violence was suppressed after October 1. Five whites also died in the incident. The governor accompanied the troops to the scene; their use had been approved by U.S. President Woodrow Wilson.
Based on the order of President Franklin D. Roosevelt given shortly after Imperial Japan's attack on Pearl Harbor, nearly 16,000 Japanese Americans were forcibly removed from the West Coast of the United States and incarcerated in two internment camp located in the Arkansas Delta. The Rohwer Camp in Desha County operated from September 1942 to November 1945 and at its peak interned 8,475 prisoners. The Jerome War Relocation Center in Drew County operated from October 1942 to June 1944 and held about 8,000.
After the Supreme Court's decision in "Brown "v." Board of Education of Topeka, Kansas" in 1954 that segregation in public schools is unconstitutional, some students worked to integrate schools in the state. The Little Rock Nine brought Arkansas to national attention in 1957 when the Federal government had to intervene to protect African-American students trying to integrate a high school in the Arkansas capital. Governor Orval Faubus had ordered the Arkansas National Guard to aid segregationists in preventing nine African-American students from enrolling at Little Rock's Central High School. After attempting three times to contact Faubus, President Dwight D. Eisenhower sent 1000 troops from the active-duty 101st Airborne Division to escort and protect the African-American students as they entered school on September 25, 1957. In defiance of federal court orders to integrate, the governor and city of Little Rock decided to close the high schools for the remainder of the school year. By the fall of 1959, the Little Rock high schools were completely integrated.
Bill Clinton, the 42nd president of the United States, was born in Hope. Before his presidency, Clinton served as the 40th and 42nd governor of Arkansas, a total of nearly 12 years.
Little Rock has been Arkansas's capital city since 1821 when it replaced Arkansas Post as the capital of the Territory of Arkansas. The state capitol was moved to Hot Springs and later Washington during the Civil War when the Union armies threatened the city in 1862, and state government did not return to Little Rock until after the war ended. Today, the Little Rock–North Little Rock–Conway metropolitan area is the largest in the state, with a population of 724,385 in 2013.
The Fayetteville–Springdale–Rogers Metropolitan Area is the second-largest metropolitan area in Arkansas, growing at the fastest rate due to the influx of businesses and the growth of the University of Arkansas and Walmart.
The state has eight cities with populations above 50,000 (based on 2010 census). In descending order of size, they are: Little Rock, Fort Smith, Fayetteville, Springdale, Jonesboro, North Little Rock, Conway, and Rogers. Of these, only Fort Smith and Jonesboro are outside the two largest metropolitan areas. Other cities are located in Arkansas such as Pine Bluff, Crossett, Bryant, Lake Village, Hot Springs, Bentonville, Texarkana, Sherwood, Jacksonville, Russellville, Bella Vista, West Memphis, Paragould, Cabot, Searcy, Van Buren, El Dorado, Blytheville, Harrison, Dumas, Rison, Warren, and Mountain Home.
The United States Census Bureau estimates that the population of Arkansas was 3,017,804 on July 1, 2019, a 3.49% increase since the 2010 United States Census.
As of 2019, Arkansas has an estimated population of 3,017,804. From fewer than 15,000 in 1820, Arkansas's population grew to 52,240 during a special census in 1835, far exceeding the 40,000 required to apply for statehood. Following statehood in 1836, the population doubled each decade until the 1870 Census conducted following the Civil War. The state recorded growth in each successive decade, although it gradually slowed in the 20th century.
It recorded population losses in the 1950 and 1960 Censuses. This outmigration was a result of multiple factors, including farm mechanization, decreasing labor demand, and young educated people leaving the state due to a lack of non-farming industry in the state. Arkansas again began to grow, recording positive growth rates ever since and exceeding the two-million mark during the 1980 Census. Arkansas's rate of change, age distributions, and gender distributions mirror national averages. Minority group data also approximates national averages. There are fewer people in Arkansas of Hispanic or Latino origin than the national average. The center of population of Arkansas for 2000 was located in Perry County, near Nogal.
In terms of race and ethnicity, the state was 80.1% white (74.2% non-Hispanic white), 15.6% black or African American, 0.9% American Indian and Alaska Native, 1.3% Asian, and 1.8% from two or more races. Hispanics or Latinos of any race made up 6.6% of the population.
As of 2011, 39.0% of Arkansas's population younger than age 1 were minorities.
European Americans have a strong presence in the northwestern Ozarks and the central part of the state. African Americans live mainly in the southern and eastern parts of the state. Arkansans of Irish, English and German ancestry are mostly found in the far northwestern Ozarks near the Missouri border. Ancestors of the Irish in the Ozarks were chiefly Scots-Irish, Protestants from Northern Ireland, the Scottish lowlands and northern England part of the largest group of immigrants from Great Britain and Ireland before the American Revolution. English and Scots-Irish immigrants settled throughout the backcountry of the South and in the more mountainous areas. Americans of English stock are found throughout the state.
A 2010 survey of the principal ancestries of Arkansas's residents revealed the following:
Most of the people identifying as American are of English descent and/or Scots-Irish descent. Their families have been in the state so long, in many cases since before statehood, that they choose to identify simply as having American ancestry or do not in fact know their own ancestry. Their ancestry primarily goes back to the original 13 colonies and for this reason many of them today simply claim American ancestry. Many people who identify themselves as Irish descent are in fact of Scots-Irish descent.
According to the 2006–2008 American Community Survey, 93.8% of Arkansas's population (over the age of five) spoke only English at home. About 4.5% of the state's population spoke Spanish at home. About 0.7% of the state's population spoke any other Indo-European languages. About 0.8% of the state's population spoke an Asian language, and 0.2% spoke other languages.
Arkansas, like most other Southern states, is part of the Bible Belt and is predominantly Protestant. The largest denominations by number of adherents in 2010 were the Southern Baptist Convention with 661,382; the United Methodist Church with 158,574; non-denominational Evangelical Protestants with 129,638; the Catholic Church with 122,662; and The Church of Jesus Christ of Latter-day Saints with 31,254. There are some residents of the state who live by other religions such as Islam, Judaism, Wicca, Paganism, Hinduism, Buddhism or who claim no religious affiliation.
Once a state with a cashless society in the uplands and plantation agriculture in the lowlands, Arkansas's economy has evolved and diversified. The state's gross domestic product (GDP) was $119 billion in 2015. Six Fortune 500 companies are based in Arkansas, including the world's #1 retailer, Walmart; Tyson Foods, J.B. Hunt, Dillard's, Murphy USA, and Windstream are also headquartered in the state. The per capita personal income in 2015 was $39,107, ranking forty-fifth in the nation. The median household income from 2011 to 2015 was $41,371, ranking forty-ninth in the nation. The state's agriculture outputs are poultry and eggs, soybeans, sorghum, cattle, cotton, rice, hogs, and milk. Its industrial outputs are food processing, electric equipment, fabricated metal products, machinery, and paper products. Mines in Arkansas produce natural gas, oil, crushed stone, bromine, and vanadium. According to CNBC, Arkansas ranks as the 20th best state for business, with the 2nd-lowest cost of doing business, 5th-lowest cost of living, 11th best workforce, 20th-best economic climate, 28th-best educated workforce, 31st-best infrastructure and the 32nd-friendliest regulatory environment. Arkansas gained twelve spots in the best state for business rankings since 2011. As of 2014, Arkansas was the most affordable U.S. state to live in.
As of October 2019, the state's unemployment rate is 3.5%.
Arkansas's earliest industries were fur trading and agriculture, with development of cotton plantations in the areas near the Mississippi River. They were dependent on slave labor through the American Civil War.
Today only about three percent of the population are employed in the agricultural sector, it remains a major part of the state's economy, ranking 13th in the nation in the value of products sold. Arkansas is the nation's largest producer of rice, broilers, and turkeys, and ranks in the top three for cotton, pullets, and aquaculture (catfish). Forestry remains strong in the Arkansas Timberlands, and the state ranks fourth nationally and first in the South in softwood lumber production. Automobile parts manufacturers have opened factories in eastern Arkansas to support auto plants in other states. Bauxite was formerly a large part of the state's economy, mined mostly around Saline County.
Tourism is also very important to the Arkansas economy; the official state nickname "The Natural State" was created for state tourism advertising in the 1970s, and is still used to this day. The state maintains 52 state parks and the National Park Service maintains seven properties in Arkansas. The completion of the William Jefferson Clinton Presidential Library in Little Rock has drawn many visitors to the city and revitalized the nearby River Market District. Many cities also hold festivals, which draw tourists to Arkansas culture, such as The Bradley County Pink Tomato Festival in Warren, King Biscuit Blues Festival, Ozark Folk Festival, Toad Suck Daze, and Tontitown Grape Festival.
As of 2010 many Arkansas local newspapers are owned by WEHCO Media, Alabama-based Lancaster Management, Kentucky-based Paxton Media Group, Missouri-based Rust Communications, Nevada-based Stephens Media, and New York-based GateHouse Media.
The culture of Arkansas includes distinct cuisine, dialect, and traditional festivals. Sports are also very important to the culture of Arkansas, ranging from football, baseball, and basketball to hunting and fishing. Perhaps the best-known piece of Arkansas's culture is the stereotype of its citizens as shiftless hillbillies. The reputation began when the state was characterized by early explorers as a savage wilderness full of outlaws and thieves. The most enduring icon of Arkansas's hillbilly reputation is "The Arkansas Traveller", a painted depiction of a folk tale from the 1840s. Although intended to represent the divide between rich southeastern plantation Arkansas planters and the poor northwestern hill country, the meaning was twisted to represent a Northerner lost in the Ozarks on a white horse asking a backwoods Arkansan for directions. The state also suffers from the racial stigma common to former Confederate states, with historical events such as the Little Rock Nine adding to Arkansas's enduring image.
Art and history museums display pieces of cultural value for Arkansans and tourists to enjoy. Crystal Bridges Museum of American Art in Bentonville was visited by 604,000 people in 2012, its first year. The museum includes walking trails and educational opportunities in addition to displaying over 450 works covering five centuries of American art. Several historic town sites have been restored as Arkansas state parks, including Historic Washington State Park, Powhatan Historic State Park, and Davidsonville Historic State Park.
Arkansas features a variety of native music across the state, ranging from the blues heritage of West Memphis, Pine Bluff, Helena–West Helena to rockabilly, bluegrass, and folk music from the Ozarks. Festivals such as the King Biscuit Blues Festival and Bikes, Blues, and BBQ pay homage to the history of blues in the state. The Ozark Folk Festival in Mountain View is a celebration of Ozark culture and often features folk and bluegrass musicians. Literature set in Arkansas such as "I Know Why the Caged Bird Sings" by Maya Angelou and "A Painted House" by John Grisham describe the culture at various time periods.
Sports have become an integral part of the culture of Arkansas, and her residents enjoy participating in and spectating various events throughout the year.
Team sports and especially collegiate football have been important to Arkansans. College football in Arkansas began from humble beginnings. The University of Arkansas first fielded a team in 1894 when football was a very dangerous game. Recent studies of the damage to team members from the concussions common in football make it clear that the danger persists.
"Calling the Hogs" is a cheer that shows support for the Razorbacks, one of the two NCAA Division I Football Bowl Subdivision (FBS) teams in the state. High school football also began to grow in Arkansas in the early 20th century. Over the years, many Arkansans have looked to the Razorbacks football team as the public image of the state. Although the University of Arkansas is based in Fayetteville, the Razorbacks have always played at least one game per season at War Memorial Stadium in Little Rock in an effort to keep fan support in central and south Arkansas.
Arkansas State University joined the University of Arkansas in FBS (then known as Division I-A) in 1992 after playing in lower divisions for nearly two decades. The two schools have never played each other, due to the University of Arkansas's policy of not playing intrastate games. Two other campuses of the University of Arkansas System are Division I members. The University of Arkansas at Pine Bluff is a member of the Southwestern Athletic Conference, a league whose members all play football in the second-level Football Championship Subdivision (FCS). The University of Arkansas at Little Rock is a member of the FBS Sun Belt Conference, but is one of two conference schools that have no football program. The state's other DivisionI member is the University of Central Arkansas, which is a full member (including football) of the FCS Southland Conference.
Seven of Arkansas's smaller colleges play in NCAA Division II, with six in the Great American Conference and one in the Lone Star Conference. Two other small Arkansas colleges compete in NCAA Division III, in which athletic scholarships are prohibited.
Baseball runs deep in Arkansas and has been popular before the state hosted Major League Baseball (MLB) spring training in Hot Springs from 1886 to the 1920s. Two minor league teams are based in the state. The Arkansas Travelers play at Dickey–Stephens Park in North Little Rock, and the Northwest Arkansas Naturals play in Arvest Ballpark in Springdale. Both teams compete in the Texas League.
Related to the state's frontier past, hunting continues in the state. The state created the Arkansas Game and Fish Commission in 1915 to regulate hunting and enforce those regulations. Today a significant portion of Arkansas's population participates in hunting duck in the Mississippi flyway and deer across the state. Millions of acres of public land are available for both bow and modern gun hunters.
Fishing has always been popular in Arkansas, and the sport and the state have benefited from the creation of reservoirs across the state. Following the completion of Norfork Dam, the Norfork Tailwater and the White River have become a destination for trout fishers. Several smaller retirement communities such as Bull Shoals, Hot Springs Village, and Fairfield Bay have flourished due to their position on a fishing lake. The Buffalo National River has been preserved in its natural state by the National Park Service and is frequented by fly fishers annually.
As of 2012, Arkansas, as with many Southern states, has a high incidence of premature death, infant mortality, cardiovascular deaths, and occupational fatalities compared to the rest of the United States. The state is tied for 43rd with New York in percentage of adults who regularly exercise. Arkansas is usually ranked as one of the least healthy states due to high obesity, smoking, and sedentary lifestyle rates. However, a Gallup poll demonstrates that Arkansas made the most immediate progress in reducing its number of uninsured residents following the passage of the Affordable Care Act. The percentage of uninsured in Arkansas dropped from 22.5 percent in 2013 to 12.4 percent in August 2014.
The Arkansas Clean Indoor Air Act went into effect in 2006, a statewide smoking ban excluding bars and some restaurants.
Healthcare in Arkansas is provided by a network of hospitals as members of the Arkansas Hospital Association. Major institutions with multiple branches include Baptist Health, Community Health Systems, and HealthSouth. The University of Arkansas for Medical Sciences (UAMS) in Little Rock operates the UAMS Medical Center, a teaching hospital ranked as high performing nationally in cancer and nephrology. The pediatric division of UAMS Medical Center is known as Arkansas Children's Hospital, nationally ranked in pediatric cardiology and heart surgery. Together, these two institutions are the state's only Level I trauma centers.
Arkansas has 1,064 state-funded kindergartens, elementary, junior and senior high schools.
The state supports a network of public universities and colleges, including two major university systems: Arkansas State University System and University of Arkansas System. The University of Arkansas, flagship campus of the University of Arkansas System in Fayetteville was ranked #63 among public schools in the nation by "U.S. News & World Report". Other public institutions include University of Arkansas at Pine Bluff, Arkansas Tech University, Henderson State University, Southern Arkansas University, and University of Central Arkansas across the state. It is also home to 11 private colleges and universities including Hendrix College, one of the nation's top 100 liberal arts colleges, according to U.S. News & World Report.
In the 1920s the state required all children to attend public schools. The school year was set at 131 days, although some areas were unable to meet that requirement.
Generally prohibited in the West at large, school corporal punishment is not unusual in Arkansas, with 20,083 public school students paddled at least one time, according to government data for the 2011–2012 school year. The rate of corporal punishment in public schools is higher only in Mississippi.
Arkansas is one of the most under-educated states in the Union. It ranks near the bottom in terms of percentage of the population with either a high school or college degree. The state's educational system has a history of under-funding, low teachers' salaries and political meddling in the curriculum.
Educational statistics during these early days are fragmentary and unreliable. Many counties did not submit full reports to the secretary of state who did double-duty as commissioner of common schools. However, the percentage of whites over twenty years of age who were illiterate was given as:
In 2010 Arkansas students earned an average score of 20.3 on the ACT exam, just below the national average of 21. These results were expected due to the large increase in the number of students taking the exam since the establishment of the Academic Challenge Scholarship. Top high schools receiving recognition from the U.S. News & World Report are spread across the state, including Haas Hall Academy in Fayetteville, KIPP Delta Collegiate in Helena-West Helena, Bentonville, Rogers, Rogers Heritage, Valley Springs, Searcy, and McCrory. A total of 81 Arkansas high schools were ranked by the U.S. News & World Report in 2012.
Arkansas ranks as the 32nd smartest state on the Morgan Quitno Smartest State Award, 44th in percentage of residents with at least a high school diploma, and 48th in percentage of bachelor's degree attainment. Arkansas has been making strides in education reform. "Education Week" has praised the state, ranking Arkansas in the top 10 of their Quality Counts Education Rankings every year since 2009 while scoring it in the top5 during 2012 and 2013. Arkansas specifically received an A in Transition and Policy Making for progress in this area consisting of early-childhood education, college readiness, and career readiness. Governor Mike Beebe has made improving education a major issue through his attempts to spend more on education. Through reforms, the state is a leader in requiring curricula designed to prepare students for postsecondary education, rewarding teachers for student achievement, and providing incentives for principals who work in lower-tier schools.
As an organized territory, and later in the early days of statehood, education was funded by the sales of federally controlled public lands. This system was inadequate and prone to local graft. In an 1854 message to the legislature, Governor Elias N. Conway said, "We have a common-school law intended as a system to establish common schools in all part of the state; but for the want of adequate means there are very few in operation under this law." At the time, only about a quarter of children were enrolled in school.
In 1867, the state legislature was still controlled by ex-Confederates. It passed a Common Schools Law that allowed public funded but limited schools to white children.
The 1868 legislature banned former Confederates and passed a more wide-ranging law detailing funding and administrative issues and allowing black children to attend school. In furtherance of this, the postwar 1868 state constitution was the first to permit a personal-property tax to fund the lands and buildings for public schools. With the 1868 elections, the first county school commissioners took office.
In 2014, the state spent $9,616 per student, compared with a national average of about $11,000 putting Arkansas in nineteenth place.
Transportation in Arkansas is overseen by the Arkansas Department of Transportation (ArDOT), headquartered in Little Rock. Several main corridors pass through Little Rock, including Interstate 30 (I-30) and I-40 (the nation's 3rd-busiest trucking corridor). Arkansas first designated a state highway system in 1924, and first numbered its roads in 1926. Arkansas had one of the first paved roads, the Dollarway Road, and one of the first members of the Interstate Highway System. The state maintains a large system of state highways today, in addition to eight Interstates and 20 U.S. Routes.
In northeast Arkansas, I-55 travels north from Memphis to Missouri, with a new spur to Jonesboro (I-555). Northwest Arkansas is served by the segment of I-49 from Fort Smith to the beginning of the Bella Vista Bypass. This segment of I-49 currently follows mostly the same route as the former section of I-540 that extended north of I-40. The state also has the 13th largest state highway system in the nation.
Arkansas is served by of railroad track divided among twenty-six railroad companies including three Class I railroads. Freight railroads are concentrated in southeast Arkansas to serve the industries in the region. The Texas Eagle, an Amtrak passenger train, serves five stations in the state Walnut Ridge, Little Rock, Malvern, Arkadelphia, and Texarkana.
Arkansas also benefits from the use of its rivers for commerce. The Mississippi River and Arkansas River are both major rivers. The United States Army Corps of Engineers maintains the McClellan-Kerr Arkansas River Navigation System, allowing barge traffic up the Arkansas River to the Port of Catoosa in Tulsa, Oklahoma.
There are four airports with commercial service: Clinton National Airport (formerly Little Rock National Airport or Adams Field), Northwest Arkansas Regional Airport, Fort Smith Regional Airport, and Texarkana Regional Airport, with dozens of smaller airports in the state.
Public transit and community transport services for the elderly or those with developmental disabilities are provided by agencies such as the Central Arkansas Transit Authority and the Ozark Regional Transit, organizations that are part of the Arkansas Transit Association.
As with the federal government of the United States, political power in Arkansas is divided into three branches: executive, legislative, and judicial. Each officer's term is four years long. Office holders are term-limited to two full terms plus any partial terms before the first full term.
The governor of Arkansas is Asa Hutchinson, a Republican, who was inaugurated on January 13, 2015. The six other elected executive positions in Arkansas are lieutenant governor, secretary of state, attorney general, treasurer, auditor, and land commissioner. The governor also appoints qualified individuals to lead various state boards, committees, and departments. Arkansas governors served two-year terms until a referendum lengthened the term to four years, effective with the 1986 general election.
In Arkansas, the lieutenant governor is elected separately from the governor and thus can be from a different political party.
The Arkansas General Assembly is the state's bicameral bodies of legislators, composed of the Senate and House of Representatives. The Senate contains 35 members from districts of approximately equal population. These districts are redrawn decennially with each US census, and in election years ending in "2", the entire body is put up for reelection. Following the election, half of the seats are designated as two-year seats and are up for reelection again in two years, these "half-terms" do not count against a legislator's term limits. The remaining half serve a full four-year term. This staggers elections such that half the body is up for re-election every two years and allows for complete body turnover following redistricting. Arkansas voters selected a 21–14 Republican majority in the Senate in 2012. Arkansas House members can serve a maximum of three two-year terms. House districts are redistricted by the Arkansas Board of Apportionment. Following the 2012 elections, Republicans gained a 51–49 majority in the House of Representatives.
The Republican Party majority status in the Arkansas State House of Representatives following the 2012 elections is the party's first since 1874. Arkansas was the last state of the old Confederacy to never have Republicans control either chamber of its house since the Civil War.
Following the term limits changes, studies have shown that lobbyists have become less influential in state politics. Legislative staff, not subject to term limits, have acquired additional power and influence due to the high rate of elected official turnover.
Arkansas's judicial branch has five court systems: Arkansas Supreme Court, Arkansas Court of Appeals, Circuit Courts, District Courts and City Courts.
Most cases begin in district court, which is subdivided into state district court and local district court. State district courts exercise district-wide jurisdiction over the districts created by the General Assembly, and local district courts are presided over by part-time judges who may privately practice law. 25 state district court judges preside over 15 districts, with more districts created in 2013 and 2017. There are 28 judicial circuits of Circuit Court, with each contains five subdivisions: criminal, civil, probate, domestic relations, and juvenile court. The jurisdiction of the Arkansas Court of Appeals is determined by the Arkansas Supreme Court, and there is no right of appeal from the Court of Appeals to the high court. The Arkansas Supreme Court can review Court of Appeals cases upon application by either a party to the litigation, upon request by the Court of Appeals, or if the Arkansas Supreme Court feels the case should have been initially assigned to it. The twelve judges of the Arkansas Court of Appeals are elected from judicial districts to renewable six-year terms.
The Arkansas Supreme Court is the court of last resort in the state, composed of seven justices elected to eight-year terms. Established by the Arkansas Constitution in 1836, the court's decisions can be appealed to only the Supreme Court of the United States.
Both Arkansas's U.S. senators, John Boozman and Tom Cotton, are Republicans. The state has four seats in U.S. House of Representatives. All four seats are held by Republicans: Rick Crawford (1st district), French Hill (2nd district), Steve Womack (3rd district), and Bruce Westerman (4th district).
Arkansas governor Bill Clinton brought national attention to the state with a long speech at the 1988 Democratic National Convention endorsing Michael Dukakis. Some journalists suggested the speech was a threat to his ambitions; Clinton defined it "a comedy of error, just one of those fluky things". Clinton won the Democratic nomination for president the following cycle. Presenting himself as a "New Democrat" and using incumbent George H. W. Bush's against him, Clinton won the 1992 presidential election (43.0% of the vote) against Republican Bush (37.4% of the vote) and billionaire populist Ross Perot, who ran as an independent (18.9% of the vote).
Most Republican strength traditionally lay mainly in the northwestern part of the state, particularly Fort Smith and Bentonville, as well as North Central Arkansas around the Mountain Home area. In the latter area, Republicans have been known to get 90 percent or more of the vote, while the rest of the state was more Democratic. After 2010, Republican strength expanded further to the Northeast and Southwest and into the Little Rock suburbs. The Democrats are mostly concentrated to central Little Rock, the Mississippi Delta, the Pine Bluff area, and the areas around the southern border with Louisiana.
Arkansas has elected only three Republicans to the U.S. Senate since Reconstruction: Tim Hutchinson, who was defeated after one term by Mark Pryor; John Boozman, who defeated incumbent Blanche Lincoln; and Tom Cotton, who defeated Mark Pryor in the 2014 elections. Before 2013, the General Assembly had not been controlled by the Republican Party since Reconstruction, with the GOP holding a 51-seat majority in the state House and a 21-seat (of 35) in the state Senate following victories in 2012. Arkansas was one of just three states among the states of the former Confederacy that sent two Democrats to the U.S. Senate (the others being Florida and Virginia) for any period during the first decade of the 21st century.
In 2010, Republicans captured three of the state's four seats in the U.S. House of Representatives. In 2012, Republicans won election for all four House seats. Arkansas held the distinction of having a U.S. House delegation composed entirely of military veterans (Rick Crawford, Army; Tim Griffin, Army Reserve; Steve Womack, Army National Guard; Tom Cotton, Army). In 2014, the last Democrat in Arkansas's congressional delegation, Mark Pryor, was defeated in his campaign to win a third term in the U.S. Senate, leaving the entire congressional delegation in GOP hands for the first time since Reconstruction.
Reflecting the state's large evangelical population, the state has a strong social conservative bent. Under the Arkansas Constitution Arkansas is a right to work state, its voters passed a ban on same-sex marriage with 75% voting yes, and the state is one of a handful with legislation on its books banning abortion in the event "Roe v. Wade" is ever overturned.
Arkansas is one of only four states (Georgia, South Carolina, Wyoming) in the U.S. to not have any legal protection against hate crimes. An anti-hate crimes measure passed the state Senate in 2001 but failed before a House panel; a similar bill failed in 2017. The forests of Arkansas and the Ozark mountain region have provided cover for clandestine hate groups. White nationalist groups such as The Covenant, The Sword, and the Arm of the Lord had a compound in the Ozarks in the 1980s, later raided by the authorities. The Knights of the KKK and the Kingdom Identity Ministries (a Christian identity organization) have also established headquarters in this area of the state, specifically in the town of Harrison. A series of riots in Harrison in the early 1900s led to most of that town's African-American population being forced from the area.
In February 2018, prosecutors in Little Rock unsealed indictments against 54 members of the New Aryan Empire (a white supremacist group that began as a prison gang). Most of the NEE members indicted in 2019 are from Russellville. Its leader briefly escaped from a Pine Bluff jail in August of that year. In May, another group of white supremacists protesters carrying the flag of Nazi Germany also disrupted a Holocaust remembrance event in Russellville. Billboards have appeared in the state displaying white supremacist slogans (e.g. "anti-racist is a code word for anti-white") or promoting white pride websites. In 2018, the Southern Poverty Law Center (an organization which tracks hate groups) identified 14 distinct hate groups in the state.
In 2019, Republican Governor Asa Hutchinson (who as federal prosecutor in the 1980s was involved in negotiations with the CSA) and Democratic Senator Joyce Elliott have called on lawmakers in the state to approve harsher penalties for hate crimes.
The Strategic Air Command facility of Little Rock Air Force Base was one of eighteen silos in the command of the 308th Strategic Missile Wing (308th SMW), specifically one of the nine silos within its 374th Strategic Missile Squadron (374th SMS). The squadron was responsible for Launch Complex 374–7, site of the 1980 explosion of a Titan II Intercontinental Ballistic Missile (ICBM) in Damascus, Arkansas.
Taxes are collected by the Arkansas Department of Finance and Administration.
Arkansas is home to many areas protected by the National Park System. These include:

</doc>
<doc id="1931" url="https://en.wikipedia.org/wiki?curid=1931" title="Atmosphere (disambiguation)">
Atmosphere (disambiguation)

An atmosphere is a gas layer around a celestial body.
Atmosphere may also refer to:

</doc>
<doc id="1933" url="https://en.wikipedia.org/wiki?curid=1933" title="Apus">
Apus

Apus is a small constellation in the southern sky. It represents a bird-of-paradise, and its name means "without feet" in Greek because the bird-of-paradise was once wrongly believed to lack feet. First depicted on a celestial globe by Petrus Plancius in 1598, it was charted on a star atlas by Johann Bayer in his 1603 "Uranometria". The French explorer and astronomer Nicolas Louis de Lacaille charted and gave the brighter stars their Bayer designations in 1756.
The five brightest stars are all reddish in hue. Shading the others at apparent magnitude 3.8 is Alpha Apodis, an orange giant that has around 48 times the diameter and 928 times the luminosity of the Sun. Marginally fainter is Gamma Apodis, another ageing giant star. Delta Apodis is a double star, the two components of which are 103 arcseconds apart and visible with the naked eye. Two star systems have been found to have planets.
Apus was one of twelve constellations published by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman who had sailed on the first Dutch trading expedition, known as the "Eerste Schipvaart", to the East Indies. It first appeared on a 35-cm (14 in) diameter celestial globe published in 1598 in Amsterdam by Plancius with Jodocus Hondius. De Houtman included it in his southern star catalogue in 1603 under the Dutch name "De Paradijs Voghel", "The Bird of Paradise", and Plancius called the constellation "Paradysvogel Apis Indica"; the first word is Dutch for "bird of paradise". "Apis" (Latin for "bee") is assumed to have been a typographical error for "avis" ("bird").
After its introduction on Plancius's globe, the constellation's first known appearance in a celestial atlas was in German cartographer Johann Bayer's "Uranometria" of 1603. Bayer called it "Apis Indica" while fellow astronomers Johannes Kepler and his son-in-law Jakob Bartsch called it "Apus" or "Avis Indica". The name "Apus" is derived from the Greek "apous", meaning "without feet". This referred to the Western misconception that the bird-of-paradise had no feet, which arose because the only specimens available in the West had their feet and wings removed. Such specimens began to arrive in Europe in 1522, when the survivors of Ferdinand Magellan's expedition brought them home. The constellation later lost some of its tail when Nicolas-Louis de Lacaille used those stars to establish Octans in the 1750s.
Covering 206.3 square degrees and hence 0.5002% of the sky, Apus ranks 67th of the 88 modern constellations by area. Its position in the Southern Celestial Hemisphere means that the whole constellation is visible to observers south of 7°N. It is bordered by Ara, Triangulum Australe and Circinus to the north, Musca and Chamaeleon to the west, Octans to the south, and Pavo to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is "Aps". The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of six segments ("illustrated in infobox"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −67.48° and −83.12°.
Lacaille gave twelve stars Bayer designations, labelling them Alpha through to Kappa, including two stars next to each other as Delta and another two stars near each other as Kappa. Within the constellation's borders, there are 39 stars brighter than or equal to apparent magnitude 6.5. Beta, Gamma and Delta Apodis form a narrow triangle, with Alpha Apodis lying to the east. The five brightest stars are all red-tinged, which is unusual among constellations.
Alpha Apodis is an orange giant of spectral type K3III located 430 ± 20 light-years away from Earth, with an apparent magnitude of 3.8. It spent much of its life as a blue-white (B-type) main sequence star before expanding, cooling and brightening as it used up its core hydrogen. It has swollen to 48 times the Sun's diameter, and shines with a luminosity approximately 928 times that of the Sun, with a surface temperature of 4312 K. Beta Apodis is an orange giant 149 ± 2 light-years away, with a magnitude of 4.2. It is around 1.84 times as massive as the Sun, with a surface temperature of 4677 K. Gamma Apodis is a yellow giant of spectral type G8III located 150 ± 4 light-years away, with a magnitude of 3.87. It is approximately 63 times as luminous the Sun, with a surface temperature of 5279 K. Delta Apodis is a double star, the two components of which are 103 arcseconds apart and visible through binoculars. Delta is a red giant star of spectral type M4III located 630 ± 30 light-years away. It is a semiregular variable that varies from magnitude +4.66 to +4.87, with pulsations of multiple periods of 68.0, 94.9 and 101.7 days. Delta is an orange giant star of spectral type K3III, located 550 ± 10 light-years away, with a magnitude of 5.3. The separate components can be resolved with the naked eye.
The fifth-brightest star is Zeta Apodis at magnitude 4.8, a star that has swollen and cooled to become an orange giant of spectral type K1III, with a surface temperature of 4649 K and a luminosity 133 times that of the Sun. It is 300 ± 4 light-years distant. Near Zeta is Iota Apodis, a binary star system 1,040 ± 60 light-years distant, that is composed of two blue-white main sequence stars that orbit each other every 59.32 years. Of spectral types B9V and B9.5 V, they are both over three times as massive as the Sun.
Eta Apodis is a white main sequence star located 140.8 ± 0.9 light-years distant. Of apparent magnitude 4.89, it is 1.77 times as massive, 15.5 times as luminous as the Sun and has 2.13 times its radius. Aged 250 ± 200 million years old, this star is emitting an excess of 24 μm infrared radiation, which may be caused by a debris disk of dust orbiting at a distance of more than 31 astronomical units from it.
Theta Apodis is a cool red giant of spectral type M7 III located 350 ± 30 light-years distant. It shines with a luminosity approximately 3879 times that of the Sun and has a surface temperature of 3151 K. A semiregular variable, it varies by 0.56 magnitudes with a period of 119 days—or approximately 4 months. It is losing mass at the rate of times the mass of the Sun per year through its stellar wind. Dusty material ejected from this star is interacting with the surrounding interstellar medium, forming a bow shock as the star moves through the galaxy. NO Apodis is a red giant of spectral type M3III that varies between magnitudes 5.71 and 5.95. Located 780 ± 20 light-years distant, it shines with a luminosity estimated at 2059 times that of the Sun and has a surface temperature of 3568 K. S Apodis is a rare R Coronae Borealis variable, an extremely hydrogen-deficient supergiant thought to have arisen as the result of the merger of two white dwarfs; fewer than 100 have been discovered as of 2012. It has a baseline magnitude of 9.7. R Apodis is a star that was given a variable star designation, yet has turned out not to be variable. Of magnitude 5.3, it is another orange giant.
Two star systems have had exoplanets discovered by doppler spectroscopy, and the substellar companion of a third star system—the sunlike star HD 131664—has since been found to be a brown dwarf with a calculated mass of the companion to 23 times that of Jupiter (minimum of 18 and maximum of 49 Jovian masses). HD 134606 is a yellow sunlike star of spectral type G6IV that has begun expanding and cooling off the main sequence. Three planets orbit it with periods of 12, 59.5 and 459 days, successively larger as they are further away from the star. HD 137388 is another star—of spectral type K2IV—that is cooler than the Sun and has begun cooling off the main sequence. Around 47% as luminous and 88% as massive as the Sun, with 85% of its diameter, it is thought to be around 7.4 ± 3.9 billion years old. It has a planet that is 79 times as massive as the Earth and orbits its sun every 330 days at an average distance of 0.89 astronomical units (AU).
The Milky Way covers much of the constellation's area. Of the deep-sky objects in Apus, there are two prominent globular clusters—NGC 6101 and IC 4499—and a large faint nebula that covers several degrees east of Beta and Gamma Apodis. NGC 6101 is a globular cluster of apparent magnitude 9.2 located around 50,000 light-years distant from Earth, which is around 160 light-years across. Around 13 billion years old, it contains a high concentration of massive bright stars known as blue stragglers, thought to be the result of two stars merging. IC 4499 is a loose globular cluster in the medium-far galactic halo; its apparent magnitude is 10.6.
The galaxies in the constellation are faint. IC 4633 is a very faint spiral galaxy surrounded by a vast amount of Milky Way line-of-sight integrated flux nebulae—large faint clouds thought to be lit by large numbers of stars.

</doc>
<doc id="1934" url="https://en.wikipedia.org/wiki?curid=1934" title="Abadan, Iran">
Abadan, Iran

Abadan ( "Ābādān", ) is a city and capital of Abadan County, Khuzestan Province, which is located in the southwest of Iran. It lies on Abadan Island ( long, 3–19 km or 2–12 miles wide). The island is bounded in the west by the Arvand waterway and to the east by the Bahmanshir outlet of the Karun River (the Arvand Rood), from the Persian Gulf, near the Iran–Iraq border. Abadan is 140 km from the provincial capital city of Ahvaz.
The earliest mention of the island of Abadan, if not the port itself, is found in works of the geographer Marcian, who renders the name "Apphadana". Earlier, the classical geographer Ptolemy notes "Apphana" as an island off the mouth of the Tigris (which is, where the modern Island of Abadan is located). An etymology for this name is presented by B. Farahvashi to be derived from the Persian word "ab" (water) and the root "pā" (guard, watch) thus "coastguard station").
In Islamic times, a pseudo-etymology was produced by the historian Ahmad ibn Yahya al-Baladhuri (d. 892) quoting a folk story that the town was presumably founded by one "Abbad bin Hosayn" from the Arabian Tribe of Banu Tamim, who established a garrison there during the governorship of "Hajjaj" in the Ummayad period.
In the subsequent centuries, the Persian version of the name had begun to come into general use before it was adopted by official decree in 1935.
The civilian population of the city dropped close to zero during the eight years of the Iran–Iraq War (1980–1988). The 1986 census recorded only 6 people. In 1991, 84,774 had returned to live in the city. By 2001, the population had jumped to 206,073, and it was 217,988, in 48,061 families, according to 2006 census. Abadan Refinery is one of the largest in the world. The population today has reached almost 350,000 people.
Only 9% of managers (of the oil company) were from Khuzestan. The proportion of natives of Tehran, the Caspian, Azerbaijan, and Kurdistan rose from 4% of blue collar workers to 22% of white collar workers to 45% of managers, thus Arabic-speakers were concentrated on the lower rungs of the work force, managers tended to be brought in from some distance. There is also a single Armenian church in the centre of the city.
Abadan is thought to have been further developed into a major port city under the Abbasids' rule. The city was then a commercial source of salt and woven mats. The siltation of the river delta forced the town further away from water; In the 14th century, however, Ibn Battutah described Abadan just as a small port in a flat salty plain. Politically, Abadan was often the subject of dispute between the nearby states. In 1847, Persia acquired it from the Ottoman Empire in which state Abadan has remained since. From the 17th century onward, the island of Abadan was part of the lands of the Arab "Ka'ab" (Bani Kaab) tribe. One section of the tribe, "Mohaysen", had its headquarters at "Mohammara" (now Khorramshahr), until the removal of Shaikh Khaz'al Khan in 1924.
It was not until the 20th century that rich oil fields were discovered in the area. On 16 July 1909, after secret negotiation with the British consul, Percy Cox, assisted by Arnold Wilson, and Sheik Khaz'al agreed to a rental agreement for the island, including Abadan. The Sheik continued to administer the island until 1924. The Anglo-Persian Oil Company built their first pipeline terminus oil refinery in Abadan, starting in 1909 and completing it in 1912, with oil flowing by August 1912 (see Abadan Refinery). Refinery throughput numbers rose from 33,000 tons in 1912–1913 to 4,338,000 tons in 1931. By 1938, it was the largest in the world.
During World War II, Abadan was the site of brief combat between Iranian forces and British and Indian troops during the Anglo-Soviet invasion of Iran. Later, Abadan was a major logistics centre for Lend-Lease aircraft being sent to the Soviet Union by the United States.
In 1951, Iran nationalised all oil properties and refining ground to a stop on the island. Rioting broke out in Abadan, after the government had decided to nationalise the oil facilities, and three British workers were killed. It was not until 1954 that a settlement was reached, which allowed a consortium of international oil companies to manage the production and refining on the island. That continued until 1973, when the NIOC took over all facilities. After the total nationalisation, Iran focused on supplying oil domestically and built a pipeline from Abadan to Tehran.
Abadan was not a major cultural or religious centre, but it played an important role in the Islamic Revolution. On 19 August 1978, the anniversary of the US-backed coup d'état that had overthrown the nationalist and popular Iranian prime minister, Mohammed Mossadegh, the Cinema Rex, a movie theatre in Abadan, was set ablaze. The Cinema Rex Fire caused 430 deaths, but more importantly, it was another event that kept the Islamic Revolution moving ahead. At the time, there was much confusion and misinformation about the perpetrators of the incident. The public largely put the blame on the local police chief and also the Shah and SAVAK. The reformist "Sobhe Emrooz" newspaper in one of its editorials revealed that the Cinema Rex was burned down by radical Islamists. The newspaper was shut down immediately after. Over time, the true culprits, radical Islamists, were apprehended, and the logic behind this act was revealed, as they were trying both to foment the general public to distrust the government even more, and they also perceived cinema as a link to the Americans. The fire was one of four during a short period in August, with other fires in Mashhad, Rizaiya, and Shiraz.
In September 1980, Abadan was almost overrun during a surprise attack on Khuzestan by Iraq, marking the beginning of the Iran–Iraq War. For 12 months, Abadan was besieged, but never captured, by Iraqi forces, and in September 1981, the Iranians broke the siege of Abadan. Much of the city, including the oil refinery, which was the world's largest refinery with capacity of 628,000 barrels per day, was badly damaged or destroyed by the siege and by bombing. Prior to the war, the city's civilian population was about 300,000, but at the war's end nearly the entire populace had sought refuge elsewhere in Iran.
After the war, the biggest concern was the rebuilding of Abadan's oil refinery, as it was operating at 10% of capacity due to damage. In 1993, the refinery began limited operation and the port reopened. By 1997, the refinery reached the same rate of production as before the war. Recently, Abadan has been the site of major labour activity as workers at the oil refineries in the city have staged walkouts and strikes to protest non-payment of wages and the political situation in the country.
To honor the 100th anniversary of the refining of oil in Abadan, city officials are planning an oil museum. The Abadan oil refinery was featured on the reverse side of Iran's 100-rial banknotes printed in 1965 and from 1971 to 1973. Abadan today has been declared as a free zone city. The healthy relationship between Iran and Iraq has become one of the transit cities connecting both countries through a 40-minute drive.
The climate in Abadan is arid (Köppen climate classification "BWh") and similar to Baghdad's, but slightly hotter due to Abadan's lower latitude. Summers are dry and extremely hot, with temperatures above almost daily and temperatures above can be almost common. Abadan is notably one of the few hottest populated places on earth and experiences a few sand and dust storms per year. Winters are mildly wet and spring-like, though subject to cold spells. Winter temperatures are around . The world's highest unconfirmed temperature was a temperature flare up during a heat burst in June 1967, with a temperature of . The lowest recorded temperature in the city range is . which was recorded on 20 January 1964 and 3 February 1967 while the highest is , recorded on 11 July 1951 and 9 August 1981.
The Abadan Institute of Technology was established in Abadan in 1939. The school specialized in engineering and petroleum chemistry, and was designed to train staff for the refinery in town. The school's name has since changed several times, but since 1989 has been considered a branch campus of the Petroleum University of Technology, centred in Tehran. Abadan University of Medical Sciences, It was founded by Ministry of Health and Medical Education in September 1941 as a Nursing Faculty and in 2012 it became an independent faculty of medical school. Program study of this school is similar to curriculum that applies most Iranian medical faculties.
Abadan was chosen for constructing a refinery because of its strategic position and proximity to other resources. The Abadan Refinery construction project started in 1909 and its operation began in 1962 by a production capacity of 2500 barrels per day.
There is an international airport in Abadan. It is represented by the IATA airport code ABD.
There is a large amount of external investment from East Asian countries that are building oil refineries and developing a lot of real estate.
Today, Abadan is known for its lively fish market where locals buy fresh catch of the day used in the many delicious seafood dishes of the city. Abadan is also part of the Arvand Free Zone, a 155 square kilometer industrial and security zone.
The city is served by Abadan-Ayatollah Jami International Airport with flights on various commercial airlines.
Nearest railway station is in Khorramshahr, about 10 km north of Abadan. Daytime trains from Ahvaz as well as overnight trains from Tehran and Mashhad are available.
Sanat Naft Abadan F.C., is one of the Iranian football clubs that is currently competing with other teams in the Iranian Football Premier League.
Takhti Stadium, the main stadium is the city and the team.

</doc>
<doc id="1935" url="https://en.wikipedia.org/wiki?curid=1935" title="Attorney">
Attorney

Attorney may refer to:

</doc>
<doc id="1937" url="https://en.wikipedia.org/wiki?curid=1937" title="Alexander Fleming">
Alexander Fleming

Sir Alexander Fleming (6 August 1881 – 11 March 1955) was a Scottish physician and microbiologist. His best-known discoveries are the enzyme lysozyme in 1923 and the world's first broadly effective antibiotic substance benzylpenicillin (Penicillin G) from the mould "Penicillium rubens" in 1928, for which he shared the Nobel Prize in Physiology or Medicine in 1945 with Howard Florey and Ernst Boris Chain. He wrote many articles on bacteriology, immunology, and chemotherapy.
Fleming was knighted for his scientific achievements in 1944. In 1999, he was named in "Time" magazine's list of the . In 2002, he was chosen in the BBC's television poll for determining the 100 Greatest Britons, and in 2009, he was also voted third "greatest Scot" in an opinion poll conducted by STV, behind only Robert Burns and William Wallace.
Born on 6 August 1881 at Lochfield farm near Darvel, in Ayrshire, Scotland, Alexander Fleming was the third of four children of farmer Hugh Fleming (1816–1888) and Grace Stirling Morton (1848–1928), the daughter of a neighbouring farmer. Hugh Fleming had four surviving children from his first marriage. He was 59 at the time of his second marriage to Grace, and died when Alexander was seven.
Fleming went to Loudoun Moor School and Darvel School, and earned a two-year scholarship to Kilmarnock Academy before moving to London, where he attended the Royal Polytechnic Institution. After working in a shipping office for four years, the twenty-year-old Alexander Fleming inherited some money from an uncle, John Fleming. His elder brother, Tom, was already a physician and suggested to him that he should follow the same career, and so in 1903, the younger Alexander enrolled at St Mary's Hospital Medical School in Paddington; he qualified with an MBBS degree from the school with distinction in 1906.
Fleming, who was a private in the London Scottish Regiment of the Volunteer Force from 1900 to 1914, had been a member of the rifle club at the medical school. The captain of the club, wishing to retain Fleming in the team, suggested that he join the research department at St Mary's, where he became assistant bacteriologist to Sir Almroth Wright, a pioneer in vaccine therapy and immunology. In 1908, he gained a BSc degree with Gold Medal in Bacteriology, and became a lecturer at St Mary's until 1914.
Commissioned lieutenant in 1914 and promoted captain in 1917, Fleming served throughout World War I in the Royal Army Medical Corps, and was Mentioned in Dispatches. He and many of his colleagues worked in battlefield hospitals at the Western Front in France. In 1918 he returned to St Mary's Hospital, where he was elected Professor of Bacteriology of the University of London in 1928. In 1951 he was elected the Rector of the University of Edinburgh for a term of three years.
During World War I, Fleming witnessed the death of many soldiers from sepsis resulting from infected wounds. Antiseptics, which were used at the time to treat infected wounds, often worsened the injuries. In an article he submitted for the medical journal "The Lancet" during World War I, Fleming described an ingenious experiment, which he was able to conduct as a result of his own glass blowing skills, in which he explained why antiseptics were killing more soldiers than infection itself during World War I. Antiseptics worked well on the surface, but deep wounds tended to shelter anaerobic bacteria from the antiseptic agent, and antiseptics seemed to remove beneficial agents produced that protected the patients in these cases at least as well as they removed bacteria, and did nothing to remove the bacteria that were out of reach. Sir Almroth Wright strongly supported Fleming's findings, but despite this, most army physicians over the course of the war continued to use antiseptics even in cases where this worsened the condition of the patients.
At St Mary's Hospital Fleming continued his investigations into antibacterial substances. Testing the nasal secretions from a patient with a heavy cold, in 1922 he found that nasal mucus had an inhibitory effect on bacterial growth. This was the first recorded discovery of lysozyme, an enzyme present in many secretions including tears, saliva, skin, hair and nails as well as mucus. Although he was able to obtain larger amounts of lysozyme from egg whites, the enzyme was only effective against small counts of harmless bacteria, and therefore had little therapeutic potential.
By 1927, Fleming had been investigating the properties of staphylococci. He was already well known from his earlier work, and had developed a reputation as a brilliant researcher. In 1928, he studied the variation of Staphylococcus aureus grown under natural condition, after the work of Joseph Warwick Bigger, who discovered that the bacterium could grow into a variety of types (strains). On 3 September 1928, Fleming returned to his laboratory having spent a holiday with his family at Suffolk. Before leaving for his holiday, he inoculated staphylococci on culture plates and left them on a bench in a corner of his laboratory. On his return, Fleming noticed that one culture was contaminated with a fungus, and that the colonies of staphylococci immediately surrounding the fungus had been destroyed, whereas other staphylococci colonies farther away were normal, famously remarking "That's funny". Fleming showed the contaminated culture to his former assistant Merlin Pryce, who reminded him, "That's how you discovered lysozyme." He identified the mould as being from the genus "Penicillium." He suspected it to be "P. chrysogenum," but a colleague Charles J. La Touche identified it as "P. rubrum." (It was later corrected as "P. notatum" and then officially accepted as "P. chrysogenum"; but finally in 2011, it was resolved as "P. rubens.") The laboratory in which Fleming discovered and tested penicillin is preserved as the Alexander Fleming Laboratory Museum in St. Mary's Hospital, Paddington.
Fleming grew the mould in a pure culture and found that the culture broth contained the antibacterial a substance. He investigated its positive anti-bacterial effect on many organisms, and noticed that it affected bacteria such as staphylococci and many other Gram-positive pathogens that cause scarlet fever, pneumonia, meningitis and diphtheria, but not typhoid fever or paratyphoid fever, which are caused by Gram-negative bacteria, for which he was seeking a cure at the time. It also affected "Neisseria gonorrhoeae," which causes gonorrhoea, although this bacterium is Gram-negative. After some months of calling it "mould juice" or "the inhibitor", he named the substance it released "penicillin" on 7 March 1929.
Fleming published his discovery in 1929, in the "British Journal of Experimental Pathology," but little attention was paid to his article. Fleming continued his investigations, but found that cultivating "Penicillium" was quite difficult, and that after having grown the mould, it was even more difficult to isolate the antibiotic agent. Fleming's impression was that because of the problem of producing it in quantity, and because its action appeared to be rather slow, penicillin would not be important in treating infection. Fleming also became convinced that penicillin would not last long enough in the human body ("in vivo") to kill bacteria effectively. Many clinical tests were inconclusive, probably because it had been used as a surface antiseptic. In the 1930s, Fleming's trials occasionally showed more promise, but Fleming largely abandoned penicillin work, leaving Howard Florey and Ernst Boris Chain at the Radcliffe Infirmary in Oxford to take up research to mass-produce it, with funds from the U.S. and British governments. They started mass production after the bombing of Pearl Harbor. By D-Day in 1944, enough penicillin had been produced to treat all the wounded in the Allied forces.
In Oxford, Ernst Boris Chain and Edward Abraham were studying the molecular structure of the antibiotic. Abraham was the first to propose the correct structure of penicillin. Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. When Chain heard that Fleming was coming, he remarked "Good God! I thought he was dead."
Norman Heatley suggested transferring the active ingredient of penicillin back into water by changing its acidity. This produced enough of the drug to begin testing on animals. There were many more people involved in the Oxford team, and at one point the entire Dunn School was involved in its production. After the team had developed a method of purifying penicillin to an effective first stable form in 1940, several clinical trials ensued, and their amazing success inspired the team to develop methods for mass production and mass distribution in 1945.
Fleming was modest about his part in the development of penicillin, describing his fame as the "Fleming Myth" and he praised Florey and Chain for transforming the laboratory curiosity into a practical drug. Fleming was the first to discover the properties of the active substance, giving him the privilege of naming it: penicillin. He also kept, grew, and distributed the original mould for twelve years, and continued until 1940 to try to get help from any chemist who had enough skill to make penicillin. But Sir Henry Harris said in 1998: "Without Fleming, no Chain; without Chain, no Florey; without Florey, no Heatley; without Heatley, no penicillin." The discovery of penicillin and its subsequent development as a prescription drug mark the start of modern antibiotics.
The accidental discovery and isolation of penicillin by Fleming in September 1928 led to one of the first broadly effective antibiotic drug in medicine. Before that, several scientists had published or pointed out that mould or "Penicillium sp." were able to inhibit bacterial growth, and even to cure bacterial infections in animals. Ernest Duchesne in 1897 in his thesis "Contribution to the study of vital competition in micro-organisms: antagonism between moulds and microbes", or also Clodomiro Picado Twight whose work at the Institut Pasteur in 1923 on the inhibiting action of fungi of the "Penicillin sp." genre in the growth of staphylococci drew little interest from the directors of the Institut at the time. Fleming was the first to push these studies further by isolating the penicillin, and by being motivated enough to promote his discovery at a larger scale.
Fleming also discovered very early that bacteria developed antibiotic resistance whenever too little penicillin was used or when it was used for too short a period. Almroth Wright had predicted antibiotic resistance even before it was noticed during experiments. Fleming cautioned about the use of penicillin in his many speeches around the world. On 26 June 1945, he made the following cautionary statements: "the microbes are educated to resist penicillin and a host of penicillin-fast organisms is bred out ... In such cases the thoughtless person playing with penicillin is morally responsible for the death of the man who finally succumbs to infection with the penicillin-resistant organism. I hope this evil can be averted." He cautioned not to use penicillin unless there was a properly diagnosed reason for it to be used, and that if it were used, never to use too little, or for too short a period, since these are the circumstances under which bacterial resistance to antibiotics develops.
The popular story of Winston Churchill's father paying for Fleming's education after Fleming's father saved young Winston from death is false. According to the biography, "Penicillin Man: Alexander Fleming and the Antibiotic Revolution" by Kevin Brown, Alexander Fleming, in a letter to his friend and colleague Andre Gratia, described this as "A wondrous fable." Nor did he save Winston Churchill himself during World War II. Churchill was saved by Lord Moran, using sulphonamides, since he had no experience with penicillin, when Churchill fell ill in Carthage in Tunisia in 1943. "The Daily Telegraph" and "The Morning Post" on 21 December 1943 wrote that he had been saved by penicillin. He was saved by the new sulphonamide drug Sulphapyridine, known at the time under the research code M&B 693, discovered and produced by May & Baker Ltd, Dagenham, Essex – a subsidiary of the French group Rhône-Poulenc. In a subsequent radio broadcast, Churchill referred to the new drug as "This admirable M&B". It is highly probable that the correct information about the sulphonamide did not reach the newspapers because, since the original sulphonamide antibacterial, Prontosil, had been a discovery by the German laboratory Bayer, and as Britain was at war with Germany at the time, it was thought better to raise British morale by associating Churchill's cure with a British discovery, penicillin.
Fleming's discovery of penicillin changed the world of modern medicine by introducing the age of useful antibiotics; penicillin has saved, and is still saving, millions of people around the world.
The laboratory at St Mary's Hospital where Fleming discovered penicillin is home to the Fleming Museum, a popular London attraction. His alma mater, St Mary's Hospital Medical School, merged with Imperial College London in 1988. The "Sir Alexander Fleming Building" on the South Kensington campus was opened in 1998, where his son Robert and his great granddaughter Claire were presented to the Queen; it is now one of the main preclinical teaching sites of the Imperial College School of Medicine.
His other alma mater, the Royal Polytechnic Institution (now the University of Westminster) has named one of its student halls of residence "Alexander Fleming House", which is near to Old Street.
On 24 December 1915, Fleming married a trained nurse, Sarah Marion McElroy of Killala, County Mayo, Ireland. Their only child, Robert Fleming (1924–2015), became a general medical practitioner. After his first wife's death in 1949, Fleming married Dr. Amalia Koutsouri-Vourekas, a Greek colleague at St. Mary's, on 9 April 1953; she died in 1986.
Fleming came from a Presbyterian background, while his first wife Sarah was a (lapsed) Roman Catholic. It is said that he was not particularly religious, and their son Robert was later received into the Anglican church, while still reportedly inheriting his two parents' fairly irreligious disposition.
From 1921 until his death in 1955, Fleming owned a country home in Barton Mills, Suffolk.
On 11 March 1955, Fleming died at his home in London of a heart attack. His ashes are buried in St Paul's Cathedral.

</doc>
<doc id="1938" url="https://en.wikipedia.org/wiki?curid=1938" title="Andrew Carnegie">
Andrew Carnegie

Andrew Carnegie (November 25, 1835August 11, 1919) was a Scottish-American industrialist and philanthropist. Carnegie led the expansion of the American steel industry in the late 19th century and became one of the richest Americans in history. He became a leading philanthropist in the United States and in the British Empire. During the last 18 years of his life, he gave away $350 million (conservatively $65 billion in 2019 dollars, based on percentage of GDP) to charities, foundations, and universities – almost 90 percent of his fortune. His 1889 article proclaiming "The Gospel of Wealth" called on the rich to use their wealth to improve society, and stimulated a wave of philanthropy.
Carnegie was born in Dunfermline, Scotland, and emigrated to the United States with his parents in 1848 at age 12. Carnegie started work as a telegrapher, and by the 1860s had investments in railroads, railroad sleeping cars, bridges, and oil derricks. He accumulated further wealth as a bond salesman, raising money for American enterprise in Europe. He built Pittsburgh's Carnegie Steel Company, which he sold to J. P. Morgan in 1901 for $303,450,000. It became the U.S. Steel Corporation. After selling Carnegie Steel, he surpassed John D. Rockefeller as the richest American for the next several years.
Carnegie devoted the remainder of his life to large-scale philanthropy, with special emphasis on local libraries, world peace, education, and scientific research. With the fortune he made from business, he built Carnegie Hall in New York, NY, and the Peace Palace and founded the Carnegie Corporation of New York, Carnegie Endowment for International Peace, Carnegie Institution for Science, Carnegie Trust for the Universities of Scotland, Carnegie Hero Fund, Carnegie Mellon University, and the Carnegie Museums of Pittsburgh, among others.
Andrew Carnegie was born to Margaret Morrison Carnegie and William Carnegie in Dunfermline, Scotland, in a typical weaver's cottage with only one main room, consisting of half the ground floor, which was shared with the neighboring weaver's family. The main room served as a living room, dining room and bedroom. He was named after his paternal grandfather. In 1836, the family moved to a larger house in Edgar Street (opposite Reid's Park), following the demand for more heavy damask, from which his father benefited. He was educated at the Free School in Dunfermline, which had been a gift to the town by the philanthropist Adam Rolland of Gask.
Carnegie's maternal uncle, George Lauder, Sr., a Scottish political leader, deeply influenced him as a boy by introducing him to the writings of Robert Burns and historical Scottish heroes such as Robert the Bruce, William Wallace, and Rob Roy. Lauder's son, also named George Lauder, grew up with Carnegie and would become his business partner. When Carnegie was thirteen, his father had fallen on very hard times as a handloom weaver; making matters worse, the country was in starvation. His mother helped support the family by assisting her brother (a cobbler), and by selling potted meats at her "sweetie shop", leaving her as the primary breadwinner. Struggling to make ends meet, the Carnegies then decided to borrow money from George Lauder, Sr. and move to Allegheny, Pennsylvania, in the United States in 1848 for the prospect of a better life. Carnegie's migration to America would be his second journey outside Dunfermline – the first being an outing to Edinburgh to see Queen Victoria.
In September 1848, Carnegie arrived with his family at their new prosperous home. Allegheny was rapidly populating in the 1840s, growing from around 10,000 to 21,262 residents. The city was very industrial and produced many products including wool and cotton cloth. The "Made in Allegheny" label used on these and other diversified products was becoming more and more popular. For his father, the promising circumstances still did not provide him any good fortune. Dealers were not interested in selling his product, and he himself struggled to sell it on his own. Eventually, the father and son both received job offers at the same Scottish-owned cotton mill, Anchor Cotton Mills. Carnegie's first job in 1848 was as a bobbin boy, changing spools of thread in a cotton mill 12 hours a day, 6 days a week in a Pittsburgh cotton factory. His starting wage was $1.20 per week ($ by inflation).
His father quit his position at the cotton mill soon after, returning to his loom and removing him as breadwinner once again. But Carnegie attracted the attention of John Hay, a Scottish manufacturer of bobbins, who offered him a job for $2.00 per week ($ by inflation). In his autobiography, Carnegie speaks of his past hardships he had to endure with this new job.
In 1849, Carnegie became a telegraph messenger boy in the Pittsburgh Office of the Ohio Telegraph Company, at $2.50 per week ($ by inflation) following the recommendation of his uncle. He was a hard worker and would memorize all of the locations of Pittsburgh's businesses and the faces of important men. He made many connections this way. He also paid close attention to his work and quickly learned to distinguish the differing sounds the incoming telegraph signals produced. He developed the ability to translate signals by ear, without using the paper slip, and within a year was promoted to operator. Carnegie's education and passion for reading was given a boost by Colonel James Anderson, who opened his personal library of 400 volumes to working boys each Saturday night. Carnegie was a consistent borrower and a "self-made man" in both his economic development and his intellectual and cultural development. He was so grateful to Colonel Anderson for the use of his library that he "resolved, if ever wealth came to me, [to see to it] that other poor boys might receive opportunities similar to those for which we were indebted to the noble man". His capacity, his willingness for hard work, his perseverance and his alertness soon brought him opportunities.
Starting in 1853, when Carnegie was around 18 years old, Thomas A. Scott of the Pennsylvania Railroad Company employed him as a secretary/telegraph operator at a salary of $4.00 per week ($ by inflation). Carnegie accepted the job with the railroad as he saw more prospects for career growth and experience there than with the telegraph company. At age 24, Scott asked Carnegie if he could handle being superintendent of the Western Division of the Pennsylvania Railroad. On December 1, 1859, Carnegie officially became superintendent of the Western Division. Carnegie then hired his sixteen-year-old brother, Tom, to be his personal secretary and telegraph operator. Not only did Carnegie hire his brother, but he also hired his cousin, Maria Hogan, who became the first female telegraph operator in the country. As superintendent Carnegie made a salary of fifteen hundred dollars a year ($ by inflation). His employment by the Pennsylvania Railroad Company would be vital to his later success. The railroads were the first big businesses in America, and the Pennsylvania was one of the largest of them all. Carnegie learned much about management and cost control during these years, and from Scott in particular.
Scott also helped him with his first investments. Many of these were part of the corruption indulged in by Scott and the Pennsylvania's president, John Edgar Thomson, which consisted of inside trading in companies that the railroad did business with, or payoffs made by contracting parties "as part of a quid pro quo". In 1855, Scott made it possible for Carnegie to invest $500 in the Adams Express, which contracted with the Pennsylvania to carry its messengers. The money was secured by his mother's placing of a $600 mortgage on the family's $700 home, but the opportunity was available only because of Carnegie's close relationship with Scott. A few years later, he received a few shares in Theodore Tuttle Woodruff's sleeping car company, as a reward for holding shares that Woodruff had given to Scott and Thomson, as a payoff. Reinvesting his returns in such inside investments in railroad-related industries: (iron, bridges, and rails), Carnegie slowly accumulated capital, the basis for his later success. Throughout his later career, he made use of his close connections to Thomson and Scott, as he established businesses that supplied rails and bridges to the railroad, offering the two men a stake in his enterprises.
Before the Civil War, Carnegie arranged a merger between Woodruff's company and that of George Pullman, the inventor of a sleeping car for first class travel, which facilitated business travel at distances over . The investment proved a success and a source of profit for Woodruff and Carnegie. The young Carnegie continued to work for the Pennsylvania's Tom Scott, and introduced several improvements in the service.
In spring 1861, Carnegie was appointed by Scott, who was now Assistant Secretary of War in charge of military transportation, as Superintendent of the Military Railways and the Union Government's telegraph lines in the East. Carnegie helped open the rail lines into Washington D.C. that the rebels had cut; he rode the locomotive pulling the first brigade of Union troops to reach Washington D.C. Following the defeat of Union forces at Bull Run, he personally supervised the transportation of the defeated forces. Under his organization, the telegraph service rendered efficient service to the Union cause and significantly assisted in the eventual victory. Carnegie later joked that he was "the first casualty of the war" when he gained a scar on his cheek from freeing a trapped telegraph wire.
Defeat of the Confederacy required vast supplies of munitions, as well as railroads (and telegraph lines) to deliver the goods. The war demonstrated how integral the industries were to American success.
In 1864, Carnegie was one of the early investors in the Columbia Oil Company in Venango County, Pennsylvania. In one year, the farm yielded over $1,000,000 in cash dividends, and petroleum from oil wells on the property sold profitably. The demand for iron products, such as armor for gunboats, cannons, and shells, as well as a hundred other industrial products, made Pittsburgh a center of wartime production. Carnegie worked with others in establishing a steel rolling mill, and steel production and control of industry became the source of his fortune. Carnegie had some investments in the iron industry before the war.
After the war, Carnegie left the railroads to devote his energies to the ironworks trade. Carnegie worked to develop several ironworks, eventually forming the Keystone Bridge Works and the Union Ironworks, in Pittsburgh. Although he had left the Pennsylvania Railroad Company, he remained connected to its management, namely Thomas A. Scott and J. Edgar Thomson. He used his connection to the two men to acquire contracts for his Keystone Bridge Company and the rails produced by his ironworks. He also gave stock to Scott and Thomson in his businesses, and the Pennsylvania was his best customer. When he built his first steel plant, he made a point of naming it after Thomson. As well as having good business sense, Carnegie possessed charm and literary knowledge. He was invited to many important social functions, which Carnegie exploited to his advantage.
Carnegie believed in using his fortune for others and doing more than making money. He wrote:
Carnegie did not want to marry during his mother's lifetime, instead choosing to take care of her in her illness towards the end of her life. After she died in 1886, the 51-year-old Carnegie married Louise Whitfield, who was 21 years his junior. In 1897, the couple had their only child, a daughter, whom they named after Carnegie's mother, Margaret.
Carnegie made his fortune in the steel industry, controlling the most extensive integrated iron and steel operations ever owned by an individual in the United States. One of his two great innovations was in the cheap and efficient mass production of steel by adopting and adapting the Bessemer process, which allowed the high carbon content of pig iron to be burnt away in a controlled and rapid way during steel production. Steel prices dropped as a result, and Bessemer steel was rapidly adopted for rails; however, it was not suitable for buildings and bridges.
The second was in his vertical integration of all suppliers of raw materials. In the late 1880s, Carnegie Steel was the largest manufacturer of pig iron, steel rails, and coke in the world, with a capacity to produce approximately 2,000 tons of pig iron per day. In 1883, Carnegie bought the rival Homestead Steel Works, which included an extensive plant served by tributary coal and iron fields, a long railway, and a line of lake steamships. Carnegie combined his assets and those of his associates in 1892 with the launching of the Carnegie Steel Company.
By 1889, the U.S. output of steel exceeded that of the UK, and Carnegie owned a large part of it. Carnegie's empire grew to include the J. Edgar Thomson Steel Works in Braddock, (named for John Edgar Thomson, Carnegie's former boss and president of the Pennsylvania Railroad), Pittsburgh Bessemer Steel Works, the Lucy Furnaces, the Union Iron Mills, the Union Mill (Wilson, Walker & County), the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines. Carnegie, through Keystone, supplied the steel for and owned shares in the landmark Eads Bridge project across the Mississippi River at St. Louis, Missouri (completed 1874). This project was an important proof-of-concept for steel technology, which marked the opening of a new steel market.
In 1901, Carnegie was 66 years of age and considering retirement. He reformed his enterprises into conventional joint stock corporations as preparation for this. John Pierpont Morgan was a banker and America's most important financial deal maker. He had observed how efficiently Carnegie produced profits. He envisioned an integrated steel industry that would cut costs, lower prices to consumers, produce in greater quantities and raise wages to workers. To this end, he needed to buy out Carnegie and several other major producers and integrate them into one company, thereby eliminating duplication and waste. He concluded negotiations on March 2, 1901, and formed the United States Steel Corporation. It was the first corporation in the world with a market capitalization over $1 billion.
The buyout, secretly negotiated by Charles M. Schwab (no relation to Charles R. Schwab), was the largest such industrial takeover in United States history to date. The holdings were incorporated in the United States Steel Corporation, a trust organized by Morgan, and Carnegie retired from business. His steel enterprises were bought out for $303,450,000.
Carnegie's share of this amounted to $225.64 million (in , $), which was paid to Carnegie in the form of 5%, 50-year gold bonds. The letter agreeing to sell his share was signed on February 26, 1901. On March 2, the circular formally filing the organization and capitalization (at $1.4 billion – 4 percent of the U.S. gross domestic product (GDP) at the time) of the United States Steel Corporation actually completed the contract. The bonds were to be delivered within two weeks to the Hudson Trust Company of Hoboken, New Jersey, in trust to Robert A. Franks, Carnegie's business secretary. There, a special vault was built to house the physical bulk of nearly $230 million worth of bonds.
Carnegie continued his business career; some of his literary intentions were fulfilled. He befriended the English poet Matthew Arnold, the English philosopher Herbert Spencer, and the American humorist Mark Twain, as well as being in correspondence and acquaintance with most of the U.S. Presidents, statesmen, and notable writers.
Carnegie constructed commodious swimming-baths for the people of his hometown in Dunfermline in 1879. In the following year, Carnegie gave £8,000 for the establishment of a Dunfermline Carnegie Library in Scotland. In 1884, he gave $50,000 to Bellevue Hospital Medical College (now part of New York University Medical Center) to found a histological laboratory, now called the Carnegie Laboratory.
In 1881, Carnegie took his family, including his 70-year-old mother, on a trip to the United Kingdom. They toured Scotland by coach, and enjoyed several receptions en route. The highlight was a return to Dunfermline, where Carnegie's mother laid the foundation stone of a Carnegie library which he funded. Carnegie's criticism of British society did not mean dislike; on the contrary, one of Carnegie's ambitions was to act as a catalyst for a close association between English-speaking peoples. To this end, in the early 1880s in partnership with Samuel Storey, he purchased numerous newspapers in England, all of which were to advocate the abolition of the monarchy and the establishment of "the British Republic". Carnegie's charm, aided by his wealth, afforded him many British friends, including Prime Minister William Ewart Gladstone.
In 1886, Carnegie's younger brother Thomas died at age 43. While owning steel works, Carnegie had purchased at low cost the most valuable of the iron ore fields around Lake Superior. The same year Carnegie became a figure of controversy. Following his tour of the UK, he wrote about his experiences in a book entitled "An American Four-in-hand in Britain".
Although actively involved in running his many businesses, Carnegie had become a regular contributor to numerous magazines, most notably "The Nineteenth Century", under the editorship of James Knowles, and the influential "North American Review", led by editor Lloyd Bryce.
In 1886, Carnegie wrote his most radical work to date, entitled "Triumphant Democracy". Liberal in its use of statistics to make its arguments, the book argued his view that the American republican system of government was superior to the British monarchical system. It gave a highly favorable and idealized view of American progress and criticized the British royal family. The cover depicted an upended royal crown and a broken scepter. The book created considerable controversy in the UK. The book made many Americans appreciate their country's economic progress and sold over 40,000 copies, mostly in the US.
In 1889, Carnegie published "Wealth" in the June issue of the "North American Review". After reading it, Gladstone requested its publication in England, where it appeared as "The Gospel of Wealth" in the "Pall Mall Gazette". Carnegie argued that the life of a wealthy industrialist should comprise two parts. The first part was the gathering and the accumulation of wealth. The second part was for the subsequent distribution of this wealth to benevolent causes. Philanthropy was key to making life worthwhile.
Carnegie was a well-regarded writer. He published three books on travel.
In the aftermath of the Spanish–American War, the United States seem poised to annex Cuba, Guam, Puerto Rico and the Philippines. Carnegie strongly opposed the idea of American colonies. He opposed the annexation of the Philippines almost to the point of supporting William Jennings Bryan against McKinley in 1900. In 1898, Carnegie tried to arrange independence for the Philippines. As the conclusion of the Spanish–American War neared, the United States purchased the Philippines from Spain for $20 million. To counter what he perceived as American imperialism, Carnegie personally offered $20 million to the Philippines so that the Filipino people could purchase their independence from the United States. However, nothing came of the offer. In 1898 Carnegie joined the American Anti-Imperialist League, in opposition to the U.S. annexation of the Philippines. Its membership included former presidents of the United States Grover Cleveland and Benjamin Harrison and literary figures such as Mark Twain.
Carnegie spent his last years as a philanthropist. From 1901 forward, public attention was turned from the shrewd business acumen which had enabled Carnegie to accumulate such a fortune, to the public-spirited way in which he devoted himself to utilizing it on philanthropic projects. He had written about his views on social subjects and the responsibilities of great wealth in "Triumphant Democracy" (1886) and "Gospel of Wealth" (1889). Carnegie bought Skibo Castle in Scotland, and made his home partly there and partly in his New York mansion located at 2 East 91st Street at Fifth Avenue. The building was completed in late 1902, and he lived there until his death in 1919. His wife Louise continued to live there until her death in 1946.
The building is now used as the Cooper-Hewitt, Smithsonian Design Museum, part of the Smithsonian Institution. The surrounding neighborhood on Manhattan's Upper East Side has come to be called Carnegie Hill. The mansion was designated as a National Historic Landmark in 1966.
Carnegie devoted the rest of his life to providing capital for purposes of public interest and social and educational advancement. He saved letters of appreciation from those he helped in a desk drawer labeled "Gratitude and Sweet Words."
He was a powerful supporter of the movement for spelling reform, as a means of promoting the spread of the English language. His organization, the Simplified Spelling Board, created the "Handbook of Simplified Spelling", which was written wholly in reformed spelling.
Among his many philanthropic efforts, the establishment of public libraries throughout the United States, Britain, Canada and other English-speaking countries was especially prominent. In this special driving interest of his, Carnegie was inspired by meetings with philanthropist Enoch Pratt (1808–1896). The Enoch Pratt Free Library (1886) of Baltimore, Maryland, impressed Carnegie deeply; he said, "Pratt was my guide and inspiration."
Carnegie turned over management of the library project by 1908 to his staff, led by James Bertram (1874–1934). The first Carnegie library opened in 1883 in Dunfermline. His method was to provide funds to build and equip the library, but only on condition that the local authority matched that by providing the land and a budget for operation and maintenance.
To secure local interest, in 1885, he gave $500,000 to Pittsburgh, Pennsylvania for a public library, and in 1886, he gave $250,000 to Allegheny City, Pennsylvania for a music hall and library; and $250,000 to Edinburgh for a free library. In total, Carnegie funded some 3,000 libraries, located in 47 US states, and also in Canada, Britain, Ireland, Australia, New Zealand, South Africa, the West Indies, and Fiji. He also donated £50,000 to help set up the University of Birmingham in 1899.
As Van Slyck (1991) showed, during the last years of the 19th century, there was increasing adoption of the idea that free libraries should be available to the American public. But the design of such libraries was the subject of prolonged and heated debate. On one hand, the library profession called for designs that supported efficiency in administration and operation; on the other, wealthy philanthropists favored buildings that reinforced the paternalistic metaphor and enhanced civic pride. Between 1886 and 1917, Carnegie reformed both library philanthropy and library design, encouraging a closer correspondence between the two.
In 1900, Carnegie gave $2 million to start the Carnegie Institute of Technology (CIT) at Pittsburgh and the same amount in 1902 to found the Carnegie Institution at Washington, D.C. He later contributed more to these and other schools. CIT is now known as Carnegie Mellon University after it merged with the Mellon Institute of Industrial Research. Carnegie also served on the Boards of Cornell University and Stevens Institute of Technology.
In 1911, Carnegie became a sympathetic benefactor to George Ellery Hale, who was trying to build the Hooker Telescope at Mount Wilson, and donated an additional ten million dollars to the Carnegie Institution with the following suggestion to expedite the construction of the telescope: "I hope the work at Mount Wilson will be vigorously pushed, because I am so anxious to hear the expected results from it. I should like to be satisfied before I depart, that we are going to repay to the old land some part of the debt we owe them by revealing more clearly than ever to them the new heavens." The telescope saw first light on November 2, 1917, with Carnegie still alive.
In 1901, in Scotland, he gave $10 million to establish the Carnegie Trust for the Universities of Scotland. It was created by a deed which he signed on June 7, 1901, and it was incorporated by Royal Charter on August 21, 1902. The establishing gift of $10 million was then an unprecedented sum: at the time, total government assistance to all four Scottish universities was about £50,000 a year. The aim of the Trust was to improve and extend the opportunities for scientific research in the Scottish universities and to enable the deserving and qualified youth of Scotland to attend a university. He was subsequently elected Lord Rector of University of St. Andrews in December 1901, and formally installed as such in October 1902, serving until 1907. He also donated large sums of money to Dunfermline, the place of his birth. In addition to a library, Carnegie also bought the private estate which became Pittencrieff Park and opened it to all members of the public, establishing the Carnegie Dunfermline Trust to benefit the people of Dunfermline. A statue of him stands there today.
He gave a further $10 million in 1913 to endow the Carnegie United Kingdom Trust, a grant-making foundation. He transferred to the trust the charge of all his existing and future benefactions, other than university benefactions in the United Kingdom. He gave the trustees a wide discretion, and they inaugurated a policy of financing rural library schemes rather than erecting library buildings, and of assisting the musical education of the people rather than granting organs to churches.
In 1901, Carnegie also established large pension funds for his former employees at Homestead and, in 1905, for American college professors. The latter fund evolved into TIAA-CREF. One critical requirement was that church-related schools had to sever their religious connections to get his money.
His interest in music led him to fund construction of 7,000 church organs. He built and owned Carnegie Hall in New York City.
Carnegie was a large benefactor of the Tuskegee Institute for African-American education under Booker T. Washington. He helped Washington create the National Negro Business League.
In 1904, he founded the Carnegie Hero Fund for the United States and Canada (a few years later also established in the United Kingdom, Switzerland, Norway, Sweden, France, Italy, the Netherlands, Belgium, Denmark, and Germany) for the recognition of deeds of heroism. Carnegie contributed $1,500,000 in 1903 for the erection of the Peace Palace at The Hague; and he donated $150,000 for a Pan-American Palace in Washington as a home for the International Bureau of American Republics.
Carnegie was honored for his philanthropy and support of the arts by initiation as an honorary member of Phi Mu Alpha Sinfonia fraternity on October 14, 1917, at the New England Conservatory of Music in Boston, Massachusetts. The fraternity's mission reflects Carnegie's values by developing young men to share their talents to create harmony in the world.
By the standards of 19th century tycoons, Carnegie was not a particularly ruthless man but a humanitarian with enough acquisitiveness to go in the ruthless pursuit of money. "Maybe with the giving away of his money," commented biographer Joseph Wall, "he would justify what he had done to get that money."
To some, Carnegie represents the idea of the American dream. He was an immigrant from Scotland who came to America and became successful. He is not only known for his successes but his enormous amounts of philanthropist works, not only to charities but also to promote democracy and independence to colonized countries.
Carnegie died on August 11, 1919, in Lenox, Massachusetts, at his Shadow Brook estate, of bronchial pneumonia. He had already given away $350,695,653 (approximately $76.9 billion, adjusted to 2015 share of GDP figures) of his wealth. After his death, his last $30,000,000 was given to foundations, charities, and to pensioners. He was buried at Sleepy Hollow Cemetery in Sleepy Hollow, New York. The grave site is located on the Arcadia Hebron plot of land at the corner of Summit Avenue and Dingle Road. Carnegie is buried only a few yards away from union organizer Samuel Gompers, another important figure of industry in the Gilded Age.
Carnegie was one of more than 50 members of the South Fork Fishing and Hunting Club, which has been blamed for the Johnstown Flood that killed 2,209 people in 1889.
At the suggestion of his friend Benjamin Ruff, Carnegie's partner Henry Clay Frick had formed the exclusive South Fork Fishing and Hunting Club high above Johnstown, Pennsylvania. The sixty-odd club members were the leading business tycoons of Western Pennsylvania and included among their number Frick's best friend, Andrew Mellon, his attorneys Philander Knox and James Hay Reed, as well as Frick's business partner, Carnegie. High above the city, near the small town of South Fork, the South Fork Dam was originally built between 1838 and 1853 by the Commonwealth of Pennsylvania as part of a canal system to be used as a reservoir for a canal basin in Johnstown. With the coming-of-age of railroads superseding canal barge transport, the lake was abandoned by the Commonwealth, sold to the Pennsylvania Railroad, and sold again to private interests and eventually came to be owned by the South Fork Fishing and Hunting Club in 1881. Prior to the flood, speculators had purchased the abandoned reservoir, made less than well-engineered repairs to the old dam, raised the lake level, built cottages and a clubhouse, and created the South Fork Fishing and Hunting Club. Less than downstream from the dam sat the city of Johnstown.
The dam was high and long. Between 1881 when the club was opened, and 1889, the dam frequently sprang leaks and was patched, mostly with mud and straw. Additionally, a previous owner removed and sold for scrap the 3 cast iron discharge pipes that previously allowed a controlled release of water. There had been some speculation as to the dam's integrity, and concerns had been raised by the head of the Cambria Iron Works downstream in Johnstown. Such repair work, a reduction in height, and unusually high snowmelt and heavy spring rains combined to cause the dam to give way on May 31, 1889, resulting in twenty million tons of water sweeping down the valley as the Johnstown Flood. When word of the dam's failure was telegraphed to Pittsburgh, Frick and other members of the South Fork Fishing and Hunting Club gathered to form the Pittsburgh Relief Committee for assistance to the flood victims as well as determining never to speak publicly about the club or the flood. This strategy was a success, and Knox and Reed were able to fend off all lawsuits that would have placed blame upon the club's members.
Although Cambria Iron and Steel's facilities were heavily damaged by the flood, they returned to full production within a year. After the flood, Carnegie built Johnstown a new library to replace the one built by Cambria's chief legal counsel Cyrus Elder, which was destroyed in the flood. The Carnegie-donated library is now owned by the Johnstown Area Heritage Association, and houses the Flood Museum.
The Homestead Strike was a bloody labor confrontation lasting 143 days in 1892, one of the most serious in U.S. history. The conflict was centered on Carnegie Steel's main plant in Homestead, Pennsylvania, and grew out of a labor dispute between the Amalgamated Association of Iron and Steel Workers (AA) and the Carnegie Steel Company.
Carnegie left on a trip to Scotland before the unrest peaked. In doing so, Carnegie left mediation of the dispute in the hands of his associate and partner Henry Clay Frick. Frick was well known in industrial circles for maintaining staunch anti-union sentiment. With the collective bargaining agreement between the union and company expiring at the end of June, Frick and the leaders of the local AA union entered into negotiations in February. With the steel industry doing well and prices higher, the AA asked for a wage increase; the AA represented about 800 of the 3,800 workers at the plant. Frick immediately countered with an average 22% wage decrease that would affect nearly half the union's membership and remove a number of positions from the bargaining unit.
The union and company failed to come to an agreement, and management locked the union out. Workers considered the stoppage a "lockout" by management and not a "strike" by workers. As such, the workers would have been well within their rights to protest, and subsequent government action would have been a set of criminal procedures designed to crush what was seen as a pivotal demonstration of the growing labor rights movement, strongly opposed by management. Frick brought in thousands of strikebreakers to work the steel mills and Pinkerton agents to safeguard them.
On July 6, the arrival of a force of 300 Pinkerton agents from New York City and Chicago resulted in a fight in which 10 men — seven strikers and three Pinkertons — were killed and hundreds were injured. Pennsylvania Governor Robert Pattison ordered two brigades of state militia to the strike site. Then allegedly in response to the fight between the striking workers and the Pinkertons, anarchist Alexander Berkman shot at Frick in an attempted assassination, wounding him. While not directly connected to the strike, Berkman was tied in for the assassination attempt. According to Berkman, "...with the elimination of Frick, responsibility for Homestead conditions would rest with Carnegie." Afterwards, the company successfully resumed operations with non-union immigrant employees in place of the Homestead plant workers, and Carnegie returned to the United States. However, Carnegie's reputation was permanently damaged by the Homestead events.
Carnegie gave "formal allegiance" to the Republican Party, though he was said to be "a violent opponent of some of the most sacred doctrines" of the party.
In his final days, Carnegie suffered from pneumonia. Before his death on August 11, 1919, Carnegie had donated $350,695,654 for various causes. The "Andrew Carnegie Dictum" was:
Carnegie was involved in philanthropic causes, but he kept himself away from religious circles. He wanted to be identified by the world as a "positivist". He was highly influenced in public life by John Bright.
As early as 1868, at age 33, he drafted a memo to himself. He wrote: "...The amassing of wealth is one of the worse species of idolatry. No idol more debasing than the worship of money." In order to avoid degrading himself, he wrote in the same memo he would retire at age 35 to pursue the practice of philanthropic giving for "... the man who dies thus rich dies disgraced." However, he did not begin his philanthropic work in all earnest until 1881, with the gift of a library to his hometown of Dunfermline, Scotland.
Carnegie wrote "The Gospel of Wealth", an article in which he stated his belief that the rich should use their wealth to help enrich society. In that article, Carnegie also expressed sympathy for the ideas of progressive taxation and an estate tax:
The following is taken from one of Carnegie's memos to himself:
Carnegie claimed to be a champion of evolutionary thought – particularly the work of Herbert Spencer, even declaring Spencer his teacher. Although Carnegie claims to be a disciple of Spencer many of his actions went against the ideas espoused by Spencer.
Spencerian evolution was for individual rights and against government interference. Furthermore, Spencerian evolution held that those unfit to sustain themselves must be allowed to perish. Spencer believed that just as there were many varieties of beetles, respectively modified to existence in a particular place in nature, so too had human society "spontaneously fallen into division of labour". Individuals who survived to this, the latest and highest stage of evolutionary progress would be "those in whom the power of self-preservation is the greatest—are the select of their generation." Moreover, Spencer perceived governmental authority as borrowed from the people to perform the transitory aims of establishing social cohesion, insurance of rights, and security. Spencerian 'survival of the fittest' firmly credits any provisions made to assist the weak, unskilled, poor and distressed to be an imprudent disservice to evolution. Spencer insisted people should resist for the benefit of collective humanity, as severe fate singles out the weak, debauched, and disabled.
Andrew Carnegie's political and economic focus during the late nineteenth and early twentieth century was the defense of laissez-faire economics. Carnegie emphatically resisted government intrusion in commerce, as well as government-sponsored charities. Carnegie believed the concentration of capital was essential for societal progress and should be encouraged. Carnegie was an ardent supporter of commercial "survival of the fittest" and sought to attain immunity from business challenges by dominating all phases of the steel manufacturing procedure. Carnegie's determination to lower costs included cutting labor expenses as well. In a notably Spencerian manner, Carnegie argued that unions impeded the natural reduction of prices by pushing up costs, which blocked evolutionary progress. Carnegie felt that unions represented the narrow interest of the few while his actions benefited the entire community.
On the surface, Andrew Carnegie appears to be a strict laissez-faire capitalist and follower of Herbert Spencer, often referring to himself as a disciple of Spencer. Conversely, Carnegie, a titan of industry, seems to embody all of the qualities of Spencerian survival of the fittest. The two men enjoyed a mutual respect for one another and maintained correspondence until Spencer's death in 1903. There are however, some major discrepancies between Spencer's capitalist evolutionary conceptions and Andrew Carnegie's capitalist practices.
Spencer wrote that in production the advantages of the superior individual are comparatively minor, and thus acceptable, yet the benefit that dominance provides those who control a large segment of production might be hazardous to competition. Spencer feared that an absence of "sympathetic self-restraint" of those with too much power could lead to the ruin of their competitors. He did not think free market competition necessitated competitive warfare. Furthermore, Spencer argued that individuals with superior resources who deliberately used investment schemes to put competitors out of business were committing acts of "commercial murder". Carnegie built his wealth in the steel industry by maintaining an extensively integrated operating system. Carnegie also bought out some regional competitors, and merged with others, usually maintaining the majority shares in the companies. Over the course of twenty years, Carnegie's steel properties grew to include the Edgar Thomson Steel Works, the Lucy Furnace Works, the Union Iron Mills, the Homestead Works, the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines among many other industry related assets. Furthermore, Carnegie's success was due to his convenient relationship with the railroad industries, which not only relied on steel for track, but were also making money from steel transport. The steel and railroad barons worked closely to negotiate prices instead of free market competition determinations.
Besides Carnegie's market manipulation, United States trade tariffs were also working in favor of the steel industry. Carnegie spent energy and resources lobbying congress for a continuation of favorable tariffs from which he earned millions of dollars a year. Carnegie tried to keep this information concealed, but legal documents released in 1900, during proceedings with the ex-chairman of Carnegie Steel, Henry Clay Frick, revealed how favorable the tariffs had been. Herbert Spencer absolutely was against government interference in business in the form of regulatory limitation, taxes, and tariffs as well. Spencer saw tariffs as a form of taxation that levied against the majority in service to "the benefit of a small minority of manufacturers and artisans".
Despite Carnegie's personal dedication to Herbert Spencer as a friend, his adherence to Spencer's political and economic ideas is more contentious. In particular, it appears Carnegie either misunderstood or intentionally misrepresented some of Spencer's principal arguments. Spencer remarked upon his first visit to Carnegie's steel mills in Pittsburgh, which Carnegie saw as the manifestation of Spencer's philosophy, "Six months' residence here would justify suicide."
On the subject of charity Andrew Carnegie's actions diverged in the most significant and complex manner from Herbert Spencer's philosophies. In his 1854 essay "Manners and Fashion", Spencer referred to public education as "Old schemes". He went on to declare that public schools and colleges fill the heads of students with inept, useless knowledge and exclude useful knowledge. Spencer stated that he trusted no organization of any kind, "political, religious, literary, philanthropic", and believed that as they expanded in influence so too did their regulations expand. In addition, Spencer thought that as all institutions grow they become evermore corrupted by the influence of power and money. The institution eventually loses its "original spirit, and sinks into a lifeless mechanism". Spencer insisted that all forms of philanthropy that uplift the poor and downtrodden were reckless and incompetent. Spencer thought any attempt to prevent "the really salutary sufferings" of the less fortunate "bequeath to posterity a continually increasing curse". Carnegie, a self-proclaimed devotee of Spencer, testified to Congress on February 5, 1915: "My business is to do as much good in the world as I can; I have retired from all other business."
Carnegie held that societal progress relied on individuals who maintained moral obligations to themselves and to society. Furthermore, he believed that charity supplied the means for those who wish to improve themselves to achieve their goals. Carnegie urged other wealthy people to contribute to society in the form of parks, works of art, libraries and other endeavors that improve the community and contribute to the "lasting good". Carnegie also held a strong opinion against inherited wealth. Carnegie believed that the sons of prosperous businesspersons were rarely as talented as their fathers. By leaving large sums of money to their children, wealthy business leaders were wasting resources that could be used to benefit society. Most notably, Carnegie believed that the future leaders of society would rise from the ranks of the poor. Carnegie strongly believed in this because he had risen from the bottom. He believed the poor possessed an advantage over the wealthy because they receive greater attention from their parents and are taught better work ethics.
Carnegie and his family belonged to the Presbyterian Church in the United States of America, also known informally as the Northern Presbyterian Church. In his early life Carnegie was skeptical of Calvinism, and religion as a whole, but reconciled with it later in his life. In his autobiography, Carnegie describes his family as moderate Presbyterian believers, writing that "there was not one orthodox Presbyterian" in his family; various members of his family having somewhat distanced themselves from Calvinism, some of them leaning more towards Swedenborgianism. Although, being a child, his family led vigorous theological and political disputes. His mother avoided the topic of religion. His father left the Presbyterian church after a sermon on infant damnation, while, according to Carnegie, still remaining very religious on his own.
Witnessing sectarianism and strife in 19th century Scotland regarding religion and philosophy, Carnegie kept his distance from organized religion and theism. Carnegie instead preferred to see things through naturalistic and scientific terms stating, "Not only had I got rid of the theology and the supernatural, but I had found the truth of evolution."
Later in life, Carnegie's firm opposition to religion softened. For many years he was a member of Madison Avenue Presbyterian Church, pastored from 1905 to 1926 by Social Gospel exponent Henry Sloane Coffin, while his wife and daughter belonged to the Brick Presbyterian Church. He also prepared (but did not deliver) an address in which he professed a belief in "an Infinite and Eternal Energy from which all things proceed". Records exist of a short period of correspondence around 1912–1913 between Carnegie and 'Abdu'l-Bahá, the eldest son of Bahá'u'lláh, founder of the Bahá'í Faith. In these letters, one of which was published in the "New York Times" in full text, Carnegie is extolled as a "lover of the world of humanity and one of the founders of Universal Peace".
Influenced by his "favorite living hero in public life" John Bright, Carnegie started his efforts in pursuit of world peace at a young age, and supported causes that opposed military intervention. His motto, "All is well since all grows better", served not only as a good rationalization of his successful business career, but also his view of international relations.
Despite his efforts towards international peace, Carnegie faced many dilemmas on his quest. These dilemmas are often regarded as conflicts between his view on international relations and his other loyalties. Throughout the 1880s and 1890s, for example, Carnegie allowed his steel works to fill large orders of armor plate for the building of an enlarged and modernized United States Navy, but he opposed American oversea expansion.
Despite that, Carnegie served as a major donor for the newly-established International Court of Arbitration's Peace Palace – brainchild of Russian Tsar Nicolas II.
His largest and in the long run most influential peace organization was the Carnegie Endowment for International Peace, formed in 1910 with a $10 million endowment. In 1913, at the dedication of the Peace Palace in The Hague, Carnegie predicted that the end of war was "as certain to come, and come soon, as day follows night."
In 1914, on the eve of the First World War, Carnegie founded the Church Peace Union (CPU), a group of leaders in religion, academia, and politics. Through the CPU, Carnegie hoped to mobilize the world's churches, religious organizations, and other spiritual and moral resources to join in promoting moral leadership to put an end to war forever. For its inaugural international event, the CPU sponsored a conference to be held on August 1, 1914, on the shores of Lake Constance in southern Germany. As the delegates made their way to the conference by train, Germany was invading Belgium.
Despite its inauspicious beginning, the CPU thrived. Today its focus is on ethics and it is known as the Carnegie Council for Ethics in International Affairs, an independent, nonpartisan, nonprofit organization, whose mission is to be the voice for ethics in international affairs.
The outbreak of the First World War was clearly a shock to Carnegie and his optimistic view on world peace. Although his promotion of anti-imperialism and world peace had all failed, and the Carnegie Endowment had not fulfilled his expectations, his beliefs and ideas on international relations had helped build the foundation of the League of Nations after his death, which took world peace to another level.
On the matter of American colonial expansion, Carnegie had always thought it is an unwise gesture for the United States. He did not oppose the annexation of the Hawaiian islands or Puerto Rico, but he opposed the annexation of the Philippines. Carnegie believed that it involved a denial of the fundamental democratic principle, and he also urged William McKinley to withdraw American troops and allow the Filipinos to live with their independence. This act strongly impressed the other American anti-imperialists, who soon elected him vice-president of the Anti-Imperialist League.
After he sold his steel company in 1901, Carnegie was able to get fully involved in the peace cause, both financially and personally. He gave away much of his fortunes to various peace-keeping agencies in order to keep them growing. When his friend, the British writer William T. Stead, asked him to create a new organization for the goal of a peace and arbitration society, his reply was:
Carnegie believed that it is the effort and will of the people, that maintains the peace in international relations. Money is just a push for the act. If world peace depended solely on financial support, it would not seem a goal, but more like an act of pity.
Like Stead, he believed that the United States and the British Empire would merge into one nation, telling him "We are heading straight to the Re-United States". Carnegie believed that the combined country's power would maintain world peace and disarmament. The creation of the Carnegie Endowment for International Peace in 1910 was regarded as a milestone on the road to the ultimate goal of abolition of war. Beyond a gift of $10 million for peace promotion, Carnegie also encouraged the "scientific" investigation of the various causes of war, and the adoption of judicial methods that should eventually eliminate them. He believed that the Endowment exists to promote information on the nations' rights and responsibilities under existing international law and to encourage other conferences to codify this law.
Carnegie was a frequent contributor to periodicals on labor issues. In addition to "Triumphant Democracy" (1886) and "The Gospel of Wealth" (1889), he also wrote "Our Coaching Trip, Brighton to Inverness" (1882), "An American Four-in-hand in Britain" (1883), "Round the World" (1884), "The Empire of Business" (1902), "The Secret of Business is the Management of Men" (1903), "James Watt" (1905) in the Famous Scots Series, "Problems of Today" (1907), and his posthumously published "Autobiography of Andrew Carnegie" (1920).
Carnegie received the honorary Doctor of Laws (DLL) from the University of Glasgow in June 1901, and received the Freedom of the City of Glasgow "in recognition of his munificence" later the same year. In July 1902 he received the Freedom of the city of St Andrews, ""in testimony of his great zeal for the welfare of his fellow-men on both sides of the Atlantic"," and in October 1902 the Freedom of the City of Perth "in testimony of his high personal worth and beneficial influence, and in recognition of widespread benefactions bestowed on this and other lands, and especially in gratitude for the endowment granted by him for the promotion of University education in Scotland" and the Freedom of the City of Dundee. He received an honorary Doctor of Laws (LLD) from the University of Aberdeen in 1906. In 1910, he received the Freedom of the City of Belfast. Carnegie received 1 July 1914 an honorary doctorate from the University of Groningen the Netherlands.
According to biographer Burton J. Hendrick:
Hendrick argues that:
Carnegie's personal papers are at the Library of Congress Manuscript Division.
The Carnegie Collections of the Columbia University Rare Book and Manuscript Library consist of the archives of the following organizations founded by Carnegie: The Carnegie Corporation of New York (CCNY); The Carnegie Endowment for International Peace (CEIP); the Carnegie Foundation for the Advancement of Teaching (CFAT);The Carnegie Council on Ethics and International Affairs (CCEIA). These collections deal primarily with Carnegie philanthropy and have very little personal material related to Carnegie. Carnegie Mellon University and the Carnegie Library of Pittsburgh jointly administer the Andrew Carnegie Collection of digitized archives on Carnegie's life.
Collections
 
 

</doc>
<doc id="1939" url="https://en.wikipedia.org/wiki?curid=1939" title="Approximant consonant">
Approximant consonant

Approximants are speech sounds that involve the articulators approaching each other but not narrowly enough nor with enough articulatory precision to create turbulent airflow. Therefore, approximants fall between fricatives, which do produce a turbulent airstream, and vowels, which produce no turbulence. This class is composed of sounds like (as in "rest") and semivowels like and (as in "yes" and "west", respectively), as well as lateral approximants like (as in "less").
Before Peter Ladefoged coined the term "approximant" in the 1960s, the term "frictionless continuant" referred to non-lateral approximants.
In phonology, "approximant" is also a distinctive feature that encompasses all sonorants except nasals, including vowels, taps and trills.
Some approximants resemble vowels in acoustic and articulatory properties and the terms "semivowel" and "glide" are often used for these non-syllabic vowel-like segments. The correlation between semivowels and vowels is strong enough that cross-language differences between semivowels correspond with the differences between their related vowels.
Vowels and their corresponding semivowels alternate in many languages depending on the phonological environment, or for grammatical reasons, as is the case with Indo-European ablaut. Similarly, languages often avoid configurations where a semivowel precedes its corresponding vowel. A number of phoneticians distinguish between semivowels and approximants by their location in a syllable. Although he uses the terms interchangeably, remarks that, for example, the final glides of English "par" and "buy" differ from French "par" ('through') and "baille" ('tub') in that, in the latter pair, the approximants appear in the syllable coda, whereas, in the former, they appear in the syllable nucleus. This means that opaque (if not minimal) contrasts can occur in languages like Italian (with the i-like sound of "piede" 'foot', appearing in the nucleus: , and that of "piano" 'slow', appearing in the syllable onset: ) and Spanish (with a near minimal pair being "abyecto" 'abject' and "abierto" 'opened').
In articulation and often diachronically, palatal approximants correspond to front vowels, velar approximants to back vowels, and labialized approximants to rounded vowels. In American English, the rhotic approximant corresponds to the rhotic vowel. This can create alternations (as shown in the above table).
In addition to alternations, glides can be inserted to the left or the right of their corresponding vowels when they occur next to a hiatus. For example, in Ukrainian, medial triggers the formation of an inserted that acts as a syllable onset so that when the affix is added to футбол ('football') to make футболіст 'football player', it is pronounced , but маоїст ('Maoist'), with the same affix, is pronounced with a glide. Dutch for many speakers has a similar process that extends to mid vowels:
Similarly, vowels can be inserted next to their corresponding glide in certain phonetic environments. Sievers' law describes this behaviour for Germanic.
Non-high semivowels also occur. In colloquial Nepali speech, a process of glide-formation occurs, where one of two adjacent vowels becomes non-syllabic; the process includes mid vowels so that ('cause to wish') features a non-syllabic mid vowel. Spanish features a similar process and even nonsyllabic can occur so that "ahorita" ('right away') is pronounced . It is not often clear, however, whether such sequences involve a semivowel (a consonant) or a diphthong (a vowel), and in many cases, it may not be a meaningful distinction.
Although many languages have central vowels , which lie between back/velar and front/palatal , there are few cases of a corresponding approximant . One is in the Korean diphthong or though it is more frequently analyzed as velar (as in the table above), and Mapudungun may be another, with three high vowel sounds, , , and three corresponding consonants, , and , and a third one is often described as a voiced unrounded velar fricative; some texts note a correspondence between this approximant and that is parallel to – and –. An example is "liq" (?) ('white').
In addition to less turbulence, approximants also differ from fricatives in the precision required to produce them. When emphasized, approximants may be slightly fricated (that is, the airstream may become slightly turbulent), which is reminiscent of fricatives. For example, the Spanish word "ayuda" ('help') features a palatal approximant that is pronounced as a fricative in emphatic speech. Spanish can be analyzed as having a meaningful distinction between fricative, approximant, and intermediate . However, such frication is generally slight and intermittent, unlike the strong turbulence of fricative consonants.
Because voicelessness has comparatively reduced resistance to air flow from the lungs, the increased air flow creates more turbulence, making acoustic distinctions between voiceless approximants (which are extremely rare cross-linguistically) and voiceless fricatives difficult. This is why, for example, no language is known to contrast the voiceless labialized velar approximant (also transcribed with the special letter ) with a voiceless labialized velar fricative . Similarly, Standard Tibetan has a voiceless lateral approximant, , and Welsh has a voiceless lateral fricative , but the distinction is not always clear from descriptions of these languages. Again, no language is known to contrast the two. Iaai is reported to have an unusually large number of voiceless approximants, with .
For places of articulation further back in the mouth, languages do not contrast voiced fricatives and approximants. Therefore, the IPA allows the symbols for the voiced fricatives to double for the approximants, with or without a lowering diacritic. 
Occasionally, the glottal "fricatives" are called approximants, since typically has no more frication than voiceless approximants, but they are often phonations of the glottis without any accompanying manner or place of articulation.
In lateral approximants, the center of tongue makes solid contact with the roof of the mouth. However, the defining location is the side of the tongue, which only approaches the teeth.
Voiceless approximants are rarely distinguished from voiceless fricatives. Iaai has an unusually large number of them, with contrasting with (as well as a large number of voiceless nasals). Attested voiceless approximants are:
Examples are:
In Portuguese, the nasal glides and historically became and in some words. In Edo, the nasalized allophones of the approximants and are nasal occlusives, and .
What are transcribed as nasal approximants may include non-syllabic elements of nasal vowels or diphthongs.

</doc>
<doc id="1940" url="https://en.wikipedia.org/wiki?curid=1940" title="Astronomer Royal">
Astronomer Royal

Astronomer Royal is a senior post in the Royal Households of the United Kingdom. There are two officers, the senior being the Astronomer Royal dating from 22 June 1675; the second is the Astronomer Royal for Scotland dating from 1834.
The post was created by King Charles II in 1675, at the same time as he founded the Royal Observatory Greenwich. He appointed John Flamsteed, instructing him "."
The Astronomer Royal was director of the Royal Observatory Greenwich from the establishment of the post in 1675 until 1972. The Astronomer Royal became an honorary title in 1972 without executive responsibilities and a separate post of Director of the Royal Greenwich Observatory was created to manage the institution.
The Astronomer Royal today receives a stipend of 100 GBP per year and is a member of the Royal Household, under the general authority of the Lord Chamberlain. After the separation of the two offices, the position of Astronomer Royal has been largely honorary, though he remains available to advise the Sovereign on astronomical and related scientific matters, and the office is of great prestige.
There was also formerly a Royal Astronomer of Ireland.
The Astronomer Royal is referenced in H.G. Wells' novel "The War of the Worlds" and in George Orwell's "Down and Out in Paris and London" (page 175 of the Penguin edition).

</doc>
<doc id="1941" url="https://en.wikipedia.org/wiki?curid=1941" title="Aeon">
Aeon

The word aeon , also spelled eon (in American English), originally meant "life", "vital force" or "being", "generation" or "a period of time", though it tended to be translated as "age" in the sense of "ages", "forever", "timeless" or "for eternity". It is a Latin transliteration from the koine Greek word ("ho aion"), from the archaic ("aiwon"). In Homer it typically refers to life or lifespan. Its latest meaning is more or less similar to the Sanskrit word "kalpa" and Hebrew word "olam". A cognate Latin word "aevum" or "aeuum" (cf. ) for "age" is present in words such as "longevity" and "mediaeval".
Although the term aeon may be used in reference to a period of a billion years (especially in geology, cosmology and astronomy), its more common usage is for any long, indefinite period. Aeon can also refer to the four aeons on the geologic time scale that make up the Earth's history, the Hadean, Archean, Proterozoic, and the current aeon, Phanerozoic.
In astronomy an aeon is defined as a billion years (10 years, abbreviated AE).
Roger Penrose uses the word "aeon" to describe the period between successive and cyclic Big Bangs within the context of conformal cyclic cosmology.
Plato used the word "aeon" to denote the eternal world of ideas, which he conceived was "behind" the perceived world, as demonstrated in his famous allegory of the cave.
Christianity's idea of "eternal life" comes from the word for life, "zoe", and a form of "aeon", which could mean life in the next aeon, the Kingdom of God, or Heaven, just as much as immortality, as in .
According to the Christian doctrine of universal reconciliation, the Greek New Testament scriptures use the word "aeon" to mean a long period (perhaps 1000 years) and the word "aeonian" to mean "during a long period"; Thus there was a time before the aeons, and the aeonian period is finite. After each man's mortal life ends, he is judged worthy of aeonian life or aeonian punishment. That is, after the period of the aeons, all punishment will cease and death is overcome and then God becomes the all in each one (). This contrasts with the conventional Christian belief in eternal life and eternal punishment.
Occultists of the Thelema and O.T.O. traditions sometimes speak of a "magical Aeon" that may last for far less time, perhaps as little as 2,000 years.
The Order of Nine Angles, a UK-based Left Hand Path/Satanic organisation propose the concept of Aeons are central to the esoteric philosophy developed by the pseudonymous Anton Long, who wrote that "an aeon is the term used [by the O9A] to describe a stage or a type of evolution. Evolution itself is taken to result from a certain specific process – and this process can be described, or explained [or 're-presented' ] via a bifurcation of time. That is, evolution is an expression of how the cosmos changes over or through or because of,'time' – this 'time' having two components. These two components are the causal and the acausal ...
"An aeon is a manifestation, in the causal, of a particular type of acausal energy. This energy re-orders, or changes, the causal. These changes have certain limits – in both causal space and causal time. That is, they have a specific beginning and a specific end. A civilization (or rather, a higher or aeonic-civilization) is how this energy becomes ordered or manifests itself in the causal: how this energy is revealed. A civilization represents the practical changes which this energy causes in the causal -in terms of the effect such energy has on individuals and this planet. A civilization is tied to, is born from, a particular aeon. By the nature of this energy, a civilization is an evolution of life – a move toward a more complex, and thus more conscious existence ..."
Aeon may also be an archaic name for omnipotent beings, such as gods.
In many Gnostic systems, the various emanations of God, who is also known by such names as the One, the Monad, "Aion teleos" ( "The Broadest Aeon"), Bythos ("depth or profundity", Greek ), "Proarkhe" ("before the beginning", Greek ), the "Arkhe" ("the beginning", Greek ), "Sophia" (wisdom), Christos (the Anointed One) are called "Aeons". In the different systems these emanations are differently named, classified, and described, but the emanation theory itself is common to all forms of Gnosticism.
In the Basilidian Gnosis they are called sonships (υἱότητες "huiotetes"; sing.: "huiotes"); according to Marcus, they are numbers and sounds; in Valentinianism they form male/female pairs called "syzygies" (Greek , from σύζυγοι "syzygoi").
Similarly, in the Greek Magical Papyri, the term "Aion" is often used to denote the All, or the supreme aspect of God.

</doc>
<doc id="1942" url="https://en.wikipedia.org/wiki?curid=1942" title="Airline">
Airline

An airline is a company that provides air transport services for traveling passengers and freight. Airlines utilize aircraft to supply these services and may form partnerships or alliances with other airlines for codeshare agreements, in which they both offer and operate the same flight. Generally, airline companies are recognized with an air operating certificate or license issued by a governmental aviation body. Airlines may be scheduled or charter operators.
The first airline was the German airship company DELAG, founded on 16 November 1909. The four oldest non-airship airlines that still exist are the Netherlands' KLM (1919), Colombia's Avianca (1919), Australia's Qantas (1921) and the Czech Republic's Czech Airlines (1923).
Airline ownership has seen a shift from mostly personal ownership until the 1930s to government-ownership of major airlines from the 1940s to 1980s and back to large-scale privatization following the mid-1980s. Since the 1980s, there has also been a trend of major airline mergers and the formation of airline alliances. The largest alliances are Star Alliance, SkyTeam and Oneworld, and these three collectively accounted for more than 60% of global commercial air traffic in 2015. Airline alliances coordinate their passenger service programs (such as lounges and frequent-flyer programs), offer special interline tickets and often engage in extensive codesharing (sometimes systemwide).
, the largest airline by passengers carried and fleet size was the American Airlines Group, while Delta Air Lines was the largest by revenue. Lufthansa Group was the largest by number of employees, FedEx Express by freight tonne-kilometres, Turkish Airlines by number of countries served and UPS Airlines by number of destinations served (though United Airlines was the largest passenger airline by number of destinations served).
DELAG, "Deutsche Luftschiffahrts-Aktiengesellschaft I" was the world's first airline. It was founded on November 16, 1909, with government assistance, and operated airships manufactured by The Zeppelin Corporation. Its headquarters were in Frankfurt. The first fixed-wing scheduled airline was started on January 1, 1914, from St. Petersburg, Florida, to Tampa, Florida, operated by the St. Petersburg and Tampa Airboat Line. The four oldest non-dirigible airlines that still exist are the Netherlands' KLM (1919), Colombia's Avianca (1919), Australia's Qantas (1921), and the Czech Republic's Czech Airlines (1923).
The earliest fixed wing airline in Europe was Aircraft Transport and Travel, formed by George Holt Thomas in 1916; via a series of takeovers and mergers, this company is an ancestor of modern-day British Airways. Using a fleet of former military Airco DH.4A biplanes that had been modified to carry two passengers in the fuselage, it operated relief flights between Folkestone and Ghent. On 15 July 1919, the company flew a proving flight across the English Channel, despite a lack of support from the British government. Flown by Lt. H Shaw in an Airco DH.9 between RAF Hendon and Paris – Le Bourget Airport, the flight took 2 hours and 30 minutes at £21 per passenger.
On 25 August 1919, the company used DH.16s to pioneer a regular service from Hounslow Heath Aerodrome to Le Bourget, the first regular international service in the world. The airline soon gained a reputation for reliability, despite problems with bad weather, and began to attract European competition. In November 1919, it won the first British civil airmail contract. Six Royal Air Force Airco DH.9A aircraft were lent to the company, to operate the airmail service between Hawkinge and Cologne. In 1920, they were returned to the Royal Air Force.
Other British competitors were quick to follow – Handley Page Transport was established in 1919 and used the company's converted wartime Type O/400 bombers with a capacity for 12 passengers, to run a London-Paris passenger service.
The first French airline was Société des lignes Latécoère, later known as Aéropostale, which started its first service in late 1918 to Spain. The Société Générale des Transports Aériens was created in late 1919, by the Farman brothers and the Farman F.60 Goliath plane flew scheduled services from Toussus-le-Noble to Kenley, near Croydon, England. Another early French airline was the Compagnie des Messageries Aériennes, established in 1919 by Louis-Charles Breguet, offering a mail and freight service between Le Bourget Airport, Paris and Lesquin Airport, Lille.
The first German airline to use heavier than air aircraft was Deutsche Luft-Reederei established in 1917 which started operating in February 1919. In its first year, the D.L.R. operated regularly scheduled flights on routes with a combined length of nearly 1000 miles. By 1921 the D.L.R. network was more than 3000 km (1865 miles) long, and included destinations in the Netherlands, Scandinavia and the Baltic Republics. Another important German airline was Junkers Luftverkehr, which began operations in 1921. It was a division of the aircraft manufacturer Junkers, which became a separate company in 1924. It operated joint-venture airlines in Austria, Denmark, Estonia, Finland, Hungary, Latvia, Norway, Poland, Sweden and Switzerland.
The Dutch airline KLM made its first flight in 1920, and is the oldest continuously operating airline in the world. Established by aviator Albert Plesman, it was immediately awarded a "Royal" predicate from Queen Wilhelmina. Its first flight was from Croydon Airport, London to Amsterdam, using a leased Aircraft Transport and Travel DH-16, and carrying two British journalists and a number of newspapers. In 1921, KLM started scheduled services.
In Finland, the charter establishing Aero O/Y (now Finnair) was signed in the city of Helsinki on September 12, 1923. Junkers F.13 D-335 became the first aircraft of the company, when Aero took delivery of it on March 14, 1924. The first flight was between Helsinki and Tallinn, capital of Estonia, and it took place on March 20, 1924, one week later.
In the Soviet Union, the Chief Administration of the Civil Air Fleet was established in 1921. One of its first acts was to help found Deutsch-Russische Luftverkehrs A.G. (Deruluft), a German-Russian joint venture to provide air transport from Russia to the West. Domestic air service began around the same time, when Dobrolyot started operations on 15 July 1923 between Moscow and Nizhni Novgorod. Since 1932 all operations had been carried under the name Aeroflot.
Early European airlines tended to favor comfort – the passenger cabins were often spacious with luxurious interiors – over speed and efficiency. The relatively basic navigational capabilities of pilots at the time also meant that delays due to the weather were commonplace.
By the early 1920s, small airlines were struggling to compete, and there was a movement towards increased rationalization and consolidation. In 1924, Imperial Airways was formed from the merger of Instone Air Line Company, British Marine Air Navigation, Daimler Airway and Handley Page Transport, to allow British airlines to compete with stiff competition from French and German airlines that were enjoying heavy government subsidies. The airline was a pioneer in surveying and opening up air routes across the world to serve far-flung parts of the British Empire and to enhance trade and integration.
The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Two French airlines also merged to form Air Union on 1 January 1923. This later merged with four other French airlines to become Air France, the country's flagship carrier to this day, on 17 May 1933.
Germany's Deutsche Luft Hansa was created in 1926 by merger of two airlines, one of them Junkers Luftverkehr. Luft Hansa, due to the Junkers heritage and unlike most other airlines at the time, became a major investor in airlines outside of Europe, providing capital to Varig and Avianca. German airliners built by Junkers, Dornier, and Fokker were among the most advanced in the world at the time.
In 1926, Alan Cobham surveyed a flight route from the UK to Cape Town, South Africa, following this up with another proving flight to Melbourne, Australia. Other routes to British India and the Far East were also charted and demonstrated at this time. Regular services to Cairo and Basra began in 1927 and were extended to Karachi in 1929. The London-Australia service was inaugurated in 1932 with the Handley Page HP 42 airliners. Further services were opened up to Calcutta, Rangoon, Singapore, Brisbane and Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
Like Imperial Airways, Air France and KLM's early growth depended heavily on the needs to service links with far-flung colonial possessions (North Africa and Indochina for the French and the East Indies for the Dutch). France began an air mail service to Morocco in 1919 that was bought out in 1927, renamed Aéropostale, and injected with capital to become a major international carrier. In 1933, Aéropostale went bankrupt, was nationalized and merged into Air France.
Although Germany lacked colonies, it also began expanding its services globally. In 1931, the airship Graf Zeppelin began offering regular scheduled passenger service between Germany and South America, usually every two weeks, which continued until 1937. In 1936, the airship Hindenburg entered passenger service and successfully crossed the Atlantic 36 times before crashing at Lakehurst, New Jersey, on May 6, 1937. In 1938, a weekly air service from Berlin to Kabul, Afghanistan, started operating.
From February 1934 until World War II began in 1939 Deutsche Lufthansa operated an airmail service from Stuttgart, Germany via Spain, the Canary Islands and West Africa to Natal in Brazil. This was the first time an airline flew across an ocean.
By the end of the 1930s Aeroflot had become the world's largest airline, employing more than 4,000 pilots and 60,000 other service personnel and operating around 3,000 aircraft (of which 75% were considered obsolete by its own standards). During the Soviet era Aeroflot was synonymous with Russian civil aviation, as it was the only air carrier. It became the first airline in the world to operate sustained regular jet services on 15 September 1956 with the Tupolev Tu-104.
Deregulation of the European Union airspace in the early 1990s has had substantial effect on the structure of the industry there. The shift towards 'budget' airlines on shorter routes has been significant. Airlines such as EasyJet and Ryanair have often grown at the expense of the traditional national airlines.
There has also been a trend for these national airlines themselves to be privatized such as has occurred for Aer Lingus and British Airways. Other national airlines, including Italy's Alitalia, have suffered – particularly with the rapid increase of oil prices in early 2008.
Finnair, the largest airline of Finland, had no fatal or hull-loss accidents since 1963, and is recognized for its safety.
Tony Jannus conducted the United States' first scheduled commercial airline flight on 1 January 1914 for the St. Petersburg-Tampa Airboat Line. The 23-minute flight traveled between St. Petersburg, Florida and Tampa, Florida, passing some above Tampa Bay in Jannus' Benoist XIV wood and muslin biplane flying boat. His passenger was a former mayor of St. Petersburg, who paid $400 for the privilege of sitting on a wooden bench in the open cockpit. The Airboat line operated for about four months, carrying more than 1,200 passengers who paid $5 each. Chalk's International Airlines began service between Miami and Bimini in the Bahamas in February 1919. Based in Ft. Lauderdale, Chalk's claimed to be the oldest continuously operating airline in the United States until its closure in 2008.
Following World War I, the United States found itself swamped with aviators. Many decided to take their war-surplus aircraft on barnstorming campaigns, performing aerobatic maneuvers to woo crowds. In 1918, the United States Postal Service won the financial backing of Congress to begin experimenting with air mail service, initially using Curtiss Jenny aircraft that had been procured by the United States Army Air Service. Private operators were the first to fly the mail but due to numerous accidents the US Army was tasked with mail delivery. During the Army's involvement they proved to be too unreliable and lost their air mail duties. By the mid-1920s, the Postal Service had developed its own air mail network, based on a transcontinental backbone between New York City and San Francisco. To supplement this service, they offered twelve contracts for spur routes to independent bidders. Some of the carriers that won these routes would, through time and mergers, evolve into Pan Am, Delta Air Lines, Braniff Airways, American Airlines, United Airlines (originally a division of Boeing), Trans World Airlines, Northwest Airlines, and Eastern Air Lines.
Service during the early 1920s was sporadic: most airlines at the time were focused on carrying bags of mail. In 1925, however, the Ford Motor Company bought out the Stout Aircraft Company and began construction of the all-metal Ford Trimotor, which became the first successful American airliner. With a 12-passenger capacity, the Trimotor made passenger service potentially profitable. Air service was seen as a supplement to rail service in the American transportation network.
At the same time, Juan Trippe began a crusade to create an air network that would link America to the world, and he achieved this goal through his airline, Pan Am, with a fleet of flying boats that linked Los Angeles to Shanghai and Boston to London. Pan Am and Northwest Airways (which began flights to Canada in the 1920s) were the only U.S. airlines to go international before the 1940s.
With the introduction of the Boeing 247 and Douglas DC-3 in the 1930s, the U.S. airline industry was generally profitable, even during the Great Depression. This trend continued until the beginning of World War II.
World War II, like World War I, brought new life to the airline industry. Many airlines in the Allied countries were flush from lease contracts to the military, and foresaw a future explosive demand for civil air transport, for both passengers and cargo. They were eager to invest in the newly emerging flagships of air travel such as the Boeing Stratocruiser, Lockheed Constellation, and Douglas DC-6. Most of these new aircraft were based on American bombers such as the B-29, which had spearheaded research into new technologies such as pressurization. Most offered increased efficiency from both added speed and greater payload.
In the 1950s, the De Havilland Comet, Boeing 707, Douglas DC-8, and Sud Aviation Caravelle became the first flagships of the Jet Age in the West, while the Eastern bloc had Tupolev Tu-104 and Tupolev Tu-124 in the fleets of state-owned carriers such as Czechoslovak ČSA, Soviet Aeroflot and East-German Interflug. The Vickers Viscount and Lockheed L-188 Electra inaugurated turboprop transport.
On 4 October 1958, British Overseas Airways Corporation started transatlantic flights between London Heathrow and New York Idlewild with a Comet 4, and Pan Am followed on 26 October with a Boeing 707 service between New York and Paris.
The next big boost for the airlines would come in the 1970s, when the Boeing 747, McDonnell Douglas DC-10, and Lockheed L-1011 inaugurated widebody ("jumbo jet") service, which is still the standard in international travel. The Tupolev Tu-144 and its Western counterpart, Concorde, made supersonic travel a reality. Concorde first flew in 1969 and operated through 2003. In 1972, Airbus began producing Europe's most commercially successful line of airliners to date. The added efficiencies for these aircraft were often not in speed, but in passenger capacity, payload, and range. Airbus also features modern electronic cockpits that were common across their aircraft to enable pilots to fly multiple models with minimal cross-training.
The 1978 U.S. airline industry deregulation lowered federally controlled barriers for new airlines just as a downturn in the nation's economy occurred. New start-ups entered during the downturn, during which time they found aircraft and funding, contracted hangar and maintenance services, trained new employees, and recruited laid-off staff from other airlines.
Major airlines dominated their routes through aggressive pricing and additional capacity offerings, often swamping new start-ups. In the place of high barriers to entry imposed by regulation, the major airlines implemented an equally high barrier called loss leader pricing. In this strategy an already established and dominant airline stomps out its competition by lowering airfares on specific routes, below the cost of operating on it, choking out any chance a start-up airline may have. The industry side effect is an overall drop in revenue and service quality. Since deregulation in 1978 the average domestic ticket price has dropped by 40%. So has airline employee pay. By incurring massive losses, the airlines of the USA now rely upon a scourge of cyclical Chapter 11 bankruptcy proceedings to continue doing business. America West Airlines (which has since merged with US Airways) remained a significant survivor from this new entrant era, as dozens, even hundreds, have gone under.
In many ways, the biggest winner in the deregulated environment was the air passenger. Although not exclusively attributable to deregulation, indeed the U.S. witnessed an explosive growth in demand for air travel. Many millions who had never or rarely flown before became regular fliers, even joining frequent flyer loyalty programs and receiving free flights and other benefits from their flying. New services and higher frequencies meant that business fliers could fly to another city, do business, and return the same day, from almost any point in the country. Air travel's advantages put long-distance intercity railroad travel and bus lines under pressure, with most of the latter having withered away, whilst the former is still protected under nationalization through the continuing existence of Amtrak.
By the 1980s, almost half of the total flying in the world took place in the U.S., and today the domestic industry operates over 10,000 daily departures nationwide.
Toward the end of the century, a new style of low cost airline emerged, offering a no-frills product at a lower price. Southwest Airlines, JetBlue, AirTran Airways, Skybus Airlines and other low-cost carriers began to represent a serious challenge to the so-called "legacy airlines", as did their low-cost counterparts in many other countries. Their commercial viability represented a serious competitive threat to the legacy carriers. However, of these, ATA and Skybus have since ceased operations.
Increasingly since 1978, US airlines have been reincorporated and spun off by newly created and internally led management companies, and thus becoming nothing more than operating units and subsidiaries with limited financially decisive control. Among some of these holding companies and parent companies which are relatively well known, are the UAL Corporation, along with the AMR Corporation, among a long list of airline holding companies sometime recognized worldwide. Less recognized are the private equity firms which often seize managerial, financial, and board of directors control of distressed airline companies by temporarily investing large sums of capital in air carriers, to rescheme an airlines assets into a profitable organization or liquidating an air carrier of their profitable and worthwhile routes and business operations.
Thus the last 50 years of the airline industry have varied from reasonably profitable, to devastatingly depressed. As the first major market to deregulate the industry in 1978, U.S. airlines have experienced more turbulence than almost any other country or region. In fact, no U.S. legacy carrier survived bankruptcy-free. Among the outspoken critics of deregulation, former CEO of American Airlines, Robert Crandall has publicly stated: "Chapter 11 bankruptcy protection filing shows airline industry deregulation was a mistake."
Congress passed the Air Transportation Safety and System Stabilization Act (P.L. 107-42) in response to a severe liquidity crisis facing the already-troubled airline industry in the aftermath of the September 11th terrorist attacks. Through the ATSB Congress sought to provide cash infusions to carriers for both the cost of the four-day federal shutdown of the airlines and the incremental losses incurred through December 31, 2001, as a result of the terrorist attacks. This resulted in the first government bailout of the 21st century. Between 2000 and 2005 US airlines lost $30 billion with wage cuts of over $15 billion and 100,000 employees laid off.
In recognition of the essential national economic role of a healthy aviation system, Congress authorized partial compensation of up to $5 billion in cash subject to review by the U.S. Department of Transportation and up to $10 billion in loan guarantees subject to review by a newly created Air Transportation Stabilization Board (ATSB). The applications to DOT for reimbursements were subjected to rigorous multi-year reviews not only by DOT program personnel but also by the Government Accountability Office and the DOT Inspector General.
Ultimately, the federal government provided $4.6 billion in one-time, subject-to-income-tax cash payments to 427 U.S. air carriers, with no provision for repayment, essentially a gift from the taxpayers. (Passenger carriers operating scheduled service received approximately $4 billion, subject to tax.) In addition, the ATSB approved loan guarantees to six airlines totaling approximately $1.6 billion. Data from the U.S. Treasury Department show that the government recouped the $1.6 billion and a profit of $339 million from the fees, interest and purchase of discounted airline stock associated with loan guarantees.
The three largest major carriers and Southwest Airlines control 70% of the U.S. passenger market.
Although Philippine Airlines (PAL) was officially founded on February 26, 1941, its license to operate as an airliner was derived from merged Philippine Aerial Taxi Company (PATCO) established by mining magnate Emmanuel N. Bachrach on December 3, 1930, making it Asia's oldest scheduled carrier still in operation. Commercial air service commenced three weeks later from Manila to Baguio, making it Asia's first airline route. Bachrach's death in 1937 paved the way for its eventual merger with Philippine Airlines in March 1941 and made it Asia's oldest airline. It is also the oldest airline in Asia still operating under its current name. Bachrach's majority share in PATCO was bought by beer magnate Andres R. Soriano in 1939 upon the advice of General Douglas MacArthur and later merged with newly formed Philippine Airlines with PAL as the surviving entity. Soriano has controlling interest in both airlines before the merger. PAL restarted service on March 15, 1941, with a single Beech Model 18 NPC-54 aircraft, which started its daily services between Manila (from Nielson Field) and Baguio, later to expand with larger aircraft such as the DC-3 and Vickers Viscount.
Cathay Pacific was one of the first airlines to be launched among the other Asian countries in 1946 along with Asiana Airlines, which later joined in 1988. The license to operate as an airliner was granted by the federal government body after reviewing the necessity at the national assembly. The Hanjin occupies the largest ownership of Korean Air as well as few low-budget airlines as of now. Korean Air is one of the four founders of SkyTeam, which was established in 2000. Asiana Airlines joined Star Alliance in 2003. Korean Air and Asiana Airlines comprise one of the largest combined airline miles and number of passenger served at the regional market of Asian airline industry
India was also one of the first countries to embrace civil aviation. One of the first Asian airline companies was Air India, which was founded as Tata Airlines in 1932, a division of Tata Sons Ltd. (now Tata Group). The airline was founded by India's leading industrialist, JRD Tata. On October 15, 1932, J. R. D. Tata himself flew a single engined De Havilland Puss Moth carrying air mail (postal mail of Imperial Airways) from Karachi to Bombay via Ahmedabad. The aircraft continued to Madras via Bellary piloted by Royal Air Force pilot Nevill Vintcent. Tata Airlines was also one of the world's first major airlines which began its operations without any support from the Government.
With the outbreak of World War II, the airline presence in Asia came to a relative halt, with many new flag carriers donating their aircraft for military aid and other uses. Following the end of the war in 1945, regular commercial service was restored in India and Tata Airlines became a public limited company on July 29, 1946, under the name Air India. After the independence of India, 49% of the airline was acquired by the Government of India. In return, the airline was granted status to operate international services from India as the designated flag carrier under the name Air India International.
On July 31, 1946, a chartered Philippine Airlines (PAL) DC-4 ferried 40 American servicemen to Oakland, California, from Nielson Airport in Makati City with stops in Guam, Wake Island, Johnston Atoll and Honolulu, Hawaii, making PAL the first Asian airline to cross the Pacific Ocean. A regular service between Manila and San Francisco was started in December. It was during this year that the airline was designated as the flag carrier of Philippines.
During the era of decolonization, newly born Asian countries started to embrace air transport. Among the first Asian carriers during the era were Cathay Pacific of Hong Kong (founded in September 1946), Orient Airways (later Pakistan International Airlines; founded in October 1946), Air Ceylon (later SriLankan Airlines; founded in 1947), Malayan Airways Limited in 1947 (later Singapore and Malaysia Airlines), El Al in Israel in 1948, Garuda Indonesia in 1949, Japan Airlines in 1951, Thai Airways in 1960, and Korean National Airlines in 1947.
Singapore Airlines had won quality awards.
Among the first countries to have regular airlines in Latin America and the Caribbean were Bolivia with Lloyd Aéreo Boliviano, Cuba with Cubana de Aviación, Colombia with Avianca (the first airline established in the Americas), Argentina with Aerolineas Argentinas, Chile with LAN Chile (today LATAM Airlines), Brazil with Varig, Dominican Republic with Dominicana de Aviación, Mexico with Mexicana de Aviación, Trinidad and Tobago with BWIA West Indies Airways (today Caribbean Airlines), Venezuela with Aeropostal, Puerto Rico with Puertorriquena; and TACA based in El Salvador and representing several airlines of Central America (Costa Rica, Guatemala, Honduras and Nicaragua). All the previous airlines started regular operations well before World War II. Puerto Rican commercial airlines such as Prinair, Oceanair, Fina Air and Vieques Air Link came much after the second world war, as did several others from other countries like Mexico's Interjet and Volaris, Venezuela's Aserca Airlines and others.
The air travel market has evolved rapidly over recent years in Latin America. Some industry estimates indicate that over 2,000 new aircraft will begin service over the next five years in this region.
These airlines serve domestic flights within their countries, as well as connections within Latin America and also overseas flights to North America, Europe, Australia, and Asia.
Only two airlines – Avianca and LATAM Airlines – have international subsidiaries and cover many destinations within the Americas as well as major hubs in other continents. LATAM with Chile as the central operation along with Peru, Ecuador, Colombia, Brazil and Argentina and formerly with some operations in the Dominican Republic. The Avianca group has its main operation in Colombia based around the hub in Bogotá, Colombia, as well as subsidiaries in various Latin American countries with hubs in San Salvador, El Salvador, as well as Lima, Peru, with a smaller operation in Ecuador.
Many countries have national airlines that the government owns and operates. Fully private airlines are subject to a great deal of government regulation for economic, political, and safety concerns. For instance, governments often intervene to halt airline labor actions to protect the free flow of people, communications, and goods between different regions without compromising safety.
The United States, Australia, and to a lesser extent Brazil, Mexico, India, the United Kingdom, and Japan have "deregulated" their airlines. In the past, these governments dictated airfares, route networks, and other operational requirements for each airline. Since deregulation, airlines have been largely free to negotiate their own operating arrangements with different airports, enter and exit routes easily, and to levy airfares and supply flights according to market demand. The entry barriers for new airlines are lower in a deregulated market, and so the U.S. has seen hundreds of airlines start up (sometimes for only a brief operating period). This has produced far greater competition than before deregulation in most markets. The added competition, together with pricing freedom, means that new entrants often take market share with highly reduced rates that, to a limited degree, full service airlines must match. This is a major constraint on profitability for established carriers, which tend to have a higher cost base.
As a result, profitability in a deregulated market is uneven for most airlines. These forces have caused some major airlines to go out of business, in addition to most of the poorly established new entrants.
In the United States, the airline industry is dominated by four large firms. Because of industry consolidation, after fuel prices dropped considerably in 2015, very little of the savings were passed on to consumers.
Groups such as the International Civil Aviation Organization establish worldwide standards for safety and other vital concerns. Most international air traffic is regulated by bilateral agreements between countries, which designate specific carriers to operate on specific routes. The model of such an agreement was the Bermuda Agreement between the US and UK following World War II, which designated airports to be used for transatlantic flights and gave each government the authority to nominate carriers to operate routes.
Bilateral agreements are based on the "freedoms of the air", a group of generalized traffic rights ranging from the freedom to overfly a country to the freedom to provide domestic flights within a country (a very rarely granted right known as cabotage). Most agreements permit airlines to fly from their home country to designated airports in the other country: some also extend the freedom to provide continuing service to a third country, or to another destination in the other country while carrying passengers from overseas.
In the 1990s, "open skies" agreements became more common. These agreements take many of these regulatory powers from state governments and open up international routes to further competition. Open skies agreements have met some criticism, particularly within the European Union, whose airlines would be at a comparative disadvantage with the United States' because of cabotage restrictions.
In 2017, 4.1 billion passengers have been carried by airlines in 41.9 million commercial scheduled flights (an average payload of passengers), for 7.75 trillion passenger kilometres (an average trip of km) over 45,091 airline routes served globally. In 2016, air transport generated $704.4 billion of revenue in 2016, employed 10.2 million workers, supported 65.5 million jobs and $2.7 trillion of economic activity: 3.6% of the global GDP.
In July 2016, the total weekly airline capacity was 181.1 billion Available Seat Kilometers (+6.9% compared to July 2015): 57.6bn in Asia-Pacific, 47.7bn in Europe, 46.2bn in North America, 12.2bn in Middle East, 12.0bn in Latin America and 5.4bn in Africa.
Airlines have substantial fixed and operating costs to establish and maintain air services: labor, fuel, airplanes, engines, spares and parts, IT services and networks, airport equipment, airport handling services, booking commissions, advertising, catering, training, aviation insurance and other costs. Thus all but a small percentage of the income from ticket sales is paid out to a wide variety of external providers or internal cost centers.
Moreover, the industry is structured so that airlines often act as tax collectors. Airline fuel is untaxed because of a series of treaties existing between countries. Ticket prices include a number of fees, taxes and surcharges beyond the control of airlines. Airlines are also responsible for enforcing government regulations. If airlines carry passengers without proper documentation on an international flight, they are responsible for returning them back to the original country.
Analysis of the 1992–1996 period shows that every player in the air transport chain is far more profitable than the airlines, who collect and pass through fees and revenues to them from ticket sales. While airlines as a whole earned 6% return on capital employed (2–3.5% less than the cost of capital), airports earned 10%, catering companies 10–13%, handling companies 11–14%, aircraft lessors 15%, aircraft manufacturers 16%, and global distribution companies more than 30%. (Source: Spinetta, 2000, quoted in Doganis, 2002)
There has been continuing cost competition from low cost airlines. Many companies emulate Southwest Airlines in various respects. The lines between full-service and low-cost airlines have become blurred – e.g., with most "full service" airlines introducing baggage check fees despite Southwest not doing so.
Many airlines in the U.S. and elsewhere have experienced business difficulty. U.S. airlines that have declared Chapter 11 bankruptcy since 1990 have included American Airlines, Continental Airlines (twice), Delta Air Lines, Northwest Airlines, Pan Am, United Airlines and US Airways (twice).
Where an airline has established an engineering base at an airport, then there may be considerable economic advantages in using that same airport as a preferred focus (or "hub") for its scheduled flights.
Fuel hedging is a contractual tool used by transportation companies like airlines to reduce their exposure to volatile and potentially rising fuel costs. Several low-cost carriers such as Southwest Airlines adopt this practice. Southwest is credited with maintaining strong business profits between 1999 and the early 2000s due to its fuel hedging policy. Many other airlines are replicating Southwest's hedging policy to control their fuel costs.
Operating costs for US major airlines are primarily aircraft operating expense including jet fuel, aircraft maintenance, depreciation and aircrew for 44%, servicing expense for 29% (traffic 11%, passenger 11% and aircraft 7%), 14% for reservations and sales and 13% for overheads (administration 6% and advertising 2%). An average US major Boeing 757-200 flies stages 11.3 block hours per day and costs $2,550 per block hour: $923 of ownership, $590 of maintenance, $548 of fuel and $489 of crew; or $13.34 per 186 seats per block hour. For a Boeing 737-500, a low-cost carrier like Southwest have lower operating costs at $1,526 than a full service one like United at $2,974, and higher productivity with 399,746 ASM per day against 264,284, resulting in a unit cost of $cts/ASM against $cts/ASM.
McKinsey observes that "newer technology, larger aircraft, and increasingly efficient operations continually drive down the cost of running an airline", from nearly 40 US cents per ASK at the beginning of the jet age, to just above 10 cents since 2000. Those improvements were passed onto the customer due to high competition: fares have been falling throughout the history of airlines.
Airlines assign prices to their services in an attempt to maximize profitability. The pricing of airline tickets has become increasingly complicated over the years and is now largely determined by computerized yield management systems.
Because of the complications in scheduling flights and maintaining profitability, airlines have many loopholes that can be used by the knowledgeable traveler. Many of these airfare secrets are becoming more and more known to the general public, so airlines are forced to make constant adjustments.
Most airlines use differentiated pricing, a form of price discrimination, to sell air services at varying prices simultaneously to different segments. Factors influencing the price include the days remaining until departure, the booked load factor, the forecast of total demand by price point, competitive pricing in force, and variations by day of week of departure and by time of day. Carriers often accomplish this by dividing each cabin of the aircraft (first, business and economy) into a number of travel classes for pricing purposes.
A complicating factor is that of origin-destination control ("O&D control"). Someone purchasing a ticket from Melbourne to Sydney (as an example) for A$200 is competing with someone else who wants to fly Melbourne to Los Angeles through Sydney on the same flight, and who is willing to pay A$1400. Should the airline prefer the $1400 passenger, or the $200 passenger plus a possible Sydney-Los Angeles passenger willing to pay $1300? Airlines have to make hundreds of thousands of similar pricing decisions daily.
The advent of advanced computerized reservations systems in the late 1970s, most notably Sabre, allowed airlines to easily perform cost-benefit analyses on different pricing structures, leading to almost perfect price discrimination in some cases (that is, filling each seat on an aircraft at the highest price that can be charged without driving the consumer elsewhere).
The intense nature of airfare pricing has led to the term "fare war" to describe efforts by airlines to undercut other airlines on competitive routes. Through computers, new airfares can be published quickly and efficiently to the airlines' sales channels. For this purpose the airlines use the Airline Tariff Publishing Company (ATPCO), who distribute latest fares for more than 500 airlines to Computer Reservation Systems across the world.
The extent of these pricing phenomena is strongest in "legacy" carriers. In contrast, low fare carriers usually offer pre-announced and simplified price structure, and sometimes quote prices for each leg of a trip separately.
Computers also allow airlines to predict, with some accuracy, how many passengers will actually fly after making a reservation to fly. This allows airlines to overbook their flights enough to fill the aircraft while accounting for "no-shows", but not enough (in most cases) to force paying passengers off the aircraft for lack of seats, stimulative pricing for low demand flights coupled with overbooking on high demand flights can help reduce this figure. This is especially crucial during tough economic times as airlines undertake massive cuts to ticket prices to retain demand.
Over January/February 2018, the cheapest airline surveyed by price comparator rome2rio was now-defunct Tigerair Australia with $0.06/km followed by AirAsia X with $0.07/km, while the most expensive was Charterlines, Inc. with $1.26/km followed by Buddha Air with $1.18/km.
For the IATA, the global airline industry revenue was $754 billion in 2017 for a $38.4 billion collective profit, and should rise by 10.7% to $834 billion in 2018 for a $33.8 billion profit forecast, down by 12% due to rising jet fuel and labor costs.
The demand for air transport will be less elastic for longer flights than for shorter flights, and more elastic for leisure travel than for business travel.
Airlines often have a strong seasonality, with traffic low in winter and peaking in summer. In Europe the most extreme market are the Greek islands with July/August having more than ten times the winter traffic, as Jet2 is the most seasonal among low-cost carriers with July having seven times the January traffic, whereas legacy carriers are much less with only 85/115% variability.
Airline financing is quite complex, since airlines are highly leveraged operations. Not only must they purchase (or lease) new airliner bodies and engines regularly, they must make major long-term fleet decisions with the goal of meeting the demands of their markets while producing a fleet that is relatively economical to operate and maintain; comparably Southwest Airlines and their reliance on a single airplane type (the Boeing 737 and derivatives), with the now defunct Eastern Air Lines which operated 17 different aircraft types, each with varying pilot, engine, maintenance, and support needs.
A second financial issue is that of hedging oil and fuel purchases, which are usually second only to labor in its relative cost to the company. However, with the current high fuel prices it has become the largest cost to an airline. Legacy airlines, compared with new entrants, have been hit harder by rising fuel prices partly due to the running of older, less fuel efficient aircraft. While hedging instruments can be expensive, they can easily pay for themselves many times over in periods of increasing fuel costs, such as in the 2000–2005 period.
In view of the congestion apparent at many international airports, the ownership of slots at certain airports (the right to take-off or land an aircraft at a particular time of day or night) has become a significant tradable asset for many airlines. Clearly take-off slots at popular times of the day can be critical in attracting the more profitable business traveler to a given airline's flight and in establishing a competitive advantage against a competing airline.
If a particular city has two or more airports, market forces will tend to attract the less profitable routes, or those on which competition is weakest, to the less congested airport, where slots are likely to be more available and therefore cheaper. For example, Reagan National Airport attracts profitable routes due partly to its congestion, leaving less-profitable routes to Baltimore-Washington International Airport and Dulles International Airport.
Other factors, such as surface transport facilities and onward connections, will also affect the relative appeal of different airports and some long-distance flights may need to operate from the one with the longest runway. For example, LaGuardia Airport is the preferred airport for most of Manhattan due to its proximity, while long-distance routes must use John F. Kennedy International Airport's longer runways.
Codesharing is the most common type of airline partnership; it involves one airline selling tickets for another airline's flights under its own airline code. An early example of this was Japan Airlines' (JAL) codesharing partnership with Aeroflot in the 1960s on Tokyo–Moscow flights; Aeroflot operated the flights using Aeroflot aircraft, but JAL sold tickets for the flights as if they were JAL flights. This practice allows airlines to expand their operations, at least on paper, into parts of the world where they cannot afford to establish bases or purchase aircraft. Another example was the Austrian–Sabena partnership on the Vienna–Brussels–New York/JFK route during the late '60s, using a Sabena Boeing 707 with Austrian livery.
Since airline reservation requests are often made by city-pair (such as "show me flights from Chicago to Düsseldorf"), an airline that can codeshare with another airline for a variety of routes might be able to be listed as indeed offering a Chicago–Düsseldorf flight. The passenger is advised however, that airline no. 1 operates the flight from say Chicago to Amsterdam, and airline no. 2 operates the continuing flight (on a different airplane, sometimes from another terminal) to Düsseldorf. Thus the primary rationale for code sharing is to expand one's service offerings in city-pair terms to increase sales.
A more recent development is the airline alliance, which became prevalent in the late 1990s. These alliances can act as virtual mergers to get around government restrictions. The largest are Star Alliance, SkyTeam and Oneworld, and these accounted for over 60% of global commercial air traffic . Alliances of airlines coordinate their passenger service programs (such as lounges and frequent-flyer programs), offer special interline tickets and often engage in extensive codesharing (sometimes systemwide). These are increasingly integrated business combinations—sometimes including cross-equity arrangements—in which products, service standards, schedules, and airport facilities are standardized and combined for higher efficiency. One of the first airlines to start an alliance with another airline was KLM, who partnered with Northwest Airlines. Both airlines later entered the SkyTeam alliance after the fusion of KLM and Air France in 2004.
Often the companies combine IT operations, or purchase fuel and aircraft as a bloc to achieve higher bargaining power. However, the alliances have been most successful at purchasing invisible supplies and services, such as fuel. Airlines usually prefer to purchase items visible to their passengers to differentiate themselves from local competitors. If an airline's main domestic competitor flies Boeing airliners, then the airline may prefer to use Airbus aircraft regardless of what the rest of the alliance chooses.
The world's largest airlines can be defined in several ways. , American Airlines Group was the largest by fleet size, passengers carried and revenue passenger mile. Delta Air Lines was the largest by revenue, assets value and market capitalization. Lufthansa Group was the largest by number of employees, FedEx Express by freight tonne-kilometres, Turkish Airlines by number of countries served and UPS Airlines by number of destinations served (though United Airlines was the largest passenger airline by number of destinations served).
Historically, air travel has survived largely through state support, whether in the form of equity or subsidies. The airline industry as a whole has made a cumulative loss during its 100-year history.
One argument is that positive externalities, such as higher growth due to global mobility, outweigh the microeconomic losses and justify continuing government intervention. A historically high level of government intervention in the airline industry can be seen as part of a wider political consensus on strategic forms of transport, such as highways and railways, both of which receive public funding in most parts of the world. Although many countries continue to operate state-owned or parastatal airlines, many large airlines today are privately owned and are therefore governed by microeconomic principles to maximize shareholder profit.
In December 1991, the collapse of Pan Am, an airline often credited for shaping the international airline industry, highlighted the financial complexities faced by major airline companies.
Following the 1978 deregulation, U.S. carriers did not manage to make an aggregate profit for 12 years in 31, including four years where combined losses amounted to $10 billion, but rebounded with eight consecutive years of profits since 2010, including its four with over $10 billion profits. They drop loss-making routes, avoid fare wars and market share battles, limit capacity growth, add hub feed with regional jets to increase their profitability. They change schedules to create more connections, buy used aircraft, reduce international frequencies and leverage partnerships to optimise capacities and benefit from overseas connectivity.
Aircraft engines emit noise pollution, gases and particulate emissions, and contribute to global dimming.
Growth of the industry in recent years raised a number of ecological questions.
Domestic air transport grew in China at 15.5 percent annually from 2001 to 2006. The rate of air travel globally increased at 3.7 percent per year over the same time. In the EU greenhouse gas emissions from aviation increased by 87% between 1990 and 2006. However it must be compared with the flights increase, only in UK, between 1990 and 2006 terminal passengers increased from 100 000 thousands to 250 000 thousands., according to AEA reports every year, 750 million passengers travel by European airlines, which also share 40% of merchandise value in and out of Europe. Without even pressure from "green activists", targeting lower ticket prices, generally, airlines do what is possible to cut the fuel consumption (and gas emissions connected therewith). Further, according to some reports, it can be concluded that the last piston-powered aircraft were as fuel-efficient as the average jet in 2005.
Despite continuing efficiency improvements from the major aircraft manufacturers, the expanding demand for global air travel has resulted in growing greenhouse gas (GHG) emissions. Currently, the aviation sector, including US domestic and global international travel, make approximately 1.6 percent of global anthropogenic GHG emissions per annum. North America accounts for nearly 40 percent of the world's GHG emissions from aviation fuel use.
CO2 emissions from the jet fuel burned per passenger on an average airline flight is about 353 kilograms (776 pounds). Loss of natural habitat potential associated with the jet fuel burned per passenger on a airline flight is estimated to be 250 square meters (2700 square feet).
In the context of climate change and peak oil, there is a debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.
The airline industry is responsible for about 11 percent of greenhouse gases emitted by the U.S. transportation sector. Boeing estimates that biofuels could reduce flight-related greenhouse-gas emissions by 60 to 80 percent. The solution would be blending algae fuels with existing jet fuel:
There are projects on electric aircraft, and some of them are fully operational as of 2013.
Main Article : Aviation call signs
Each operator of a scheduled or charter flight uses an airline call sign when communicating with airports or air traffic control centres. Most of these call-signs are derived from the airline's trade name, but for reasons of history, marketing, or the need to reduce ambiguity in spoken English (so that pilots do not mistakenly make navigational decisions based on instructions issued to a different aircraft), some airlines and air forces use call-signs less obviously connected with their trading name. For example, British Airways uses a "Speedbird" call-sign, named after the logo of one of its predecessors, BOAC, while SkyEurope used "Relax".
The various types of airline personnel include Flight crew, responsible for the operation of the aircraft. Flight crew members include: Pilots (Captain and First Officer: some older aircraft also required a Flight Engineer and/or a Navigator); Flight attendants (led by a purser on larger aircraft); In-flight security personnel on some airlines (most notably El Al)
Groundcrew, responsible for operations at airports, include Aerospace and avionics engineers responsible for certifying the aircraft for flight and management of aircraft maintenance; Aerospace engineers, responsible for airframe, powerplant and electrical systems maintenance; Avionics engineers responsible for avionics and instruments maintenance; Airframe and powerplant technicians; Electric System technicians, responsible for maintenance of electrical systems; Flight dispatchers; Baggage handlers; Ramp Agents; Remote centralised weight and balancing; Gate agents; Ticket agents; Passenger service agents (such as airline lounge employees); Reservation agents, usually (but not always) at facilities outside the airport; Crew schedulers.
Airlines follow a corporate structure where each broad area of operations (such as maintenance, flight operations (including flight safety), and passenger service) is supervised by a vice president. Larger airlines often appoint vice presidents to oversee each of the airline's hubs as well. Airlines employ lawyers to deal with regulatory procedures and other administrative tasks.
The pattern of ownership has been privatized since the mid-1980s, that is, the ownership has gradually changed from governments to private and individual sectors or organizations. This occurs as regulators permit greater freedom and non-government ownership, in steps that are usually decades apart. This pattern is not seen for all airlines in all regions. Many major airlines operating between the 1940s and 1980s were government-owned or government-established. However, most airlines from the earliest days of air travel in the 1920s and 1930s were personal businesses.
Growth rates are not consistent in all regions, but countries with a de-regulated airline industry have more competition and greater pricing freedom. This results in lower fares and sometimes dramatic spurts in traffic growth. The U.S., Australia, Canada, Japan, Brazil, India and other markets exhibit this trend. The industry has been observed to be cyclical in its financial performance. Four or five years of poor earnings precede five or six years of improvement. But profitability even in the good years is generally low, in the range of 2–3% net profit after interest and tax. In times of profit, airlines lease new generations of airplanes and upgrade services in response to higher demand. Since 1980, the industry has not earned back the cost of capital during the best of times. Conversely, in bad times losses can be dramatically worse. Warren Buffett in 1999 said "the money that had been made since the dawn of aviation by all of this country's airline companies was zero. Absolutely zero."
As in many mature industries, consolidation is a trend. Airline groupings may consist of limited bilateral partnerships, long-term, multi-faceted alliances between carriers, equity arrangements, mergers, or takeovers. Since governments often restrict ownership and merger between companies in different countries, most consolidation takes place within a country. In the U.S., over 200 airlines have merged, been taken over, or gone out of business since the Airline Deregulation Act in 1978. Many international airline managers are lobbying their governments to permit greater consolidation to achieve higher economy and efficiency.

</doc>
<doc id="1943" url="https://en.wikipedia.org/wiki?curid=1943" title="Australian Democrats">
Australian Democrats

The Australian Democrats is a centrist political party in Australia. Founded in 1977 from a merger of the Australia Party and the New Liberal Movement, both of which were descended from Liberal Party dissenting splinter groups, it was Australia's largest minor party from its formation in 1977 through to 2004 and frequently held the balance of power in the Senate during that time. The party was founded on the principle of participatory democracy ("bottom-up" control by members) but, , a new constitution specified representative democracy or, effectively, "top-down" control by a national executive—a fundamental change to the party.
The Democrats' inaugural leader was Don Chipp, a former Liberal cabinet minister, who famously promised to "keep the bastards honest". At the 1977 federal election, the Democrats polled 11.1 percent of the Senate vote and secured two seats. The party would retain a presence in the Senate for the next 30 years, at its peak (between 1999 and 2002) holding nine out of 76 seats, though never securing a seat in the lower house. The party's share of the vote collapsed at the 2004 election and was further diminished in 2007 with the last senators leaving office in 2008.
Due to the party's numbers in the Senate, both Liberal and Labor governments required the assistance of the Democrats to pass contentious legislation, most notably in the case of the Howard Government's goods and services tax (GST). Ideologically, the Democrats were usually regarded as centrists, occupying the political middle ground between the Liberal Party and the Labor Party.
The party was formally deregistered in 2016 for not having the required 500 members.
In 2018 the Australian Democrats merged with Country Minded, an Australian political party seeking accountable regional and agricultural representation. 
On 7 April 2019 the merged entity regained registration of the name "Australian Democrats" with the Australian Electoral Commission. The party unsuccessfully contested the lower-house seat of Adelaide and a total of six Senate seats (two in each state of New South Wales, Victoria and South Australia) at the 2019 federal election.
The party was founded on principles of honesty, tolerance, compassion and direct democracy through postal ballots of all members, so that "there should be no hierarchical structure ... by which a carefully engineered elite could make decisions for the members." From the outset, members' participation was fiercely protected in national and divisional constitutions prescribing internal elections, regular meeting protocols, annual conferences—and monthly journals for open discussion and balloting. Dispute resolution procedures were established, with final recourse to a party ombudsman and membership ballot.
Policies determined by the unique participatory method promoted environmental awareness and sustainability, opposition to the primacy of economic rationalism (Australian neoliberalism), preventative approaches to human health and welfare, animal rights, rejection of nuclear technology and weapons.
The Australian Democrats were the first representatives of green politics at the federal level in Australia. They "were in the vanguard of environmentalism in Australia. From the early 1980s they were unequivocally opposed to the building of the Franklin Dam in Tasmania and they opposed the mining and export of uranium and the development of nuclear power plants in Australia." In particular, leader Don Chipp, and Tasmanian state Democrat Norm Sanders, played crucial legislative roles in protecting the Franklin Dam.
The party's centrist role made it subject to criticism from both the right and left of the political spectrum. In particular, Chipp's former conservative affiliation was frequently recalled by opponents on the left. This problem was to torment later leaders and strategists who, by 1991, were proclaiming "the electoral objective" as a higher priority than the rigorous participatory democracy espoused by the party's founders.
Because of their numbers on the cross benches during the Hawke and Keating governments, the Democrats were sometimes regarded as exercising a balance of power—which attracted electoral support from a significant sector of the electorate which had been alienated by both Labor and Coalition policies and practices.
Over three decades, the Australian Democrats achieved representation in the legislatures of the ACT, South Australia, New South Wales, Western Australia and Tasmania as well as Senate seats in all six states. However, at the 2004 and 2007 federal elections, all seven of its Senate seats were lost. The last remaining State parliamentarian, David Winderlich, left the party and was defeated as an independent in 2010.
The Australian Democrats were formed in May 1977 from an amalgamation of the Australia Party and the New Liberal Movement.
The two groups found a common basis for a new political movement in the widespread discontent with the two major parties. In the former Liberal Government Minister, Don Chipp, the two groups found their leader.
The party's broad aim was to achieve a balance of power in one or more parliaments and to exercise it responsibly in line with policies determined by membership.
The first Australian Democrat parliamentarian was Robin Millhouse, the sole New LM member of the South Australian House of Assembly, who joined the Democrats in 1977. Millhouse held his seat (Mitcham) at the 1977 and 1979 state elections. In 1982, Millhouse resigned to take up a senior judicial appointment, and Heather Southcott won the by-election for the Democrats, but lost the seat to the Liberals later that year at the 1982 state election. Mitcham was the only single-member lower-house seat anywhere in Australia to be won by the Democrats.
The first Democrat federal parliamentarian was Senator Janine Haines, who in 1977 was nominated by the South Australian Parliament to fill the casual vacancy caused by the resignation of Liberal Senator Steele Hall. Hall had been elected as a Liberal Movement senator, before rejoining the Liberal Party in 1976, and South Australian premier Don Dunstan nominated Haines on the basis that the Democrats was the successor party to the Liberal Movement.
At the 1977 election, the Australian Democrats secured two seats in the Senate with the election of Colin Mason (NSW) and Don Chipp (VIC), though Haines lost her seat in South Australia. At the 1980 election, this increased to five seats with the election of Michael Macklin (QLD) and John Siddons (VIC) and the return of Janine Haines (SA). Thereafter they frequently held enough seats to give them the balance of power in the upper chamber.
At a Melbourne media conference on 19 September 1980, in the midst of the 1980 election campaign, Chipp described his party's aim as to "keep the bastards honest"—the "bastards" being the major parties and/or politicians in general. This became a long-lived slogan for the Democrats.
Don Chipp resigned from the Senate on 18 August 1986, being succeeded as party leader by Janine Haines and replaced as a senator for Victoria by Janet Powell.
At the 1987 election following a double dissolution, the reduced quota of 7.7% necessary to win a seat assisted the election of three new senators. 6-year terms were won by Paul McLean (NSW) and incumbents Janine Haines (South Australia) and Janet Powell (Victoria). In South Australia, a second senator, John Coulter, was elected for a 3-year term, as were incumbent Michael Macklin (Queensland) and Jean Jenkins (Western Australia).
1990 saw the voluntary departure from the Senate of Janine Haines (a step with which not all Democrats agreed) and the failure of her strategic goal of winning the House of Representatives seat of Kingston.
The casual vacancy was filled by Meg Lees several months before the election of Cheryl Kernot in place of retired deputy leader Michael Macklin. The ambitious Kernot immediately contested the party's national parliamentary deputy leadership. Being unemployed at the time, she requested and obtained party funds to pay for her travel to address members in all seven divisions. In the event, Victorian Janet Powell was elected as leader and John Coulter was chosen as deputy leader.
Despite the loss of Haines and the WA Senate seat (through an inconsistent national preference agreement with the ALP), the 1990 federal election heralded something of a rebirth for the party, with a dramatic rise in primary vote. This was at the same time as an economic recession was building, and events such as the Gulf War in Kuwait were beginning to shepherd issues of globalisation and transnational trade on to national government agendas.
The Australian Democrats had a long-standing policy to oppose war and so opposed Australia's support of, and participation in, the Gulf War. Whereas the House of Representatives was able to avoid any debate about the war and Australia's participation, the Democrats took full advantage of the opportunity to move for a debate in the Senate.
Because of the party's pacifist-based opposition to the Gulf War, there was mass-media antipathy and negative publicity which some construed as poor media performance by Janet Powell, the party's standing having stalled at about 10%. Before 12 months of her leadership had passed, the South Australian and Queensland divisions were circulating the party's first-ever petition to criticise and oust the parliamentary leader. The explicit grounds related to Powell's alleged responsibility for poor AD ratings in Gallup and other media surveys of potential voting support. When this charge was deemed insufficient, interested party officers and senators reinforced it with negative media 'leaks' concerning her openly established relationship with Sid Spindler and exposure of administrative failings resulting in excessive overtime to a staff member. With National Executive blessing, the party room pre-empted the ballot by replacing the leader with deputy John Coulter. In the process, severe internal divisions were generated. One major collateral casualty was the party whip Paul McLean who resigned and quit the Senate in disgust at what he perceived as in-fighting between close friends. The casual NSW vacancy created by his resignation was filled by Karin Sowada. Powell duly left the party, along with many leading figures of the Victorian branch of the party, and unsuccessfully stood as an Independent candidate when her term expired. In later years, she campaigned for the Australian Greens.
The party's parliamentary influence was weakened in 1996 after the Howard Government was elected, and a Labor senator, Mal Colston, resigned from the Labor Party. Since the Democrats now shared the parliamentary balance of power with two Independent senators, the Coalition government was able on occasion to pass legislation by negotiating with Colston and Brian Harradine.
In October 1997, party leader Cheryl Kernot resigned, announcing that she would be joining the Australian Labor Party. (Five years later it was revealed that she had been in a sexual relationship with Labor deputy leader Gareth Evans). Kernot resigned from the Senate and was replaced by Andrew Bartlett, while deputy Meg Lees became the new party leader.
Under Lees' leadership, in the 1998 federal election, the Democrats' candidate John Schumann came within 2 per cent of taking Liberal Foreign Minister Alexander Downer's seat of Mayo in the Adelaide Hills under Australia's preferential voting system. The party's representation increased to nine senators, and they regained the balance of power, holding it until the Coalition gained a Senate majority at the 2004 election.
Internal conflict and leadership tensions from 2000 to 2002, blamed on the party's support for the Government's Goods and Services Tax (GST), was damaging to the Democrats. Opposed by the Labor Party, the Australian Greens and independent Senator Harradine, the GST required Democrat support to pass. In an election fought on tax, the Democrats publicly stated that they liked neither the Liberal (GST) tax package nor the Labor package, but pledged to work with whichever party was elected to make their tax package better. They campaigned with the slogan "No GST on food".
In 1999, after negotiations with Prime Minister Howard, Meg Lees, Andrew Murray and the party room senators agreed to support the A New Tax System (ANTS) legislation with exemptions from GST for most food and some medicines, as well as many environmental and social concessions. Five Australian Democrats senators voted in favour. However, two dissident senators on the party's left Natasha Stott Despoja and Andrew Bartlett voted against the GST.
In 2001, a leadership spill saw Meg Lees replaced as leader by Natasha Stott Despoja after a very public and bitter leadership battle. Despite criticism of Stott Despoja's youth and lack of experience, the 2001 election saw the Democrats receive similar media coverage to the previous election. Despite the internal divisions, the Australian Democrats' election result in 2001 was quite good. However, it was not enough to prevent the loss of Vicki Bourne's Senate seat in NSW.
The 2002 South Australian election was the last time an Australian Democrat would be elected to an Australian parliament. Sandra Kanck was re-elected to a second eight-year term from an upper house primary vote of 7.3 percent.
Resulting tensions between Stott Despoja and Lees led to Meg Lees leaving the party in 2002, becoming an independent and forming the Australian Progressive Alliance. Stott Despoja stood down from the leadership following a loss of confidence by her party room colleagues. It led to a protracted leadership battle in 2002, which eventually led to the election of Senator Andrew Bartlett as leader. While the public fighting stopped, the public support for the party remained at record lows.
On 6 December 2003, Bartlett stepped aside temporarily as leader of the party, after an incident in which he swore at Liberal Senator Jeannie Ferris on the floor of Parliament while intoxicated. The party issued a statement stating that deputy leader Lyn Allison would serve as the acting leader of the party. Bartlett apologised to the Democrats, Jeannie Ferris and the Australian public for his behaviour and assured all concerned that it would never happen again. On 29 January 2004, after seeking medical treatment, Bartlett returned to the Australian Democrats leadership, vowing to abstain from alcohol.
Following internal conflict over GST (1998–2001) and resultant leadership changes, a dramatic decline occurred in the Democrats' membership and voting support in all states. Simultaneously, an increase was recorded in support for the Australian Greens who, by 2004, were supplanting the Democrats as a substantial third party. The trend was noted that year by political scientists Dean Jaensch et al.
Support for the Australian Democrats fell significantly at the 2004 federal election in which they achieved only 2.4 per cent of the national vote. Nowhere was this more noticeable than in their key support base of suburban Adelaide in South Australia, where they received between 1 and 4 percent of the lower house vote; by comparison, they tallied between 7 and 31 per cent of the vote in 2001. No Democrat senators were elected, though four kept their seats due to being elected in 2001, thus their representation fell from eight senators to four. Three incumbent senators were defeated: Aden Ridgeway (NSW), Brian Greig (WA) and John Cherry (Qld). Following the loss, the customary post-election leadership ballot installed Allison as leader, with Bartlett as her deputy. From 1 July 2005 the Australian Democrats lost official parliamentary party status, being represented by only four senators while the governing Liberal-National Coalition gained a majority and potential control of the Senate—the first time this advantage had been enjoyed by any government since 1980.
On 28 August 2006, the founder of the Australian Democrats, Don Chipp, died. Former prime minister Bob Hawke said: "... there is a coincidental timing almost between the passing of Don Chipp and what I think is the death throes of the Democrats." In November 2006, the Australian Democrats fared very poorly in the Victorian state election, receiving a Legislative Council vote tally of only 0.83%, less than half of the party's result in 2002 (1.79 per cent).
The Democrats again had no success at the 2007 federal election, and lost all four of their remaining Senate seats. Two incumbent senators, Lyn Allison (Victoria) and Andrew Bartlett (Queensland), were defeated, their seats both reverting to major parties. Their two remaining colleagues, Andrew Murray (WA) and Natasha Stott Despoja (SA), retired. All four senators' terms expired on 30 June 2008—leaving the Australian Democrats with no federal representation for the first time since its founding in 1977. Later, in 2009, Jaensch suggested it was possible the Democrats could make a political comeback at the 2010 South Australian election, but this did not occur.
The Tasmanian division of the party was deregistered for having insufficient members in January 2006. 
At the 2006 South Australian election, the Australian Democrats were reduced to 1.7 per cent of the Legislative Council (upper house) vote. Their sole councillor up for re-election, Kate Reynolds, was defeated. In July 2006, Richard Pascoe, national and South Australian party president, resigned, citing slumping opinion polls and the poor result in the 2006 South Australian election as well as South Australian parliamentary leader Sandra Kanck's comments regarding the drug MDMA which he saw as damaging to the party.
In the New South Wales state election of March 2007, the Australian Democrats lost their last remaining NSW Upper House representative, Arthur Chesterfield-Evans. The party fared poorly, gaining only 1.8 per cent of the Legislative Council vote. 
On 13 September 2007, the ACT Democrats (Australian Capital Territory Division of the party) was deregistered by the ACT Electoral Commissioner, being unable to demonstrate a minimum membership of 100 electors.
These losses left Sandra Kanck, in South Australia, as the party's only parliamentarian. She retired in 2009 and was replaced by David Winderlich, making him (as of 2020) the last Democrat to sit in any Australian parliament. The Democrats lost all representation when Winderlich resigned from the party in October 2009. He sat the remainder of his term as an independent, and lost his seat at the 2010 South Australian election.
On 16 April 2015, the Australian Electoral Commission deregistered the Australian Democrats as a political party for failure to demonstrate the requisite 500 members to maintain registration. However, the party did run candidates and remain registered for a period of time thereafter in the New South Wales Democrats and Queensland Democrat divisions.
In November 2018 there was a report that CountryMinded, a de-registered microparty, would merge with the Australian Democrats in a new bid to seek membership growth, electoral re-registration and financial support. In February 2019, application for registration was submitted to the AEC and was upheld on 7 April 2019, despite an objection from the Australian Democrats (Queensland Division).

</doc>
<doc id="1944" url="https://en.wikipedia.org/wiki?curid=1944" title="Australian Capital Territory">
Australian Capital Territory

The Australian Capital Territory (ACT), known as the Federal Capital Territory (FCT) until 1938, is a federal territory of Australia containing the Australian capital city of Canberra and some surrounding townships. It is located in the south-east of the country and is an enclave within the state of New South Wales. Founded after Federation as the seat of government for the new nation, all important institutions of the Australian Government are headquartered in the territory.
On 1 January 1901, federation of the colonies of Australia was achieved. Section 125 of the new Australian Constitution provided that land, situated in New South Wales and at least from Sydney, would be ceded to the new federal government. Following discussion and exploration of various areas within New South Wales, the "Seat of Government Act 1908" was passed in 1908 which specified a capital in the Yass-Canberra region. The territory was transferred to the federal government by New South Wales in 1911, two years prior to the capital city being founded and formally named as Canberra in 1913.
While the overwhelming majority of the population reside in the city of Canberra in the ACT's north-east, the territory also includes some surrounding townships such as Williamsdale, Naas, Uriarra, Tharwa and Hall. The ACT also includes the Namadgi National Park which comprises the majority of land area of the territory. Despite a common misconception, the Jervis Bay Territory is not part of the ACT although the laws of the Australian Capital Territory apply as if Jervis Bay did form part of the ACT. The territory has a relatively dry, continental climate experiencing warm to hot summers and cool to cold winters.
The Australian Capital Territory is home to many important institutions of the federal government, national monuments and museums. This includes the Parliament of Australia, the High Court of Australia, the Australian Defence Force Academy and the Australian War Memorial. It also hosts the majority of foreign embassies in Australia as well as regional headquarters of many international organisations, not-for-profit groups, lobbying groups and professional associations. Several major universities also have campuses in the ACT including the Australian National University, the University of Canberra, the University of New South Wales, Charles Sturt University and the Australian Catholic University.
A locally elected legislative assembly has governed the territory since 1988. However, the Commonwealth maintains authority over the territory and may overturn local laws. It still maintains control over the area known as the Parliamentary Triangle through the National Capital Authority. Residents of the territory elect three members of the House of Representatives and two senators.
With 428,060 residents, the Australian Capital Territory is the second smallest mainland state or territory by population. At the , the median weekly income for people in the territory aged over 15 was $998 and higher than the national average of $662. The average level of degree qualification in the ACT is also higher than the national average. Within the ACT, 37.1% of the population hold a bachelor's degree level or above education compared to the national figure of 20%.
Indigenous Australian peoples have long inhabited the area. Evidence indicates habitation dating back at least 25,000 years. It is possible that the area was inhabited for considerably longer, with evidence of an Aboriginal presence at Lake Mungo in south-western New South Wales dating back around 40,000 years. The principal group occupying the region were the Ngunnawal people.
Following European settlement, the growth of the new colony of New South Wales led to an increasing demand for arable land. Governor Lachlan Macquarie supported expeditions to open up new lands to the south of Sydney. The 1820s saw further exploration in the Canberra area associated with the construction of a road from Sydney to the Goulburn plains. While working on the project, Charles Throsby learned of a nearby lake and river from the local Indigenous peoples and he accordingly sent Wild to lead a small party to investigate the site. The search was unsuccessful, but they did discover the Yass River and it is surmised that they would have set foot on part of the future territory.
A second expedition was mounted shortly thereafter and they became the first Europeans to camp at the Molonglo (Ngambri) and Queanbeyan (Jullergung) Rivers. However, they failed to find the Murrumbidgee River. The issue of the Murrumbidgee was solved in 1821 when Throsby mounted a third expedition and successfully reached the watercourse, on the way providing the first detailed account of the land where Canberra now resides. The last expedition in the region before settlement was undertaken by Allan Cunningham in 1824. He reported that the region was suitable for grazing and the settlement of the Limestone Plains followed immediately thereafter.
The first land grant in the region was made to Joshua John Moore in 1823 and European settlement in the area began in 1824 with the construction of a homestead by his stockmen on what is now the Acton Peninsula. Moore formally purchased the site in 1826 and named the property "Canberry" or "Canberra".
A significant influx of population and economic activity occurred around the 1850s goldrushes. The goldrushes prompted the establishment of communication between Sydney and the region by way of the Cobb & Co coaches, which transported mail and passengers. The first post offices opened in Ginninderra in 1859 and at Lanyon in 1860.
During colonial times, the European communities of Ginninderra, Molonglo and Tuggeranong settled and farmed the surrounding land. The region was also called the Queanbeyan-Yass district, after the two largest towns in the area. The villages of Ginninderra and Tharwa developed to service the local agrarian communities.
During the first 20 years of settlement, there was only limited contact between the settlers and Aboriginal people. Over the succeeding years, the Ngunnawal and other local indigenous people effectively ceased to exist as cohesive and independent communities adhering to their traditional ways of life. Those who had not succumbed to disease and other predations either dispersed to the local settlements or were relocated to more distant Aboriginal reserves set up by the New South Wales government in the latter part of the 19th century.
In 1898, a referendum on a proposed Constitution was held in four of the colonies – New South Wales, Victoria, South Australia and Tasmania. Although the referendum achieved a majority in all four colonies, the New South Wales referendum failed to gain the minimum number of votes needed for the bill to pass. Following this result, a meeting of the four Premiers in 1898 heard from George Reid, the Premier of New South Wales, who argued that locating the future capital in New South Wales would be sufficient to ensure the passage of the Bill. The 1899 referendum on this revised bill was successful and passed with sufficient numbers. Section 125 of the Australian Constitution thus provided that, following Federation in 1901, land would be ceded freely to the new Federal Government.
This, however, left open the question of where to locate the capital. In 1906 and after significant deliberations, New South Wales agreed to cede sufficient land on the condition that it was in the Yass-Canberra region, this site being closer to Sydney. Initially, Dalgety, New South Wales remained at the forefront, but Yass-Canberra prevailed after voting by federal representatives. The "Seat of Government Act 1908" was passed in 1908, which repealed the 1904 Act and specified a capital in the Yass-Canberra region. Government surveyor Charles Scrivener was deployed to the region in the same year to map out a specific site and, after an extensive search, settled upon the present location.
The territory was transferred to the Commonwealth by New South Wales in 1911, two years before the naming of Canberra as the national capital in 1913.
In 1911, an international competition to design the future capital was held, which was won by the Chicago architect Walter Burley Griffin in 1912. The official naming of Canberra occurred on 12 March 1913 and construction began immediately.
After Griffin's departure following difficulty in implementing his project, the Federal Capital Advisory Committee was established in 1920 to advise the government of the construction efforts. The Committee had limited success meeting its goals. However, the chairman, John Sulman, was instrumental in applying the ideas of the garden city movement to Griffin's plan. The Committee was replaced in 1925 by the Federal Capital Commission.
In 1930, the ACT Advisory Council was established to advise the minister for territories on the community's concerns. In 1934, Supreme Court of the Australian Capital Territory was established.
From 1938 to 1957, the National Capital Planning and Development Committee continued to plan the further expansion of Canberra. However, the National Capital Planning and Development Committee did not have executive power, and decisions were made on the development of Canberra without the Committee's consultation. During this time, Prime Minister Robert Menzies regarded the state of the national capital as an embarrassment.
After World War II, there was a shortage of housing and office space in Canberra. A Senate Select Committee hearing was held in 1954 to address its development requirements. This Committee recommended the creation of a single planning body with executive power. Consequently, the National Capital Planning and Development Committee was replaced by the National Capital Development Commission in 1957. The National Capital Development Commission ended four decades of disputes over the shape and design of Lake Burley Griffin and construction was completed in 1964 after four years of work. The completion of the centrepiece of Griffin's design finally the laid the platform for the development of Griffin's Parliamentary Triangle.
In 1978, an advisory referendum was held to determine the views of ACT citizens about whether there should be self-government. Just under 64 percent of voters rejected devolved government options, in favour of the status quo. Nevertheless, in 1988, the new minister for the Australian Capital Territory Gary Punch received a report recommending the abolition of the National Capital Development Commission and the formation of a locally elected government. Punch recommended that the Hawke government accept the report's recommendations and subsequently Clyde Holding introduced legislation to grant self-government to the territory in October 1988.
The enactment on 6 December 1988 of the "Australian Capital Territory (Self-Government) Act 1988" established the framework for self-government. The first election for the 17-member Australian Capital Territory Legislative Assembly was held on 4 March 1989.
The initial years of self-government were difficult and unstable. A majority of ACT residents had opposed self-government and had it imposed upon them by the federal parliament. At the first election, 4 of the 17 seats were won by anti-self-government single-issue parties due to a protest vote by disgruntled territorians and a total of 8 were won by minor parties and independents.
In 1992, Labor won eight seats and the minor parties and independents won only three. Stability increased, and in 1995, Kate Carnell became the first elected Liberal chief minister. In 1998, Carnell became the first chief minister to be re-elected.
The Australian Capital Territory is the smallest mainland territory (aside from the Jervis Bay Territory) and covers a total land area of .
It is bounded by the Goulburn-Cooma railway line in the east, the watershed of Naas Creek in the south, the watershed of the Cotter River in the west and the watershed of the Molonglo River in the north-east. These boundaries were set to give the ACT an adequate water supply. The ACT extends about North-South between 35.124°S and 35.921°S, and West-East between 148.763°E and 149.399°E. The city area of Canberra occupies the north-eastern corner of this area.
The Australian Capital Territory includes the city of Canberra and some other townships such as Williamsdale, Naas, Uriarra, Tharwa and Hall. The Australian Capital Territory also contains agricultural land (sheep, dairy cattle, vineyards and small amounts of crops) and a large area of national park (Namadgi National Park), much of it mountainous and forested.
Tidbinbilla is a locality to the south-west of Canberra that features the Tidbinbilla Nature Reserve and the Canberra Deep Space Communication Complex, operated by the United States' National Aeronautics and Space Administration (NASA) as part of its Deep Space Network.
There are a large range of mountains, rivers and creeks throughout the territory and are largely contained within the Namadgi National Park. These include the Naas and Murrumbidgee Rivers.
The territory has a relatively dry, continental climate experiencing warm to hot summers and cool to cold winters. Under Köppen-Geiger classification, the territory has an oceanic climate ("Cfb").
January is the hottest month with an average high of . July is the coldest month when the average high drops to . The highest maximum temperature recorded in the territory was on 4 January 2020. The lowest minimum temperature was on 11 July 1971.
Rainfall varies significantly across the territory. Much higher rainfall occurs in the mountains to the west of Canberra compared to the east. The mountains act as a barrier during winter with the city receiving less rainfall. Average annual rainfall in the territory is and there is an average of 108 rain days annually. The wettest month is October with an average rainfall of and the driest month is June with an average of .
Frost is common in the winter months. Snow is rare in Canberra's city centre, but the surrounding areas get annual snowfall through winter and often the snow-capped mountains can be seen from the city. The last significant snowfall in the city centre was in 1968.
Smoke haze became synonymous with the 2019/2020 Australian summer. On 1 January 2020 Canberra had the worst air quality of any major city in the world, with an AQI of 7700 (USAQI 949).
Notable geological formations in the Australian Capital Territory include the "Canberra Formation", the "Pittman Formation", "Black Mountain Sandstone" and "State Circle Shale".
In the 1840s fossils of brachiopods and trilobites from the Silurian period were discovered at Woolshed Creek near Duntroon. At the time, these were the oldest fossils discovered in Australia, though this record has now been far surpassed. Other specific geological places of interest include the State Circle cutting and the Deakin anticline.
The oldest rocks in the ACT date from the Ordovician around 480 million years ago. During this period the region along with most of Eastern Australia was part of the ocean floor; formations from this period include the "Black Mountain Sandstone" formation and the "Pittman Formation" consisting largely of quartz-rich sandstone, siltstone and shale. These formations became exposed when the ocean floor was raised by a major volcanic activity in the Devonian forming much of the east coast of Australia
The environments range from alpine area on the higher mountains, to sclerophyll forest and to woodland. Much of the ACT has been cleared for grazing and is also burnt off by bushfires several times per century. The kinds of plants can be grouped into vascular plants, that include gymnosperms, flowering plants, and ferns, as well as bryophytes, lichens, fungi and freshwater algae. Four flowering plants are endemic to the ACT. Several lichens are unique to the territory. Most plants in the ACT are characteristic of the Flora of Australia and include well known plants such as Grevillea, Eucalyptus trees and kangaroo grass.
The native forest in the Canberra region was almost wholly eucalypt species and provided a resource for fuel and domestic purposes. By the early 1960s, logging had depleted the eucalypt, and concern about water quality led to the forests being closed. Interest in forestry began in 1915 with trials of a number of species including "Pinus radiata" on the slopes of Mount Stromlo. Since then, plantations have been expanded, with the benefit of reducing erosion in the Cotter catchment, and the forests are also popular recreation areas.
The fauna of the territory includes representatives from most major Australian animal groups. This includes kangaroos, wallabies, koalas, platypus, echidna, emu, kookaburras and dragon lizards.
The ACT has internal self-government, but Australia's Constitution does not afford a territory legislature the high degree of independence provided to that of a state. Instead, each territory is governed under a Commonwealth statutefor the ACT, the Australian Capital Territory (Self-Government) Act 1988. The chief minister performs many of the roles that a state governor normally holds in the context of a state; however, the Speaker of the Legislative Assembly gazettes the laws and summons meetings of the Assembly.
Laws are made in a 25-member Legislative Assembly that combines both state and local government functions (prior to 2016, the Assembly was made up of 17 members).
Members of the Legislative Assembly are elected via the Hare–Clark system.
The executive of the Australian Capital Territory, also known as the ACT Government, consists of the chief minister and such other ministers as are appointed by the chief minister. The ACT chief minister (currently Andrew Barr, Labor) is elected by members of the Legislative Assembly. The chief minister represents the ACT Government as a member of the Council of Australian Governments.
Unlike other self-governing Australian territories (for example, the Northern Territory), the ACT does not have an Administrator. The Crown is represented in government of the ACT by the Australian Governor-General. Until 4 December 2011, the decisions of the assembly could be overruled by the Governor-General (effectively by the national government) under section 35 of the Australian Capital Territory (Self-Government) Act 1988, although the federal parliament voted in 2011 to abolish this veto power, instead requiring a majority of both houses of the federal parliament to override an enactment of the ACT.
The court system of the territory consists of the Supreme Court of the Australian Capital Territory, the Magistrates Court of the Australian Capital Territory and the ACT Civil and Administrative Tribunal. It is unique in that the territory does not have an intermediary court like other mainland states and territories; there is only the superior court and a court of summary jurisdiction. the Chief Justice is Helen Murrell and the current Chief Magistrate is Lorraine Walker.
ACT Policing (part of the Australian Federal Police) is responsible for providing policing services to the ACT. Canberra had the lowest rate of crime of any capital city in Australia . 
In Australia's Federal Parliament, the ACT is represented by five federal members: three members of the House of Representatives represent the Division of Bean, the Division of Canberra and the Division of Fenner, and it is one of only two territories to be represented in the Senate, with two Senators (the other being the Northern Territory). The Member for Bean and the ACT Senators also represent the constituents of Norfolk Island. The Member for Fenner and the ACT Senators also represent the constituents of the Jervis Bay Territory.
In 1915, the "Jervis Bay Territory Acceptance Act 1915" created the Jervis Bay Territory as an annexe to the Federal Capital Territory. While the Act's use of the language of "annexed" is sometimes interpreted as implying that the Jervis Bay Territory was to form part of the Federal Capital Territory, the accepted legal position is that it has been a legally distinct territory from its creation despite being subject to ACT law and, prior to ACT self-government in 1988, being administratively treated as part of the ACT.
In 1988, when the ACT gained self-government, Jervis Bay was formally pronounced as a separate territory administered by the Commonwealth known as the Jervis Bay Territory. However, the laws of the ACT continue to apply to the Jervis Bay Territory. Magistrates from the ACT regularly travel to the Jervis Bay Territory to conduct court.
Another occasional misconception is that the ACT retains a small area of territory on the coast on the Beecroft Peninsula, consisting of a strip of coastline around the northern headland of Jervis Bay. While the land is owned by the Commonwealth Government, that area itself is still considered to be under the jurisdiction of New South Wales government, not a separate territory nor a part of the ACT.
The Australian Bureau of Statistics estimates that the population of the territory was 419,200 on 31 March 2019. The population is projected to reach to approximately 700,000 by 2058.
The overwhelming majority of the population reside in the city of Canberra.
At the , the median weekly income for people in the territory aged over 15 was $998 while the national average was $662.
The average level of degree qualification in the ACT is higher than the national average. Within the ACT, 37.1% of the population hold a bachelor's degree level or above education compared to the national figure of 20%.
The Australian Capital Territory consists of the city of Canberra and some surrounding townships including Williamsdale, Naas, Uriarra, Tharwa and Hall.
The urban areas of Canberra are organised into a hierarchy of districts, town centres, group centres, local suburbs as well as other industrial areas and villages. There are seven districts (with an eighth currently under construction), each of which is divided into smaller suburbs, and most of which have a town centre which is the focus of commercial and social activities. The districts were settled in the following chronological order:
The North and South Canberra districts are substantially based on Walter Burley Griffin's designs. In 1967, the then National Capital Development Commission adopted the "Y Plan" which laid out future urban development in Canberra around a series of central shopping and commercial area known as the 'town centres' linked by freeways, the layout of which roughly resembled the shape of the letter Y, with Tuggeranong at the base of the Y and Belconnen and Gungahlin located at the ends of the arms of the Y.
At the 2016 census, the most commonly nominated ancestries were: 
The 2016 census showed that 32% of the ACT's inhabitants were born overseas. Of inhabitants born outside of Australia, the most prevalent countries of birth were England, China, India, New Zealand and the Philippines.
1.6% of the population, or 6,476 people, identified as Indigenous Australians (Aboriginal Australians and Torres Strait Islanders) in 2016.
At the 2016 census, 72.7% of people spoke only English at home. The other languages most commonly spoken at home were Mandarin (3.1%), Vietnamese (1.1%), Cantonese (1%), Hindi (0.9%) and Spanish (0.8%).
The most common responses in the for religion in the territory were No Religion (36.2%), Catholic (22.3%), Anglican (10.8%), Not stated (9.2%) and Hinduism (2.6%). In Australian Capital Territory, Christianity was the largest religious group reported overall (49.9%).
Almost all educational institutions in the Australian Capital Territory are located within Canberra. The ACT public education system schooling is normally split up into Pre-School, Primary School (K-6), High School (7–10) and College (11–12) followed by studies at university or CIT (Canberra Institute of Technology). Many private high schools include years 11 and 12 and are referred to as colleges. Children are required to attend school until they turn 17 under the ACT Government's "Learn or Earn" policy.
In February 2004 there were 140 public and non-governmental schools in ACT; 96 were operated by the Government and 44 are non-Government. In 2005, there were 60,275 students in the ACT school system. 59.3% of the students were enrolled in government schools with the remaining 40.7% in non-government schools. There were 30,995 students in primary school, 19,211 in high school, 9,429 in college and a further 340 in special schools.
As of May 2004, 30% of people in the ACT aged 15–64 had a level of educational attainment equal to at least a bachelor's degree, significantly higher than the national average of 19%. The two main tertiary institutions are the Australian National University (ANU) in Acton and the University of Canberra (UC) in Bruce. There are also two religious university campuses in Canberra: Signadou is a campus of the Australian Catholic University and St Mark's Theological College is a campus of Charles Sturt University. Tertiary level vocational education is also available through the multi-campus Canberra Institute of Technology.
The Australian Defence Force Academy (ADFA) and the Royal Military College, Duntroon (RMC) are in the suburb of Campbell in Canberra's inner northeast. ADFA teaches military undergraduates and postgraduates and is officially a campus of the University of New South Wales while Duntroon provides Australian Army Officer training.
The Academy of Interactive Entertainment (AIE) offers courses in computer game development and 3D animation.
The Australian Capital Territory is home to a number of major professional sports league franchise teams including the ACT Brumbies (Rugby Union), Canberra United (Football), Canberra Raiders (Rugby League) and the Canberra Capitals (Basketball).
The Prime Minister's XI (Cricket), started by Robert Menzies in the 1950s and revived by Bob Hawke in 1984, has been played every year at Manuka Oval against an overseas touring team.
The Greater Western Sydney Giants (Australian Rules) play three regular season matches a year and one pre-season match in Canberra at Manuka Oval.
The territory is home to many national monuments and institutions such as the Australian War Memorial, the National Gallery of Australia, the National Portrait Gallery, the National Library, the National Archives, the Australian Academy of Science, the National Film and Sound Archive and the National Museum. Many Commonwealth government buildings in Canberra are open to the public, including Parliament House, the High Court and the Royal Australian Mint.
Lake Burley Griffin is the site of the Captain James Cook Memorial and the National Carillon. Other sites of interest include the Telstra Tower, the Australian National Botanic Gardens, the National Zoo and Aquarium, the National Dinosaur Museum and Questacon – the National Science and Technology Centre.
The Canberra Museum and Gallery in the city is a repository of local history and art, housing a permanent collection and visiting exhibitions. Several historic homes are open to the public: Lanyon and Tuggeranong Homesteads in the Tuggeranong Valley, Mugga-Mugga in Symonston, and Blundells' Cottage in Parkes all display the lifestyle of the early European settlers. Calthorpes' House in Red Hill is a well-preserved example of a 1920s house from Canberra's very early days.
Canberra has many venues for live music and theatre: the Canberra Theatre and Playhouse which hosts many major concerts and productions; and Llewellyn Hall (within the ANU School of Music), a world-class concert hall are two of the most notable. The Albert Hall was Canberra's first performing arts venue, opened in 1928. It was the original performance venue for theatre groups such as the Canberra Repertory Society.
There are numerous bars and nightclubs which also offer live entertainment, particularly concentrated in the areas of Dickson, Kingston and the city. Most town centres have facilities for a community theatre and a cinema, and they all have a library. Popular cultural events include the National Folk Festival, the Royal Canberra Show, the Summernats car festival, Enlighten festival and the National Multicultural Festival in February.
Canberra and the territory have a daily newspaper, "The Canberra Times", which was established in 1926. There are also several free weekly publications, including news magazines "CityNews" and "Canberra Weekly."
There are a number of AM and FM stations broadcasting throughout the ACT (AM/FM Listing). The main commercial operators are the Capital Radio Network (2CA and 2CC), and Austereo/ARN (104.7 and Mix 106.3). There are also several community operated stations as well as the local and national stations of the Australian Broadcasting Corporation.
A DAB+ digital radio trial is also in operation, it simulcasts some of the AM/FM stations, and also provides several digital only stations (DAB+ Trial Listing).
Five free-to-air television stations service the territory:
Each station broadcasts a primary channel and several multichannels.
Pay television services are available from Foxtel (via satellite) and telecommunications company TransACT (via cable).
The Australian Capital Territory has two large public hospitals both located in Canberra: the approximately 600-bed Canberra Hospital in Garran and the 174-bed Calvary Public Hospital in Bruce. Both are teaching institutions. The largest private hospital is the Calvary John James Hospital in Deakin. Calvary Private Hospital in Bruce and Healthscope's National Capital Private Hospital in Garran are also major healthcare providers.
Canberra has 10 aged care facilities. Canberra's hospitals receive emergency cases from throughout southern New South Wales, and ACT Ambulance Service is one of four operational agencies of the ACT Emergency Services Authority. NETS provides a dedicated ambulance service for inter-hospital transport of sick newborns within the ACT and into surrounding New South Wales.
The automobile is by far the dominant form of transport in Canberra and the territory. The city is laid out so that arterial roads connecting inhabited clusters run through undeveloped areas of open land or forest, which results in a low population density; this also means that idle land is available for the development of future transport corridors if necessary without the need to build tunnels or acquire developed residential land. In contrast, other capital cities in Australia have substantially less green space.
Canberra's districts are generally connected by parkways—limited access dual carriageway roads with speed limits generally set at a maximum of . An example is the Tuggeranong Parkway which links Canberra's CBD and Tuggeranong, and bypasses Weston Creek. In most districts, discrete residential suburbs are bounded by main arterial roads with only a few residential linking in, to deter non-local traffic from cutting through areas of housing.
ACTION, the government-operated bus service, provides public transport throughout Canberra. Qcity Transit provides bus services between Canberra and nearby areas of New South Wales through their Transborder Express brand (Murrumbateman and Yass) and as Qcity Transit (Queanbeyan). A light rail line that opened in April 2019 links the CBD with the northern district of Gungahlin. At the 2016 census, 7.1% of the journeys to work involved public transport while 4.5% were on foot.
There are two local taxi companies. Aerial Capital Group enjoyed monopoly status until the arrival of Cabxpress in 2007. In October 2015, the ACT Government passed legislation to regulate ride sharing, allowing ride share services including Uber to operate legally in Canberra. The ACT Government was the first jurisdiction in Australia to enact legislation to regulate the service.
An interstate NSW TrainLink railway service connects Canberra to Sydney. Canberra's railway station is in the inner south suburb of Kingston. Train services to Melbourne are provided by way of a NSW TrainLink bus service which connects with a rail service between Sydney and Melbourne in Yass, about a one-hour drive from Canberra.
Canberra is about three hours by road from Sydney on the Federal Highway (National Highway 23), which connects with the Hume Highway (National Highway 31) near Goulburn, and seven hours by road from Melbourne on the Barton Highway (National Highway 25), which joins the Hume Highway at Yass. It is a two-hour drive on the Monaro Highway (National Highway 23) to the ski fields of the Snowy Mountains and the Kosciuszko National Park. Batemans Bay, a popular holiday spot on the New South Wales coast, is also two hours away via the Kings Highway.
Canberra Airport provides direct domestic services to Sydney, Melbourne, Brisbane, Adelaide, Gold Coast and Perth, with connections to other domestic centres. There are also direct flights to small regional towns: Dubbo and Newcastle in New South Wales. Regular direct international flights operate to Singapore and Doha from the airport daily, but both with a stopover in Sydney before Canberra. Canberra Airport is, as of September 2013, designated by the Australian Government Department of Infrastructure and Regional Development as a restricted use designated international airport. Until 2003, the civilian airport shared runways with RAAF Base Fairbairn. In June of that year, the Air Force base was decommissioned and from that time the airport was fully under civilian control.
The government-owned ACTEW Corporation manages the territory's water and sewerage infrastructure. ActewAGL is a joint venture between ACTEW and AGL, and is the retail provider of Canberra's utility services including water, natural gas, electricity, and also some telecommunications services via a subsidiary TransACT.
Canberra's water is stored in four reservoirs, the Corin, Bendora and Cotter dams on the Cotter River and the Googong Dam on the Queanbeyan River. Although the Googong Dam is located in New South Wales, it is managed by the ACT government. ACTEW Corporation owns Canberra's two wastewater treatment plants, located at Fyshwick and on the lower reaches of the Molonglo River.
Electricity for Canberra mainly comes from the national power grid through substations at Holt and Fyshwick (via Queanbeyan). Power was first supplied from a thermal plant built in 1913, near the Molonglo River, but this was finally closed in 1957. The ACT has four solar farms, which were opened between 2014 and 2017: Royalla (rated output of 20 megawatts, 2014), Mount Majura (2.3 MW, 2016), Mugga Lane (13 MW, 2017) and Williamsdale (11 MW, 2017). In addition numerous houses in Canberra have photovoltaic panels and/or solar hot water systems. In 2015/16, rooftop solar systems supported by the ACT government's feed-in tariff had a capacity of 26.3 megawatts, producing 34,910 MWh. In the same year, retailer-supported schemes had a capacity of 25.2 megawatts and exported 28,815 MWh to the grid (power consumed locally was not recorded).
The ACT has the highest rate with internet access at home (94 per cent of households in 2014–15).
The economic activity of the Australian Capital Territory is heavily concentrated around the city of Canberra.
A stable housing market, steady employment and rapid population growth in the 21st century have led to economic prosperity and, in 2011, CommSec ranked the ACT as the second best performing economic region in the country. This trend continued into 2016, when the territory was ranked the third best performing out of all of Australia's states and territories.
In 2017–18, the ACT had the fastest rate of growth in the nation due to a rapid growth in population, a strongly performing higher education sector as well as a significant housing and infrastructure investment.
Higher education is the territory's largest export industry. Canberra is home to a significant number of universities and higher education providers. The other major services exports of the ACT in 2017-18 were government services and personal travel. The major goods exports of the territory in 2017-18 were gold coin, legal tender coin, metal structures and fish, though these represent a small proportion of the economy compared to services exports.
The economy of the ACT is largely dependent on the public sector with 30% of the jobs in the territory being in the public sector. Decisions by the federal government regarding the public service can have a significant impact on the territory's economy.
The ACT's gross state product in 2017-18 was $39.8 billion which represented 2.2% of the overall gross domestic product of Australia. In 2017-18 the ACT economy grew by 4.0 per cent, the highest growth rate of any jurisdiction in Australia. This brought real economic growth over the three years to June 2018 to 12 per cent.

</doc>
<doc id="1946" url="https://en.wikipedia.org/wiki?curid=1946" title="Unit of alcohol">
Unit of alcohol

Units of alcohol are used in the United Kingdom (UK) as a measure to quantify the actual alcoholic content within a given volume of an alcoholic beverage, in order to provide guidance on total alcohol consumption.
A number of other countries (including Australia, Canada, New Zealand, and the US) use the concept of a "standard drink", the definition of which varies from country to country, for the same purpose. "Standard drinks" were referred to in the first UK guidelines (1984) that published "safe limits" for drinking, but these were replaced by references to "alcohol units" in the 1987 guidelines and the latter term has been used in all subsequent UK guidance.
One unit of alcohol (UK) is defined as 10 millilitres (8 grams) of pure alcohol. Typical drinks (i.e., typical quantities or servings of common alcoholic drinks) may contain 1–3 units of alcohol.
Containers of alcoholic drinks sold directly to UK consumers are normally labelled to indicate the number of units of alcohol in a typical serving (optional) and in the full container (can or bottle), as well as information about responsible drinking.
As an approximate guideline, a typical healthy adult can metabolise (break down) about one unit of alcohol per hour, although this may vary depending on sex, age, weight, health and many other factors.
The number of UK units of alcohol in a drink can be determined by multiplying the volume of the drink (in millilitres) by its percentage ABV, and dividing by 1000.
For example, one imperial pint (568 ml) of beer at 4% alcohol by volume (ABV) contains:
The formula uses . This results in exactly one unit per percentage point per litre, of any alcoholic beverage.
The formula can be simplified for everyday use by expressing the serving size in centilitres and the alcohol content literally as a percentage:
Thus, a 750 ml bottle of wine at 12% ABV contains 75 cl × 12% = 9 units. Alternatively, the serving size in litres multiplied by the alcohol content as a number, the above example giving 0.75 × 12 = 9 units:
Both pieces of input data are usually mentioned in this form on the bottle, so is easy to retrieve.
UK alcohol companies pledged in March 2011 to implement an innovative health labelling scheme to provide more information about responsible drinking on alcohol labels and containers. This voluntary scheme is the first of its kind in Europe and has been developed in conjunction with the UK Department of Health. The pledge stated:
At the end of 2014, 101 companies had committed to the pledge labelling scheme.
There are five elements included within the overall labelling scheme, the first three being mandatory, and the last two optional:
Drinks companies had pledged to display the three mandatory items on 80% of drinks containers on shelves in the UK off-trade by the end of December 2013. A report published in November 2014, confirmed that UK drinks producers had delivered on that pledge with a 79.3% compliance with the pledge elements as measured by products on shelf. Compared with labels from 2008 on a like-for-like basis, information on Unit alcohol content had increased by 46%; 91% of products displayed alcohol and pregnancy warnings (18% in 2008); and 75% showed the Chief Medical Officers' lower risk daily guidelines (6% in 2008).
It is sometimes misleadingly stated that there is one unit per half-pint of beer, or small glass of wine, or single measure of spirits. However, such statements do not take into account the various strengths and volumes supplied in practice.
For example, the ABV of beer typically varies from 3.5% to 5.5%. A typical "medium" glass of wine with 175 ml at 12% ABV has 2.1 units. And spirits, although typically 35–40% ABV, have single measures of 25 ml or 35 ml (so 1 or 1.4 units) depending on location.
The misleading nature of "one unit per half-pint of beer, or small glass of wine, or single measure of spirits" can lead to people underestimating their alcohol intake.
Most spirits sold in the United Kingdom have 40% ABV or slightly less. In England, a single pub measure (25 ml) of a spirit contains one unit. However, a larger 35 ml measure is increasingly used (and in particular is standard in Northern Ireland), which contains 1.4 units of alcohol at 40% ABV. Sellers of spirits by the glass must state the capacity of their standard measure in ml.
On average, it takes about one hour for the body to metabolise (break down) one unit of alcohol. However, this will vary with body weight, sex, age, personal metabolic rate, recent food intake, the type and strength of the alcohol, and medications taken. Alcohol may be metabolised more slowly if liver function is impaired.
From 1992 to 1995, the UK government advised that men should drink no more than 21 units per week, and women no more than 14. (The difference between the sexes was due to the typically lower weight and water-to-body-mass ratio of women). "The Times" claimed in October 2007 that these limits had been "plucked out of the air" and had no scientific basis.
This was changed after a government study showed that many people were in effect "saving up" their units and using them at the end of the week, a form of binge drinking. Since 1995 the advice was that regular consumption of 3–4 units a day for men, or 2–3 units a day for women, would not pose significant health risks, but that consistently drinking four or more units a day (men), or three or more units a day (women), is not advisable.
An international study of about 6,000 men and 11,000 women for a total of 75,000 person-years found that people who reported that they drank more than a threshold value of 2 units of alcohol a day had a higher risk of fractures than non-drinkers. For example, those who drank over 3 units a day had nearly twice the risk of a hip fracture.

</doc>
<doc id="1947" url="https://en.wikipedia.org/wiki?curid=1947" title="Aotus">
Aotus

Aotus (the name is derived from the Ancient Greek words for "earless" in both cases: the monkey is missing external ears, and the pea is missing earlike bracteoles) may refer to:

</doc>
<doc id="1948" url="https://en.wikipedia.org/wiki?curid=1948" title="Ally McBeal">
Ally McBeal

Ally McBeal is an American legal comedy-drama television series, originally aired on Fox from September 8, 1997, to May 20, 2002. Created by David E. Kelley, the series stars Calista Flockhart in the title role as a lawyer working in the Boston law firm Cage and Fish, with other lawyers whose lives and loves were eccentric, humorous, and dramatic. The series received critical acclaim in its early seasons, winning the Golden Globe Award for Best Television Series – Musical or Comedy in 1997 and 1998, and also winning the Emmy Award for Outstanding Comedy Series in 1999.
The series, set in the fictional Boston law firm Cage and Fish, begins with main character Allison Marie "Ally" McBeal joining the firm co-owned by her law school classmate Richard Fish (Greg Germann) after leaving her previous job due to sexual harassment. On her first day, Ally is horrified to find that she will be working alongside her ex-boyfriend Billy Thomas (Gil Bellows)—whom she has never gotten over. To make things worse, Billy is now married to fellow lawyer Georgia (Courtney Thorne-Smith), who later joins Cage and Fish. The triangle among the three forms the basis for the main plot for the show's first three seasons.
Although ostensibly a legal drama, the main focus of the series was the romantic and personal lives of the main characters, often using legal proceedings as plot devices to contrast or reinforce a character's drama. For example, bitter divorce litigation of a client might provide a backdrop for Ally's decision to break up with a boyfriend. Legal arguments were also frequently used to explore multiple sides of various social issues.
Cage and Fish (which becomes Cage/Fish & McBeal or Cage, Fish, & Associates towards the end of the series), the fictional law firm where most of the characters work, is depicted as a highly sexualized environment symbolized by its unisex restroom. Lawyers and secretaries in the firm routinely date, flirt with, or have a romantic history with each other and frequently run into former or potential romantic interests in the courtroom or on the street outside.
The series had many offbeat and frequently surreal running gags and themes, such as Ally's tendency to immediately fall over whenever she met somebody she found attractive, Richard Fish's wattle fetish and humorous mottos ("Fishisms" & "Bygones"), John's gymnastic dismounts out of the office's unisex bathroom stalls, or the dancing twins (played by Eric & Steve Cohen) at the bar, that ran through the series. The show also used vivid, dramatic fantasy sequences for Ally's and other characters' wishful thinking; of particular note is the early internet sensation the dancing baby.
The series also featured regular visits to a local bar where singer Vonda Shepard regularly performed (though occasionally handing over the microphone to the characters). Star contemporary singers also performed in the bar at the end of the shows, including acts such as Mariah Carey, Barry White and Anastacia. The series also took place in the same continuity as David E. Kelley's legal drama "The Practice" (which aired on ABC), as the two shows crossed over with one another on occasion, a very rare occurrence for two shows that aired on different networks.
Ultimately, in the last installment of the fifth and final season, "Bygones", Ally decided to resign from Cage & Fish, leave Boston, and go to New York City.
Fox canceled "Ally McBeal" after five seasons. In addition to being the lowest-rated season of "Ally McBeal" and the grounds for the show's cancellation, the fifth season was also the only season of the show that failed to win any Emmy or Golden Globe awards.
In Australia, "Ally McBeal" was aired by the Seven Network from 1997 to 2002. In 2010, it was aired repeatedly by Network 10.
Seymore Walsh, a stern judge often exasperated by the eccentricities of the Cage & Fish lawyers and played by actor Albert Hall, was also a recurring character on "The Practice". In addition, Judge Jennifer (Whipper) Cone appears on "The Practice" episode "Line of Duty" (S02 E15), while Judge Roberta Kittelson, a recurring character on "The Practice", has a featured guest role in the "Ally McBeal" episode "Do you Wanna Dance?"
Most of the primary "Practice" cast members guest starred in the "Ally McBeal" episode "The Inmates" (S01 E20), in a storyline that concluded with the "Practice" episode "Axe Murderer" (S02 E26), featuring Calista Flockhart and Gil Bellows reprising their "Ally" characters. What is unusual about this continuing storyline is that "Ally McBeal" and "The Practice" aired on different networks. Bobby Donnell, the main character of "The Practice" played by Dylan McDermott, was featured heavily in both this crossover and another "Ally McBeal" episode, "These are the Days".
Regular "Practice" cast members Lara Flynn Boyle and Michael Badalucco each had a cameo in "Ally McBeal" (Boyle as a woman who trades insults with Ally in the episode "Making Spirits Bright" and Badalucco as one of Ally's dates in the episode "I Know him by Heart") but it is unclear whether they were playing the same characters they play on "The Practice".
Upon premiering in 1997, the show was an instant hit, averaging around 11 million viewers per episode. The show's second season saw an increase in ratings and soon became a top 20 show, averaging around 13 million viewers per episode. The show's ratings began to decline in the third season, but stabilized in the fourth season after Robert Downey Jr. joined the regular cast as Ally's boyfriend Larry Paul, and a fresher aesthetic was created by new art director Matthew DeCoste. However, Downey's character was written out after the end of the season due to the actor's troubles with drug addiction.
The first two seasons, as well as the fourth, remain the most critically acclaimed and saw the most awards success at the Emmys, SAG Awards and the Golden Globes. In 2007, "Ally McBeal" placed #48 on "Entertainment Weekly" 2007 "New TV Classics" list.
"Ally McBeal" received some criticism from TV critics and feminists who found the title character annoying and demeaning to women (specifically regarding professional women) because of her perceived flightiness, lack of demonstrated legal knowledge, short skirts, and emotional instability. Perhaps the most notorious example of the debate sparked by the show was the June 29, 1998, cover story of "Time" magazine, which juxtaposed McBeal with three pioneering feminists (Susan B. Anthony, Betty Friedan, Gloria Steinem) and asked "Is Feminism Dead?" In episode 12 of the second season of the show, Ally talks to her co-worker John Cage about a dream she had, saying "You know, I had a dream that they put my face on the cover of "Time" magazine as 'the face of feminism'."
"Ally McBeal" was a heavily music-oriented show. Vonda Shepard, a virtually unknown musician at the time, was featured continually on the show. Her song "Searchin' My Soul" became the show's theme song. Many of the songs Shepard performed were established hits with lyrics that paralleled the events of the episode, including "Both Sides Now", "Hooked on a Feeling" and "Tell Him". Besides recording background music for the show, Shepard frequently appeared at the ends of episodes as a musician performing at a local piano bar frequented by the main characters. On rare occasions, her character would have conventional dialogue. A portion of "Searchin' My Soul" was played at the beginning of each episode, but the song was never played in its entirety.
Several of the characters had a musical leitmotif that played when they appeared. John Cage's was "You're the First, the Last, My Everything", Ling Woo's was the Wicked Witch of the West theme from "The Wizard of Oz", and Ally McBeal herself picked "Tell Him", when told by a psychiatrist that she needed a theme.
Due to the popularity of the show and Shepard's music, a soundtrack titled "Songs from Ally McBeal" was released in 1998, as well as a successor soundtrack titled "" in 1999. Two compilation albums from the show featuring Shepard were also released in 2000 and 2001. A Christmas album was also released under the title "Ally McBeal: A Very Ally Christmas". The album received positive reviews, and Shephard's version of Kay Starr's Christmas song "(Everybody's Waitin' For) The Man with the Bag", received considerable airplay during the holiday season.
Other artists featured on the show include Michael Jackson, Barry White, Al Green, Tina Turner, Macy Gray, Gloria Gaynor, Chayanne, Barry Manilow, Anastacia, Elton John, Sting and Mariah Carey. Josh Groban played the role of Malcolm Wyatt in the May 2001 season finale, performing "You're Still You". The series creator, David E. Kelley, was impressed with Groban's performance at The Family Celebration event and based on the audience reaction to Groban's singing, Kelley created a character for him in that finale. The background score for the show was composed by Danny Lux.
Due to music licensing issues, none of the seasons of "Ally McBeal" were available on DVD in the United States (only 6 random episodes could be found on the R1 edition) until 2009, though the show had been available in Italy, Belgium, the Netherlands, Japan, Hong Kong, Portugal, Spain, France, Germany, the United Kingdom, Mexico, Taiwan, Australia, Brazil, and the Czech Republic with all the show's music intact since 2005. In the UK, Ireland, and Spain all seasons are available in a complete box set.
20th Century Fox released the complete first season on DVD in Region 1 on October 6, 2009. They also released a special complete series edition on the same day. Season 1 does not contain any special features, but the complete series set contains several bonus features, including featurettes, an all-new retrospective, the episode of "The Practice" in which Calista Flockhart guest-starred, and a bonus disc entitled "The Best of Ally McBeal Soundtrack." In addition, both releases contain all of the original music. Season 2 was released on April 6, 2010. Seasons 3, 4, and 5 were all released on October 5, 2010.
In 1999, at the height of the show's popularity, a half-hour version entitled "Ally" began airing in parallel with the main program. This version, designed in a sitcom format, used re-edited scenes from the main program, along with previously unseen footage. The intention was to further develop the plots in the comedy-drama in a sitcom style. It also focused only on Ally's personal life, cutting all the courtroom plots. The repackaged show was cancelled partway through its initial run. While 13 episodes of "Ally" were produced, only ten aired.
In episode 2, season 3 of the British comedy "The Adam and Joe Show", the show was parodied as "Ally McSqueal" using soft toys.
Episode 12, season 1 of the show "Futurama", "When Aliens Attack", centers on an invasion of Earth by the Omicronians precipitated by a signal loss during the climax of an episode of "Single Female Lawyer", whose main character is Jenny McNeal.

</doc>
<doc id="1949" url="https://en.wikipedia.org/wiki?curid=1949" title="Andreas Capellanus">
Andreas Capellanus

Andreas Capellanus ("Capellanus" meaning "chaplain"), also known as Andrew the Chaplain, and occasionally by a French translation of his name, André le Chapelain, was the 12th-century author of a treatise commonly known as "De amore" ("About Love"), and often known in English, somewhat misleadingly, as "The Art of Courtly Love", though its realistic, somewhat cynical tone suggests that it is in some measure an antidote to courtly love. Little is known of Andreas Capellanus's life, but he is presumed to have been a courtier of Marie de Champagne, and probably of French origin.
"De Amore" was written at the request of Marie de Champagne, daughter of King Louis VII of France and of Eleanor of Aquitaine. In it, the author informs a young pupil, Walter, of the pitfalls of love. A dismissive allusion in the text to the "wealth of Hungary" has suggested the hypothesis that it was written after 1184, at the time when Bela III of Hungary had sent to the French court a statement of his income and had proposed marriage to Marie's half-sister Marguerite of France, but before 1186, when his proposal was accepted.
"De Amore" is made up of three books. The first book covers the etymology and definition of love and is written in the manner of an academic lecture. The second book consists of sample dialogues between members of different social classes; it outlines how the romantic process between the classes should work. This second work is largely considered to be an inferior to the first. Book three is made of stories from actual courts of love presided over by noble women.
John Jay Parry, the editor of one modern edition of "De Amore", quotes critic Robert Bossuat as describing "De Amore" as "one of those capital works which reflect the thought of a great epoch, which explains the secret of a civilization". It may be viewed as didactic, mocking, or merely descriptive; in any event it preserves the attitudes and practices that were the foundation of a long and significant tradition in Western literature.
The social system of "courtly love", as gradually elaborated by the Provençal troubadours from the mid twelfth century, soon spread. One of the circles in which this poetry and its ethic were cultivated was the court of Eleanor of Aquitaine (herself the granddaughter of an early troubadour poet, William IX of Aquitaine). It has been claimed that "De Amore" codifies the social and sexual life of Eleanor's court at Poitiers between 1170 and 1174, though it was evidently written at least ten years later and, apparently, at Troyes. It deals with several specific themes that were the subject of poetical debate among late twelfth century troubadours and trobairitz.
The meaning of "De Amore" has been debated over the centuries. In the years immediately following its release many people took Andreas’ opinions concerning Courtly Love seriously. In more recent times, however, scholars have come to view the priest's work as satirical. Many scholars now agree that Andreas was commenting on the materialistic, superficial nature of medieval nobles. Andreas seems to have been warning young Walter, his protégé, about love in the Middle Ages.

</doc>
<doc id="1950" url="https://en.wikipedia.org/wiki?curid=1950" title="American Civil Liberties Union">
American Civil Liberties Union

The American Civil Liberties Union (ACLU) is a nonprofit organization founded in 1920 "to defend and preserve the individual rights and liberties guaranteed to every person in this country by the Constitution and laws of the United States". Officially nonpartisan, the organization has been supported and criticized by liberal and conservative organizations alike. The ACLU works through litigation and lobbying and it has over 1,200,000 members and an annual budget of over $300 million. Local affiliates of the ACLU are active in all 50 states, the District of Columbia, and Puerto Rico. The ACLU provides legal assistance in cases when it considers civil liberties to be at risk. Legal support from the ACLU can take the form of direct legal representation or preparation of "amicus curiae" briefs expressing legal arguments when another law firm is already providing representation.
In addition to representing persons and organizations in lawsuits, the ACLU lobbies for policy positions that have been established by its board of directors. Current positions of the ACLU include opposing the death penalty; supporting same-sex marriage and the right of LGBT people to adopt; supporting reproductive rights such as birth control and abortion rights; eliminating discrimination against women, minorities, and LGBT people; decarceration in the United States; supporting the rights of prisoners and opposing torture; and upholding the separation of church and state by opposing government preference for religion over non-religion or for particular faiths over others.
Legally, the ACLU consists of two separate but closely affiliated nonprofit organizations, namely the American Civil Liberties Union, a 501(c)(4) social welfare group; and the ACLU Foundation, a 501(c)(3) public charity. Both organizations engage in civil rights litigation, advocacy, and education, but only donations to the 501(c)(3) foundation are tax deductible, and only the 501(c)(4) group can engage in unlimited political lobbying. The two organizations share office space and employees.
The ACLU was founded in 1920 by a committee including Helen Keller, Roger Nash Baldwin, Crystal Eastman, Walter Nelles, Morris Ernst, Albert DeSilver, Arthur Garfield Hays, Jane Addams, Felix Frankfurter, Elizabeth Gurley Flynn, and Rose Schneiderman. Its focus was on freedom of speech, primarily for anti-war protesters. It was founded in response to the controversial Palmer raids, which saw thousands of radicals arrested in matters which violated their constitutional search and seizures protection. During the 1920s, the ACLU expanded its scope to include protecting the free speech rights of artists and striking workers, and working with the National Association for the Advancement of Colored People (NAACP) to decrease racism and discrimination. During the 1930s, the ACLU started to engage in work combating police misconduct and supporting Native American rights. Many of the ACLU's cases involved the defense of Communist Party members and Jehovah's Witnesses. In 1940, the ACLU leadership voted to exclude communists from its leadership positions, a decision rescinded in 1968. During World War II, the ACLU defended Japanese-American citizens, unsuccessfully trying to prevent their forcible relocation to internment camps. During the Cold War, the ACLU headquarters was dominated by anti-communists, but many local affiliates defended members of the Communist Party.
By 1964, membership had risen to 80,000, and the ACLU participated in efforts to expand civil liberties. In the 1960s, the ACLU continued its decades-long effort to enforce separation of church and state. It defended several anti-war activists during the Vietnam War. The ACLU was involved in the "Miranda" case, which addressed conduct by police during interrogations, and in the "New York Times" case, which established new protections for newspapers reporting on government activities. In the 1970s and 1980s, the ACLU ventured into new legal areas, involving the rights of homosexuals, students, prisoners, and the poor. In the twenty-first century, the ACLU has fought the teaching of creationism in public schools and challenged some provisions of anti-terrorism legislation as infringing on privacy and civil liberties. Fundraising and membership spiked after the 2016 presidential election and the ACLU's current membership is more than 1.2 million.
The ACLU is led by a president and an executive director, Susan N. Herman and Anthony Romero, respectively, in 2015. The president acts as chair of the ACLU's board of directors, leads fundraising, and facilitates policy-setting. The executive director manages the day-to-day operations of the organization. The board of directors consists of 80 persons, including representatives from each state affiliate, as well as at-large delegates. The organization has its headquarters in 125 Broad Street, a 40-story skyscraper located in Lower Manhattan, New York City.
The leadership of the ACLU does not always agree on policy decisions; differences of opinion within the ACLU leadership have sometimes grown into major debates. In 1937, an internal debate erupted over whether to defend Henry Ford's right to distribute anti-union literature. In 1939, a heated debate took place over whether to prohibit communists from serving in ACLU leadership roles. During the early 1950s and Cold War McCarthyism, the board was divided on whether to defend communists. In 1968, a schism formed over whether to represent Benjamin Spock's anti-war activism. In 1973, as the Watergate Scandal continued to unfold, leadership was initially divided over whether to call for President Nixon's impeachment and removal from office. In 2005, there was internal conflict about whether or not a gag rule should be imposed on ACLU employees to prevent publication of internal disputes.
In the year ending March 31, 2014, the ACLU and the ACLU Foundation had a combined income from support and revenue of $100.4 million, originating from grants (50.0%), membership donations (25.4%), donated legal services (7.6%), bequests (16.2%), and revenue (0.9%). Membership dues are treated as donations; members choose the amount they pay annually, averaging approximately $50 per member per year. In the year ending March 31, 2014, the combined expenses of the ACLU and ACLU Foundation were $133.4 million, spent on programs (86.2%), management (7.4%), and fundraising (8.2%). (After factoring in other changes in net assets of +$30.9 million, from sources such as investment income, the organization had an overall decrease in net assets of $2.1 million.) Over the period from 2011 to 2014 the ACLU Foundation, on the average, has accounted for roughly 70% of the combined budget, and the ACLU roughly 30%.
The ACLU solicits donations to its charitable foundation. The ACLU is accredited by the Better Business Bureau, and the Charity Navigator has ranked the ACLU with a four-star rating. The local affiliates solicit their own funding; however, some also receive funds from the national ACLU, with the distribution and amount of such assistance varying from state to state. At its discretion, the national organization provides subsidies to smaller affiliates that lack sufficient resources to be self-sustaining; for example, the Wyoming ACLU chapter received such subsidies until April 2015, when, as part of a round of layoffs at the national ACLU, the Wyoming office was closed.
In October 2004, the ACLU rejected $1.5 million from both the Ford Foundation and Rockefeller Foundation because the foundations had adopted language from the USA PATRIOT Act in their donation agreements, including a clause stipulating that none of the money would go to "underwriting terrorism or other unacceptable activities." The ACLU views this clause, both in federal law and in the donors' agreements, as a threat to civil liberties, saying it is overly broad and ambiguous.
Due to the nature of its legal work, the ACLU is often involved in litigation against governmental bodies, which are generally protected from adverse monetary judgments; a town, state or federal agency may be required to change its laws or behave differently, but not to pay monetary damages except by an explicit statutory waiver. In some cases, the law permits plaintiffs who successfully sue government agencies to collect money damages or other monetary relief. In particular, the Civil Rights Attorney's Fees Award Act of 1976 leaves the government liable in some civil rights cases. Fee awards under this civil rights statute are considered "equitable relief" rather than damages, and government entities are not immune from equitable relief. Under laws such as this, the ACLU and its state affiliates sometimes share in monetary judgments against government agencies. In 2006, the Public Expressions of Religion Protection Act sought to prevent monetary judgments in the particular case of violations of church-state separation.
The ACLU has received court awarded fees from opponents, for example, the Georgia affiliate was awarded $150,000 in fees after suing a county demanding the removal of a Ten Commandments display from its courthouse; a second Ten Commandments case in the state, in a different county, led to a $74,462 judgment. The State of Tennessee was required to pay $50,000, the State of Alabama $175,000, and the State of Kentucky $121,500, in similar Ten Commandments cases.
Most of the organization's workload is performed by its local affiliates. There is at least one affiliate organization in each state, as well as one in Washington, D.C., and in Puerto Rico. California has three affiliates. The affiliates operate autonomously from the national organization; each affiliate has its own staff, executive director, board of directors, and budget. Each affiliate consists of two non-profit corporations: a 501(c)(3) corporation that does not perform lobbying, and a 501(c)(4) corporation which is entitled to lobby.
ACLU affiliates are the basic unit of the ACLU's organization and engage in litigation, lobbying, and public education. For example, in a twenty-month period beginning January 2004, the ACLU's New Jersey chapter was involved in fifty-one cases according to their annual reportthirty-five cases in state courts, and sixteen in federal court. They provided legal representation in thirty-three of those cases, and served as amicus in the remaining eighteen. They listed forty-four volunteer attorneys who assisted them in those cases. 
, the ACLU's official position statements included the following policies:
The ACLU is supported by a variety of persons and organizations. There were over 1,000,000 members in 2017, and the ACLU annually receives thousands of grants from hundreds of charitable foundations. Allies of the ACLU in legal actions have included the National Association for the Advancement of Colored People, the American Jewish Congress, People for the American Way, the National Rifle Association, the Electronic Frontier Foundation, Americans United for Separation of Church and State and the National Organization for Women.
The ACLU has been criticized by liberals such as when it excluded communists from its leadership ranks, when it defended Neo-Nazis, when it declined to defend Paul Robeson, or when it opposed the passage of the National Labor Relations Act. Conversely, it has been criticized by conservatives such as when it argued against official prayer in public schools, or when it opposed the Patriot Act. The ACLU has supported conservative figures such as Rush Limbaugh, George Wallace, Henry Ford and Oliver North as well as liberal figures such as Dick Gregory, Rockwell Kent and Benjamin Spock.
A major source of criticism are legal cases in which the ACLU represents an individual or organization that promotes offensive or unpopular viewpoints such as the Ku Klux Klan, neo-Nazis, the Nation of Islam, the North American Man/Boy Love Association, the Westboro Baptist Church or the Unite the Right rally. As of 2000, the ACLU has historically responded to this criticism by stating "[i]t is easy to defend freedom of speech when the message is something many people find at least reasonable. But the defense of freedom of speech is most critical when the message is one most people find repulsive." However, after the Unite the Right rally on August 17, 2017, the executive director of the ACLU announced that "the ACLU will no longer defend hate groups protesting with firearms."
The ACLU developed from the National Civil Liberties Bureau (CLB), co-founded in 1917 during World War I by Crystal Eastman, an attorney activist, and Roger Nash Baldwin. The focus of the CLB was on freedom of speech, primarily anti-war speech, and on supporting conscientious objectors who did not want to serve in World War I.
Three United States Supreme Court decisions in 1919 each upheld convictions under laws against certain kinds of anti-war speech. In 1919, the Court upheld the conviction of Socialist Party leader Charles Schenck for publishing anti-war literature. In "Debs v. United States," the court upheld the conviction of Eugene Debs. While the Court upheld a conviction a third time in "Abrams v. United States", Justice Oliver Wendell Holmes wrote an important dissent which has gradually been absorbed as an American principle: he urged the court to treat freedom of speech as a fundamental right, which should rarely be restricted.
In 1918, Crystal Eastman resigned from the organization due to health issues. After assuming sole leadership of the CLB, Baldwin insisted that the organization be reorganized. He wanted to change its focus from litigation to direct action and public education.
The CLB directors concurred, and on January 19, 1920, they formed an organization under a new name, the American Civil Liberties Union. Although a handful of other organizations in the United States at that time focused on civil rights, such as the National Association for the Advancement of Colored People (NAACP) and Anti-Defamation League (ADL), the ACLU was the first that did not represent a particular group of persons, or a single theme. Like the CLB, the NAACP pursued litigation to work on civil rights, including efforts to overturn the disfranchisement of African Americans in the South that had taken place since the turn of the century.
During the first decades of the ACLU, Baldwin continued as its leader. His charisma and energy attracted many supporters to the ACLU board and leadership ranks. Baldwin was ascetic, wearing hand-me-down clothes, pinching pennies, and living on a very small salary. The ACLU was directed by an executive committee, and it was not particularly democratic or egalitarian. The ACLU's base in New York resulted in its being dominated by people from the city and state. Most ACLU funding came from philanthropies, such as the Garland Fund.
In the 1920s, government censorship was commonplace. Magazines were routinely confiscated under the anti-obscenity Comstock laws; permits for labor rallies were often denied; and virtually all anti-war or anti-government literature was outlawed. Right-wing conservatives wielded vast amounts of power, and activists that promoted unionization, socialism, or government reform were often denounced as un-American or unpatriotic. In one typical instance in 1923, author Upton Sinclair was arrested for trying to read the First Amendment during an Industrial Workers of the World rally.
ACLU leadership was divided on how to challenge the civil rights violations. One faction, including Baldwin, Arthur Garfield Hays and Norman Thomas, believed that direct, militant action was the best path. Hays was the first of many successful attorneys that relinquished their private practices to work for the ACLU. Another group, including Walter Nelles and Walter Pollak felt that lawsuits taken to the Supreme Court were the best way to achieve change. Both groups worked in tandem, but equally revered the Bill of Rights and the US Constitution.
During the 1920s, the ACLU's primary focus was on freedom of speech in general, and speech within the labor movement particularly. Because most of the ACLU's efforts were associated with the labor movement, the ACLU itself came under heavy attack from conservative groups, such as the American Legion, the National Civic Federation, and Industrial Defense Association and the Allied Patriotic Societies.
In addition to labor, the ACLU also led efforts in non-labor arenas, for example, promoting free speech in public schools. The ACLU itself was banned from speaking in New York public schools in 1921. The ACLU, working with the NAACP, also supported racial discrimination cases. The ACLU defended free speech regardless of the opinions being espoused. For example, the reactionary, anti-Catholic, anti-black Ku Klux Klan (KKK) was a frequent target of ACLU efforts, but the ACLU defended the KKK's right to hold meetings in 1923. There were some civil rights that the ACLU did not make an effort to defend in the 1920s, including censorship of the arts, government search and seizure issues, right to privacy, or wiretapping.
The Communist Party USA was routinely harassed and oppressed by government officials, leading it to be the primary client of the ACLU. At the same time, the Communists were very aggressive in their tactics, often engaging in illegal conduct such as denying their party membership under oath. This led to frequent conflicts between the Communists and ACLU. Communist leaders sometimes attacked the ACLU, particularly when the ACLU defended the free speech rights of conservatives, whereas Communists tried to disrupt speeches by critics of the USSR. This uneasy relationship between the two groups continued for decades.
When 1925 arrived five years after the ACLU was formed the organization had virtually no success to show for its efforts. That changed in 1925, when the ACLU persuaded John T. Scopes to defy Tennessee's anti-evolution law in "The State of Tennessee v. John Thomas Scopes". Clarence Darrow, a member of the ACLU National Committee, headed Scopes' legal team. The prosecution, led by William Jennings Bryan, contended that the Bible should be interpreted literally in teaching creationism in school. The ACLU lost the case and Scopes was fined $100. The Tennessee Supreme Court later upheld the law but overturned the conviction on a technicality.
The Scopes trial was a phenomenal public relations success for the ACLU. The ACLU became well known across America, and the case led to the first endorsement of the ACLU by a major US newspaper. The ACLU continued to fight for the separation of church and state in schoolrooms, decade after decade, including the 1982 case "McLean v. Arkansas" and the 2005 case "Kitzmiller v. Dover Area School District".
Baldwin himself was involved in an important free speech victory of the 1920s, after he was arrested for attempting to speak at a rally of striking mill workers in New Jersey. Although the decision was limited to the state of New Jersey, the appeals court's judgement in 1928 declared that constitutional guarantees of free speech must be given "liberal and comprehensive construction", and it marked a major turning point in the civil rights movement, signaling the shift of judicial opinion in favor of civil rights.
The most important ACLU case of the 1920s was "Gitlow v. New York", in which Benjamin Gitlow was arrested for violating a state law against inciting anarchy and violence, when he distributed literature promoting communism. Although the Supreme Court did not overturn Gitlow's conviction, it adopted the ACLU's stance (later termed the incorporation doctrine) that the First Amendment freedom of speech applied to state laws, as well as federal laws.
After the First World War, many native-born Americans had a revival of concerns about assimilation of immigrants and worries about "foreign" values; they wanted public schools to teach children to be American. Numerous states drafted laws designed to use schools to promote a common American culture, and in 1922, the voters of Oregon passed the Oregon Compulsory Education Act. The law was primarily aimed at eliminating parochial schools, including Catholic schools. It was promoted by groups such as the Knights of Pythias, the Federation of Patriotic Societies, the Oregon Good Government League, the Orange Order, and the Ku Klux Klan.
The Oregon Compulsory Education Act required almost all children in Oregon between eight and sixteen years of age to attend public school by 1926. Associate Director Roger Nash Baldwin, a personal friend of Luke E. Hart, the then–Supreme Advocate and future Supreme Knight of the Knights of Columbus, offered to join forces with the Knights to challenge the law. The Knights of Columbus pledged an immediate $10,000 to fight the law and any additional funds necessary to defeat it.
The case became known as "Pierce v. Society of Sisters", a seminal United States Supreme Court decision that significantly expanded coverage of the Due Process Clause in the Fourteenth Amendment. In a unanimous decision, the court held that the act was unconstitutional and that parents, not the state, had the authority to educate children as they thought best. It upheld the religious freedom of parents to educate their children in religious schools.
Leaders of the ACLU were divided on the best tactics to use to promote civil liberties. Felix Frankfurter felt that legislation was the best long-term solution, because the Supreme Court could not (and in his opinion should not) mandate liberal interpretations of the Bill of Rights. But Walter Pollack, Morris Ernst, and other leaders felt that Supreme Court decisions were the best path to guarantee civil liberties. A series of Supreme Court decisions in the 1920s foretold a changing national atmosphere; anti-radical emotions were diminishing, and there was a growing willingness to protect freedom of speech and assembly via court decisions.
Censorship was commonplace in the early 20th century. State laws and city ordinances routinely outlawed speech deemed to be obscene or offensive, and prohibited meetings or literature that promoted unions or labor organization. Starting in 1926, the ACLU began to expand its free speech activities to encompass censorship of art and literature. In that year, H. L. Mencken deliberately broke Boston law by distributing copies of his banned "American Mercury" magazine; the ACLU defended him and won an acquittal. The ACLU went on to win additional victories, including the landmark case "United States v. One Book Called Ulysses" in 1933, which reversed a ban by the Customs Department against the book "Ulysses" by James Joyce. The ACLU only achieved mixed results in the early years, and it was not until 1966 that the Supreme Court finally clarified the obscenity laws in the "Roth v. United States" and "Memoirs v. Massachusetts" cases.
The Comstock laws banned distribution of sex education information, based on the premise that it was obscene and led to promiscuous behavior Mary Ware Dennett was fined $300 in 1928, for distributing a pamphlet containing sex education material. The ACLU, led by Morris Ernst, appealed her conviction and won a reversal, in which judge Learned Hand ruled that the pamphlet's main purpose was to "promote understanding".
The success prompted the ACLU to broaden their freedom of speech efforts beyond labor and political speech, to encompass movies, press, radio and literature. The ACLU formed the National Committee on Freedom from Censorship in 1931 to coordinate this effort. By the early 1930s, censorship in the United States was diminishing.
Two major victories in the 1930s cemented the ACLUs campaign to promote free speech. In "Stromberg v. California", decided in 1931, the Supreme Court sided with the ACLU and affirmed the right of a communist party member to salute a communist flag. The result was the first time the Supreme Court used the Due Process Clause of the 14th amendment to subject states to the requirements of the First Amendment. In "Near v. Minnesota", also decided in 1931, the Supreme Court ruled that states may not exercise prior restraint and prevent a newspaper from publishing, simply because the newspaper had a reputation for being scandalous.
The late 1930s saw the emergence of a new era of tolerance in the United States. National leaders hailed the Bill of Rights, particularly as it protected minorities, as the essence of democracy. The 1939 Supreme Court decision in "Hague v. Committee for Industrial Organization" affirmed the right of communists to promote their cause. Even conservative elements, such as the American Bar Association began to campaign for civil liberties, which were long considered to be the domain of left-leaning organizations. By 1940, the ACLU had achieved many of the goals it set in the 1920s, and many of its policies were the law of the land.
In 1929, after the Scopes and Dennett victories, Baldwin perceived that there was vast, untapped support for civil liberties in the United States. Baldwin proposed an expansion program for the ACLU, focusing on police brutality, Native American rights, African American rights, censorship in the arts, and international civil liberties. The board of directors approved Baldwin's expansion plan, except for the international efforts.
The ACLU played a major role in passing the 1932 Norris–La Guardia Act, a federal law which prohibited employers from preventing employees from joining unions, and stopped the practice of outlawing strikes, unions, and labor organizing activities with the use of injunctions. The ACLU also played a key role in initiating a nationwide effort to reduce misconduct (such as extracting false confessions) within police departments, by publishing the report "Lawlessness in Law Enforcement" in 1931, under the auspices of Herbert Hoover's Wickersham Commission. In 1934, the ACLU lobbied for the passage of the Indian Reorganization Act, which restored some autonomy to Native American tribes, and established penalties for kidnapping Native American children.
Although the ACLU deferred to the NAACP for litigation promoting civil liberties for African Americans, the ACLU did engage in educational efforts, and published "Black Justice" in 1931, a report which documented institutional racism throughout the South, including lack of voting rights, segregation, and discrimination in the justice system. Funded by the Garland Fund, the ACLU also participated in producing the influential Margold Report, which outlined a strategy to fight for civil rights for blacks. The ACLU's plan was to demonstrate that the "separate but equal" policies governing the Southern discrimination were illegal because blacks were never, in fact, treated equally.
In 1932twelve years after the ACLU was foundedit had achieved significant success; the Supreme Court had embraced the free speech principles espoused by the ACLU, and the general public was becoming more supportive of civil rights in general. But the Great Depression brought new assaults on civil liberties; the year 1930 saw a large increase in the number of free speech prosecutions, a doubling of the number of lynchings, and all meetings of unemployed persons were banned in Philadelphia.
The Franklin D. Roosevelt administration proposed the New Deal to combat the depression. ACLU leaders were of mixed opinions about the New Deal, since many felt that it represented an increase in government intervention into personal affairs, and because the National Recovery Administration suspended antitrust legislation. Roosevelt was not personally interested in civil rights, but did appoint many civil libertarians to key positions, including Interior Secretary Harold Ickes, a member of the ACLU.
The economic policies of the New Deal leaders were often aligned with ACLU goals, but social goals were not. In particular, movies were subject to a barrage of local ordinances banning screenings that were deemed immoral or obscene. Even public health films portraying pregnancy and birth were banned; as was "Life" magazine's April 11, 1938, issue which included photos of the birth process. The ACLU fought these bans, but did not prevail.
The Catholic Church attained increasing political influence in the 1930s, and used its influence to promote censorship of movies, and to discourage publication of birth control information. This conflict between the ACLU and the Catholic Church led to the resignation of the last Catholic priest from ACLU leadership in 1934; a Catholic priest would not be represented there again until the 1970s.
The ACLU took no official position on president Franklin Delano Roosevelt's 1937 court-packing plan, which threatened to increase the number of Supreme Court justices, unless the Supreme Court reversed its course and began approving New Deal legislation. The Supreme Court responded by making a major shift in policy, and no longer applied strict constitutional limits to government programs, and also began to take a more active role in protecting civil liberties.
The first decision that marked the court's new direction was "De Jonge v. Oregon", in which a communist labor organizer was arrested for calling a meeting to discuss unionization. The ACLU attorney Osmond Fraenkel, working with International Labor Defense, defended De Jonge in 1937, and won a major victory when the Supreme Court ruled that "peaceable assembly for lawful discussion cannot be made a crime." The De Jonge case marked the start of an era lasting for a dozen years, during which Roosevelt appointees (led by Hugo Black, William O. Douglas, and Frank Murphy) established a body of civil liberties law. In 1938, Justice Harlan F. Stone wrote the famous "footnote four" in "United States v. Carolene Products Co." in which he suggested that state laws which impede civil liberties wouldhenceforthrequire compelling justification.
Senator Robert F. Wagner proposed the National Labor Relations Act in 1935, which empowered workers to unionize. Ironically, the ACLU, after 15 years of fighting for workers' rights, initially opposed the act (it later took no stand on the legislation) because some ACLU leaders feared the increased power the bill gave to the government. The newly formed National Labor Relations Board (NLRB) posed a dilemma for the ACLU, because in 1937 it issued an order to Henry Ford, prohibiting Ford from disseminating anti-union literature. Part of the ACLU leadership habitually took the side of labor, and that faction supported the NLRB's action. But part of the ACLU supported Ford's right to free speech. ACLU leader Arthur Garfield Hays proposed a compromise (supporting the auto workers union, yet also endorsing Ford's right to express personal opinions), but the schism highlighted a deeper divide that would become more prominent in the years to come.
The ACLU's support of the NLRB was a major development for the ACLU, because it marked the first time it accepted that a government agency could be responsible for upholding civil liberties. Until 1937, the ACLU felt that civil rights were best upheld by citizens and private organizations.
Some factions in the ACLU proposed new directions for the organization. In the late 1930s, some local affiliates proposed shifting their emphasis from civil liberties appellate actions, to becoming a legal aid society, centered on store front offices in low income neighborhoods. The ACLU directors rejected that proposal. Other ACLU members wanted the ACLU to shift focus into the political arena, and to be more willing to compromise their ideals in order to strike deals with politicians. This initiative was also rejected by the ACLU leadership.
The ACLU's support of defendants with unpopular, sometimes extreme, viewpoints have produced many landmark court cases and established new civil liberties. One such defendant was the Jehovah's Witnesses, who were involved in a large number of Supreme Court cases. Cases that the ACLU supported included "Lovell v. City of Griffin" (which struck down a city ordinance that required a permit before a person could distribute "literature of any kind"); "Martin v. Struthers" (which struck down an ordinance prohibiting door-to-door canvassing); and "Cantwell v. Connecticut" (which reversed the conviction of a Witness who was reciting offensive speech on a street corner).
The most important cases involved statutes requiring flag salutes. The Jehovah's Witnesses felt that saluting a flag was contrary to their religious beliefs. Two children were convicted in 1938 of not saluting the flag. The ACLU supported their appeal to the Supreme Court, but the court affirmed the conviction, in 1940. But three years later, in "West Virginia State Board of Education v. Barnette", the Supreme court reversed itself and wrote "If there is any fixed star in our constitutional constellation, it is that no official, high or petty, can prescribe what shall be orthodox in politics, nationalism, religion, or other matters of opinion or force citizens to confess by word or act their faith therein." To underscore its decision, the Supreme Court announced it on Flag Day.
The rise of totalitarian regimes in Germany, Russia, and other countries who rejected freedom of speech and association had a large impact on the civil liberties movement in the US; anti-Communist sentiment rose and civil liberties were curtailed.
The ACLU leadership was divided over whether or not to defend pro-Nazi speech in the United States; pro-labor elements within the ACLU were hostile towards Nazism and fascism, and objected when the ACLU defended Nazis. Several states passed laws outlawing the hate speech directed at ethnic groups. The first person arrested under New Jersey's 1935 hate speech law was a Jehovah's Witness who was charged with disseminating anti-Catholic literature. The ACLU defended the Jehovah's Witnesses, and the charges were dropped. The ACLU proceeded to defend numerous pro-Nazi groups, defending their rights to free speech and free association.
In the late 1930s, the ACLU allied itself with the Popular Front, a coalition of liberal organizations coordinated by the United States Communist Party. The ACLU benefited because affiliates from the Popular Front could often fight local civil rights battles much more effectively than the New York-based ACLU. The association with the Communist Party led to accusations that the ACLU was a "Communist front", particularly because Harry F. Ward was both chairman of the ACLU and chairman of the American League Against War and Fascism, a Communist organization.
The House Un-American Activities Committee (HUAC) was created in 1938 to uncover sedition and treason within the United States. When witnesses testified at its hearings, the ACLU was mentioned several times, leading the HUAC to mention the ACLU prominently in its 1939 report. This damaged the ACLU's reputation severely, even though the report said that it could not "definitely state whether or not" the ACLU was a Communist organization.
While the ACLU rushed to defend its image against allegations of being a Communist front, it also worked to protect witnesses who were being harassed by the HUAC. The ACLU was one of the few organizations to protest (unsuccessfully) against passage of the Smith Act in 1940, which would later be used to imprison many persons who supported Communism. The ACLU defended many persons who were prosecuted under the Smith Act, including labor leader Harry Bridges.
ACLU leadership was split on whether to purge its leadership of Communists. Norman Thomas, John Haynes Holmes, and Morris Ernst were anti-Communists who wanted to distance the ACLU from Communism; opposing them were Harry F. Ward, Corliss Lamont, and Elizabeth Gurley Flynn, who rejected any political test for ACLU leadership. A bitter struggle ensued throughout 1939, and the anti-Communists prevailed in February 1940, when the board voted to prohibit anyone who supported totalitarianism from ACLU leadership roles. Ward immediately resigned, andfollowing a contentious six-hour debateFlynn was voted off the ACLU's board. The 1940 resolution was considered by many to be a betrayal of its fundamental principles. The resolution was rescinded in 1968, and Flynn was posthumously reinstated to the ACLU in 1970.
When World War II engulfed the United States, the Bill of Rights was enshrined as a hallowed document, and numerous organizations defended civil liberties. Chicago and New York proclaimed "Civil Rights" weeks, and President Franklin Delano Roosevelt announced a national Bill of Rights day. Eleanor Roosevelt was the keynote speaker at the 1939 ACLU convention. In spite of this newfound respect for civil rights, Americans were becoming adamantly anti-communist, and believed that excluding communists from American society was an essential step to preserve democracy.
Contrasted with World War I, there was relatively little violation of civil liberties during World War II. President Roosevelt was a strong supporter of civil liberties, butmore importantlythere were few anti-war activists during World War II. The most significant exception was the internment of Japanese Americans.
Two months after the Japanese attack on Pearl Harbor, Roosevelt authorized the creation of military "exclusion zones" with Executive Order 9066, paving the way for the detention of all West Coast Japanese Americans in inland camps. In addition to the non-citizen Issei (prohibited from naturalization as members of an "unassimilable" race), over two-thirds of those swept up were American-born citizens. The ACLU immediately protested to Roosevelt, comparing the evacuations to Nazi concentration camps. The ACLU was the only major organization to object to the internment plan, and their position was very unpopular, even within the organization. Not all ACLU leaders wanted to defend the Japanese Americans; Roosevelt loyalists such as Morris Ernst wanted to support Roosevelt's war effort, but pacifists such as Baldwin and Norman Thomas felt that Japanese Americans needed access to due process before they could be imprisoned. In a March 20, 1942, letter to Roosevelt, Baldwin called on the administration to allow Japanese Americans to prove their loyalty at individual hearings, describing the constitutionality of the planned removal "open to grave question." His suggestions went nowhere, and opinions within the organization became increasingly divided as the Army began the "evacuation" of the West Coast. In May, the two factions, one pushing to fight the exclusion orders then being issued, the other advocating support for the President's policy of removing citizens whose "presence may endanger national security," brought their opposing resolutions to a vote before the board and the ACLU's national leaders. They decided not to challenge the eviction of Japanese American citizens, and on June 22 instructions were sent to West Coast branches not to support cases that argued the government had no constitutional right to do so.
The ACLU offices on the West Coast had been more directly involved in addressing the tide of anti-Japanese prejudice from the start, as they were geographically closer to the issue, and were already working on cases challenging the exclusion by this time. The Seattle office, assisting in Gordon Hirabayashi's lawsuit, created an unaffiliated committee to continue the work the ACLU had started, while in Los Angeles, attorney A.L. Wirin continued to represent Ernest Kinzo Wakayama but without addressing the case's constitutional questions. (Wirin would lose private clients because of his defense of Wakayama and other Japanese Americans.) However, the San Francisco branch, led by Ernest Besig, refused to discontinue its support for Fred Korematsu, whose case had been taken on prior to the June 22 directive, and attorney Wayne Collins, with Besig's full support, centered his defense on the illegality of Korematsu's exclusion.
The West Coast offices had wanted a test case to take to court, but had a difficult time finding a Japanese American who was both willing to violate the internment orders and able to meet the ACLU's desired criteria of a sympathetic, Americanized plaintiff. Of the 120,000 Japanese Americans affected by the order, only 12 disobeyed, and Korematsu, Hirabayashi, and two others were the only resisters whose cases eventually made it to the Supreme Court. "Hirabayashi v. United States" came before the Court in May 1943, and the justices upheld the government's right to exclude Japanese Americans from the West Coast; although it had earlier forced its local office in L.A. to stop aiding Hirabayashi, the ACLU donated $1,000 to the case (over a third of the legal team's total budget) and submitted an "amicus" brief. Besig, dissatisfied with Osmond Fraenkel's tamer defense, filed an additional "amicus" brief that directly addressed Hirabayashi's constitutional rights. In the meantime, A.L. Wirin served as one of the attorneys in "Yasui v. United States" (decided the same day as the Hirabayashi case, and with the same results), but he kept his arguments within the perimeters established by the national office. The only case to receive a favorable ruling, "ex parte Endo", was also aided by two "amicus" briefs from the ACLU, one from the more conservative Fraenkel and another from the more putative Wayne Collins.
"Korematsu v. United States" proved to be the most controversial of these cases, as Besig and Collins refused to bow to the national ACLU office's pressure to pursue the case without challenging the government's right to remove citizens from their homes. The ACLU board threatened to revoke the San Francisco branch's national affiliation, while Baldwin tried unsuccessfully to convince Collins to step down so he could replace him as lead attorney in the case. Eventually Collins agreed to present the case alongside Charles Horsky, although their arguments before the Supreme Court remained based in the unconstitutionality of the exclusion order Korematsu had disobeyed. The case was decided in December 1944, when the Court once again upheld the government's right to relocate Japanese Americans, although Korematsu's, Hirabayashi's and Yasui's convictions were later overturned in "coram nobis" proceedings in the 1980s.
The national office of the ACLU was even more reluctant to defend anti-war protesters. A majority of the board passed a resolution in 1942 which declared the ACLU unwilling to defend anyone who interfered with the United States' war effort. Included in this group were the thousands of Nisei who renounced their US citizenship during the war but later regretted the decision and tried to revoke their applications for "repatriation." (A significant number of those slated to "go back" to Japan had never actually been to the country and were in fact being deported rather than repatriated.) Ernest Besig had in 1944 visited the Tule Lake Segregation Center, where the majority of these "renunciants" were concentrated, and subsequently enlisted Wayne Collins' help to file a lawsuit on their behalf, arguing the renunciations had been given under duress. The national organization prohibited local branches from representing the renunciants, forcing Collins to pursue the case on his own, although Besig and the Northern California office provided some support.
During his 1944 visit to Tule Lake, Besig had also became aware of a hastily constructed stockade in which Japanese American internees were routinely being brutalized and held for months without due process. Besig was forbidden by the national ACLU office to intervene on behalf of the stockade prisoners or even to visit the Tule Lake camp without prior written approval from Baldwin. Unable to help directly, Besig turned to Wayne Collins for assistance. Collins, using the threat of habeas corpus suits managed to have the stockade closed down. A year later, after learning that the stockade had been reestablished, he returned to the camp and had it closed down for good.
When the war ended in 1945, the ACLU was 25 years old, and had accumulated an impressive set of legal victories. President Harry S. Truman sent a congratulatory telegram to the ACLU on the occasion of their 25th anniversary. American attitudes had changed since World War I, and dissent by minorities was tolerated with more willingness. The Bill of Rights was more respected, and minority rights were becoming more commonly championed. During their 1945 annual conference, the ACLU leaders composed a list of important civil rights issues to focus on in the future, and the list included racial discrimination and separation of church and state.
The ACLU supported the African-American defendants in "Shelley v. Kraemer", when they tried to occupy a house they had purchased in a neighborhood which had racially restrictive housing covenants. The African-American purchasers won the case in 1945.
Anti-Communist sentiment gripped the United States during the Cold War beginning in 1946. Federal investigations caused many persons with Communist or left-leaning affiliations to lose their jobs, become blacklisted, or be jailed. During the Cold War, although the United States collectively ignored the civil rights of Communists, other civil libertiessuch as due process in law and separation of church and statecontinued to be reinforced and even expanded.
The ACLU was internally divided when it purged Communists from its leadership in 1940, and that ambivalence continued as it decided whether to defend alleged Communists during the late 1940s. Some ACLU leaders were anti-Communist, and felt that the ACLU should not defend any victims. Some ACLU leaders felt that Communists were entitled to free speech protections, and the ACLU should defend them. Other ACLU leaders were uncertain about the threat posed by Communists, and tried to establish a compromise between the two extremes. This ambivalent state of affairs would last until 1954, when the civil liberties faction prevailed, leading to the resignation of most of the anti-Communist leaders.
In 1947, President Truman issued Executive Order 9835, which created the Federal Loyalty Program. This program authorized the Attorney General to create a list of organizations which were deemed to be subversive. Any association with these programs was ground for barring the person from employment. Listed organizations were not notified that they were being considered for the list, nor did they have an opportunity to present counterarguments; nor did the government divulge any factual basis for inclusion in the list. Although ACLU leadership was divided on whether to challenge the Federal Loyalty Program, some challenges were successfully made.
Also in 1947, the House Un-American Activities Committee (HUAC) subpoenaed ten Hollywood directors and writers, the "Hollywood Ten", intending to ask them to identify Communists, but the witnesses refused to testify. All were imprisoned for contempt of Congress. The ACLU supported the appeals of several of the artists, but lost on appeal. The Hollywood establishment panicked after the HUAC hearings, and created a blacklist which prohibited anyone with leftist associations from working. The ACLU supported legal challenges to the blacklist, but those challenges failed. The ACLU was more successful with an education effort; the 1952 report "The Judges and the Judged", prepared at the ACLU's direction in response to the blacklisting of actress Jean Muir, described the unfair and unethical actions behind the blacklisting process, and it helped gradually turn public opinion against McCarthyism.
The federal government took direct aim at the US Communist Party in 1948 when it indicted its top twelve leaders in the Foley Square trial. The case hinged on whether or not mere membership in a totalitarian political party was sufficient to conclude that members advocated the overthrow of the United States government. The ACLU chose to not represent any of the defendants, and they were all found guilty and sentenced to three to five years in prison. Their defense attorneys were all cited for contempt, went to prison and were disbarred. When the government indicted additional party members, the defendants could not find attorneys to represent them. Communists protested outside the courthouse; a bill to outlaw picketing of courthouses was introduced in Congress, and the ACLU supported the anti-picketing law.
The ACLU, in a change of heart, supported the party leaders during their appeal process. The Supreme Court upheld the convictions in the "Dennis v. United States" decision by softening the free speech requirements from a "clear and present danger" test, to a "grave and probable" test. The ACLU issued a public condemnation of the "Dennis" decision, and resolved to fight it. One reason for the Supreme Court's support of Cold War legislation was the 1949 deaths of Supreme Court justices Frank Murphy and Wiley Rutledge, leaving Hugo Black and William O. Douglas as the only remaining civil libertarians on the Court.
The "Dennis" decision paved the way for the prosecution of hundreds of other Communist party members. The ACLU supported many of the Communists during their appeals (although most of the initiative originated with local ACLU affiliates, not the national headquarters) but most convictions were upheld. The two California affiliates, in particular, felt the national ACLU headquarters was not supporting civil liberties strongly enough, and they initiated more cold war cases than the national headquarters did.
The ACLU also challenged many loyalty oath requirements across the country, but the courts upheld most of the loyalty oath laws. California ACLU affiliates successfully challenged the California state loyalty oath. The Supreme Court, until 1957, upheld nearly every law which restricted the liberties of Communists.
The ACLU, even though it scaled back its defense of Communists during the Cold War, still came under heavy criticism as a "front" for Communism. Critics included the American Legion, Senator Joseph McCarthy, the HUAC, and the FBI. Several ACLU leaders were sympathetic to the FBI, and as a consequence, the ACLU rarely investigated any of the many complaints alleging abuse of power by the FBI during the Cold War.
In 1950, Raymond L. Wise, ACLU board member 1933–1951, defended William Perl, one of the other spies embroiled in the atomic espionage cases (made famous by the execution of Julius Rosenberg and Ethel Rosenberg).
In 1950, the ACLU board of directors asked executive director Baldwin to resign, feeling that he lacked the organizational skills to lead the 9,000 (and growing) member organization. Baldwin objected, but a majority of the board elected to remove him from the position, and he was replaced by Patrick Murphy Malin. Under Malin's guidance, membership tripled to 30,000 by 1955the start of a 24-year period of continual growth leading to 275,000 members in 1974. Malin also presided over an expansion of local ACLU affiliates.
The ACLU, which had been controlled by an elite of a few dozen New Yorkers, became more democratic in the 1950s. In 1951, the ACLU amended its bylaws to permit the local affiliates to participate directly in voting on ACLU policy decisions. A bi-annual conference, open to the entire membership, was instituted in the same year, and in later decades it became a pulpit for activist members, who suggested new directions for the ACLU, including abortion rights, death penalty, and rights of the poor.
During the early 1950s, the ACLU continued to steer a moderate course through the Cold War. When leftist singer Paul Robeson was denied a passport in 1950, even though he was not accused of any illegal acts, the ACLU chose to not defend him. The ACLU later reversed their stance, and supported William Worthy and Rockwell Kent in their passport confiscation cases, which resulted in legal victories in the late 1950s.
In response to communist witch-hunts, many witnesses and employees chose to use the fifth amendment protection against self-incrimination to avoid divulging information about their political beliefs. Government agencies and private organizations, in response, established policies which inferred communist party membership for anyone who invoked the fifth amendment. The national ACLU was divided on whether to defend employees who had been fired merely for pleading the fifth amendment, but the New York affiliate successfully assisted teacher Harry Slochower in his Supreme Court case which reversed his termination.
The fifth amendment issue became the catalyst for a watershed event in 1954, which finally resolved the ACLU's ambivalence by ousting the anti-communists from ACLU leadership. In 1953, the anti-communists, led by Norman Thomas and James Fly, proposed a set of resolutions that inferred guilt of persons that invoked the fifth amendment. These resolutions were the first that fell under the ACLU's new organizational rules permitting local affiliates to participate in the vote; the affiliates outvoted the national headquarters, and rejected the anti-communist resolutions. Anti-communists leaders refused to accept the results of the vote, and brought the issue up for discussion again at the 1954 bi-annual convention. ACLU member Frank Graham, president of the University of North Carolina, attacked the anti-communists with a counter-proposal, which stated that the ACLU "stand[s] against guilt by association, judgment by accusation, the invasion of privacy of personal opinions and beliefs, and the confusion of dissent with disloyalty." The anti-communists continued to battle Graham's proposal, but were outnumbered by the affiliates. The anti-communists finally gave up and departed the board of directors in late 1954 and 1955, ending an eight-year reign of ambivalence within the ACLU leadership ranks. Thereafter, the ACLU proceeded with firmer resolve against Cold War anti-communist legislation. The period from the 1940 resolution (and the purge of Elizabeth Flynn) to the 1954 resignation of the anti-communist leaders is considered by many to be an era in which the ACLU abandoned its core principles.
McCarthyism declined in late 1954 after television journalist Edward R. Murrow and others publicly chastised McCarthy. The controversies over the Bill of Rights that were generated by the Cold War ushered in a new era in American Civil liberties. In 1954, in "Brown v. Board of Education", the Supreme Court unanimously overturned state-sanctioned school segregation, and thereafter a flood of civil rights victories dominated the legal landscape.
The Supreme Court handed the ACLU two key victories in 1957, in "Watkins v. United States" and "Yates v. United States", both of which undermined the Smith Act and marked the beginning of the end of communist party membership inquiries. In 1965, the Supreme Court produced some decisions, including "Lamont v. Postmaster General" (in which the plaintiff was Corliss Lamont, a former ACLU board member), which upheld fifth amendment protections and brought an end to restrictions on political activity.
The decade from 1954 to 1964 was the most successful period in the ACLU's history. Membership rose from 30,000 to 80,000, and by 1965 it had affiliates in seventeen states. During the ACLU's bi-annual conference in Colorado in 1964, the Supreme Court issued rulings on eight cases in which the ACLU was involved; the ACLU prevailed on seven of the eight. The ACLU played a role in Supreme Court decisions reducing censorship of literature and arts, protecting freedom of association, prohibiting racial segregation, excluding religion from public schools, and providing due process protection to criminal suspects. The ACLU's success arose from changing public attitudes; the American populace was more educated, more tolerant, and more willing to accept unorthodox behavior.
Legal battles concerning the separation of church and state originated in laws dating to 1938 which required religious instruction in school, or provided state funding for religious schools. The Catholic church was a leading proponent of such laws; and the primary opponents (the "separationists") were the ACLU, Americans United for Separation of Church and State, and the American Jewish Congress. The ACLU led the challenge in the 1947 "Everson v. Board of Education" case, in which Justice Hugo Black wrote "[t]he First Amendment has erected a wall between church and state... That wall must be kept high and impregnable." It was not clear that the Bill of Rights forbid state governments from supporting religious education, and strong legal arguments were made by religious proponents, arguing that the Supreme Court should not act as a "national school board", and that the Constitution did not govern social issues. However, the ACLU and other advocates of church/state separation persuaded the Court to declare such activities unconstitutional. Historian Samuel Walker writes that the ACLU's "greatest impact on American life" was its role in persuading the Supreme Court to "constitutionalize" so many public controversies.
In 1948, the ACLU prevailed in the "McCollum v. Board of Education" case, which challenged public school religious classes taught by clergy paid for from private funds. The ACLU also won cases challenging schools in New Mexico which were taught by clergy and had crucifixes hanging in the classrooms. In the 1960s, the ACLU, in response to member insistence, turned its attention to in-class promotion of religion. In 1960, 42 percent of American schools included Bible reading. In 1962, the ACLU published a policy statement condemning in-school prayers, observation of religious holidays, and Bible reading. The Supreme Court concurred with the ACLU's position, when it prohibited New York's in-school prayers in the 1962 "Engel v. Vitale" decision. Religious factions across the country rebelled against the anti-prayer decisions, leading them to propose the School Prayer Constitutional Amendment, which declared in-school prayer legal. The ACLU participated in a lobbying effort against the amendment, and the 1966 congressional vote on the amendment failed to obtain the required two-thirds majority.
However, not all cases were victories; ACLU lost cases in 1949 and 1961 which challenged state laws requiring commercial businesses to close on Sunday, the Christian Sabbath. The Supreme Court has never overturned such laws, although some states subsequently revoked many of the laws under pressure from commercial interests.
During the 1940s and 1950s, the ACLU continued its battle against censorship of art and literature. In 1948, the New York affiliate of the ACLU received mixed results from the Supreme Court, winning the appeal of Carl Jacob Kunz, who was convicted for speaking without a police permit, but losing the appeal of Irving Feiner who was arrested to prevent a breach of the peace, based on his oration denouncing president Truman and the American Legion. The ACLU lost the case of Joseph Beauharnais, who was arrested for group libel when he distributed literature impugning the character of African Americans.
Cities across America routinely banned movies because they were deemed to be "harmful", "offensive", or "immoral"censorship which was validated by the 1915 "Mutual v. Ohio" Supreme Court decision which held movies to be mere commerce, undeserving of first amendment protection. The film "The Miracle" was banned in New York in 1951, at the behest of the Catholic Church, but the ACLU supported the film's distributor in an appeal of the ban, and won a major victory in the 1952 decision "Joseph Burstyn, Inc. v. Wilson". The Catholic Church led efforts throughout the 1950s attempting to persuade local prosecutors to ban various books and movies, leading to conflict with the ACLU when the ACLU published it statement condemning the church's tactics. Further legal actions by the ACLU successfully defended films such as "M" and "la Ronde", leading the eventual dismantling of movie censorship. Hollywood continued employing self-censorship with its own Production Code, but in 1956 the ACLU called on Hollywood to abolish the Code.
The ACLU defended beat generation artists, including Allen Ginsberg who was prosecuted for his poem "Howl"; andin an unorthodox case the ACLU helped a coffee house regain its restaurant license which was revoked because its Beat customers were allegedly disturbing the peace and quiet of the neighborhood.
The ACLU lost an important press censorship case when, in 1957, the Supreme Court upheld the obscenity conviction of publisher Samuel Roth for distributing adult magazines. As late as 1953, books such as "Tropic of Cancer" and "From Here to Eternity" were still banned. But public standards rapidly became more liberal though the 1960s, and obscenity was notoriously difficult to define, so by 1971 prosecutions for obscenity had halted.
A major aspect of civil liberties progress after World War II was the undoing centuries of racism in federal, state, and local governments an effort generally associated with the civil rights movement. Several civil liberties organizations worked together for progress, including the National Association for the Advancement of Colored People (NAACP), the ACLU, and the American Jewish Congress. The NAACP took primary responsibility for Supreme Court cases (often led by lead NAACP attorney Thurgood Marshall), with the ACLU focusing on police misconduct, and supporting the NAACP with amicus briefs. The NAACP achieved a key victory in 1950 with the "Henderson v. United States" decision that ended segregation in interstate bus and rail transportation.
In 1954, the ACLU filed an amicus brief in the case of "Brown v. Board of Education", which led to the ban on racial segregation in US public schools. Southern states instituted a McCarthyism-style witch-hunt against the NAACP, attempting it to disclose membership lists. The ACLU's fight against racism was not limited to segregation; in 1964 the ACLU provided key support to plaintiffs, primarily lower income urban residents, in "Reynolds v. Sims", which required states to establish the voting districts in accordance with the "one person, one vote" principle.
The ACLU regularly tackled police misconduct issues, starting with the 1932 case "Powell v. Alabama" (right to an attorney), and including 1942's "Betts v. Brady" (right to an attorney), and 1951's "Rochin v. California" (involuntary stomach pumping). In the late 1940s, several ACLU local affiliates established permanent committees to address policing issues. During the 1950s and 1960s, the ACLU was responsible for substantially advancing the legal protections against police misconduct. The Philadelphia affiliate was responsible for causing the City of Philadelphia, in 1958, to create the nation's first civilian police review board. In 1959, the Illinois affiliate published the first report in the nation, "Secret Detention by the Chicago Police", which documented unlawful detention by police.
Some of the most well known ACLU successes came in the 1960s, when the ACLU prevailed in a string of cases limiting the power of police to gather evidence; in 1961's "Mapp v. Ohio", the Supreme court required states to obtain a warrant before searching a person's home. The "Gideon v. Wainwright" decision in 1963 provided legal representation to indigents. In 1964, the ACLU persuaded the Court, in "Escobedo v. Illinois", to permit suspects to have an attorney present during questioning. And, in 1966, "Miranda v. Arizona" federal decision required police to notify suspects of their constitutional rights, which was later extended to juveniles in the following year's "in re Gault" (1967) federal ruling. Although many law enforcement officials criticized the ACLU for expanding the rights of suspects, police officers themselves took advantage of the ACLU. For example, when the ACLU represented New York City policemen in their lawsuit which objected to searches of their workplace lockers. In the late 1960s, civilian review boards in New York City and Philadelphia were abolished, over the ACLU's objection.
The 1960s was a tumultuous era in the United States, and public interest in civil liberties underwent an explosive growth. Civil liberties actions in the 1960s were often led by young people, and often employed tactics such as sit ins and marches. Protests were often peaceful, but sometimes employed militant tactics. The ACLU played a central role in all major civil liberties debates of the 1960s, including new fields such as gay rights, prisoner's rights, abortion, rights of the poor, and the death penalty. Membership in the ACLU increased from 52,000 at the beginning of the decade, to 104,000 in 1970. In 1960, there were affiliates in seven states, and by 1974 there were affiliates in 46 states. During the 1960s, the ACLU underwent a major transformation tactics; it shifted emphasis from legal appeals (generally involving amicus briefs submitted to the Supreme Court) to direct representation of defendants when they were initially arrested. At the same time, the ACLU transformed its style from "disengaged and elitist" to "emotionally engaged". The ACLU published a breakthrough document in 1963, titled "How Americans Protest", which was borne of frustration with the slow progress in battling racism, and which endorsed aggressive, even militant protest techniques.
African-American protests in the South accelerated in the early 1960s, and the ACLU assisted at every step. After four African-American college students staged a sit-in in a segregated North Carolina department store, the sit-in movement gained momentum across the United States. During 1960–61, the ACLU defended black students arrested for demonstrating in North Carolina, Florida, and Louisiana. The ACLU also provided legal help for the Freedom Rides in 1961, the integration of the University of Mississippi, the Birmingham campaign in 1963, and the 1964 Freedom Summer.
The NAACP was responsible for managing most sit-in related cases that made it to the Supreme Court, winning nearly every decision. But it fell to the ACLU and other legal volunteer efforts to provide legal representation to hundreds of protestorswhite and blackwho were arrested while protesting in the South. The ACLU joined with other civil liberties groups to form the Lawyers Constitutional Defense Committee (LCDC) which subsequently provided legal representation to many of the protesters. The ACLU provided the majority of the funding for the LCDC.
In 1964, the ACLU opened up a major office in Atlanta, Georgia, dedicated to serving Southern issues. Much of the ACLU's progress in the South was due to Charles Morgan Jr., the charismatic leader of the Atlanta office. He was responsible for desegregating juries ("Whitus v. Georgia"), desegregating prisons ("Lee v. Washington"), and reforming election laws. The ACLU's southern office also defended African-American congressman Julian Bond in "Bond v. Floyd", when the Georgia congress refused to formally induct Bond into the legislature. Another widely publicized case defended by Morgan was that of Army doctor Howard Levy, who was convicted of refusing to train Green Berets. Despite raising the defense that the Green Berets were committing war crimes in Vietnam, Levy lost on appeal in "Parker v. Levy", 417 US 733 (1974).
In 1969, the ACLU won a major victory for free speech, when it defended Dick Gregory after he was arrested for peacefully protesting against the mayor of Chicago. The court ruled in "Gregory v. Chicago" that a speaker cannot be arrested for disturbing the peace when the hostility is initiated by someone in the audience, as that would amount to a "heckler's veto".
The ACLU was at the center of several legal aspects of the Vietnam war: defending draft resisters, challenging the constitutionality of the war, the potential impeachment of Richard Nixon, and the use of national security concerns to preemptively censor newspapers.
David J. Miller was the first person prosecuted for burning his draft card. The New York affiliate of the ACLU appealed his 1965 conviction (367 F.2d 72: "United States of America v. David J. Miller", 1966), but the Supreme Court refused to hear the appeal. Two years later, the Massachusetts affiliate took the card-burning case of David O'Brien to the Supreme Court, arguing that the act of burning was a form of symbolic speech, but the Supreme Court upheld the conviction in "United States v. O'Brien", 391 US 367 (1968). Thirteen-year-old Junior High student Mary Tinker wore a black armband to school in 1965 to object to the war, and was suspended from school. The ACLU appealed her case to the Supreme Court and won a victory in "Tinker v. Des Moines Independent Community School District". This critical case established that the government may not establish "enclaves" such as schools or prisons where all rights are forfeit.
The ACLU defended Sydney Street, who was arrested for burning an American flag to protest the reported assassination of civil rights leader James Meredith. In the "Street v. New York" decision, the court agreed with the ACLU that encouraging the country to abandon one of its national symbols was constitutionally protected form of expression. The ACLU successfully defended Paul Cohen, who was arrested for wearing a jacket with the words "fuck the draft" on its back, while he walked through the Los Angeles courthouse. The Supreme Court, in "Cohen v. California", held that the vulgarity of the wording was essential to convey the intensity of the message.
Non-war related free speech rights were also advanced during the Vietnam war era; in 1969, the ACLU defended a Ku Klux Klan member who advocated long-term violence against the government, and the Supreme Court concurred with the ACLU's argument in the landmark decision "Brandenburg v. Ohio", which held that only speech which advocated "imminent" violence could be outlawed.
A major crisis gripped the ACLU in 1968 when a debate erupted over whether to defend Benjamin Spock and the Boston Five against federal charges that they encouraged draftees to avoid the draft. The ACLU board was deeply split over whether to defend the activists; half the board harbored anti-war sentiments, and felt that the ACLU should lend its resources to the cause of the Boston Five. The other half of the board believed that civil liberties were not at stake, and the ACLU would be taking a political stance. Behind the debate was the longstanding ACLU tradition that it was politically impartial, and provided legal advice without regard to the political views of the defendants. The board finally agreed to a compromise solution that permitted the ACLU to defend the anti-war activists, without endorsing the activist's political views. Some critics of the ACLU suggest that the ACLU became a partisan political organization following the Spock case. After the Kent State shootings in 1970, ACLU leaders took another step towards politics by passing a resolution condemning the Vietnam War. The resolution was based in a variety of legal arguments, including civil liberties violations and a claim that the war was illegal.
Also in 1968, the ACLU held an internal symposium to discuss its dual roles: providing "direct" legal support (defense for accused in their initial trial, benefiting only the individual defendant), and appellate support (providing amicus briefs during the appeal process, to establish widespread legal precedent). Historically, the ACLU was known for its appellate work which led to landmark Supreme Court decisions, but by 1968, 90% of the ACLU's legal activities involved direct representation. The symposium concluded that both roles were valid for the ACLU.
The ACLU supported "The New York Times" in its 1971 suit against the government, requesting permission to publish the Pentagon papers. The court upheld the "Times" and ACLU in the "New York Times Co. v. United States" ruling, which held that the government could not preemptively prohibit the publication of classified information and had to wait until after it was published to take action.
On September 30, 1973, the ACLU became first national organization to publicly call for the impeachment and removal from office of President Richard Nixon. Six civil liberties violations were cited as grounds: “specific proved violations of the rights of political dissent; usurpation of Congressional war‐making powers; establishment of a personal secret police which committed crimes; attempted interference in the trial of Daniel Ellsberg; distortion of the system of justice and perversion of other Federal agencies.” One month later, after the House of Representatives began an impeachment inquiry against him, the organization released a 56‐page handbook detailing “17 things citizens could do to bring about the impeachment of President Nixon.“ This resolution, when placed beside the earlier resolution opposing the Vietnam war, convinced many ACLU critics, particularly conservatives, that the organization had transformed into a liberal political organization.
The decade from 1965 to 1975 saw an expansion of the field of civil liberties. Administratively, the ACLU responded by appointing Aryeh Neier to take over from Pemberton as Executive Director in 1970. Neier embarked on an ambitious program to expand the ACLU; he created the ACLU Foundation to raise funds, and he created several new programs to focus the ACLU's legal efforts. By 1974, ACLU membership had reached 275,000.
During those years, the ACLU led the way in expanding legal rights in three directions: new rights for persons within government-run "enclaves", new rights for victim groups, and privacy rights for mainstream citizens. At the same time, the organization grew substantially. The ACLU helped develop the field of constitutional law that governs "enclaves", which are groups of persons that live in conditions under government control. Enclaves include mental hospital patients, members of the military, and prisoners, and students (while at school). The term enclave originated with Supreme Court justice Abe Fortas's use of the phrase "schools may not be enclaves of totalitarianism" in the "Tinker v. Des Moines" decision.
The ACLU initiated the legal field of student's rights with the "Tinker v. Des Moines" case, and expanded it with cases such as "Goss v. Lopez" which required schools to provide students an opportunity to appeal suspensions.
As early as 1945, the ACLU had taken a stand to protect the rights of the mentally ill, when it drafted a model statute governing mental commitments. In the 1960s, the ACLU opposed involuntary commitments, unless it could be demonstrated that the person was a danger to himself or the community. In the landmark 1975 "O'Connor v. Donaldson" decision the ACLU represented a non-violent mental health patient who had been confined against his will for 15 years, and persuaded the Supreme Court to rule such involuntary confinements illegal. The ACLU has also defended the rights of mentally ill individuals who are not dangerous, but who create disturbances. The New York chapter of the ACLU defended Billie Boggs, a mentally ill woman who exposed herself and defecated and urinated in public.
Prior to 1960, prisoners had virtually no recourse to the court system, because courts considered prisoners to have no civil rights. That changed in the late 1950s, when the ACLU began representing prisoners that were subject to police brutality, or deprived of religious reading material. In 1968, the ACLU successfully sued to desegregate the Alabama prison system; and in 1969, the New York affiliate adopted a project to represent prisoners in New York prisons. Private attorney Phil Hirschkop discovered degrading conditions in Virginia prisons following the Virginia State Penitentiary strike, and won an important victory in 1971's "Landman v. Royster" which prohibited Virginia from treating prisoners in inhumane ways. In 1972, the ACLU consolidated several prison rights efforts across the nation and created the National Prison Project. The ACLU's efforts led to landmark cases such as "Ruiz v. Estelle" (requiring reform of the Texas prison system) and in 1996 US Congress enacted the Prison Litigation Reform Act (PLRA) which codified prisoners' rights.
The ACLU, during the 1960s and 1970s, expanded its scope to include what it referred to as "victim groups", namely women, the poor, and homosexuals. Heeding the call of female members, the ACLU endorsed the Equal Rights Amendment in 1970 and created the Women's Rights Project in 1971. The Women's Rights Project dominated the legal field, handling more than twice as many cases as the National Organization for Women, including breakthrough cases such as "Reed v. Reed", "Frontiero v. Richardson", and " Taylor v. Louisiana".
ACLU leader Harriet Pilpel raised the issue of the rights of homosexuals in 1964, and two years later the ACLU formally endorsed gay rights. In 1972, ACLU cooperating attorneys in Oregon filed the first federal civil rights case involving a claim of unconstitutional discrimination against a gay or lesbian public school teacher. The US District Court held that a state statute that authorized school districts to fire teachers for "immorality" was unconstitutionally vague, and awarded monetary damages to the teacher. The court refused to reinstate the teacher, and the Ninth Circuit Court of Appeals affirmed that refusal by a 2 to 1 vote. "Burton v. Cascade School District", 353 F. Supp. 254 (D. Or. 1972), aff'd 512 F.2d 850 (1975). In 1973, the ACLU created the Sexual Privacy Project (later the Gay and Lesbian Rights Project) which combated discrimination against homosexuals. This support continues even today. After then-Senator Larry Craig was arrested for soliciting sex in a public restroom, the ACLU wrote an amicus brief for Craig, saying that sex between consenting adults in public places was protected under privacy rights.
Rights of the poor was another area that was expanded by the ACLU. In 1966 and again in 1968, activists within the ACLU encouraged the organization to adopt a policy overhauling the welfare system, and guaranteeing low-income families a baseline income; but the ACLU board did not approve the proposals. The ACLU played a key role in the 1968 "King v. Smith" decision, where the Supreme Court ruled that welfare benefits for children could not be denied by a state simply because the mother cohabited with a boyfriend.
The Reproductive Freedom Project is an institution founded in 1974 (within the larger context of ACLU) that is committed to defend individuals who feel abused by the government, especially with cases pertaining to a lack of access to abortions, birth control, or sexual education.
The ACLU continues to defend individuals who feel abused or improperly treated by the government. Often the American Civil Liberties Union is the group to stand up for an individual when being discriminated against because of their religion, sex, gender, sexuality, race, or class, even when they are not the popular opinion. The Reproductive Freedom Project, however, goes deeper than the ACLU. The Project promotes sexual and reproductive health by providing lessons about contraception, knowing about one's reproductive rights and assisting with the financial burdens of abortions and all of the logistics that may go into that.
The Reproductive Freedom Project of ACLU, according to their mission statement, actively works provide access to any and all reproductive health care for any human, regardless of race, gender, socioeconomic status, sexual orientation, or political standing. In some cases, Reproductive Freedom Programs fund ultrasounds and abortions and any lodging, meals, or transportation that go with that. Because women have reported finding it necessary to cross state lines or wait weeks for an abortion, The Reproductive Freedom Project states that they want to fight for individuals "state by state and law by law" until every individual can pursue the kind of lifestyle they want. As stated on their website, "states have enacted more restrictions to abortion than they did in the previous 10 years combined". The ACLU claims to be committed to fighting injustices with access to education on what accessibilities one has to abortions, birth control, religious rights, as well as trying to diminish abstinence-only sexual education, for ACLU claims that abstinence only education promotes a lack of willingness to use contraceptives.
As referenced in the larger ACLU article, in 1929, the ACLU defended Margaret Sanger's right to educate the general public about forms of birth control. In 1980, the Project filed "Poe v. Lynchburg Training School" after 8,000 women had been sterilized without their authorization. In 1985, the state decided to provide counseling and medical treatment for problems caused by what had happened 5 years prior. In 1977, the ACLU took part in and litigated "Walker v. Pierce", the Supreme Court case that created federal regulations to prevent Medicaid patients from being sterilized without their knowledge or consent. In 1981–1990, the Project litigated "Hodgson v. Minnesota", a case defending the rights of teenagers who chose not to comply with a state law requiring them to receive parental permission for an abortion. In the 1990s, the Project provided legal assistance and resource kits to those who were being attacked for educating about sexuality and AIDS. In 1995, the Project filed "Curtis v. School Committee of Falmouth", the US's first condom availability program.
The Reproductive Freedom Project is presently working on three ideas: (1) to "reverse the shortage of trained abortion providers throughout the country" (2) to "block state and federal welfare "reform" proposals that cut off benefits for children who are born to women already receiving welfare, unmarried women, or teenagers" and (3) to "stop the elimination of vital reproductive health services as a result of hospital mergers and health care networks". The Project says they are hoping to achieve these goals through legal action and litigation.
The right to privacy is not explicitly identified in the US Constitution, but the ACLU led the charge to establish such rights in the indecisive 1961 "Poe v. Ullman" case, which addressed a state statute outlawing contraception. The issue arose again in "Griswold v. Connecticut" (1965), and this time the Supreme Court adopted the ACLU's position, and formally declared a right to privacy. The New York affiliate of the ACLU pushed to eliminate anti-abortion laws starting in 1964, a year before "Griswold" was decided, and in 1967 the ACLU itself formally adopted the right to abortion as a policy. The ACLU led the defense in "United States v. Vuitch" which expanded the right of physicians to determine when abortions were necessary. These efforts culminated in one of the most controversial Supreme Court decisions of all time, "Roe v. Wade", which legalized abortion in the first three months of pregnancy. The ACLU successfully argued against state bans on interracial marriage, in the case of "Loving v. Virginia" (1967).
Related to privacy, the ACLU engaged in several battles to ensure that government records about individuals were kept private, and to give individuals the right to review their records. The ACLU supported several measures, including the 1970 Fair Credit Reporting Act required credit agencies to divulge credit information to individuals; the 1973 Family Educational Rights and Privacy Act, which provided students the right to access their records; and the 1974 Privacy Act which prevented the federal government from disclosing personal information without good cause.
In the early 1970s, conservatives and libertarians began to criticize the ACLU for being too political and too liberal. Legal scholar Joseph W. Bishop wrote that the ACLU's trend to partisanship started with its defense of Spock's anti-war protests. Critics also blamed the ACLU for encouraging the Supreme Court to embrace judicial activism. Critics claimed that the ACLU's support of controversial decisions like "Roe v. Wade" and "Griswold v. Connecticut" violated the intention of the authors of the Bill of Rights. The ACLU became an issue in the 1988 presidential campaign, when Republican candidate George H. W. Bush accused Democratic candidate Michael Dukakis (a member of the ACLU) of being a "card carrying member of the ACLU".
It is the policy of the ACLU to support the civil liberties of defendants regardless of their ideological stance. The ACLU takes pride in defending individuals with unpopular viewpoints, such as George Wallace, George Lincoln Rockwell, and KKK members. The ACLU has defended American Nazis many times, and their actions often brought protests, particularly from American Jews.
In 1977, a small group of American Nazis, led by Frank Collin, applied to the town of Skokie, Illinois, for permission to hold a demonstration in the town park. Skokie at the time had a majority population of Jews, totaling 40,000 of 70,000 citizens, some of whom were survivors of Nazi concentration camps. Skokie refused to grant permission, and an Illinois judge supported Skokie and prohibited the demonstration. Skokie immediately passed three ordinances aimed at preventing the group from meeting in Skokie. The ACLU assisted Collin and appealed to federal court. The appeal dragged on for a year, and the ACLU eventually prevailed in "Smith v. Collin", 447 F. Supp. 676.
The Skokie case was heavily publicized across America, partially because Jewish groups such as the Jewish Defense League and Anti Defamation League strenuously objected to the demonstration, leading many members of the ACLU to cancel their memberships. The Illinois affiliate of the ACLU lost about 25% of its membership and nearly one-third of its budget. The financial strain from the controversy led to layoffs at local chapters. After the membership crisis died down, the ACLU sent out a fund-raising appeal which explained their rationale for the Skokie case, and raised over $500,000 ($ in dollars).
The inauguration of Ronald Reagan as president in 1981, ushered in an eight-year period of conservative leadership in the US government. Under Reagan's leadership, the government pushed a conservative social agenda.
Fifty years after the Scopes trial, the ACLU found itself fighting another classroom case, the Arkansas 1981 creationism statute, which required schools to teach the biblical account of creation as a scientific alternative to evolution. The ACLU won the case in the "McLean v. Arkansas" decision.
In 1982, the ACLU became involved in a case involving the distribution of child pornography ("New York v. Ferber"). In an amicus brief, the ACLU argued that child pornography that violates the three prong obscenity test should be outlawed, but that the law in question was overly restrictive because it outlawed artistic displays and otherwise non-obscene material. The court did not adopt the ACLU's position.
During the 1988 presidential election, Vice President George H. W. Bush noted that his opponent Massachusetts Governor Michael Dukakis had described himself as a "card-carrying member of the ACLU" and used that as evidence that Dukakis was "a strong, passionate liberal" and "out of the mainstream". The phrase subsequently was used by the organization in an advertising campaign.
In 1990, the ACLU defended Lieutenant Colonel Oliver North, whose conviction was tainted by coerced testimonya violation of his fifth amendment rightsduring the Iran–Contra affair, where Oliver North was involved in illegal weapons sales to Iran in order to illegally fund the Contra guerillas.
In 1997, ruling unanimously in the case of "Reno v. American Civil Liberties Union", the Supreme Court voted down anti-indecency provisions of the Communications Decency Act (the CDA), finding they violated the freedom of speech provisions of the First Amendment. In their decision, the Supreme Court held that the CDA's "use of the undefined terms 'indecent' and 'patently offensive' will provoke uncertainty among speakers about how the two standards relate to each other and just what they mean."
In 2000, Marvin Johnson, a legislative counsel for the ACLU, stated that proposed anti-spam legislation infringed on free speech by denying anonymity and by forcing spam to be labeled as such, "Standardized labeling is compelled speech." He also stated, "It's relatively simple to click and delete." The debate found the ACLU joining with the Direct Marketing Association and the Center for Democracy and Technology in 2000 in criticizing a bipartisan bill in the House of Representatives. As early as 1997, the ACLU had taken a strong position that nearly all spam legislation was improper, although it has supported "opt-out" requirements in some cases. The ACLU opposed the 2003 CAN-SPAM act suggesting that it could have a chilling effect on speech in cyberspace. It has been criticized for this position.
In November 2000, 15 African-American residents of Hearne, Texas, were indicted on drug charges after being arrested in a series of "drug sweeps". The ACLU filed a class-action lawsuit, "Kelly v. Paschall", on their behalf, alleging that the arrests were unlawful. The ACLU contended that 15 percent of Hearne's male African-American population aged 18 to 34 were arrested based only on the "uncorroborated word of a single unreliable confidential informant coerced by police to make cases." On May 11, 2005, the ACLU and Robertson County announced a confidential settlement of the lawsuit, an outcome which "both sides stated that they were satisfied with." The District Attorney dismissed the charges against the plaintiffs of the suit. The 2009 film "American Violet" depicts this case.
In 2000, the ACLU's Massachusetts affiliate represented the North American Man Boy Love Association (NAMBLA), on first amendment grounds, in the "Curley v. NAMBLA" wrongful death civil suit. The organization was sued because a man who raped and murdered a child had visited the NAMBLA website. Also in 2000, the ACLU lost the "Boy Scouts of America v. Dale" case, which had asked the Supreme Court to require the Boy Scouts of America to drop their policy of prohibiting homosexuals from becoming Boy Scout leaders.
In March 2004, the ACLU, along with Lambda Legal and the National Center for Lesbian Rights, sued the state of California on behalf of six same-sex couples who were denied marriage licenses. That case, "Woo v. Lockyer", was eventually consolidated into "In re Marriage Cases", the California Supreme Court case which led to same-sex marriage being available in that state from June 16, 2008, until Proposition 8 was passed on November 4, 2008. The ACLU, Lambda Legal and the National Center for Lesbian Rights then challenged Proposition 8 and won.
During the 2004 trial regarding allegations of Rush Limbaugh's drug abuse, the ACLU argued that his privacy should not have been compromised by allowing law enforcement examination of his medical records. In June 2004, the school district in Dover, Pennsylvania, required that its high school biology students listen to a statement which asserted that the theory of evolution is not fact and mentioning intelligent design as an alternative theory. Several parents called the ACLU to complain, because they believed that the school was promoting a religious idea in the classroom and violating the Establishment Clause of the First Amendment. The ACLU, joined by Americans United for Separation of Church and State, represented the parents in a lawsuit against the school district. After a lengthy trial, Judge John E. Jones III ruled in favor of the parents in the "Kitzmiller v. Dover Area School District" decision, finding that intelligent design is not science and permanently forbidding the Dover school system from teaching intelligent design in science classes.
In April 2006, Edward Jones and the ACLU sued the City of Los Angeles, on behalf of Robert Lee Purrie and five other homeless people, for the city's violation of the 8th and 14th Amendments to the US Constitution, and Article I, sections 7 and 17 of the California Constitution (supporting due process and equal protection, and prohibiting cruel and unusual punishment). The Court ruled in favor of the ACLU, stating that, "the LAPD cannot arrest people for sitting, lying, or sleeping on public sidewalks in Skid Row." Enforcement of section 41.18(d) 24 hours a day against persons who have nowhere else to sit, lie, or sleep, other than on public streets and sidewalks, is breaking these amendments. The Court said that the anti-camping ordinance is "one of the most restrictive municipal laws regulating public spaces in the United States". Jones and the ACLU wanted a compromise in which the LAPD is barred from enforcing section 41.18(d) (arrest, seizure, and imprisonment) in Skid Row between the hours of 9:00 p.m. and 6:30 am. The compromise plan permits the homeless to sleep on the sidewalk, provided they are not "within 10 feet of any business or residential entrance" and only between these hours. One of the motivations for the compromise is the shortage of space in the prison system. Downtown development business interests and the Central City Association (CCA) were against the compromise. Police Chief William Bratton said the case had slowed the police effort to fight crime and clean up Skid Row, and that when he was allowed to clean up Skid Row, real estate profited. On September 20, 2006, the Los Angeles City Council voted to reject the compromise. On October 3, 2006, police arrested Skid Row's transients for sleeping on the streets for the first time in months.
In 2006, the ACLU of Washington State joined with a pro-gun rights organization, the Second Amendment Foundation, and prevailed in a lawsuit against the North Central Regional Library District (NCRL) in Washington for its policy of refusing to disable restrictions upon an adult patron's request. Library patrons attempting to access pro-gun web sites were blocked, and the library refused to remove the blocks. In 2012, the ACLU sued the same library system for refusing to temporarily, at the request of an adult patron, disable Internet filters which blocked access to Google Images.
In 2006, the ACLU challenged a Missouri law that prohibited picketing outside of veterans' funerals. The suit was filed in support of the Westboro Baptist Church and Shirley Phelps-Roper, who were threatened with arrest. The Westboro Baptist Church is well known for their picket signs that contain messages such as, "God Hates Fags", "Thank God for Dead Soldiers", and "Thank God for 9/11". The ACLU issued a statement calling the legislation a "law that infringes on Shirley Phelps-Roper's rights to religious liberty and free speech". The ACLU prevailed in the lawsuit.
In light of the Supreme Court's "Heller" decision recognizing that the Constitution protects an individual right to bear arms, ACLU of Nevada took a position of supporting "the individual's right to bear arms subject to constitutionally permissible regulations" and pledged to "defend this right as it defends other constitutional rights". Since 2008, the ACLU has increasingly assisted gun owners in recovering firearms that have been seized illegally by law enforcement.
In 2009, the ACLU filed an amicus brief in "Citizens United v. FEC", arguing that the Bipartisan Campaign Reform Act of 2002 violated the First Amendment right to free speech by curtailing political speech. This stance on the landmark "Citizens United" case caused considerable disagreement within the organization, resulting in a discussion about its future stance during a quarterly board meeting in 2010. On March 27, 2012, the ACLU reaffirmed its stance in support of the Supreme Court's "Citizens United" ruling, at the same time voicing support for expanded public financing of election campaigns and stating the organization would firmly oppose any future constitutional amendment limiting free speech.
In 2010, the ACLU of Illinois was inducted into the Chicago Gay and Lesbian Hall of Fame as a Friend of the Community.
In 2011, the ACLU started its Don't Filter Me project, countering LGBT-related Internet censorship in public schools in the United States.
On January 7, 2013, the ACLU reached a settlement with the federal government in "Collins v. United States" that provided for the payment of full separation pay to servicemembers discharged under "don't ask, don't tell" since November 10, 2004, who had previously been granted only half that. Some 181 were expected to receive about $13,000 each.
After the September 11 attacks, the federal government instituted a broad range of new measures to combat terrorism, including the passage of the Patriot Act. The ACLU challenged many of the measures, claiming that they violated rights regarding due process, privacy, illegal searches, and cruel and unusual punishment. An ACLU policy statement states:
During the ensuing debate regarding the proper balance of civil liberties and security, the membership of the ACLU increased by 20%, bringing the group's total enrollment to 330,000. The growth continued, and by August 2008 ACLU membership was greater than 500,000. It remained at that level through 2011.
The ACLU has been a vocal opponent of the USA PATRIOT Act of 2001, the PATRIOT 2 Act of 2003, and associated legislation made in response to the threat of domestic terrorism. In response to a requirement of the USA PATRIOT Act, the ACLU withdrew from the Combined Federal Campaign charity drive. The campaign imposed a requirement that ACLU employees must be checked against a federal anti-terrorism watch list. The ACLU has stated that it would "reject $500,000 in contributions from private individuals rather than submit to a government 'blacklist' policy."
In 2004, the ACLU sued the federal government in "American Civil Liberties Union v. Ashcroft" on behalf of Nicholas Merrill, owner of an Internet service provider. Under the provisions of the Patriot Act, the government had issued national security letters to Merrill to compel him to provide private Internet access information from some of his customers. In addition, the government placed a gag order on Merrill, forbidding him from discussing the matter with anyone.
In January 2006, the ACLU filed a lawsuit, "ACLU v. NSA", in a federal district court in Michigan, challenging government spying in the NSA warrantless surveillance controversy. On August 17, 2006, that court ruled that the warrantless wiretapping program is unconstitutional and ordered it ended immediately. However, the order was stayed pending an appeal. The Bush administration did suspend the program while the appeal was being heard. In February 2008, the US Supreme Court turned down an appeal from the ACLU to let it pursue a lawsuit against the program that began shortly after the September 11 terror attacks.
The ACLU and other organizations also filed separate lawsuits around the country against telecommunications companies. The ACLU filed a lawsuit in Illinois ("Terkel v. AT&T") which was dismissed because of the state secrets privilege and two others in California requesting injunctions against AT&T and Verizon. On August 10, 2006, the lawsuits against the telecommunications companies were transferred to a federal judge in San Francisco.
The ACLU represents a Muslim-American who was detained but never accused of a crime in "Ashcroft v. al-Kidd", a civil suit against former Attorney General John Ashcroft. In January 2010, the American military released the names of 645 detainees held at the Bagram Theater Internment Facility in Afghanistan, modifying its long-held position against publicizing such information. This list was prompted by a Freedom of Information Act lawsuit filed in September 2009 by the ACLU, whose lawyers had also requested detailed information about conditions, rules and regulations.
The ACLU has also criticized targeted killings of American citizens who fight against the United States. In 2011, the ACLU criticized the killing of radical Muslim cleric Anwar al-Awlaki on the basis that it was a violation of his Fifth Amendment right to not be deprived of life, liberty, or property without due process of law.
Following Donald Trump's election as President on November 8, 2016, the ACLU responded on Twitter saying: "Should President-elect Donald Trump attempt to implement his unconstitutional campaign promises, we'll see him in court." On January 27, 2017, President Trump signed an executive order indefinitely barring "Syrian refugees from entering the United States, suspended all refugee admissions for 120 days and blocked citizens of seven Muslim-majority countries, refugees or otherwise, from entering the United States for 90 days: Iran, Iraq, Libya, Somalia, Sudan, Syria and Yemen". The ACLU responded by filing a lawsuit against the ban on behalf of Hameed Khalid Darweesh and Haider Sameer Abdulkhaleq Alshawi, who had been detained at JFK International Airport. On January 28, 2017, a US District Court Judge Ann Donnelly granted a temporary injunction against the immigration order, saying it was difficult to see any harm from allowing the newly arrived immigrants from entering the country.
In response to Trump's order, the ACLU raised more than $24 million from more than 350,000 individual online donations in a two-day period. This amounted to six times what the ACLU normally receives in online donations in a year. Celebrities donating included Chris Sacca (who offered to match other people's donations and ultimately gave $150,000), Rosie O'Donnell, Judd Apatow, Sia, John Legend, and Adele. The number of members of the ACLU doubled in the time from the election to end of January to 1 million.
Grants and contributions increased from $106,628,381 USD reported by the 2016 year-end income statement to $274,104,575 by the 2017 year-end statement. The primary source of revenue from the segment came from individual contributions in response to the Trump presidency's alleged infringements on civil liberties. The surge in donations more than doubled the total support and revenue of the non-profit organization year over year from 2016 to 2017. Besides filing more lawsuits than during previous presidential administrations, the ACLU has spent more money on advertisements and messaging as well, weighing in on elections and pressing political concerns. This increased public profile has drawn some accusations that the organization has become more politically partisan than in previous decades.
Following WikiLeaks founder Julian Assange's arrest, Ben Wizner from the ACLU said that if authorities were to prosecute Assange "for violating U.S. secrecy laws [it] would set an especially dangerous precedent for U.S. journalists, who routinely violate foreign secrecy laws to deliver information vital to the public's interest."
On August 10, 2020, in an opinion article for "USA Today" by Anthony D. Romero, the ACLU called for the dismantling of the United States Department of Homeland Security over the deployment of federal forces in July 2020 during the George Floyd protests. On August 26, 2020, the ACLU filed a lawsuit on behalf of seven protesters and three veterans from the following the protests in Portland, Oregon, which accused the Trump Administration of using excessive force and unlawful arrests with federal officers.
The ACLU of Tennessee protested the shooting of Jocques Clemmons which occurred in Nashville, Tennessee, on February 10, 2017. On May 11, 2017, as Glenn Funk, the district attorney of Davidson County, decided not to prosecute police officer Joshua Lippert, they called for an independent community review board and for Nashville police officers to wear body cameras, which was approved by local voters in a referendum.
On June 21, 2018, a leaked memo showed that the ACLU has explicitly endorsed the view that free speech can harm marginalized groups by undermining their civil rights. "Speech that denigrates such groups can inflict serious harms and is intended to and often will impede progress toward equality," the ACLU declared in guidelines governing case selection and "Conflicts Between Competing Values or Priorities." The ACLU had previously defended the free speech rights of the KKK and Nazis.
The ACLU argued that a Massachusetts law, later unanimously struck down by the Supreme Court, was constitutional. The law prohibited sidewalk counselors from approaching women outside abortion facilities and offering them alternatives to abortion but allowed escorts to speak with them and accompany them into the building. In overturning the law in "McCullen v. Coakley", the Supreme Court unanimously ruled that it violated the counselors' freedom of speech and that it was viewpoint discrimination.

</doc>
<doc id="1955" url="https://en.wikipedia.org/wiki?curid=1955" title="Adobe Inc.">
Adobe Inc.

Adobe Inc. ( ) is an American multinational computer software company. Incorporated in Delaware 
and headquartered in San Jose, California, it has historically focused upon the creation of multimedia and creativity software products, with a more recent foray towards digital marketing software. Adobe is best known for its Adobe Flash web software ecosystem, Photoshop image editing software, Adobe Illustrator vector graphics editor, Acrobat Reader, the Portable Document Format (PDF), and Adobe Creative Suite, as well as its successor Adobe Creative Cloud.
Adobe was founded in December 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution.
, Adobe has more than 21,000 employees worldwide, about 40% of whom work in San Jose. Adobe also has major development operations in the United States in Newton, New York City, Minneapolis, Lehi, Seattle, and San Francisco. It also has major development operations in Noida and Bangalore in India.
The company was started in John Warnock's garage. The name of the company, "Adobe", comes from Adobe Creek in Los Altos, California, which ran behind Warnock's house. That creek is so named because of the type of clay found there, which alludes to the creative nature of the company's software. Adobe's corporate logo features a stylized "A" and was designed by Marva Warnock, a graphic designer who is also John Warnock's wife.
Steve Jobs attempted to buy the company for $5 million in 1982, but Warnock and Geschke refused. Their investors urged them to work something out with Jobs, so they agreed to sell him shares worth 19 percent of the company. Jobs paid a five-times multiple of their company's valuation at the time, plus a five-year license fee for PostScript, in advance. The purchase and advance made Adobe the first company in the history of Silicon Valley to become profitable in its first year.
Warnock and Geschke considered various business options including a copy-service business and a turnkey system for office printing. Then they chose to focus on developing specialized printing software and created the Adobe PostScript page description language.
PostScript was the first truly international standard for computer printing as it included algorithms describing the letter-forms of many languages. Adobe added kanji printer products in 1988. Warnock and Geschke were also able to bolster the credibility of PostScript by connecting with a typesetting manufacturer. They weren't able to work with Compugraphic, but then worked with Linotype to license the Helvetica and Times Roman fonts (through the Linotron 100). By 1987, PostScript had become the industry-standard printer language with more than 400 third-party software programs and licensing agreements with 19 printer companies.
Warnock described the language as "extensible", in its ability to apply graphic arts standards to office printing.
Adobe's first products after PostScript were digital fonts, which they released in a proprietary format called Type 1, worked on by Bill Paxton after he left Stanford. Apple subsequently developed a competing standard, TrueType, which provided full scalability and precise control of the pixel pattern created by the font's outlines, and licensed it to Microsoft.
In the mid-1980s, Adobe entered the consumer software market with Illustrator, a vector-based drawing program for the Apple Macintosh. Illustrator, which grew from the firm's in-house font-development software, helped popularize PostScript-enabled laser printers.
Adobe entered the NASDAQ Composite index in August 1986. Its revenue has grown from roughly $1 billion in 1999 to $4bn in 2012. Adobe's fiscal years run from December to November. For example, the 2007 fiscal year ended on November 30, 2007.
In 1989, Adobe introduced what was to become its flagship product, a graphics editing program for the Macintosh called Photoshop. Stable and full-featured, Photoshop 1.0 was ably marketed by Adobe and soon dominated the market.
In 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software. PDF is now an International Standard: ISO 32000-1:2008.
In December 1991, Adobe released Adobe Premiere, which Adobe rebranded as Adobe Premiere Pro in 2003. In 1992, Adobe acquired OCR Systems, Inc. In 1994, Adobe acquired Aldus and added PageMaker and After Effects to its product line later in the year; it also controls the TIFF file format. In the same year, Adobe acquired LaserTools Corp and Compution Inc. In 1995, Adobe added FrameMaker, the long-document DTP application, to its product line after Adobe acquired Frame Technology Corp. In 1996, Adobe acquired Ares Software Corp. In 2002, Adobe acquired Canadian company Accelio (also known as JetForm).
In May 2003 Adobe purchased audio editing and multitrack recording software Cool Edit Pro from Syntrillium Software for $16.5 million, as well as a large loop library called "Loopology". Adobe then renamed Cool Edit Pro to "Adobe Audition" and included it in the Creative Suite.
On December 3, 2005, Adobe acquired its main rival, Macromedia, in a stock swap valued at about $3.4 billion, adding ColdFusion, Contribute, Captivate, Breeze (rebranded as Adobe Connect), Director, Dreamweaver, Fireworks, Flash, FlashPaper, Flex, FreeHand, HomeSite, JRun, Presenter, and Authorware to Adobe's product line.
Adobe released Adobe Media Player in April 2008. On April 27, Adobe discontinued development and sales of its older HTML/web development software, GoLive, in favor of Dreamweaver. Adobe offered a discount on Dreamweaver for GoLive users and supports those who still use GoLive with online tutorials and migration assistance. On June 1, Adobe launched Acrobat.com, a series of web applications geared for collaborative work. Creative Suite 4, which includes Design, Web, Production Premium, and Master Collection came out in October 2008 in six configurations at prices from about US$1,700 to $2,500 or by individual application. The Windows version of Photoshop includes 64-bit processing. On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak economic environment.
On September 15, 2009, Adobe Systems announced that it would acquire online marketing and web analytics company Omniture for $1.8 billion. The deal was completed on October 23, 2009. Former Omniture products were integrated into the Adobe Marketing Cloud.
On November 10, 2009, the company laid off a further 680 employees.
Adobe's 2010 was marked by continuing front-and-back arguments with Apple over the latter's non-support for Adobe Flash on its iPhone, iPad and other products. Former Apple CEO Steve Jobs claimed that Flash was not reliable or secure enough, while Adobe executives have argued that Apple wish to maintain control over the iOS platform. In April 2010, Steve Jobs published a post titled "Thoughts on Flash" where he outlined his thoughts on Flash and the rise of HTML 5.
In July 2010, Adobe bought Day Software integrating their line of CQ Products: WCM, DAM, SOCO, and Mobile
In January 2011, Adobe acquired DemDex, Inc. with the intent of adding DemDex's audience-optimization software to its online marketing suite. At Photoshop World 2011, Adobe unveiled a new mobile photo service. Carousel is a new application for iPhone, iPad, and Mac that uses Photoshop Lightroom technology for users to adjust and fine-tune images on all platforms. Carousel will also allow users to automatically sync, share and browse photos. The service was later renamed to "Adobe Revel".
In October 2011, Adobe acquired Nitobi Software, the makers of the mobile application development framework "PhoneGap". As part of the acquisition, the source code of PhoneGap was submitted to the Apache Foundation, where it became Apache Cordova.
In November 2011, Adobe announced that they would cease development of Flash for mobile devices following version 11.1. Instead, it would focus on HTML 5 for mobile devices. In December 2011, Adobe announced that it entered into a definitive agreement to acquire privately held Efficient Frontier.
In December 2012, Adobe opened a new 280,000 square foot corporate campus in Lehi, Utah.
In 2013, Adobe endured a major security breach. Vast portions of the source code for the company's software were stolen and posted online and over 150 million records of Adobe's customers have been made readily available for download. In 2012, about 40 million sets of payment card information were compromised by a hack of Adobe.
A class-action lawsuit alleging that the company suppressed employee compensation was filed against Adobe, and three other Silicon Valley-based companies in a California federal district court in 2013. In May 2014, it was revealed the four companies, Adobe, Apple, Google, and Intel had reached agreement with the plaintiffs, 64,000 employees of the four companies, to pay a sum of $324.5 million to settle the suit.
In March 2018, at Adobe Summit, the company and NVIDIA publicized a key association to quickly upgrade their industry-driving AI and profound learning innovations. Expanding on years of coordinated effort, the organizations will work to streamline the Adobe Sensei AI and machine learning structure for NVIDIA GPUs. The joint effort will speed time to showcase and enhance the execution of new Sensei-powered services for Adobe Creative Cloud and Experience Cloud clients and engineers.
Adobe and NVIDIA have co-operated for over 10 years on empowering GPU quickening for a wide arrangement of Adobe's creative and computerized encounter items. This incorporates Sensei-powered features, for example, auto lip-sync in Adobe Character Animator CC and face-aware editing in Photoshop CC, and also cloud-based AI/ML items and features, for example, picture investigation for Adobe Stock and Lightroom CC and auto-labeling in Adobe Experience Supervisor.
In May 2018, Adobe stated they would buy e-commerce services provider Magento Commerce from private equity firm Permira for $1.68 billion. This deal will help bolster its Experience Cloud business, which provides services including analytics, advertising, and marketing. The deal is expected to close during Adobe's fiscal third quarter in 2018.
In September 2018, Adobe announced its acquisition of marketing automation software company Marketo.
In October 2018, Adobe officially changed its name from Adobe Systems Incorporated to Adobe Inc.
In January 2019, Adobe announced its acquisition of 3D texturing company Allegorithmic.
In 2020, the annual Adobe Summit was canceled due to the COVID-19 pandemic. The event is said to take place online this year.
The software giant has imposed a ban on the political ads features on its digital advert sales platform as the United States presidential elections approach.
Adobe Stock
A microstock agency that presently provides over 57 million high-resolution, royalty-free images and videos available to license (via subscription or credit purchase methods). On December 11, 2014, Adobe announced it was buying Fotolia for $800  million in cash, aiming at integrating the service to its Creative Cloud solution. The purchase was completed in January 2015. It is run as a stand-alone website.
Adobe Experience Platform
In March 2019, Adobe released its Adobe Experience Platform, which consists family of content, development, and customer relationship management products, with what it calls the "next generation" of its Sensei artificial intelligence and machine learning framework.
From 1995 to 2013, "Fortune" ranked Adobe as "an outstanding place to work". Adobe was rated the 5th best U.S. company to work for in 2003, 6th in 2004, 31st in 2007, 40th in 2008, 11th in 2009, 42nd in 2010, 65th in 2011, 41st in 2012, and 83rd in 2013. In October 2008, Adobe Systems Canada Inc. was named one of "Canada's Top 100 Employers" by Mediacorp Canada Inc. and was featured in "Maclean's" newsmagazine.
Adobe has a five-star privacy rating from the Electronic Frontier Foundation.
Adobe has been criticized for its pricing practices, with retail prices being up to twice as much in non-US countries. For example, it is significantly cheaper to pay for a return airfare ticket to the United States and purchase one particular collection of Adobe's software there than to buy it locally in Australia.
After Adobe revealed the pricing for the Creative Suite 3 Master Collection, which was £1,000 higher for European customers, a petition to protest over "unfair pricing" was published and signed by 10,000 users. In June 2009, Adobe further increased its prices in the UK by 10% in spite of weakening of the pound against the dollar, and UK users were not allowed to buy from the US store.
Adobe's Reader and Flash programs were listed on "The 10 most hated programs of all time" article by "TechRadar".
Hackers have exploited vulnerabilities in Adobe programs, such as Adobe Reader, to gain unauthorized access to computers. Adobe's Flash Player has also been criticized for, among other things, suffering from performance, memory usage and security problems (see criticism of Flash Player). A report by security researchers from Kaspersky Lab criticized Adobe for producing the products having top 10 security vulnerabilities.
Observers noted that Adobe was spying on its customers by including spyware in the Creative Suite 3 software and quietly sending user data to a firm named Omniture. When users became aware, Adobe explained what the suspicious software did and admitted that they: "could and should do a better job taking security concerns into account". When a security flaw was later discovered in Photoshop CS5, Adobe sparked outrage by saying it would leave the flaw unpatched, so anyone who wanted to use the software securely would have to pay for an upgrade. Following a fierce backlash Adobe decided to provide the software patch.
Adobe has been criticized for pushing unwanted software including third-party browser toolbars and free virus scanners, usually as part of the Flash update process, and for pushing a third-party scareware program designed to scare users into paying for unneeded system repairs.
On October 3, 2013, the company initially revealed that 2.9 million customers' sensitive and personal data was stolen in security breach which included encrypted credit card information. Adobe later admitted that 38 million active users have been affected and the attackers obtained access to their IDs and encrypted passwords, as well as to many inactive Adobe accounts. The company did not make it clear if all the personal information was encrypted, such as email addresses and physical addresses, though data privacy laws in 44 states require this information to be encrypted.
A 3.8 GB file stolen from Adobe and containing 152 million usernames, reversibly encrypted passwords and unencrypted password hints was posted on AnonNews.org. LastPass, a password security firm, said that Adobe failed to use best practices for securing the passwords and has not salted them. Another security firm, Sophos, showed that Adobe used a weak encryption method permitting the recovery of a lot of information with very little effort. According to IT expert Simon Bain, Adobe has failed its customers and 'should hang their heads in shame'.
Many of the credit cards were tied to the Creative Cloud software-by-subscription service. Adobe offered its affected US customers a free membership in a credit monitoring service, but no similar arrangements have been made for non-US customers. When a data breach occurs in the US, penalties depend on the state where the victim resides, not where the company is based.
After stealing the customers' data, cyber-thieves also accessed Adobe's source code repository, likely in mid-August 2013. Because hackers acquired copies of the source code of Adobe proprietary products, they could find and exploit any potential weaknesses in its security, computer experts warned. Security researcher Alex Holden, chief information security officer of Hold Security, characterized this Adobe breach, which affected Acrobat, ColdFusion and numerous other applications, as "one of the worst in US history". Adobe also announced that hackers stole parts of the source code of Photoshop, which according to commentators could allow programmers to copy its engineering techniques and would make it easier to pirate Adobe's expensive products.
Published on a server of a Russian-speaking hacker group, the "disclosure of encryption algorithms, other security schemes, and software vulnerabilities can be used to bypass protections for individual and corporate data" and may have opened the gateway to new generation zero-day attacks. Hackers already used ColdFusion exploits to make off with usernames and encrypted passwords of PR Newswire's customers, which has been tied to the Adobe security breach. They also used a ColdFusion exploit to breach Washington state court and expose up to 200,000 Social Security numbers.
In 1994, Adobe acquired Aldus Corp., a software vendor that sold FreeHand, a competing product. Freehand was direct competition to Adobe Illustrator, Adobe's flagship vector-graphics editor. The Federal Trade Commission intervened and forced Adobe to sell FreeHand back to Altsys, and also banned Adobe from buying back FreeHand or any similar program for the next 10 years (1994–2004). Altsys was then bought by Macromedia, which released versions 5 to 11. When Adobe acquired Macromedia in December 2005, it stalled development of Freehand in 2007, effectively rendering it obsolete. With FreeHand and Illustrator, Adobe controlled the only two products that compete in the professional illustration program market for Macintosh operating systems.
In 2011, a group of 5,000 Freehand graphic designers convened under the banner "Free Freehand", and filed a civil antitrust complaint in the US District Court for the Northern District of California against Adobe. The suit alleged that "Adobe has violated federal and state antitrust laws by abusing its dominant position in the professional vector graphic illustration software market" and that "Adobe has engaged in a series of exclusionary and anti-competitive acts and strategies designed to kill FreeHand, the dominant competitor to Adobe's Illustrator software product, instead of competing on the basis of product merit according to the principals of free market capitalism." Adobe had no response to the claims and the lawsuit was eventually settled. The FreeHand community believes Adobe should release the product to an open-source community if it cannot update it internally.
, on its FreeHand product page, Adobe stated, "While we recognize FreeHand has a loyal customer base, we encourage users to migrate to the new Adobe Illustrator CS4 software which supports both PowerPC and Intel-based Macs and Microsoft Windows XP and Windows Vista." , the Freehand page no longer exists; instead, it simply redirects to the Illustrator page. Adobe's software FTP server still contains a directory for FreeHand, but it is empty.

</doc>
<doc id="1957" url="https://en.wikipedia.org/wiki?curid=1957" title="Alexander Technique">
Alexander Technique

The Alexander Technique, named after its creator Frederick Matthias Alexander, is an educational process that was created to retrain habitual patterns of movement and posture. Alexander believed that poor habits in posture and movement damaged spatial self-awareness as well as health, and that movement efficiency could support overall physical well-being. He saw the technique as a mental training technique as well.
Alexander began developing his technique's principles in the 1890s in an attempt to address voice loss during public speaking. He credited his method with allowing him to pursue his passion for reciting in Shakespearean theater.
Some proponents of the Alexander Technique say that it addresses a variety of health conditions related to cumulative physical behaviors, but there is little evidence to support many of the claims made about the technique. As of 2015 there was evidence suggesting the Alexander Technique may be helpful for long-term back pain, long-term neck pain, and may help people cope with Parkinson's disease. However, both Aetna and the Australian Department of Health have conducted reviews and concluded that the technique has insufficient evidence to warrant insurance coverage.
The Alexander Technique is used and taught by classically trained vocal coaches and musicians in schools and private lessons. Its advocates state that it allows for a balanced use of all aspects of the vocal tract by consciously increasing air-flow, allowing improved vocal skill and tone. The method is said by actors to reduce stage fright and to increase spontaneity.
The Alexander Technique is a frequent component in acting training, because it can assist the actor in being more natural in performance.
According to Alexander Technique instructor Michael J. Gelb, people tend to study the Alexander Technique for reasons of personal development.
A review of evidence for Alexander Technique for various health conditions provided by UK NHS Choices last updated in 2018 said that advocates of the technique made claims for it that were not supported by evidence, but that there was evidence suggesting that it might help with:
"NHS Choices" also states that "some research has also suggested the Alexander technique may improve general long-term pain, stammering and balance skills in elderly people to help them avoid falls. But the evidence in these areas is limited and more studies are needed. There's currently little evidence to suggest the Alexander technique can help improve other health conditions, including asthma, headaches, osteoarthritis, difficulty sleeping (insomnia) and stress."
A review published in "BMC Complementary and Alternative Medicine" in 2014 focused on "the evidence for the effectiveness of AT sessions on musicians' performance, anxiety, respiratory function and posture" concluded that: "Evidence from RCTs and CTs suggests that AT sessions may improve performance anxiety in musicians. Effects on music performance, respiratory function and posture yet remain inconclusive."
A review published in the "International Journal of Clinical Practice" in 2012 found: "Strong evidence exists for the effectiveness of Alexander Technique lessons for chronic back pain and moderate evidence in Parkinson’s-associated disability. Preliminary evidence suggests that Alexander Technique lessons may lead to improvements in balance skills in the elderly, in general chronic pain, posture, respiratory function and stuttering, but there is insufficient evidence to support recommendations in these areas."
A 2012 Cochrane systematic review found that there is no conclusive evidence that the Alexander technique is effective for treating asthma, and randomized clinical trials are needed in order to assess the effectiveness of this type of treatment approach.
A review by Aetna last updated in 2016 stated: "Aetna considers the following alternative medicine interventions experimental and investigational, because there is inadequate evidence in the peer-reviewed published medical literature of their effectiveness." Included is Alexander technique in that list.
A review published in 2015 and conducted for the Australia Department of Health in order to determine what services the Australian government should pay for, reviewed clinical trials published to date and found that: "Overall, the evidence was limited by the small number of participants in the intervention arms, wide confidence intervals or a lack of replication of results." It concluded that: "The Alexander technique may improve short-term pain and disability in people with low back pain, but the longer-term effects remain uncertain. For all other clinical conditions, the effectiveness of Alexander technique was deemed to be uncertain, due to insufficient evidence." It also noted that: "Evidence for the safety of
Alexander technique was lacking, with most trials not reporting on this outcome. Subsequently in 2017 the Australian government named the Alexander Technique as a practice that would not qualify for insurance subsidy, saying this step would "ensure taxpayer funds are expended appropriately and not directed to therapies lacking evidence".
The Alexander Technique is most commonly taught privately in a series of 10 to 40 private lessons which may last from 30 minutes to an hour. Students are often performers, such as actors, dancers, musicians, athletes and public speakers, people who work on computers, or those who are in frequent pain for other reasons. Instructors observe their students, then show them how to move with better poise and less strain. Sessions include chair work – often in front of a mirror, during which the instructor and the student will stand, sit and lie down, moving efficiently while maintaining a comfortable relationship between the head, neck and spine, and table work or physical manipulation.
To qualify as a teacher of Alexander Technique, instructors are required to complete 1,600 hours, spanning three years, of supervised teacher training. The result must be satisfactory to qualified peers to gain membership in professional societies.
Alexander's approach emphasizes awareness strategies applied to conducting oneself while in action, (which could be now called "mindful" action, though in his four books he did not use that term.)
Actions such as sitting, squatting, lunging or walking are often selected by the teacher. Other actions may be selected by the student that is tailored to their interests or work activities; hobbies, computer use, lifting, driving or artistic performance or practice, sports, speech or horseback riding. Alexander teachers often use themselves as examples. They demonstrate, explain, and analyze a student's moment-to-moment responses as well as using mirrors, video feedback or classmate observations. Guided modelling with a highly skilled light hand contact is the primary tool for detecting and guiding the student into a more coordinated state in movement and at rest during in-person lessons. Suggestions for improvements are often student-specific, as everyone starts out with slightly different habits.
Exercise as a teaching tool is deliberately omitted because of a common mistaken assumption that there exists a "correct" position. There are only two specific procedures that are practiced by the student; the first is lying semi-supine. Resting in this way uses "mechanical advantage" as a means of redirecting long-term and short-term accumulated muscular tension into a more integrated and balanced state. This position is sometimes referred to as "constructive rest", or "the balanced resting state". It's also a specific time to practice Alexander's principle of conscious "directing" without "doing". The second exercise is the "Whispered Ah", which is used to co-ordinate freer breathing and vocal production.
Freedom, efficiency and patience are the prescribed values. Proscribed are unnecessary effort, self-limiting habits as well as mistaken perceptual conclusions about the nature of training and experimentation. Students are led to change their largely automatic routines that are interpreted by the teacher to currently or cumulatively be physically limiting, inefficient, or not in keeping with best "use" of themselves as a whole. The Alexander teacher provides verbal coaching while monitoring, guiding and preventing unnecessary habits at their source with a specialized hands-on assistance.
This specialized hands-on skill also allows Alexander teachers to bring about a balanced working of the student's supportive musculature as it relates to gravity's downward pull from moment to moment. Often, students require a great deal of hands-on work in order to first gain an experience of a fully poised relation to gravity and themselves. The hands-on skill requires Alexander teachers to maintain in themselves from moment-to-moment their own improved psycho-physical co-ordination that the teacher is communicating to the student.
Alexander developed terminology to describe his methods, outlined in his four books that explain the experience of learning and substituting new improvements.
"Directing" serves to counteract the common backward and downward pull and shortening in stature that can be detected at the beginning of every movement – particularly addressing a startle pattern of "fight, flight or freeze". A mere thought, as a projection of intention, shapes preparatory movement below the level of sensing it. Alexander used these words for reshaping these subliminal preparations: "The neck to be free, the head to go forward and up, the back to lengthen and widen". Some teachers have shortened this to a suggestion of, "Freer?" Negative directions (that use Alexander's other preventive principle of "inhibition") have also been found to be effective, because negative directions leave the positive response open-ended.
Whichever is used, all "Directing" is suggestively thought, (rather than willfully accomplished.) This is because the neuro-muscular responses to "Directing" often occur underneath one's ability to perceive how they are actually carried out neuro-physiologically and neuro-cognitively. As freedom of expression or movement is the objective, the most appropriate responses cannot be anticipated or expected, only observed and chosen in the moment. 
Teacher trainees gradually learn to include a constant attending to their lengthening in stature in every movement. It becomes a basis for initiating and continuing every action, every response to stimuli or while remaining constructively at rest.
Frederick Matthias Alexander (1869–1955) was a Shakespearean orator from Tasmania, who developed voice loss during his unamplified performances. After doctors found no physical cause, Alexander reasoned that he was inadvertently damaging himself while speaking. He observed himself in multiple mirrors and saw that he was contracting his posture in preparation for any speech. He hypothesized that a habitual conditioned pattern (of pulling his head backwards and downwards) needlessly was disrupting the normal working of his total postural, breathing, and vocal processes.
With experimentation, Alexander developed the ability to stop the unnecessary and habitual contracting in his neck, displacement of his head, and shortening of his stature. As he became practised at speaking without these interferences, he found that his problem with recurrent voice loss was resolved. While on a recital tour in New Zealand (1895), he came to believe in the wider significance of improved carriage for overall physical functioning although evidence from his own publications appears to indicate it happened less systematically and over a long period of time.
The American philosopher and educator John Dewey became impressed with the Alexander Technique after his headaches, neck pains, blurred vision, and stress symptoms largely improved during the time he used Alexander's advice to change his posture. In 1923, Dewey wrote the introduction to Alexander's "Constructive Conscious Control of the Individual".
Aldous Huxley had transformative lessons with Alexander, and continued doing so with other teachers after moving to the US. He rated Alexander's work highly enough to base the character of the doctor who saves the protagonist in "Eyeless in Gaza" (an experimental form of autobiographical work) on F.M. Alexander, putting many of his phrases into the character's mouth. Huxley's work "The Art of Seeing" also discusses his views on the technique.
Sir Stafford Cripps, George Bernard Shaw, Henry Irving and other stage grandees, Lord Lytton and other eminent people of the era also wrote positive appreciations of his work after taking lessons with Alexander.
Since Alexander's work in the field came at the start of the 20th century, his ideas influenced many originators in the field of mind-body improvement. Fritz Perls, who originated Gestalt therapy, credited Alexander as an inspiration for his psychological work. The Mitzvah Technique was influenced by the Alexander Technique; as was the Feldenkrais Method – who expanded on the one exercise in Alexander Technique called "The Whispered Ah."

</doc>
<doc id="1960" url="https://en.wikipedia.org/wiki?curid=1960" title="Andrea Alciato">
Andrea Alciato

Andrea Alciato (8 May 149212 January 1550), commonly known as Alciati (Andreas Alciatus), was an Italian jurist and writer. He is regarded as the founder of the French school of legal humanists.
Alciati was born in Alzate Brianza, near Milan, and settled in France in the early 16th century. He displayed great literary skill in his exposition of the laws, and was one of the first to interpret the civil law by the history, languages and literature of antiquity, and to substitute original research for the servile interpretations of the glossators. He published many legal works, and some annotations on Tacitus and accumulated a sylloge of Roman inscriptions from Milan and its territories, as part of his preparation for his history of Milan, written in 1504–05.
Alciati is most famous for his "Emblemata," published in dozens of editions from 1531 onward. This collection of short Latin verse texts and accompanying woodcuts created an entire European genre, the emblem book, which attained enormous popularity in continental Europe and Great Britain.
Alciati died at Pavia in 1550.

</doc>
<doc id="1962" url="https://en.wikipedia.org/wiki?curid=1962" title="Apparent magnitude">
Apparent magnitude

Apparent magnitude () is a measure of the brightness of a star or other astronomical object observed from Earth. An object's apparent magnitude depends on its intrinsic luminosity, its distance from Earth, and any extinction of the object's light caused by interstellar dust along the line of sight to the observer.
The magnitude scale is reverse logarithmic: the brighter an object is, the lower its magnitude. An object that is measured to be 5 magnitudes "higher" than another object is 100 times "dimmer". Consequently, a difference of 1.0 in magnitude corresponds to a brightness ratio of , or about 2.512. For example, a star of magnitude 2.0 is 2.512 times brighter than a star of magnitude 3.0 and is 100 times brighter than one of magnitude 7.0. The brightest astronomical objects have negative apparent magnitudes: for example, Venus at −4.2 or Sirius at −1.46. The faintest stars visible with the naked eye on the darkest night have apparent magnitudes of about +6.5, though this varies depending on a person's eyesight and with altitude and atmospheric conditions. The apparent magnitudes of known objects range from the Sun at −26.7 to objects in deep Hubble Space Telescope images of around magnitude +30.
The measurement of apparent magnitude is called
photometry. Photometric measurements are made in the ultraviolet, visible, or infrared wavelength bands using standard passband filters belonging to photometric systems such as the UBV system or the Strömgren "uvbyβ" system.
Absolute magnitude is a measure of the intrinsic luminosity of a celestial object rather than its apparent brightness and is expressed on the same reverse logarithmic scale. Absolute magnitude is defined as the apparent magnitude that a star or object would have if it were observed from a distance of . When referring to just "magnitude", apparent magnitude rather than absolute magnitude is normally intended.
The scale used to indicate magnitude originates in the Hellenistic practice of dividing stars visible to the naked eye into six "magnitudes". The brightest stars in the night sky were said to be of first magnitude ( = 1), whereas the faintest were of sixth magnitude ( = 6), which is the limit of human visual perception (without the aid of a telescope). Each grade of magnitude was considered twice the brightness of the following grade (a logarithmic scale), although that ratio was subjective as no photodetectors existed. This rather crude scale for the brightness of stars was popularized by Ptolemy in his "Almagest" and is generally believed to have originated with Hipparchus. This cannot be proved or disproved because Hipparchus's original star catalogue is lost. The only preserved text by Hipparchus himself (a commentary to Aratus) clearly documents that he did not have a system to describe brightnesses with numbers: He always uses terms like "big" or "small", "bright" or "faint" or even descriptions like "visible at fullmoon".
In 1856, Norman Robert Pogson formalized the system by defining a first magnitude star as a star that is 100 times as bright as a sixth-magnitude star, thereby establishing the logarithmic scale still in use today. This implies that a star of magnitude is about 2.512 times as bright as a star of magnitude . This figure, the fifth root of 100, became known as Pogson's Ratio. The zero point of Pogson's scale was originally defined by assigning Polaris a magnitude of exactly 2. Astronomers later discovered that Polaris is slightly variable, so they switched to Vega as the standard reference star, assigning the brightness of Vega as the definition of zero magnitude at any specified wavelength.
Apart from small corrections, the brightness of Vega still serves as the definition of zero magnitude for visible and near infrared wavelengths, where its spectral energy distribution (SED) closely approximates that of a black body for a temperature of . However, with the advent of infrared astronomy it was revealed that Vega's radiation includes an infrared excess presumably due to a circumstellar disk consisting of dust at warm temperatures (but much cooler than the star's surface). At shorter (e.g. visible) wavelengths, there is negligible emission from dust at these temperatures. However, in order to properly extend the magnitude scale further into the infrared, this peculiarity of Vega should not affect the definition of the magnitude scale. Therefore, the magnitude scale was extrapolated to "all" wavelengths on the basis of the black-body radiation curve for an ideal stellar surface at uncontaminated by circumstellar radiation. On this basis the spectral irradiance (usually expressed in janskys) for the zero magnitude point, as a function of wavelength, can be computed. Small deviations are specified between systems using measurement apparatuses developed independently so that data obtained by different astronomers can be properly compared, but of greater practical importance is the definition of magnitude not at a single wavelength but applying to the response of standard spectral filters used in photometry over various wavelength bands.
With the modern magnitude systems, brightness over a very wide range is specified according to the logarithmic definition detailed below, using this zero reference. In practice such apparent magnitudes do not exceed 30 (for detectable measurements). The brightness of Vega is exceeded by four stars in the night sky at visible wavelengths (and more at infrared wavelengths) as well as the bright planets Venus, Mars, and Jupiter, and these must be described by "negative" magnitudes. For example, Sirius, the brightest star of the celestial sphere, has a magnitude of −1.4 in the visible. Negative magnitudes for other very bright astronomical objects can be found in the table below.
Astronomers have developed other photometric zeropoint systems as alternatives to the Vega system. The most widely used is the AB magnitude system, in which photometric zeropoints are based on a hypothetical reference spectrum having constant flux per unit frequency interval, rather than using a stellar spectrum or blackbody curve as the reference. The AB magnitude zeropoint is defined such that an object's AB and Vega-based magnitudes will be approximately equal in the V filter band.
Precision measurement of magnitude (photometry) requires calibration of the photographic or (usually) electronic detection apparatus. This generally involves contemporaneous observation, under identical conditions, of standard stars whose magnitude using that spectral filter is accurately known. Moreover, as the amount of light actually received by a telescope is reduced due to transmission through the Earth's atmosphere, the airmasses of the target and calibration stars must be taken into account. Typically one would observe a few different stars of known magnitude which are sufficiently similar. Calibrator stars close in the sky to the target are favoured (to avoid large differences in the atmospheric paths). If those stars have somewhat different zenith angles (altitudes) then a correction factor as a function of airmass can be derived and applied to the airmass at the target's position. Such calibration obtains the brightnesses as would be observed from above the atmosphere, where apparent magnitude is defined.
The dimmer an object appears, the higher the numerical value given to its magnitude, with a difference of 5 magnitudes corresponding to a brightness factor of exactly 100. Therefore, the magnitude , in the spectral band , would be given by
which is more commonly expressed in terms of common (base-10) logarithms as
where is the observed flux density using spectral filter , and is the reference flux (zero-point) for that photometric filter. Since an increase of 5 magnitudes corresponds to a decrease in brightness by a factor of exactly 100, each magnitude increase implies a decrease in brightness by the factor ≈ 2.512 (Pogson's ratio). Inverting the above formula, a magnitude difference implies a brightness factor of
"What is the ratio in brightness between the Sun and the full Moon?"
The apparent magnitude of the Sun is −26.74 (brighter), and the mean magnitude of the full moon is −12.74 (dimmer).
Difference in magnitude: 
Brightness factor: 
The Sun appears about times brighter than the full moon.
Sometimes one might wish to add brightnesses. For example, photometry on closely separated double stars may only be able to produce a measurement of their combined light output. How would we reckon the combined magnitude of that double star knowing only the magnitudes of the individual components? This can be done by adding the brightnesses (in linear units) corresponding to each magnitude.
Solving for formula_7 yields
where is the resulting magnitude after adding the brightnesses referred to by and .
While magnitude generally refers to a measurement in a particular filter band corresponding to some range of wavelengths, the apparent or absolute bolometric magnitude (m) is a measure of an object's apparent or absolute brightness integrated over all wavelengths of the electromagnetic spectrum (also known as the object's irradiance or power, respectively). The zeropoint of the apparent bolometric magnitude scale is based on the definition that an apparent bolometric magnitude of 0 mag is equivalent to a received irradiance of 2.518×10 watts per square metre (W·m).
While apparent magnitude is a measure of the brightness of an object as seen by a particular observer, absolute magnitude is a measure of the "intrinsic" brightness of an object. Flux decreases with distance according to an inverse-square law, so the apparent magnitude of a star depends on both its absolute brightness and its distance (and any extinction). For example, a star at one distance will have the same apparent magnitude as a star four times brighter at twice that distance. In contrast, the intrinsic brightness of an astronomical object, does not depend on the distance of the observer or any extinction.
The absolute magnitude , of a star or astronomical object is defined as the apparent magnitude it would have as seen from a distance of . The absolute magnitude of the Sun is 4.83 in the V band (visual), 4.68 in the Gaia satellite's G band (green) and 5.48 in the B band (blue).
In the case of a planet or asteroid, the absolute magnitude rather means the apparent magnitude it would have if it were from both the observer and the Sun, and fully illuminated at maximum opposition (a configuration that is only theoretically achievable, with the observer situated on the surface of the Sun). 
The magnitude scale is a reverse logarithmic scale. A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber–Fechner law), but it is now believed that the response is a power law (see Stevens' power law).
Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350 nm, in the near ultraviolet), B (about 435 nm, in the blue region) and V (about 555 nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the human eye. When an apparent magnitude is discussed without further qualification, the V magnitude is generally understood.
Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum, their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.
Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.
For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. For objects at very great distances (far beyond the Milky Way), this relationship must be adjusted for redshifts and for non-Euclidean distance measures due to general relativity.
For planets and other Solar System bodies, the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.
Some of the listed magnitudes are approximate. Telescope sensitivity depends on observing time, optical bandpass, and interfering light from scattering and airglow.

</doc>
<doc id="1963" url="https://en.wikipedia.org/wiki?curid=1963" title="Absolute magnitude">
Absolute magnitude

Absolute magnitude () is a measure of the luminosity of a celestial object, on an inverse logarithmic astronomical magnitude scale. An object's absolute magnitude is defined to be equal to the apparent magnitude that the object would have if it were viewed from a distance of exactly , without extinction (or dimming) of its light due to absorption by interstellar matter and cosmic dust. By hypothetically placing all objects at a standard reference distance from the observer, their luminosities can be directly compared on a magnitude scale.
As with all astronomical magnitudes, the absolute magnitude can be specified for different wavelength ranges corresponding to specified filter bands or passbands; for stars a commonly quoted absolute magnitude is the absolute visual magnitude, which uses the visual (V) band of the spectrum (in the UBV photometric system). Absolute magnitudes are denoted by a capital M, with a subscript representing the filter band used for measurement, such as M for absolute magnitude in the V band.
The more luminous an object, the smaller the numerical value of its absolute magnitude. A difference of 5 magnitudes between the absolute magnitudes of two objects corresponds to a ratio of 100 in their luminosities, and a difference of n magnitudes in absolute magnitude corresponds to a luminosity ratio of 100. For example, a star of absolute magnitude M=3.0 would be 100 times as luminous as a star of absolute magnitude M=8.0 as measured in the V filter band. The Sun has absolute magnitude M=+4.83. Highly luminous objects can have negative absolute magnitudes: for example, the Milky Way galaxy has an absolute B magnitude of about −20.8.
An object's absolute "bolometric" magnitude (M) represents its total luminosity over all wavelengths, rather than in a single filter band, as expressed on a logarithmic magnitude scale. To convert from an absolute magnitude in a specific filter band to absolute bolometric magnitude, a bolometric correction (BC) is applied.
For Solar System bodies that shine in reflected light, a different definition of absolute magnitude (H) is used, based on a standard reference distance of one astronomical unit.
In stellar and galactic astronomy, the standard distance is 10 parsecs (about 32.616 light-years, 308.57 petameters or 308.57 trillion kilometres). A star at 10 parsecs has a parallax of 0.1″ (100 milliarcseconds). Galaxies (and other extended objects) are much larger than 10 parsecs, their light is radiated over an extended patch of sky, and their overall brightness cannot be directly observed from relatively short distances, but the same convention is used. A galaxy's magnitude is defined by measuring all the light radiated over the entire object, treating that integrated brightness as the brightness of a single point-like or star-like source, and computing the magnitude of that point-like source as it would appear if observed at the standard 10 parsecs distance. Consequently, the absolute magnitude of any object "equals" the apparent magnitude it "would have" if it were 10 parsecs away.
The measurement of absolute magnitude is made with an instrument called a bolometer. When using an absolute magnitude, one must specify the type of electromagnetic radiation being measured. When referring to total energy output, the proper term is bolometric magnitude. The bolometric magnitude usually is computed from the visual magnitude plus a bolometric correction, . This correction is needed because very hot stars radiate mostly ultraviolet radiation, whereas very cool stars radiate mostly infrared radiation (see Planck's law).
Some stars visible to the naked eye have such a low absolute magnitude that they would appear bright enough to outshine the planets and cast shadows if they were at 10 parsecs from the Earth. Examples include Rigel (−7.0), Deneb (−7.2), Naos (−6.0), and Betelgeuse (−5.6). For comparison, Sirius has an absolute magnitude of only 1.4, which is still brighter than the Sun, whose absolute visual magnitude is 4.83. The Sun's absolute bolometric magnitude is set arbitrarily, usually at 4.75.
Absolute magnitudes of stars generally range from −10 to +17. The absolute magnitudes of galaxies can be much lower (brighter). For example, the giant elliptical galaxy M87 has an absolute magnitude of −22 (i.e. as bright as about 60,000 stars of magnitude −10). Some active galactic nuclei (quasars like CTA-102) can reach absolute magnitudes in excess of −32, making them the most luminous objects in the observable universe.
The Greek astronomer Hipparchus established a numerical scale to describe the brightness of each star appearing in the sky. The brightest stars in the sky were assigned an apparent magnitude , and the dimmest stars visible to the naked eye are assigned . The difference between them corresponds to a factor of 100 in brightness. For objects within the immediate neighborhood of the Sun, the absolute magnitude and apparent magnitude from any distance (in parsecs, with 1 pc = 3.2616 light-years) are related by
where is the radiant flux measured at distance (in parsecs), the radiant flux measured at distance . Using the common logarithm, the equation can be written as
where it is assumed that extinction from gas and dust is negligible. Typical extinction rates within the Milky Way galaxy are 1 to 2 magnitudes per kiloparsec, when dark clouds are taken into account.
For objects at very large distances (outside the Milky Way) the luminosity distance (distance defined using luminosity measurements) must be used instead of , because the Euclidean approximation is invalid for distant objects. Instead, general relativity must be taken into account. Moreover, the cosmological redshift complicates the relationship between absolute and apparent magnitude, because the radiation observed was shifted into the red range of the spectrum. To compare the magnitudes of very distant objects with those of local objects, a K correction might have to be applied to the magnitudes of the distant objects.
The absolute magnitude can also be written in terms of the apparent magnitude and stellar parallax :
or using apparent magnitude and distance modulus :
Rigel has a visual magnitude of 0.12 and distance of about 860 light-years:
Vega has a parallax of 0.129″, and an apparent magnitude of 0.03:
The Black Eye Galaxy has a visual magnitude of 9.36 and a distance modulus of 31.06:
The bolometric magnitude , takes into account electromagnetic radiation at all wavelengths. It includes those unobserved due to instrumental passband, the Earth's atmospheric absorption, and extinction by interstellar dust. It is defined based on the luminosity of the stars. In the case of stars with few observations, it must be computed assuming an effective temperature.
Classically, the difference in bolometric magnitude is related to the luminosity ratio according to:
which makes by inversion:
where
In August 2015, the International Astronomical Union passed Resolution B2 defining the zero points of the absolute and apparent bolometric magnitude scales in SI units for power (watts) and irradiance (W/m), respectively. Although bolometric magnitudes had been used by astronomers for many decades, there had been systematic differences in the absolute magnitude-luminosity scales presented in various astronomical references, and no international standardization. This led to systematic differences in bolometric corrections scales. Combined with incorrect assumed absolute bolometric magnitudes for the Sun, this could lead to systematic errors in estimated stellar luminosities (and other stellar properties, such as radii or ages, which rely on stellar luminosity to be calculated).
Resolution B2 defines an absolute bolometric magnitude scale where corresponds to luminosity , with the zero point luminosity set such that the Sun (with nominal luminosity ) corresponds to absolute bolometric magnitude 4.74. Placing a radiation source (e.g. star) at the standard distance of 10 parsecs, it follows that the zero point of the apparent bolometric magnitude scale corresponds to irradiance . Using the IAU 2015 scale, the nominal total solar irradiance ("solar constant") measured at 1 astronomical unit () corresponds to an apparent bolometric magnitude of the Sun of −26.832.
Following Resolution B2, the relation between a star's absolute bolometric magnitude and its luminosity is no longer directly tied to the Sun's (variable) luminosity:
where
The new IAU absolute magnitude scale permanently disconnects the scale from the variable Sun. However, on this SI power scale, the nominal solar luminosity corresponds closely to 4.74, a value that was commonly adopted by astronomers before the 2015 IAU resolution.
The luminosity of the star in watts can be calculated as a function of its absolute bolometric magnitude as:
using the variables as defined previously.
For planets and asteroids, a definition of absolute magnitude that is more meaningful for non-stellar objects is used. The absolute magnitude, commonly called formula_12, is defined as the apparent magnitude that the object would have if it were one astronomical unit (AU) from both the Sun and the observer, and in conditions of ideal solar opposition (an arrangement that is impossible in practice). Solar System bodies are illuminated by the Sun, therefore the magnitude varies as a function of illumination conditions, described by the phase angle. This relationship is referred to as the phase curve. The absolute magnitude is the brightness at phase angle zero, an arrangement known as opposition, from a distance of one AU.
The absolute magnitude formula_12 can be used to calculate the apparent magnitude formula_14 of a body. For an object reflecting sunlight, formula_12 and formula_14 are connected by the relation
where formula_18 is the phase angle, the angle between the body-Sun and body–observer lines. formula_19 is the phase integral (the integration of reflected light; a number in the 0 to 1 range).
By the law of cosines, we have:
Distances:
The value of formula_19 depends on the properties of the reflecting surface, in particular on its roughness. In practice, different approximations are used based on the known or assumed properties of the surface.
Planetary bodies can be approximated reasonably well as ideal diffuse reflecting spheres. Let formula_18 be the phase angle in degrees, then
A full-phase diffuse sphere reflects two-thirds as much light as a diffuse flat disk of the same diameter. A quarter phase (formula_25) has formula_26 as much light as full phase (formula_27).
For contrast, a "diffuse disk reflector model" is simply formula_28, which isn't realistic, but it does represent the opposition surge for rough surfaces that reflect more uniform light back at low phase angles.
The definition of the geometric albedo formula_29, a measure for the reflectivity of planetary surfaces, is based on the diffuse disk reflector model. The absolute magnitude formula_12, diameter formula_31 (in kilometers) and geometric albedo formula_29 of a body are related by
Example: The Moon's absolute magnitude formula_12 can be calculated from its diameter formula_35 and geometric albedo formula_36:
We have formula_38, formula_39
At quarter phase, formula_40 (according to the diffuse reflector model), this yields an apparent magnitude of formula_41 The actual value is somewhat lower than that, formula_42 The phase curve of the Moon is too complicated for the diffuse reflector model.
Because Solar System bodies are never perfect diffuse reflectors, astronomers use different models to predict apparent magnitudes based on known or assumed properties of the body. For planets, approximations for the correction term formula_43 in the formula for have been derived empirically, to match observations at different phase angles. The approximations recommended by the Astronomical Almanac are (with formula_18 in degrees):
Here formula_45 is the effective inclination of Saturn's rings (their tilt relative to the observer), which as seen from Earth varies between 0° and 27° over the course of one Saturn orbit, and formula_46 is a small correction term depending on Uranus' sub-Earth and sub-solar latitudes. formula_47 is the Common Era year. Neptune's absolute magnitude is changing slowly due to seasonal effects as the planet moves along its 165-year orbit around the Sun, and the approximation above is only valid after the year 2000. For some circumstances, like formula_48 for Venus, no observations are available, and the phase curve is unknown in those cases.
Example: On 1 January 2019, Venus was formula_49 from the Sun, and formula_50 from Earth, at a phase angle of formula_51 (near quarter phase). Under full-phase conditions, Venus would have been visible at formula_52 Accounting for the high phase angle, the correction term above yields an actual apparent magnitude of formula_53 This is close to the value of formula_54 predicted by the Jet Propulsion Laboratory.
Earth's albedo varies by a factor of 6, from 0.12 in the cloud-free case to 0.76 in the case of altostratus cloud. The absolute magnitude here corresponds to an albedo of 0.434. Earth's apparent magnitude cannot be predicted as accurately as that of most other planets.
If an object has an atmosphere, it reflects light more or less isotropically in all directions, and its brightness can be modelled as a diffuse reflector. Atmosphereless bodies, like asteroids or moons, tend to reflect light more strongly to the direction of the incident light, and their brightness increases rapidly as the phase angle approaches formula_55. This rapid brightening near opposition is called the opposition effect. Its strength depends on the physical properties of the body's surface, and hence it differs from asteroid to asteroid.
In 1985, the IAU adopted the semi-empirical formula_56-system, based on two parameters formula_12 and formula_58 called "absolute magnitude" and "slope", to model the opposition effect for the ephemerides published by the Minor Planet Center.
where
and
This relation is valid for phase angles formula_68, and works best when formula_69.
The slope parameter formula_58 relates to the surge in brightness, typically , when the object is near opposition. It is known accurately only for a small number of asteroids, hence for most asteroids a value of formula_71 is assumed. In rare cases, formula_58 can be negative. An example is 101955 Bennu, with formula_73.
In 2012, the formula_56-system was officially replaced by an improved system with three parameters formula_12, formula_76 and formula_77, which produces more satisfactory results if the opposition effect is very small or restricted to very small phase angles. However, as of 2019, this formula_78-system has not been adopted by either the Minor Planet Center nor Jet Propulsion Laboratory.
The apparent magnitude of asteroids varies as they rotate, on time scales of seconds to weeks depending on their rotation period, by up to formula_79 or more. In addition, their absolute magnitude can vary with the viewing direction, depending on their axial tilt. In many cases, neither the rotation period nor the axial tilt are known, limiting the predictability. The models presented here do not capture those effects.
The brightness of comets is given separately as "total magnitude" (formula_80, the brightness integrated over the entire visible extend of the coma) and "nuclear magnitude" (formula_81, the brightness of the core region alone). Both are different scales than the magnitude scale used for planets and asteroids, and can not be used for a size comparison with an asteroid's absolute magnitude .
The activity of comets varies with their distance from the Sun. Their brightness can be approximated as
where formula_84 are the total and nuclear apparent magnitudes of the comet, respectively, formula_85 are its "absolute" total and nuclear magnitudes, formula_86 and formula_87 are the body-sun and body-observer distances, formula_88 is the Astronomical Unit, and formula_89 are the slope parameters characterising the comet's activity. For formula_90, this reduces to the formula for a purely reflecting body.
For example, the lightcurve of comet C/2011 L4 (PANSTARRS) can be approximated by formula_91 On the day of its perihelion passage, 10 March 2013, comet PANSTARRS was formula_92 from the Sun and formula_93 from Earth. The total apparent magnitude formula_80 is predicted to have been formula_95 at that time. The Minor Planet Center gives a value close to that, formula_96.
The absolute magnitude of any given comet can vary dramatically. It can change as the comet becomes more or less active over time, or if it undergoes an outburst. This makes it difficult to use the absolute magnitude for a size estimate. When comet 289P/Blanpain was discovered in 1819, its absolute magnitude was estimated as formula_97. It was subsequently lost, and was only rediscovered in 2003. At that time, its absolute magnitude had decreased to formula_98, and it was realised that the 1819 apparition coincided with an outburst. 289P/Blanpain reached naked eye brightness (5–8 mag) in 1819, even though it is the comet with the smallest nucleus that has ever been physically characterised, and usually doesn't become brighter than 18 mag.
For some comets that have been observed at heliocentric distances large enough to distinguish between light reflected from the coma, and light from the nucleus itself, an absolute magnitude analogous to that used for asteroids has been calculated, allowing to estimate the sizes of their nuclei.
For a meteor, the standard distance for measurement of magnitudes is at an altitude of at the observer's zenith.
/5}</math>, where formula_99, the absolute magnitude of the Sun, and formula_100</ref>

</doc>
<doc id="1965" url="https://en.wikipedia.org/wiki?curid=1965" title="Apollo 1">
Apollo 1

Apollo 1, initially designated AS-204, was the first crewed mission of the United States Apollo program, the undertaking to land the first humans on the Moon. Planned as the first low Earth orbital test of the Apollo command and service module, to launch on February 21, 1967, the mission never flew; a cabin fire during a launch rehearsal test at Cape Kennedy Air Force Station Launch Complex 34 on January 27 killed all three crew members—Command Pilot Virgil I. "Gus" Grissom, Senior Pilot Ed White, and Pilot Roger B. Chaffee—and destroyed the command module (CM). The name Apollo1, chosen by the crew, was made official by NASA in their honor after the fire.
Immediately after the fire, NASA convened the Apollo 204 Accident Review Board to determine the cause of the fire, and both houses of the United States Congress conducted their own committee inquiries to oversee NASA's investigation. The ignition source of the fire was determined to be electrical, and the fire spread rapidly due to combustible nylon material, and the high pressure, pure oxygen cabin atmosphere. Rescue was prevented by the plug door hatch, which could not be opened against the internal pressure of the cabin. Because the rocket was unfueled, the test had not been considered hazardous, and emergency preparedness for it was poor.
During the Congressional investigation, Senator Walter Mondale publicly revealed a NASA internal document citing problems with prime Apollo contractor North American Aviation, which became known as the Phillips Report. This disclosure embarrassed NASA Administrator James E. Webb, who was unaware of the document's existence, and attracted controversy to the Apollo program. Despite congressional displeasure at NASA's lack of openness, both congressional committees ruled that the issues raised in the report had no bearing on the accident.
Crewed Apollo flights were suspended for 20 months while the command module's hazards were addressed. However, the development and uncrewed testing of the lunar module (LM) and SaturnV rocket continued. The SaturnIB launch vehicle for Apollo1, SA-204, was used for the first LM test flight, Apollo5. The first successful crewed Apollo mission was flown by Apollo1's backup crew on Apollo7 in October 1968.
AS-204 was to be the first crewed test flight of the Apollo command and service module (CSM) to Earth orbit, launched on a Saturn IB rocket. AS-204 was to test launch operations, ground tracking and control facilities and the performance of the Apollo-Saturn launch assembly and would have lasted up to two weeks, depending on how the spacecraft performed.
The CSM for this flight, number 012 built by North American Aviation (NAA), was a Block I version designed before the lunar orbit rendezvous landing strategy was chosen; therefore it lacked capability of docking with the lunar module. This was incorporated into the Block II CSM design, along with lessons learned in Block I. Block II would be test-flown with the LM when the latter was ready, and would be used on the Moon landing flights.
Director of Flight Crew Operations Deke Slayton selected the first Apollo crew in January 1966, with Grissom as Command Pilot, White as Senior Pilot, and rookie Donn F. Eisele as Pilot. But Eisele dislocated his shoulder twice aboard the KC135 weightlessness training aircraft, and had to undergo surgery on January 27. Slayton replaced him with Chaffee, and NASA announced the crew selection on March 21, 1966. James McDivitt, David Scott and Russell Schweickart were named as the backup crew.
On September 29, Walter Schirra, Eisele, and Walter Cunningham were named as the prime crew for a second Block I CSM flight, AS-205. NASA planned to follow this with an uncrewed test flight of the LM (AS-206), then the third crewed mission would be a dual flight designated AS-278 (or AS-207/208), in which AS-207 would launch the first crewed Block II CSM, which would then rendezvous and dock with the LM launched uncrewed on AS-208.
In March, NASA was studying the possibility of flying the first Apollo mission as a joint space rendezvous with the final Project Gemini mission, Gemini 12 in November 1966. But by May, delays in making Apollo ready for flight just by itself, and the extra time needed to incorporate compatibility with the Gemini, made that impractical. This became moot when slippage in readiness of the AS-204 spacecraft caused the last-quarter 1966 target date to be missed, and the mission was rescheduled for February 21, 1967.
In October 1966, NASA announced the flight would carry a small television camera to broadcast live from the command module. The camera would also be used to allow flight controllers to monitor the spacecraft's instrument panel in flight. Television cameras were carried aboard all crewed Apollo missions.
Grissom's crew received approval in June 1966 to design a mission patch with the name "Apollo1" (though the approval was subsequently withdrawn pending a final decision on the mission designation, which wasn't resolved until after the fire). The design's center depicts a command and service module flying over the southeastern United States with Florida (the launch point) prominent. The Moon is seen in the distance, symbolic of the eventual program goal. A yellow border carries the mission and astronaut names with another border set with stars and stripes, trimmed in gold. The insignia was designed by the crew, with the artwork done by North American Aviation employee Allen Stevens.
The Apollo command and service module was much bigger and far more complex than any previously implemented spacecraft design. In October 1963, Joseph F. Shea was named Apollo Spacecraft Program Office (ASPO) manager, responsible for managing the design and construction of both the CSM and the LM.
In a spacecraft review meeting held with Shea on August 19, 1966 (a week before delivery), the crew expressed concern about the amount of flammable material (mainly nylon netting and Velcro) in the cabin, which both astronauts and technicians found convenient for holding tools and equipment in place. Although Shea gave the spacecraft a passing grade, after the meeting they gave him a crew portrait they had posed with heads bowed and hands clasped in prayer, with the inscription:
Shea gave his staff orders to tell North American to remove the flammables from the cabin, but did not supervise the issue personally.
North American shipped spacecraft CM-012 to Kennedy Space Center on August 26, 1966, under a conditional Certificate of Flight Worthiness: 113 significant incomplete planned engineering changes had to be completed at KSC. But that was not all; an additional 623 engineering change orders were made and completed after delivery. Grissom became so frustrated with the inability of the training simulator engineers to keep up with the spacecraft changes, that he took a lemon from a tree by his house and hung it on the simulator.
The command and service modules were mated in the KSC altitude chamber in September, and combined system testing was performed. Altitude testing was performed first uncrewed, then with both the prime and backup crews, from October 10 through December 30. During this testing, the environmental control unit in the command module was found to have a design flaw, and was sent back to the manufacturer for design changes and rework. The returned ECU then leaked water/glycol coolant, and had to be returned a second time. Also during this time, a propellant tank in service module 017 had ruptured during testing at NAA, prompting the separation of the modules and removal from the chamber so the service module could be tested for signs of the tank problem. These tests were negative.
In December, the second Block I flight AS-205 was canceled as unnecessary; and Schirra, Eisele and Cunningham were reassigned as the backup crew for Apollo1. McDivitt's crew was now promoted to prime crew of the Block II / LM mission, re-designated AS-258 because the AS-205 launch vehicle would be used in place of AS-207. A third crewed mission was planned to launch the CSM and LM together on a SaturnV (AS-503) to an elliptical medium Earth orbit (MEO), to be crewed by Frank Borman, Michael Collins and William Anders. McDivitt, Scott and Schweickart had started their training for AS-258 in CM-101 at the NAA plant in Downey, California, when the Apollo1 accident occurred.
Once all outstanding CSM-012 hardware problems were fixed, the reassembled spacecraft finally completed a successful altitude chamber test with Schirra's backup crew on December 30. According to the final report of the accident investigation board, "At the post-test debriefing the backup flight crew expressed their satisfaction with the condition and performance of the spacecraft." This would appear to contradict the account given in "Lost Moon: The Perilous Voyage of Apollo13" by Jeffrey Kluger and astronaut James Lovell, that "When the trio climbed out of the ship... Schirra made it clear that he was not pleased with what he had seen," and that he later warned Grissom and Shea that "there's nothing wrong with this ship that I can point to, but it just makes me uncomfortable. Something about it just doesn't ring right," and that Grissom should get out at the first sign of trouble.
Following the successful altitude tests, the spacecraft was removed from the altitude chamber on January 3, 1967, and mated to its Saturn IB launch vehicle on pad 34 on January 6.
Grissom said in a February 1963 interview that NASA could not eliminate risk despite precautions:
"I suppose that someday we are going to have a failure. In every other business there are failures, and they are bound to happen sooner or later", he added. Grissom was asked about the fear of potential catastrophe in a December 1966 interview:
The launch simulation on January 27, 1967, on pad 34, was a "plugs-out" test to determine whether the spacecraft would operate nominally on (simulated) internal power while detached from all cables and umbilicals. Passing this test was essential to making the February 21 launch date. The test was considered non-hazardous because neither the launch vehicle nor the spacecraft was loaded with fuel or cryogenics, and all pyrotechnic systems (explosive bolts) were disabled.
At 1:00 pm EST (1800 GMT) on January 27, first Grissom, then Chaffee, and White entered the command module fully pressure-suited, and were strapped into their seats and hooked up to the spacecraft's oxygen and communication systems. Grissom immediately noticed a strange odor in the air circulating through his suit which he compared to "sour buttermilk", and the simulated countdown was put on hold at 1:20 pm, while air samples were taken. No cause of the odor could be found, and the countdown was resumed at 2:42 pm. The accident investigation found this odor not to be related to the fire.
Three minutes after the count was resumed, the hatch installation was started. The hatch consisted of three parts: a removable inner hatch, which stayed inside the cabin; a hinged outer hatch, which was part of the spacecraft's heat shield; and an outer hatch cover, which was part of the boost protective cover enveloping the entire command module to protect it from aerodynamic heating during launch, and from launch escape rocket exhaust in the event of a launch abort. The boost hatch cover was partially, but not fully, latched in place because the flexible boost protective cover was slightly distorted by some cabling run under it to provide the simulated internal power. (The spacecraft's fuel cell reactants were not loaded for this test.) After the hatches were sealed, the air in the cabin was replaced with pure oxygen at , higher than atmospheric pressure.
Movement by the astronauts was detected by the spacecraft's inertial measurement unit and the astronaut's biomedical sensors, and also indicated by increases in oxygen spacesuit flow, and sounds from Grissom's stuck-open microphone. There was no evidence to identify the movement, or whether it was related to the fire. The stuck microphone was part of a problem with the communications loop connecting the crew, the Operations and Checkout Building, and the Complex 34 blockhouse control room. The poor communications led Grissom to remark: "How are we going to get to the Moon if we can't talk between two or three buildings?" The simulated countdown was put on hold again at 5:40 pm while attempts were made to troubleshoot the communications problem. All countdown functions up to the simulated internal power transfer had been successfully completed by 6:20 pm, but at 6:30 the count remained on hold at T minus 10 minutes.
The crew members were using the time to run through their checklist again, when a momentary increase in AC Bus2 voltage occurred. Nine seconds later (at 6:31:04.7), one of the astronauts (some listeners and laboratory analysis indicate Grissom) exclaimed "Hey!", "Fire!", or "Flame!"; this was followed by two seconds of scuffling sounds through Grissom's open microphone. This was immediately followed at 6:31:06.2 (23:31:06.2 GMT) by someone (believed by most listeners, and supported by laboratory analysis, to be Chaffee) saying, "[I've, or We've] got a fire in the cockpit." After 6.8 seconds of silence a second, badly garbled transmission was heard by various listeners as:
The transmission lasted 5.0 seconds and ended with a cry of pain.
Some blockhouse witnesses said that they saw White on the television monitors, reaching for the inner hatch release handle as flames in the cabin spread from left to right.
The intensity of the fire fed by pure oxygen caused the pressure to rise to , which ruptured the command module's inner wall at 6:31:19 (23:31:19 GMT, initial phase of the fire). Flames and gases then rushed outside the command module through open access panels to two levels of the pad service structure. Intense heat, dense smoke, and ineffective gas masks designed for toxic fumes rather than heavy smoke hampered the ground crew's attempts to rescue the men. There were fears the command module had exploded, or soon would, and that the fire might ignite the solid fuel rocket in the launch escape tower above the command module, which would have likely killed nearby ground personnel, and possibly have destroyed the pad.
As the pressure was released by the cabin rupture, the convective rush of air caused the flames to spread across the cabin, beginning the second phase. The third phase began when most of the oxygen was consumed and was replaced with atmospheric air, essentially quenching the fire, but causing high concentrations of carbon monoxide and heavy smoke to fill the cabin, and large amounts of soot to be deposited on surfaces as they cooled.
It took five minutes for the pad workers to open all three hatch layers, and they could not drop the inner hatch to the cabin floor as intended, so they pushed it out of the way to one side. Although the cabin lights remained lit, they were at first unable to find the astronauts through the dense smoke. As the smoke cleared, they found the bodies, but were not able to remove them. The fire had partly melted Grissom's and White's nylon space suits and the hoses connecting them to the life support system. Grissom had removed his restraints and was lying on the floor of the spacecraft. White's restraints were burned through, and he was found lying sideways just below the hatch. It was determined that he had tried to open the hatch per the emergency procedure, but was not able to do so against the internal pressure. Chaffee was found strapped into his right-hand seat, as procedure called for him to maintain communication until White opened the hatch. Because of the large strands of melted nylon fusing the astronauts to the cabin interior, removing the bodies took nearly 90 minutes.
Deke Slayton was possibly the first NASA official to examine the spacecraft interior. His testimony contradicted the official report concerning the position of Grissom's body. Slayton said of Grissom and White's bodies, "It is very difficult for me to determine the exact relationships of these two bodies. They were sort of jumbled together, and I couldn't really tell which head even belonged to which body at that point. I guess the only thing that was real obvious is that both bodies were at the lower edge of the hatch. They were not in the seats. They were almost completely clear of the seat areas."
As a result of the in-flight failure of the Gemini 8 mission on March 17, 1966, NASA Deputy Administrator Robert Seamans wrote and implemented "Management Instruction 8621.1" on April 14, 1966, defining "Mission Failure Investigation Policy And Procedures". This modified NASA's existing accident procedures, based on military aircraft accident investigation, by giving the Deputy Administrator the option of performing independent investigations of major failures, beyond those for which the various Program Office officials were normally responsible. It declared, "It is NASA policy to investigate and document the causes of all major mission failures which occur in the conduct of its space and aeronautical activities and to take appropriate corrective actions as a result of the findings and recommendations."
Immediately after the Apollo1 fire, to avoid appearance of a conflict of interest, NASA Administrator James E. Webb asked President Lyndon B. Johnson to allow NASA to handle the investigation according to its established procedure, promising to be truthful in assessing blame, and to keep the appropriate leaders of Congress informed. Seamans then directed establishment of the "Apollo 204 Review Board" chaired by Langley Research Center director Floyd L. Thompson, which included astronaut Frank Borman, spacecraft designer Maxime Faget, and six others. On February 1, Cornell University professor Frank A. Long left the board, and was replaced by Dr. Robert W. Van Dolah, of the U.S. Bureau of Mines. The next day, North American's chief engineer for Apollo, George Jeffs, also left.
Seamans immediately ordered all Apollo1 hardware and software impounded, to be released only under control of the board. After thorough stereo photographic documentation of the CM-012 interior, the board ordered its disassembly using procedures tested by disassembling the identical CM-014, and conducted a thorough investigation of every part. The board also reviewed the astronauts' autopsy results and interviewed witnesses. Seamans sent Webb weekly status reports of the investigation's progress, and the board issued its final report on April 5, 1967.
According to the Board, Grissom suffered severe third-degree burns on over one-third of his body and his spacesuit was mostly destroyed. White suffered third-degree burns on almost half of his body and a quarter of his spacesuit had melted away. Chaffee suffered third-degree burns over almost a quarter of his body and a small portion of his spacesuit was damaged. The autopsy report confirmed that the primary cause of death for all three astronauts was cardiac arrest caused by high concentrations of carbon monoxide. Burns suffered by the crew were not believed to be major factors, and it was concluded that most of them had occurred postmortem. Asphyxiation occurred after the fire melted the astronauts' suits and oxygen tubes, exposing them to the lethal atmosphere of the cabin.
The review board identified several major factors which combined to cause the fire and the astronauts' deaths:
The review board determined that the electrical power momentarily failed at 23:30:55 GMT, and found evidence of several electric arcs in the interior equipment. They were unable to conclusively identify a single ignition source. They determined that the fire most likely started near the floor in the lower left section of the cabin, close to the Environmental Control Unit. It spread from the left wall of the cabin to the right, with the floor being affected only briefly.
The board noted that a silver-plated copper wire, running through an environmental control unit near the center couch, had become stripped of its Teflon insulation and abraded by repeated opening and closing of a small access door.
This weak point in the wiring also ran near a junction in an ethylene glycol/water cooling line that had been prone to leaks. The electrolysis of ethylene glycol solution with the silver anode was discovered at the Manned Spacecraft Center on May 29, 1967, to be a hazard capable of causing a violent exothermic reaction, igniting the ethylene glycol mixture in the Command Module's pure oxygen atmosphere. Experiments at the Illinois Institute of Technology confirmed the hazard existed for silver-plated wires, but not for copper-only or nickel-plated copper. In July, ASPO directed both North American and Grumman to ensure no silver or silver-coated electrical contacts existed in the vicinity of possible glycol spills in the Apollo spacecraft.
The plugs-out test had been run to simulate the launch procedure, with the cabin pressurized with pure oxygen at the nominal launch level of , above standard sea level atmospheric pressure. This is more than five times the partial pressure of oxygen in the atmosphere, and provides an environment in which materials not normally considered flammable will be highly flammable and burst into flame.
The high-pressure oxygen atmosphere was similar to that which had been used successfully in the Mercury and Gemini programs. The pressure before launch was deliberately greater than ambient in order to drive out the nitrogen-containing air and replace it with pure oxygen, and also to seal the plug door hatch cover. During launch, the pressure would have been gradually reduced to the in-flight level of , providing sufficient oxygen for the astronauts to breathe while reducing the fire risk. The Apollo1 crew had successfully tested this procedure with their spacecraft in the Operations and Checkout Building altitude (vacuum) chamber on October 18 and 19, 1966, and the backup crew of Schirra, Eisele and Cunningham had repeated it on December 30. The investigation board noted that, during these tests, the command module had been fully pressurized with pure oxygen four times, for a total of six hours and fifteen minutes, two and a half hours longer than it had been during the plugs-out test.
The review board cited "many types and classes of combustible material" close to ignition sources. The NASA crew systems department had installed of Velcro throughout the spacecraft, almost like carpeting. This Velcro was found to be flammable in a high-pressure 100% oxygen environment. Astronaut Buzz Aldrin states in his book "Men From Earth" that the flammable material had been removed per the crew's August 19 complaints and Joseph Shea's order, but was replaced before the August 26 delivery to Cape Kennedy.
The inner hatch cover used a plug door design, sealed by higher pressure inside the cabin than outside. The normal pressure level used for launch ( above ambient) created sufficient force to prevent removing the cover until the excess pressure was vented. Emergency procedure called for Grissom to open the cabin vent valve first, allowing White to remove the cover, but Grissom was prevented from doing this because the valve was located to the left, behind the initial wall of flames. Also, while the system could easily vent the normal pressure, its flow capacity was utterly incapable of handling the rapid increase to absolute caused by the intense heat of the fire.
North American had originally suggested the hatch open outward and use explosive bolts to blow the hatch in case of emergency, as had been done in Project Mercury. NASA did not agree, arguing the hatch could accidentally open, as it had on Grissom's "Liberty Bell 7" flight, so the Manned Spacecraft Center designers rejected the explosive design in favor of a mechanically operated one for the Gemini and Apollo programs. Before the fire, the Apollo astronauts had recommended changing the design to an outward-opening hatch, and this was already slated for inclusion in the Block II command module design. According to Donald K. Slayton's testimony before the House investigation of the accident, this was based on ease of exit for spacewalks and at the end of flight, rather than for emergency exit.
The board noted that the test planners had failed to identify the test as hazardous; the emergency equipment (such as gas masks) were inadequate to handle this type of fire; that fire, rescue, and medical teams were not in attendance; and that the spacecraft work and access areas contained many hindrances to emergency response such as steps, sliding doors, and sharp turns.
When designing the Mercury spacecraft, NASA had considered using a nitrogen/oxygen mixture to reduce the fire risk near launch, but rejected it based on a number of considerations. First, a pure oxygen atmosphere is comfortably breathable by humans at five psi, greatly reducing the pressure load on the spacecraft in the vacuum of space. Second, nitrogen used with the in-flight pressure reduction carried the risk of decompression sickness (known as "the bends"). But the decision to eliminate the use of any gas but oxygen was crystalized when a serious accident occurred on April 21, 1960, in which McDonnell Aircraft test pilot G. B. North passed out and was seriously injured when testing a Mercury cabin / spacesuit atmosphere system in a vacuum chamber. The problem was found to be nitrogen-rich (oxygen-poor) air leaking from the cabin into his spacesuit feed. North American Aviation had suggested using an oxygen/nitrogen mixture for Apollo, but NASA overruled this. The pure oxygen design was judged to be safer, less complicated, and lighter in weight. In his monograph "Project Apollo: The Tough Decisions", Deputy Administrator Seamans wrote that NASA's worst mistake in engineering judgment was not to run a fire test on the command module before the plugs-out test. In the first episode of the 2009 BBC documentary series "NASA: Triumph and Tragedy", Jim McDivitt said that NASA had no idea how a 100% oxygen atmosphere would influence burning. Similar remarks by other astronauts were expressed in the 2007 documentary film "In the Shadow of the Moon".
Several fires in high-oxygen test environments had occurred before the Apollo fire. In 1962, USAF Colonel B. Dean Smith was conducting a test of the Gemini space suit with a colleague in a pure oxygen chamber at Brooks Air Force Base in San Antonio, Texas, when a fire broke out, destroying the chamber. Smith and his partner narrowly escaped. On February 16, 1965, United States Navy Divers Fred Jackson and John Youmans were killed in a decompression chamber fire at the Experimental Diving Unit in Washington, D.C., shortly after additional oxygen was added to the chamber's atmospheric mix.
Other oxygen fire occurrences are documented in reports archived in the National Air and Space Museum, such as:
Incidents had also occurred in the Soviet space program, but due to the Soviet government's policy of secrecy, these were not disclosed until well after the Apollo1 fire. Cosmonaut Valentin Bondarenko died on March 23, 1961, from burns sustained in a fire while participating in a 15-day endurance experiment in a high-oxygen isolation chamber, less than three weeks before the first Vostok crewed space flight; this was disclosed on January 28, 1986.
During the Voskhod 2 mission in March 1965, cosmonauts Pavel Belyayev and Alexei Leonov could not completely seal the spacecraft hatch after Leonov's historic first walk in space. The spacecraft's environmental control system responded to the leaking air by adding more oxygen to the cabin, causing the concentration level to rise as high as 45%. The crew and ground controllers worried about the possibility of fire, remembering Bondarenko's death four years earlier.
On January 31, 1967, four days after the Apollo1 fire, United States Air Force airmen William F. Bartley Jr. and Richard G. Harmon were killed in a flash fire while tending laboratory rabbits in the Two Man Space Environment Simulator, a pure oxygen chamber at the School of Aerospace Medicine at Brooks Air Force Base. Like the Apollo1 fire, the School fire was caused by an electrical spark in a pure oxygen environment. The widows of the Apollo1 crew sent condolence letters to Bartley and Harmon's families.
Committees in both houses of the United States Congress with oversight of the space program soon launched investigations, including the Senate Committee on Aeronautical and Space Sciences, chaired by Senator Clinton P. Anderson. Seamans, Webb, Manned Space Flight Administrator Dr. George E. Mueller, and Apollo Program Director Maj Gen Samuel C. Phillips were called to testify before Anderson's committee.
In the February 27 hearing, Senator Walter F. Mondale asked Webb if he knew of a report of extraordinary problems with the performance of North American Aviation on the Apollo contract. Webb replied he did not, and deferred to his subordinates on the witness panel. Mueller and Phillips responded they too were unaware of any such "report".
However, in late 1965, just over a year before the accident, Phillips had headed a "tiger team" investigating the causes of inadequate quality, schedule delays, and cost overruns in both the Apollo CSM and the Saturn V second stage (for which North American was also prime contractor). He gave an oral presentation (with transparencies) of his team's findings to Mueller and Seamans, and also presented them in a memo to North American president John L. Atwood, to which Mueller appended his own strongly worded memo to Atwood.
During Mondale's 1967 questioning about what was to become known as the "Phillips Report", Seamans was afraid Mondale might actually have seen a hard copy of Phillips' presentation, and responded that contractors have occasionally been subjected to on-site progress reviews; perhaps this was what Mondale's information referred to. Mondale continued to refer to "the Report" despite Phillips' refusal to characterize it as such, and angered by what he perceived as Webb's deception and concealment of important program problems from Congress, he questioned NASA's selection of North American as prime contractor. Seamans later wrote that Webb roundly chastised him in the cab ride leaving the hearing, for volunteering information which led to the disclosure of Phillips' memo.
On May 11, Webb issued a statement defending NASA's November 1961 selection of North American as the prime contractor for Apollo. This was followed on June9 by Seamans filing a seven-page memorandum documenting the selection process. Webb eventually provided a controlled copy of Phillips' memo to Congress. The Senate committee noted in its final report NASA's testimony that "the findings of the [Phillips] task force had no effect on the accident, did not lead to the accident, and were not related to the accident", but stated in its recommendations:
Freshman Senators Edward W. Brooke III and Charles H. Percy jointly wrote an "Additional Views" section appended to the committee report, chastising NASA more strongly than Anderson for not having disclosed the Phillips review to Congress. Mondale wrote his own, even more strongly worded Additional View, accusing NASA of "evasiveness... lack of candor... patronizing attitude toward Congress... refusal to respond fully and forthrightly to legitimate Congressional inquiries, and... solicitous concern for corporate sensitivities at a time of national tragedy".
The potential political threat to Apollo blew over, due in large part to the support of President Lyndon B. Johnson, who at the time still wielded a measure of influence with the Congress from his own Senatorial experience. He was a staunch supporter of NASA since its inception, had even recommended the Moon program to President John F. Kennedy in 1961, and was skilled at portraying it as part of Kennedy's legacy.
Relations between NASA and North American deteriorated over assignment of blame. North American argued unsuccessfully it was not responsible for the fatal error in spacecraft atmosphere design. Finally, Webb contacted Atwood, and demanded either he or Chief Engineer Harrison A. Storms resign. Atwood elected to fire Storms.
On the NASA side, Joseph Shea resorted to barbiturates and alcohol in order to help him cope. NASA administrator James Webb became increasingly worried about Shea's mental state. Shea was asked to take an extended voluntary leave of absence, but Shea refused, threatening to resign rather than take leave. As a compromise, he agreed to meet with a psychiatrist and to abide by an independent assessment of his psychological fitness. This approach to remove Shea from his position was also unsuccessful. Finally, six months after the fire, Shea's superiors reassigned him to NASA headquarters in Washington, D.C. Shea felt that his new post was a "non-job," and left after only two months.
Gene Kranz called a meeting of his staff in Mission Control three days after the accident, delivering a speech which has subsequently become one of NASA's principles. Speaking of the errors and overall attitude surrounding the Apollo program before the accident, he said: "We were too 'gung-ho' about the schedule and we blocked out all of the problems we saw each day in our work. Every element of the program was in trouble and so were we." He reminded the team of the perils and mercilessness of their endeavor, and stated the new requirement that every member of every team in mission control be "tough and competent", requiring nothing less than perfection throughout NASA's programs. In 2003, following the Space Shuttle "Columbia" disaster, NASA administrator Sean O'Keefe quoted Kranz's speech, applying it to the "Columbia" crew.
After the fire, the Apollo program was grounded for review and redesign. The command module was found to be extremely hazardous and, in some instances, carelessly assembled (for example, a misplaced wrench socket was found in the cabin).
It was decided that remaining Block I spacecraft would be used only for uncrewed Saturn V test flights. All crewed missions would use the Block II spacecraft, to which many command module design changes were made:
Thorough protocols were implemented for documenting spacecraft construction and maintenance.
The astronauts' widows asked that "Apollo 1" be reserved for the flight their husbands never made, and on April 24, 1967, Mueller, as Associate Administrator for Manned Space Flight, announced this change officially: AS-204 would be recorded as Apollo1, "first crewed Apollo Saturn flight – failed on ground test". Even though three uncrewed Apollo missions (AS-201, AS-202, and AS-203) had previously occurred, only AS-201 and AS-202 carried spacecraft. Therefore, the next mission, the first uncrewed Saturn V test flight (AS-501) would be designated Apollo4, with all subsequent flights numbered sequentially in the order flown. The first three flights would not be renumbered, and the names "Apollo2" and "Apollo3" would officially go unused. Mueller considered AS-201 and AS-202, the first and second flights of the Apollo Block I CSM, as Apollo2 and3 respectively.
The crewed flight hiatus allowed work to catch up on the Saturn V and lunar module, which were encountering their own delays. Apollo4 flew in November 1967. Apollo1's (AS-204) Saturn IB rocket was taken down from Launch Complex 34, later reassembled at Launch complex 37B and used to launch Apollo5, an uncrewed Earth orbital test flight of the first lunar module, LM-1, in January 1968. A second uncrewed Saturn V AS-502 flew as Apollo6 in April 1968, and Grissom's backup crew of Wally Schirra, Don Eisele, and Walter Cunningham, finally flew the orbital test mission as Apollo7 (AS-205), in a Block II CSM in October 1968.
Gus Grissom and Roger Chaffee were buried at Arlington National Cemetery. Ed White was buried at West Point Cemetery on the grounds of the United States Military Academy in West Point, New York. Their names are among those of several astronauts and cosmonauts who have died in the line of duty, listed on the Space Mirror Memorial at the Kennedy Space Center Visitor Complex in Merritt Island, Florida. President Jimmy Carter awarded the Congressional Space Medal of Honor posthumously to Grissom on October 1, 1978. President Bill Clinton awarded it to White and Chaffee on December 17, 1997.
An Apollo 1 mission patch was left on the Moon's surface after the first crewed lunar landing by Apollo11 crew members Neil Armstrong and Buzz Aldrin. The Apollo15 mission left on the surface of the Moon a tiny memorial statue, "Fallen Astronaut", along with a plaque containing the names of the Apollo1 astronauts, among others including Soviet cosmonauts, who perished in the pursuit of human space flight.
After the Apollo 1 fire, Launch Complex 34 was subsequently used only for the launch of Apollo7 and later dismantled down to the concrete launch pedestal, which remains at the site () along with a few other concrete and steel-reinforced structures. The pedestal bears two plaques commemorating the crew. Each year the families of the Apollo1 crew are invited to the site for a memorial, and the Kennedy Space Center Visitor Complex includes the site during the tour of the historic Cape Canaveral launch sites.
In January 2005, three granite benches, built by a college classmate of one of the astronauts, were installed at the site on the southern edge of the launch pad. Each bears the name of one of the astronauts and his military service insignia.
The Apollo 1 command module has never been on public display. After the accident, the spacecraft was removed and taken to Kennedy Space Center to facilitate the review board's disassembly in order to investigate the cause of the fire. When the investigation was complete, it was moved to the NASA Langley Research Center in Hampton, Virginia, and placed in a secured storage warehouse.
On February 17, 2007, the parts of CM-012 were moved approximately to a newer, environmentally controlled warehouse. Only a few weeks earlier, Gus Grissom's brother Lowell publicly suggested CM-012 be permanently entombed in the concrete remains of Launch Complex 34.
On January 27, 2017, the 50th anniversary of the fire, NASA put the hatch from Apollo1 on display at the Saturn V Rocket Center at Kennedy Space Center Visitors Complex. KSC's Visitor Complex also houses memorials that include parts of "Challenger" and "Columbia," which is located in the Space Shuttle "Atlantis" exhibit. "This is way, way, way long overdue. But we're excited about it," said Scott Grissom, Gus Grissom's older son.
Notes
Citations

</doc>
<doc id="1966" url="https://en.wikipedia.org/wiki?curid=1966" title="Apollo 10">
Apollo 10

Apollo 10 was a May 1969 human spaceflight, the fourth crewed mission in the United States Apollo program, and the second (after Apollo8) to orbit the Moon. It was the Fmission: a "dress rehearsal" for the first Moon landing, testing all the components and procedures just short of actually landing. While astronaut John Young remained in the Command Module orbiting the Moon, astronauts Thomas Stafford and Gene Cernan flew the Apollo Lunar Module (LM) to a descent orbit within of the lunar surface, the point where powered descent for landing would begin. After orbiting the Moon 31 times Apollo 10 returned safely to Earth, and its success enabled the first actual landing (Apollo 11) two months later.
Apollo 10 set the record for the highest speed attained by a crewed vehicle: 39,897 km/h (11.08 km/s or 24,791 mph) on May 26, 1969, during the return from the Moon.
The mission's call signs were the names of the "Peanuts" characters Charlie Brown and Snoopy, who became Apollo 10's semi-official mascots. "Peanuts" creator Charles Schulz also drew mission-related artwork for NASA.
Apollo 10 and Apollo 11 were the only Apollo missions whose crew were all veterans of spaceflight. Thomas P. Stafford had flown on Gemini 6 and Gemini 9; John W. Young had flown on Gemini 3 and Gemini 10, and Eugene A. Cernan had flown with Stafford on Gemini9.
In addition, Apollo 10 was the only SaturnV flight from Launch Complex 39B, as preparations for Apollo 11 at LC-39A had begun in March almost immediately after Apollo9's launch.
They were also the only Apollo crew all of whose members went on to fly subsequent missions aboard Apollo spacecraft: Young later commanded Apollo 16, Cernan commanded Apollo 17 and Stafford commanded the U.S. vehicle on the Apollo–Soyuz Test Project. It was on Apollo 10 that John Young became the first human to fly solo around the Moon, while Stafford and Cernan flew the LM in lunar orbit as part of the preparations for Apollo 11. Young was also backup commander of Apollo 13 and Apollo 17 and Cernan was backup commander of Apollo 14.
The Apollo 10 crew are also the humans who have traveled the farthest away from home, some from their homes and families in Houston. While most Apollo missions orbited the Moon at the same from the lunar surface, the distance between the Earth and Moon varies by about , between perigee and apogee, throughout each lunar month, and the Earth's rotation makes the distance to Houston vary by another each day. The Apollo 10 crew reached the farthest point in their orbit around the far side of the Moon at about the same time Earth's rotation put Houston nearly a full Earth diameter away.
By the normal rotation in place during Apollo, the backup crew would have been scheduled to fly on Apollo 13. 
However, Alan Shepard, then number two at the Astronaut Office, gave himself the Apollo 13 command slot instead. L. Gordon Cooper Jr., Commander of the Apollo 10 backup crew, was enraged and resigned from NASA. Deke Slayton, the Director of the Flight Crew Operations also removed Donn F. Eisele from the crew due to the personal misconduct and a professional misconduct in the Apollo 7 mission and was replaced by Stuart Roosa. Later, Shepard's crew was forced to switch places with Jim Lovell's tentative Apollo 14 crew.
Slayton wrote in his memoirs that Cooper and Eisele were never intended to rotate to another mission as both were out of favor with NASA management for various reasons (Cooper for his lax attitude towards training, and Eisele for incidents aboard Apollo7 plus an extramarital affair) and were assigned to the backup crew simply because of a lack of qualified manpower in the Astronaut Office at the time the assignment needed to be made. Cooper, Slayton noted, had a very small chance of receiving the Apollo 13 command if he did an outstanding job with the assignment, which he did not. Eisele, despite his issues with management, was always intended for future assignment to the Apollo Applications Program (which was eventually cut down to only the Skylab component) and not a lunar mission.
This dress rehearsal for a Moon landing brought the Apollo Lunar Module to from the lunar surface, at the point where powered descent would begin on the actual landing. Practicing this approach orbit would refine knowledge of the lunar gravitational field needed to calibrate the powered descent guidance system to within needed for a landing. Earth-based observations, uncrewed spacecraft, and Apollo 8 had respectively allowed calibration to within , , and . Except for this final stretch, the mission was designed to duplicate how a landing would have gone, both in space and for ground control, putting NASA's flight controllers and extensive tracking and control network through a rehearsal.
The ascent stage was loaded with the amount of fuel and oxidizer it would have had remaining if it had lifted off from the surface and reached the altitude at which the Apollo 10 ascent stage fired; this was only about half the total amount required for lift off and rendezvous with the CSM. The mission-loaded LM weighed , compared to for the Apollo 11 LM which made the first landing. Craig Nelson wrote in his book "Rocket Men" that NASA took special precaution to ensure Stafford and Cernan would not attempt to make the first landing. Nelson quoted Cernan as saying "A lot of people thought about the kind of people we were: 'Don't give those guys an opportunity to land, 'cause they might!' So the ascent module, the part we lifted off the lunar surface with, was short-fueled. The fuel tanks weren't full. So had we literally tried to land on the Moon, we couldn't have gotten off."
On May 22, 1969, at 20:35:02 UTC, a 27.4 second LM descent propulsion system burn inserted the LM into a descent orbit of so that the resulting lowest point in the orbit occurred about 15° from lunar landing site2 (the Apollo 11 landing site). The lowest measured point in the trajectory was above the lunar surface at 21:29:43 UTC.
Shortly after trans-lunar injection, Young performed the transposition, docking, and extraction maneuver, separating the command and service module (CSM) from the S-IVB stage, turning around, and docking its nose to the top of the lunar module (LM), before separating from the S-IVB. Apollo 10 was the first mission to carry a color television camera inside the spacecraft, and made the first live color TV transmissions from space.
After reaching lunar orbit three days later, Young remained in the command module (CM) "Charlie Brown" while Stafford and Cernan entered the LM "Snoopy" and flew it separately. The LM crew performed the descent orbit insertion maneuver by firing their descent engine, and tested their craft's landing radar as they approached the altitude where the subsequent Apollo 11 mission would begin powered descent to actually land on the Moon. They surveyed the future Apollo 11 landing site in the Sea of Tranquility, then jettisoned the descent stage and fired the engine of the ascent stage to return to "Charlie Brown" Command Module. The descent stage was left in orbit, but eventually may have crashed onto the lunar surface because of the Moon's non-uniform gravitational field. Its location is unknown as it was not tracked.
During descent stage separation, the lunar module began to roll unexpectedly because the crew accidentally duplicated commands into the flight computer which took the LM out of abort mode, the correct configuration for this maneuver. The live network broadcasts caught Cernan and Stafford uttering several expletives before regaining control of the LM. Decades later, Cernan said he observed the horizon spinning eight times over, indicating eight rolls of the spacecraft under ascent engine power. Recordings from the flight do not support this dramatic memory. While the incident was downplayed by NASA, the roll was just several revolutions from being unrecoverable, which would have resulted in the LM crashing into the lunar surface.
After Stafford and Cernan docked with and re-entered "Charlie Brown", "Snoopy's" engine was fired to fuel depletion to send the ascent stage on a trajectory past the Moon and into a heliocentric orbit. This maneuver was unlike the fate of the subsequent Apollo 11 ascent stage, which was left in lunar orbit to eventually crash (post-Apollo 11 ascent stages were steered into the Moon to obtain readings from seismometers placed on the surface, except for Apollo 13's ascent stage, which the crew used as a "life boat" to get safely back to Earth before releasing it to burn up in Earth's atmosphere).
"Snoopy"'s ascent stage orbit was not tracked after 1969, and its current location is unknown. In 2011, a group of amateur astronomers in the UK started a project to search for it. In 2019, the Royal Astronomical Society announced a possible rediscovery of "Snoopy," determining that small Earth-crossing asteroid "2018 AV2" is likely the capsule with "98%" certainty. It is the only once-crewed spacecraft still in outer space without a crew.
Splashdown occurred in the Pacific Ocean on May 26, 1969, at 16:52:23 UTC, about east of American Samoa. The astronauts were recovered by , and subsequently flown to Pago Pago International Airport in Tafuna for a greeting reception, before being flown on a C-141 cargo plane to Honolulu.
After Apollo 10, NASA required astronauts to choose more "dignified" names for their command and lunar modules. This proved unenforceable: Apollo 16 astronauts Young, Mattingly and Duke chose "Casper", as in Casper the Friendly Ghost, for their command module name. The idea was to give children a way to identify with the mission by using humor.
The Smithsonian has been accountable for the command module "Charlie Brown" since 1970. The spacecraft was on display in several countries until it was placed on loan to the London Science Museum in 1978. "Charlie Brown"'s service module (SM) was jettisoned just before re-entry and burned up in the Earth's atmosphere.
After translunar injection, the Saturn V's S-IVB third stage was accelerated past Earth escape velocity and became a object where , it remains in a heliocentric orbit.
The ascent stage of the Apollo Lunar Module "Snoopy" was jettisoned into a heliocentric orbit. On June 10, 2019, Nick Howes, a fellow of the Royal Astronomical Society, announced that he and his colleagues had located "Snoopy", whose location was previously unknown, based on radar astronomy data with 98% certainty.
"Snoopy's" descent stage was jettisoned in lunar orbit; its current location is unknown. Further, it is unclear whether the descent stage impacted the lunar surface, or if it remains in lunar orbit. Phil Stooke, a planetary scientist who studied the lunar crash sites of the LM's ascent stages, wrote that the descent stage "crashed at an unknown location", and another source stated that the descent stage "eventually impact(ed) within a few degrees of the equator on the near side". However, Richard Orloff and an official NASA mission summary stated simply that the descent stage entered lunar orbit, remaining silent on the question of whether the stage later impacted the Moon. An amateur astronomy blog begun in early 2020 explored the possibility that the descent stage may still be in lunar orbit, using computer simulation.
The shield-shaped emblem for the flight shows a large, three-dimensional Roman numeral X sitting on the Moon's surface, in Stafford's words, "to show that we had left our mark." Although it did not land on the Moon, the prominence of the number represents the significant contributions the mission made to the Apollo program. A CSM circles the Moon as an LM ascent stage flies up from its low pass over the lunar surface with its engine firing. The Earth is visible in the background. On the mission patch, a wide, light blue border carries the word APOLLO at the top and the crew names around the bottom. The patch is trimmed in gold. The insignia was designed by Allen Stevens of Rockwell International.
In February 2016 Discovery Channel broadcast a TV show suggesting that the mission witnessed mysterious or alien signals while on the far side of the Moon. The astronauts mention the odd whistling sound that lasted nearly an hour. It was speculated that this is an evidence for UFO coverup.
According to space journalist James Oberg, the sound was most probably radio interference between the command module and the lunar module landing vehicles. Describing it as "outer-space type music" was most probably due to priming, as suggested by Benjamin Radford.
NASA reports
Multimedia

</doc>
<doc id="1967" url="https://en.wikipedia.org/wiki?curid=1967" title="Apollo 12">
Apollo 12

Apollo 12 was the sixth crewed flight in the United States Apollo program and the second to land on the Moon. It was launched on November 14, 1969, from the Kennedy Space Center, Florida, four months after Apollo 11. Commander Charles "Pete" Conrad and Apollo Lunar Module Pilot Alan L. Bean performed just over one day and seven hours of lunar surface activity while Command Module Pilot Richard F. Gordon remained in lunar orbit. The landing site for the mission was located in the southeastern portion of the Ocean of Storms.
On November 19 Conrad and Bean achieved a precise landing at their expected location within walking distance of the site of the Surveyor 3 robotic probe, which had landed on April 20, 1967. They carried the first color television camera to the lunar surface on an Apollo flight, but transmission was lost after Bean accidentally pointed the camera at the Sun and the camera's sensor was destroyed. On one of two moonwalks they visited Surveyor 3 and removed some parts for return to Earth.
Lunar Module "Intrepid" lifted off from the Moon on November 20 and docked with the command module, which then, after completing its 45th lunar orbit, traveled back to Earth. The Apollo 12 mission ended on November 24 with a successful splashdown.
Commander Pete Conrad flew on Gemini 5 in 1965, and as command pilot on Gemini 11 in 1966. Command Module Pilot Richard F. Gordon Jr. flew with Conrad on Gemini 11. Originally, Conrad's Lunar Module pilot was Clifton C. Williams Jr., who was killed in October 1967 when the T-38 he was flying crashed near Tallahassee. When forming his crew, Conrad had wanted Alan L. Bean, a former student of his at the United States Naval Test Pilot School at Patuxent River NAS, Maryland, but had been told by Director of Flight Crew Operations Deke Slayton that Bean was unavailable due to an assignment to the Apollo Applications Program. After Williams's death, Conrad asked for Bean again, and this time Slayton yielded.
Apollo 12 launched on schedule from Kennedy Space Center, under completely overcast rainy skies, encountering wind speeds of during ascent, the highest of any Apollo mission.
Lightning struck the Saturn V 36.5 seconds after lift-off, triggered by the vehicle itself, discharging down to the Earth through the ionized exhaust plume. Protective circuits on the fuel cells in the service module (SM) detected overloads and took all three fuel cells offline, along with much of the command and service module (CSM) instrumentation. A second strike at 52 seconds knocked out the "8-ball" attitude indicator. The telemetry stream at Mission Control was garbled. However, the Saturn V continued to fly normally; the strikes had not affected the Saturn V instrument unit guidance system, which functions independently from the CSM.
The loss of all three fuel cells put the CSM entirely on batteries, which were unable to maintain normal 75-ampere launch loads on the 28-volt DC bus. One of the AC inverters dropped offline. These power supply problems lit nearly every warning light on the control panel and caused much of the instrumentation to malfunction.
Electrical, Environmental and Consumables Manager (EECOM) John Aaron remembered the telemetry failure pattern from an earlier test when a power supply malfunctioned in the CSM signal conditioning electronics (SCE), which converted raw signals from instrumentation to standard voltages for the spacecraft instrument displays and telemetry encoders.
Aaron made a call, "Flight, EECOM. Try SCE to Aux", which switched the SCE to a backup power supply. The switch was fairly obscure, and neither Flight Director Gerald Griffin, CAPCOM Gerald Carr, nor Mission Commander Pete Conrad immediately recognized it. Lunar Module Pilot Alan Bean, flying in the right seat as the spacecraft systems engineer, remembered the SCE switch from a training incident a year earlier when the same failure had been simulated. Aaron's quick thinking and Bean's memory saved what could have been an aborted mission, and earned Aaron the reputation of a "steely-eyed missile man". Bean put the fuel cells back on line, and with telemetry restored, the launch continued successfully. Once in Earth parking orbit, the crew carefully checked out their spacecraft before re-igniting the S-IVB third stage for trans-lunar injection. The lightning strikes had caused no serious permanent damage.
Initially, it was feared that the lightning strike could have caused the explosive bolts that open the Command Module's parachute compartment to fire prematurely, rendering the parachutes useless which would have made safe return impossible. The decision was made not to share this with the astronauts since there was little that could be done to verify or resolve the problem if it existed. The parachutes deployed and functioned normally at the end of the mission.
After LM separation, the S-IVB was intended to fly into solar orbit. The S-IVB auxiliary propulsion system was fired, and the remaining propellants vented to slow it down to fly past the Moon's trailing edge (the Apollo spacecraft always approached the Moon's leading edge). The Moon's gravity would then slingshot the stage into solar orbit. However, a small error in the state vector in the Saturn's guidance system caused the S-IVB to fly past the Moon at too high an altitude to achieve Earth escape velocity. It remained in a semi-stable Earth orbit after passing the Moon on November 18, 1969. It finally escaped Earth orbit in 1971 but was briefly recaptured in Earth orbit 31 years later. It was discovered by amateur astronomer Bill Yeung who gave it the temporary designation J002E3 before it was determined to be an artificial object.
The Apollo 12 mission landed on November 19, 1969, on an area of the Ocean of Storms (Latin "Oceanus Procellarum") that had been visited earlier by several robotic missions ("Luna 5", Surveyor 3, and Ranger 7). The International Astronomical Union, recognizing this, christened this region "Mare Cognitum (Known Sea)". The Lunar coordinates of the landing site were 3.01239° S latitude, 23.42157° W longitude. The landing site would thereafter be listed as "Statio Cognitum" on lunar maps. Conrad and Bean did not formally name their landing site, though Conrad nicknamed the intended touchdown area "Pete's Parking Lot".
The second lunar landing was an exercise in precision targeting, which would be needed for future Apollo missions. Most of the descent was automatic, with manual control assumed by Conrad during the final few hundred feet of descent. Unlike Apollo 11, where Neil Armstrong had to use the manual control to direct his lander downrange of the computer's target, which was strewn with boulders, Apollo 12 succeeded in landing at its intended target—within walking distance of the "Surveyor 3" probe, which had landed on the Moon in April 1967. This is the first and thus far only occasion in which humans have revisited a probe sent to land on another world.
Conrad actually landed "Intrepid" short of "Pete's Parking Lot", because it looked rougher during final approach than anticipated, and was a little under from Surveyor3, a distance that was chosen to eliminate the possibility of lunar dust (being kicked up by "Intrepid's" descent engine during landing) from covering Surveyor3. But the actual touchdown point—approximately from Surveyor3—did cause high velocity sandblasting of the probe. It was later determined that the sandblasting removed more dust than it delivered onto the Surveyor, because the probe was covered by a thin layer that gave it a tan hue as observed by the astronauts, and every portion of the surface exposed to the direct sandblasting was lightened back toward the original white color through the removal of lunar dust.
When Conrad, who was somewhat shorter than Neil Armstrong, stepped onto the lunar surface, his first words were "Whoopie! Man, that may have been a small one for Neil, but that's a long one for me." This was not an off-the-cuff remark: Conrad had made a bet with reporter Oriana Fallaci he would say these words, after she had queried whether NASA had instructed Neil Armstrong what to say as he stepped onto the Moon. Conrad later said he was never able to collect the money.
To improve the quality of television pictures from the Moon, a color camera was carried on Apollo 12 (unlike the monochrome camera on Apollo 11). Unfortunately, when Bean carried the camera to the place near the LM where it was to be set up, he inadvertently pointed it directly into the Sun, destroying the Secondary Electron Conduction (SEC) tube. Television coverage of this mission was thus terminated almost immediately.
Apollo 12 successfully landed within walking distance of the Surveyor 3 probe. Conrad and Bean removed pieces of the probe to be taken back to Earth for analysis. It is claimed that the common bacterium "Streptococcus mitis" was found to have accidentally contaminated the spacecraft's camera prior to launch and survived dormant in this harsh environment for two and a half years. However, this finding has since been disputed: see Reports of "Streptococcus mitis" on the Moon.
Astronauts Conrad and Bean also collected rocks and set up equipment that took measurements of the Moon's seismicity, solar wind flux and magnetic field, and relayed the measurements to Earth. The instruments were part of the first complete nuclear-powered ALSEP station set up by astronauts on the Moon to relay long-term data from the lunar surface. The instruments on Apollo 11 were not as extensive or designed to operate long term. The astronauts also took photographs, although by accident Bean left several rolls of exposed film on the lunar surface. Meanwhile, Gordon, on board the "Yankee Clipper" in lunar orbit, took multi-spectral photographs of the surface.
The lunar plaque attached to the descent stage of "Intrepid" is unique in that unlike the other plaques, it (a) did not have a depiction of the Earth, and (b) it was textured differently: The other plaques had black lettering on polished stainless steel while the Apollo 12 plaque had the lettering in polished stainless steel while the background was brushed flat.
"Intrepid's" ascent stage was dropped (per normal procedures) after Conrad and Bean rejoined Gordon in orbit. It impacted the Moon on November 20, 1969, at . The seismometers the astronauts had left on the lunar surface registered the vibrations for more than an hour.
The crew stayed an extra day in lunar orbit taking photographs, for a total lunar surface stay of 31 and a half hours and a total time in lunar orbit of eighty-nine hours.
On the return flight to Earth after leaving lunar orbit, the crew of Apollo 12 witnessed (and photographed) a solar eclipse, though this one was of the Earth eclipsing the Sun.
"Yankee Clipper" returned to Earth on November 24, 1969, at 20:58 UTC (3:58pm EST, 10:58am HST), in the Pacific Ocean, approximately 500 nautical miles (800 km) east of American Samoa. During splashdown, a 16 mm film camera dislodged from storage and struck Bean in the forehead, rendering him briefly unconscious. He suffered a mild concussion and needed six stitches. After recovery by , they were flown to Pago Pago International Airport in Tafuna for a reception, before being flown on a C-141 cargo plane to Honolulu.
The Apollo 12 mission patch shows the crew's navy background; all three astronauts at the time of the mission were U.S. Navy commanders. It features a clipper ship arriving at the Moon, representing the CM "Yankee Clipper". The ship trails fire, and flies the flag of the United States. The mission name APOLLO XII and the crew names are on a wide gold border, with a small blue trim. Blue and gold are traditional U.S. Navy colors. The patch has four stars on it – one each for the three astronauts who flew the mission and one for Clifton Williams, a U.S. Marine Corps aviator and astronaut who was killed on October 5, 1967, after a mechanical failure caused the controls of his T-38 trainer to stop responding, resulting in a crash. He trained with Conrad and Gordon as part of the backup crew for what would be the Apollo 9 mission, and would have been assigned as lunar module pilot for Apollo 12.
The Apollo 12 command module "Yankee Clipper" is on display at the Virginia Air and Space Center in Hampton, Virginia.
In 2002, astronomers thought they might have discovered another moon orbiting Earth, which they designated J002E3, that turned out to be the S-IVB third stage of the Apollo 12 SaturnV rocket.
The Lunar Module "Intrepid" impacted the Moon November 20, 1969, at 22:17:17.7 UT (5:17pm EST) . In 2009, the Lunar Reconnaissance Orbiter (LRO) photographed the Apollo 12 landing site. The "Intrepid" lunar module descent stage, experiment package (ALSEP), Surveyor3 spacecraft, and astronaut footpaths are all visible. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
The Apollo 12 third stage, an S-IVB, after passing the Moon on 18 November 1969 and not entering a Lunar orbit like the Command Module and Lunar Lander, entered an elliptical Earth orbit with a period of approximately 43 days, and thus became a derelict satellite orbiting Earth. Some time later, the spent third stage entered a heliocentric orbit, but was not observed doing so. The likely mechanism was via one of the Sun-Earth Lagrangian points, which serve "as a 'portal' between the regions of space gravitationally controlled by the Earth and Sun."
Notably, in April 2002, it reentered a temporary orbit around the Earth-Moon system near the Sun-Earth L1 Lagrangian point—"a location where the gravity of the Earth and Sun approximately cancel"—and remained in orbital synchrony with both the Moon and the Earth for a little over a year, reentering a heliocentric orbit in May 2003. The object was "discovered" by near-Earth object watcher Bill Yeung on 3September 2002, labeled object "J002E3" by the Minor Planet Center, and was subsequently identified as the former Apollo 12 stage only several days later. The rocket stage is "the first known case of an object being captured by the Earth, although Jupiter has been known to capture comets via the same mechanism."
Portions of the Apollo 12 mission are dramatized in the 1998 miniseries "From the Earth to the Moon" episode entitled "That's All There Is." Conrad, Gordon, and Bean were portrayed by Paul McCrane, Tom Verica, and Dave Foley, respectively. Conrad had been portrayed by a different actor, Peter Scolari, in the first episode.
NASA reports
Multimedia

</doc>
<doc id="1968" url="https://en.wikipedia.org/wiki?curid=1968" title="Apollo 14">
Apollo 14

Apollo 14 was the eighth crewed mission in the United States Apollo program, the third to land on the Moon, and the first to land in the lunar highlands. It was the last of the "H missions," landings at specific sites of scientific interest on the Moon for two-day stays with two lunar extravehicular activities (EVAs or moonwalks).
The mission was originally scheduled for 1970, but was postponed because of the investigation following the failure of Apollo 13 to reach the Moon's surface, and the need for modifications to the spacecraft as a result. Commander Alan Shepard, Command Module Pilot Stuart Roosa, and Lunar Module Pilot Edgar Mitchell launched on their nine-day mission on Sunday, January 31, 1971, at 4:03:02 p.m. EST, following a weather delay of forty minutes and two seconds. En route to the lunar landing, the crew overcame a series of malfunctions that might have resulted in a second consecutive aborted mission, and possibly, the premature end of the Apollo program.
Shepard and Mitchell made their lunar landing on February5 in the Fra Mauro formation – originally the target of Apollo 13. During the two walks on the surface, of Moon rocks were collected, and several scientific experiments were deployed. To the dismay of some geologists, Shepard and Mitchell did not reach the rim of Cone crater as had been planned, though they came close. In Apollo 14's most famous incident, Shepard hit two golf balls he had brought with him with a makeshift club.
While Shepard and Mitchell were on the surface, Roosa remained in lunar orbit aboard the Command and Service Module, performing scientific experiments and photographing the Moon, including the landing site of the future Apollo 16 mission. He took several hundred seeds on the mission, many of which were germinated on return, resulting in the so-called Moon trees, that were widely distributed in the following years. After liftoff from the surface and a successful docking, the spacecraft was flown back to Earth where the three astronauts splashed down safely in the Pacific Ocean on February 9.
The mission commander of Apollo 14, Alan Shepard, one of the original Mercury Seven astronauts, became the first American to enter space with a suborbital fight on May 5, 1961. Thereafter, he was grounded by Ménière's disease, a disorder of the ear, and served as Chief Astronaut, the administrative head of the Astronaut Office. He had experimental surgery in 1968 which was successful and allowed his return to flight status. Shepard, at age 47, was the oldest U.S. astronaut to fly when he made his trip aboard Apollo 14, and he is the oldest person to walk on the Moon.
Apollo 14's Command Module Pilot (CMP), Stuart Roosa, aged 37 when the mission flew, had been a smoke jumper before joining the Air Force in 1953. He became a fighter pilot and then in 1965 successfully completed Aerospace Research Pilot School (ARPS) at Edwards Air Force Base in California prior to his selection as a Group 5 astronaut the following year. He served as a capsule communicator (CAPCOM) for Apollo 9. The Lunar Module Pilot (LMP), Edgar Mitchell, aged 40 at the time of Apollo 14, joined the Navy in 1952 and served as a fighter pilot, beginning in 1954. He was assigned to squadrons aboard aircraft carriers before returning to the United States to further his education while in the Navy, also completing the ARPS prior to his selection as a Group 5 astronaut. He served on the support crew for Apollo 9 and was the LMP of the backup crew for Apollo 10.
Shepard and his crew had originally been designated by Deke Slayton, Director of Flight Crew Operations and one of the Mercury Seven, as the crew for Apollo 13. NASA management felt that Shepard needed more time for training given he had not flown in space since 1961, and chose him and his crew for Apollo 14 instead. The crew originally designated for Apollo 14, Jim Lovell as the commander, Ken Mattingly as CMP and Fred Haise as LMP, all of whom had backed up Apollo 11, was made the prime crew for Apollo 13 instead.
Mitchell's commander on the Apollo 10 backup crew had been another of the original seven, Gordon Cooper, who had tentatively been scheduled to command Apollo 13, but according to author Andrew Chaikin, his casual attitude toward training resulted in his nonselection. Also on that crew, but excluded from further flights, was Donn Eisele, likely because of problems aboard Apollo 7, which he had flown, and because he had been involved in a messy divorce.
Apollo 14's backup crew was Eugene A. Cernan as commander, Ronald E. Evans Jr. as CMP and Joe H. Engle as LMP. The backup crew, with Harrison Schmitt replacing Engle, would become the prime crew of Apollo 17. Schmitt flew instead of Engle because there was intense pressure on NASA to fly a scientist to the Moon (Schmitt was a geologist) and Apollo 17 was the last lunar flight. Engle, who had flown the X-15 to the edge of outer space, flew into space for NASA in 1981 on STS-2, the second Space Shuttle flight.
During projects Mercury and Gemini, each mission had a prime and a backup crew. Apollo 9 commander James McDivitt believed meetings that required a member of the flight crew were being missed, so for Apollo a third crew of astronauts was added, known as the support crew. Usually low in seniority, support crew members assembled the mission's rules, flight plan, and checklists, and kept them updated; for Apollo 14, they were Philip K. Chapman, Bruce McCandless II, William R. Pogue and C. Gordon Fullerton. CAPCOMs, the individuals in Mission Control responsible for communications with the astronauts were Evans, McCandless, Fullerton and Haise. A veteran of Apollo 13, which had aborted before reaching the Moon, Haise put his training for that mission to use, especially during the EVAs, since both missions were targeted at the same place on the Moon. Had Haise walked on the Moon, he would have been the first Group 5 astronaut to do so, an honor that went to Mitchell.
The flight directors during Apollo had a one-sentence job description, "The flight director may take any actions necessary for crew safety and mission success." For Apollo 14, they were: Pete Frank, Orange team; Glynn Lunney, Black team; Milt Windler, Maroon team and Gerry Griffin, Gold team.
Prime and backup crews for both Apollo 13 and 14 were announced on August 6, 1969. Apollo 14 was scheduled for July 1970, but in January of that year, due to budget cuts that saw the cancellation of Apollo 20, NASA decided there would be two Apollo missions per year with 1970 to see Apollo 13 in April and Apollo 14 likely in October or November.
The investigation into the accident which caused an abort of Apollo 13 delayed Apollo 14. On May 7, 1970, NASA Administrator Thomas O. Paine announced that Apollo 14 would launch no earlier than December 3, and the landing would be close to the site targeted by Apollo 13. The Apollo 14 astronauts continued their training. On June 30, 1970, following the release of the accident report and a NASA review of what changes to the spacecraft would be necessary, NASA announced that the launch would slip to no earlier than January 31, 1971.
The crew of Apollo 14 trained together for 19 months after assignment to the mission, longer than any other Apollo crew to that point. In addition to the normal training workload, they had to supervise the changes to the command and service module (CSM) made as a result of the Apollo 13 investigation, much of which was delegated by Shepard to Roosa. Mitchell later stated, "We realized that if our mission failed—if we had to turn back—that was probably the end of the Apollo program. There was no way NASA could stand two failures in a row. We figured there was a heavy mantle on our shoulders to make sure we got it right."
Before the abort of the Apollo 13 mission, the plan was to have Apollo 14 land near Littrow crater, in Mare Serenitatis, where there are features that were thought to be volcanic. After Apollo 13 returned, it was decided that its landing site, near Cone crater in the Fra Mauro formation, was scientifically more important than Littrow. The Fra Mauro formation is composed of ejecta from the impact event that formed Mare Imbrium, and scientists hoped for samples that originated deep under the Moon's surface. Cone crater was the result of a young, deep impact, and large enough to have torn through whatever debris was deposited since the Imbrium Event, which geologists hoped to be able to date. Landing at Fra Mauro would also allow orbital photography of another candidate landing site, the Descartes Highlands, which became the landing site for Apollo 16. Although Littrow went unvisited, a nearby area, Taurus-Littrow, was the landing site for Apollo 17. Apollo 14's landing site was located slightly closer to Cone crater than the point designated for Apollo 13.
The change in landing site from Littrow to Fra Mauro affected the geological training for Apollo 14. Before the switch, the astronauts had been taken to volcanic sites on Earth; afterwards, they visited crater sites, such as the Ries Crater in West Germany and an artificial crater field created for astronaut training in Arizona's Verde Valley. The effectiveness of the training was limited by a lack of enthusiasm shown by Shepard, which set the tone for Mitchell. Harrison Schmitt suggested that the commander had other things on his mind, such as overcoming a ten-year absence from spaceflight and ensuring a successful mission after the near-disaster of Apollo 13.
Roosa undertook training for his period alone in lunar orbit, when he would make observations of the Moon and take photographs. He had been impressed by the training given to Apollo 13 prime crew CMP Mattingly by geologist Farouk El-Baz and got El-Baz to agree to undertake his training. The two men pored over lunar maps depicting the areas the CSM would pass over. When Shepard and Mitchell were on their geology field trips, Roosa would be overhead in an airplane taking photographs of the site and making observations. El-Baz had Roosa make observations while flying his T-38 jet at a speed and altitude simulating the speed at which the lunar surface would pass below the CSM.
Another issue that had marked Apollo 13 was the last-minute change of crew due to exposure to communicable disease. To prevent another such occurrence, for Apollo 14 NASA instituted what was called the Flight Crew Health Stabilization Program. Beginning 21 days before launch, the crew lived in quarters at the launch site, Florida's Kennedy Space Center (KSC), with their contacts limited to their spouses, the backup crew, mission technicians and others directly involved in training. Those individuals were given physical examinations and immunizations, and crew movements were limited as much as possible at KSC and nearby areas.
The Command and Service Modules were delivered to KSC on November 19, 1969; the ascent stage of the LM arrived on November 21 with the descent stage three days later. Thereafter, checkout, testing and equipment installation proceeded. The launch vehicle stack, with the spacecraft on top, was rolled out from the Vehicle Assembly Building to Pad 39A on November 9, 1970.
The Apollo 14 spacecraft consisted of Command Module (CM) 110 and Service Module (SM) 110 (together CSM-110), called "Kitty Hawk", and Lunar Module 8 (LM-8), called "Antares". Roosa had chosen the CSM's call sign after the town in North Carolina where the Wright Brothers first flew. Antares was the star, in the constellation Scorpius, that the astronauts in the LM would use to orient the craft for its lunar landing; it had been named by Mitchell. Also considered part of the spacecraft were the Launch Escape System and the Spacecraft/Launch Vehicle Adapter.
The changes to the Apollo spacecraft between Apollo 13 and 14 were more numerous than with earlier missions, not only because of the problems with Apollo 13, but because of the more extensive lunar activities planned for Apollo 14. The Apollo 13 accident had been caused by the explosive failure of an oxygen tank, after the insulation of the internal wiring had been damaged by heating of the tank contents pre-launch—that the oxygen had gotten hot enough to damage the insulation had not been realized, since the protective thermostatic switches had failed because they were, through an error, not designed to handle the voltage applied during ground testing. The explosion damaged the other tank or its tubing, causing its contents to leak away. 
The changes in response included a redesign of the oxygen tanks, with the thermostats being upgraded to handle the proper voltage. A third tank was also added, placed in Bay1 of the SM, on the side opposite the other two, and was given a valve that could isolate it in an emergency, and allow it to feed the CM's environmental system only. The quantity probe in each tank was upgraded from aluminum to stainless steel.
Also in response to the Apollo 13 accident, the electrical wiring in Bay4 (where the explosion had happened) was sheathed in stainless steel. The fuel cell oxygen supply valves were redesigned to isolate the Teflon-coated wiring from the oxygen. The spacecraft and Mission Control monitoring systems were modified to give more immediate and visible warnings of anomalies. The Apollo 13 astronauts had suffered shortages of water and of power after the accident. Accordingly, an emergency supply of of water was stored in Apollo 14's CM, and an emergency battery, identical to those that powered the LM's descent stage, was placed in the SM. The LM was modified to make transfer of power from LM to CM easier.
Other changes included the installation of anti-slosh baffles in the LM descent stage's propellant tanks. This would prevent the low fuel light coming on prematurely, as had happened on Apollo 11 and 12. Structural changes were made to accommodate the equipment to be used on the lunar surface, including the Modular Equipment Transporter.
The Saturn V used for Apollo 14 was designated SA-509, and was similar to those used on Apollo 8 through 13. At , it was the heaviest vehicle yet flown by NASA, heavier than the launch vehicle for Apollo 13.
A number of changes were made to avoid pogo oscillations, that had caused an early shutdown of the center J-2 engine on Apollo 13's S-II second stage. These included a helium gas accumulator installed in the liquid oxygen (LOX) line of the center engine, a backup cutoff device for that engine, and a simplified 2-position propellant utilization valve on each of the five J-2 engines.
The Apollo Lunar Surface Experiments Package (ALSEP) array of scientific instruments carried by Apollo 14 consisted of the Passive Seismic Experiment (PSE), Active Seismic Experiment (ASE), Suprathermal Ion Detector (SIDE), Cold Cathode Ion Gauge (CCIG),and Charged Particle Lunar Environmental Experiment (CPLEE). Two additional lunar surface experiments not part of the ALSEP were also flown, the Laser Ranging Retro-Reflector (LRRR or LR3), to be deployed in the ALSEP's vicinity, and the Lunar Portable Magnetometer (LPM), to be used by the astronauts during their second EVA. The PSE had been flown on Apollo 12 and 13, the ASE on Apollo 13, the SIDE on Apollo 12, the CCIG on Apollo 12 and 13 and the LRRR on Apollo 11. The LPM was new, but resembled equipment flown on Apollo 12. The ALSEP components flown on Apollo 13 were destroyed when its LM burned up in Earth's atmosphere.
Deployment of the ALSEP, and of the other instruments, each formed one of Apollo 14's mission objectives.
The PSE was a seismometer, similar to one left on the Moon by Apollo 12, and was to measure seismic activity in the Moon. The Apollo 14 instrument would be calibrated by the impact, after being jettisoned, of the LM's ascent stage, since an object of known mass and velocity would be impacting at a known location on the Moon. The Apollo 12 instrument would also be activated by the spent Apollo 14 S-IVB booster, which would impact the Moon after the mission entered lunar orbit. The two seismometers would, in combination with those left by later Apollo missions, constitute a network of such instruments at different locations on the Moon.
The ASE would also measure seismic waves. It consisted of two parts. In the first, one of the crew members would deploy three geophones at distances up to from the ALSEP's Central Station, and on his way back from the furthest, fire thumpers every . The second consisted of four mortars (with their launch tubes), of different properties and set to impact at different distances from the experiment. It was hoped that the waves generated from the impacts would provide data about seismic wave transmission in the Moon's regolith. The mortar shells were not to be fired until the astronauts had returned to Earth, and in the event were never fired for fear they would damage other experiments. A similar experiment was successfully deployed, and the mortars launched, on Apollo 16.
The LPM was to be carried during the second EVA and used to measure the Moon's magnetic field at various points.
The SIDE measured ions on the lunar surface, including from the solar wind. It was combined with the CCIG, which was to measure the lunar atmosphere and detect if it varied over time. The CPLEE measured the particle energies of protons and electrons generated by the Sun that reached the lunar surface. The LRRR acts as a passive target for laser beams, allowing the measurement of the Earth/Moon distance and how it changes over time. The LRRRs from Apollo 11, 14 and 15 are the only experiments left on the Moon by the Apollo astronauts that are still returning data.
Flown for the first time on Apollo 14 was the Buddy Secondary Life Support System (BSLSS), a set of flexible hoses which would enable Shepard and Mitchell to share cooling water should one of their Primary Life Support System (PLSS) backpacks fail. In such an emergency, the astronaut with the failed equipment would get oxygen from his Oxygen Purge System (OPS) backup cylinder, but the BSLSS would ensure he did not have to use oxygen for cooling, extending the life of the OPS. The OPSs used on Apollo 14 were modified from those used on previous missions in that the internal heaters were removed as unnecessary.
Also taken to the lunar surface were water bags, dubbed "Gunga Dins", for insertion in the astronauts' helmets, allowing them sips of water during the EVAs. These had been flown on Apollo 13, but Shepard and Mitchell were the first to use them on the Moon. Similarly, Shepard was the first on the lunar surface to wear a space suit with commander's stripes: red stripes on arms, legs, and on the helmet, though one had been worn by Lovell on Apollo 13. These were instituted because of the difficulty in telling one spacesuited astronaut from the other in photographs.
The Modular Equipment Transporter (MET) was a two-wheeled handcart, used only on Apollo 14, intended to allow the astronauts to take tools and equipment with them, and store lunar samples, without needing to carry them. On later Apollo program missions, the self-propelled Lunar Roving Vehicle (LRV) was flown instead.
The MET, when deployed for use on the lunar surface, was about long, wide and high. It had pressurized rubber tires wide and in diameter, containing nitrogen and inflated to about . The first use of tires on the Moon, these were developed by Goodyear and were dubbed their XLT (Experimental Lunar Tire) model. Fully loaded, the MET weighed about . Two legs combined with the wheels to provide four-point stability when at rest.
Apollo 14 launched from Launch Complex 39-A at KSC at 4:03:02 pm (21:03:02 UTC), January 31, 1971. This followed a launch delay due to weather of 40 minutes and 2 seconds; the first such delay in the Apollo program. The original planned time, 3:23 pm, was at the very start of the launch window of just under four hours; had Apollo 14 not launched during it, it could not have departed until March. Apollo 12 had launched during poor weather, but had twice been struck by lightning, as a result of which the rules had been tightened. Among those present to watch the launch were U.S. Vice President Spiro T. Agnew and the Prince of Spain, the future King Juan Carlos I. The mission would take a faster trajectory to the Moon than planned, and thus make up the time in flight. Because it had, just over two days after launch, the mission timers would be put ahead by 40 minutes and 3 seconds so that later events would take place at the times scheduled in the flight plan.
After the vehicle reached orbit, the S-IVB third stage shut down, and the astronauts performed checks of the spacecraft before restarting the stage for translunar injection (TLI), the burn that placed the vehicle on course for the Moon. After TLI, the CSM separated from the S-IVB, and Roosa performed the transposition maneuver, turning it around in order to dock with the LM before the entire spacecraft separated from the stage. Roosa, who had practiced the maneuver many times, hoped to break the record for the least amount of propellent used in docking. But when he gently brought the modules together, the docking mechanism would not activate. He made several attempts over the next two hours, as mission controllers huddled and sent advice. If the LM could not be extracted from its place on the S-IVB, no lunar landing could take place, and with consecutive failures, the Apollo program might end. Mission Control proposed that they try it again with the docking probe retracted, hoping the contact would trigger the latches. This worked, and within an hour the joined spacecraft had separated from the S-IVB. The stage was set on a course to impact the Moon, which it did just over three days later, causing the Apollo 12 seismometer to register vibrations for over three hours.
The crew settled in for its voyage to Fra Mauro. At 60:30 Ground Elapsed Time, Shepard and Mitchell entered the LM to check its systems; while there they photographed a wastewater dump from the CSM, part of a particle contamination study in preparation for Skylab. Two midcourse corrections were performed on the translunar coast, with one burn lasting 10.19 seconds and one lasting 0.65 seconds.
At 81:56:40.70 into the mission (February 4 at 1:59:43 am EST; 06:59:43 UTC), the Service Propulsion System engine in the SM was fired for 370.84 seconds to send the craft into a lunar orbit with apocynthion of and pericynthion of . A second burn, at 86:10:52 mission time, sent the spacecraft into an orbit of by . This was done in preparation for the release of the LM "Antares". Apollo 14 was the first mission on which the CSM propelled the LM to the lower orbit—though Apollo 13 would have done so had the abort not already occurred. This was done to increase the amount of hover time available to the astronauts, a safety factor since Apollo 14 was to land in rough terrain. 
After separating from the command module in lunar orbit, the LM "Antares" had two serious problems. First, the LM computer began getting an ABORT signal from a faulty switch. NASA believed the computer might be getting erroneous readings like this if a tiny ball of solder had shaken loose and was floating between the switch and the contact, closing the circuit. The immediate solution – tapping on the panel next to the switch – did work briefly, but the circuit soon closed again. If the problem recurred after the descent engine fired, the computer would think the signal was real and would initiate an auto-abort, causing the ascent stage to separate from the descent stage and climb back into orbit. NASA and the software teams at the Massachusetts Institute of Technology scrambled to find a solution. The software was hard-wired, preventing it from being updated from the ground. The fix made it appear to the system that an abort had already happened, and it would ignore incoming automated signals to abort. This would not prevent the astronauts from piloting the ship, though if an abort became necessary, they might have to initiate it manually. Mitchell entered the changes with minutes to go until planned ignition.
A second problem occurred during the powered descent, when the LM landing radar failed to lock automatically onto the Moon's surface, depriving the navigation computer of vital information on the vehicle's altitude and vertical descent speed. After the astronauts cycled the landing radar breaker, the unit successfully acquired a signal near . Mission rules required an abort if the landing radar was out at , though Shepard might have tried to land without it. With the landing radar, Shepard steered the LM to a landing which was the closest to the intended target of the six missions that landed on the Moon.
Shepard stated, after stepping onto the lunar surface, "And it's been a long way, but we're here." The first EVA began at 9:42 am EST (14:42 UTC) on February 5, 1971, having been delayed by a problem with the communications system which set back the start of the first EVA to five hours after landing. The astronauts devoted much of the first EVA to equipment offloading, deployment of the ALSEP and the US flag, as well as setting up and loading the MET. These activities were televised back to Earth, though the picture tended to degenerate during the latter portion of the EVA. Mitchell deployed the ASE's geophone lines, unreeling and emplacing the two lines leading out from the ALSEP's Central Station. He then fired the thumper explosives, vibrations from which would give scientists back on Earth information about the depth and composition of the lunar regolith. Of the 21 thumpers, five failed to fire. On the way back to the LM, the astronauts collected and documented lunar samples, and took photographs of the area. The first EVA lasted 4 hours, 47 minutes, 50 seconds.
The astronauts had been surprised by the undulating ground, expecting flatter terrain in the area of the landing, and this became an issue on the second EVA, as they set out, MET in tow, for the rim of Cone crater. The craters that Shepard and Mitchell planned to use for navigational landmarks looked very different on the ground than on the maps they had, based on overhead shots taken from lunar orbit. Additionally, they consistently overestimated the distance they traveled. Mission Control and the CAPCOM, Fred Haise, could see nothing of this, as the television camera remained near the LM, but they worried as the clock ticked on the EVA, and monitored the heavy breathing and rapid heartbeats of the astronauts. They topped one ridge that they expected was the crater rim, only to view more such terrain beyond. Although Mitchell strongly suspected the rim was nearby, they had become physically exhausted from the effort, and were instructed by Haise to sample where they were and then start moving back towards the LM. Later analysis using the pictures they took determined that they had come within about of the crater's rim. Images from the Lunar Reconnaissance Orbiter (LRO) show the tracks of the astronauts and the MET come to within 30 m of the rim. The difficulties faced by Shepard and Mitchell would emphasize the need for a means of transportation on the lunar surface with a navigation system, which was met by the Lunar Roving Vehicle, already planned to fly on Apollo 15.
Once the astronauts returned to the vicinity of the LM and were again within view of the television camera, Shepard performed a stunt he had been planning for years in the event he reached the Moon, and which is probably what Apollo 14 is best remembered for. Shepard brought along a six iron golf club head which he could attach to the handle of a lunar excavation tool, and two golf balls, and took several one-handed swings (due to the limited flexibility of the EVA suit). He exuberantly exclaimed that the second ball went "miles and miles and miles" in the low lunar gravity. Mitchell then threw a lunar scoop handle as if it were a javelin. The "javelin" and one of the golf balls wound up in a crater together, with Mitchell's projectile a little bit further. In an interview with Ottawa Golf, Shepard stated the other landed near the ALSEP. The second EVA lasted 4 hours, 34 minutes, 41 seconds.
Some geologists were pleased enough with the close approach to Cone crater to send a case of scotch to the astronauts while they were in post-mission quarantine, though their enthusiasm was tempered by the fact that Shepard and Mitchell had documented few of the samples they brought back, making it hard and sometimes impossible to discern where they came from. Others were less happy; Don Wilhelms wrote in his book on the geological aspects of Apollo, "the golf game did not set well with most geologists in light of the results at Cone crater. The total haul from the rim-flank of Cone ... was 16 Hasselblad photographs (out of a mission total of 417), six rock-size samples heavier than 50 g, and a grand total of 10 kg of samples, 9 kg of which are in one rock (sample 14321 [i.e., Big Bertha]). That is to say, apart from 14321 we have less than 1 kg of rock—962 g to be exact—from what in my opinion is the most important single point reached by astronauts on the Moon." Geologist Lee Silver stated, "The Apollo 14 crews did not have the right attitude, did not learn enough about their mission, had the burden of not having the best possible preflight photography, and they weren’t ready." In their sourcebook on Apollo, Richard W. Orloff and David M. Harland doubted that if Apollo 13 had reached the Moon, that Lovell and Haise, given a more distant landing point, could have gotten as close to Cone crater as Shepard and Mitchell did.
A total of of Moon rocks, or lunar samples, were brought back from Apollo 14. Most are breccias, which are rocks composed of fragments of other, older rocks. Breccias form when the heat and pressure of meteorite impacts fuse small rock fragments together. There were a few basalts that were collected in this mission in the form of clasts (fragments) in breccia. The Apollo 14 basalts are generally richer in aluminum and sometimes richer in potassium than other lunar basalts. Most lunar mare basalts collected during the Apollo program were formed from 3.0 to 3.8 billion years ago. The Apollo 14 basalts were formed 4.0 to 4.3 billion years ago, older than the volcanism known to have occurred at any of the mare locations reached during the Apollo program.
In January 2019 research showed that Big Bertha, which weighs , has characteristics that make it likely to be a terrestrial (Earth) meteorite. Granite and quartz, which are commonly found on Earth but very rarely found on the Moon, were confirmed to exist on Big Bertha. To find the sample's age, the research team from Curtin University looked at bits of the mineral zircon embedded in its structure. "By determining the age of zircon found in the sample, we were able to pinpoint the age of the host rock at about four billion years old, making it similar to the oldest rocks on Earth," researcher Alexander Nemchin said, adding that "the chemistry of the zircon in this sample is very different from that of every other zircon grain ever analyzed in lunar samples, and remarkably similar to that of zircons found on Earth." This would mean Big Bertha is both the first discovered terrestrial meteorite and the oldest known Earth rock.
Roosa spent almost two days alone aboard "Kitty Hawk", performing the first intensive program of scientific observation from lunar orbit, much of which was intended to have been done by Apollo 13. After "Antares" separated and its crew began preparations to land, Roosa in "Kitty Hawk" performed a SPS burn to send the CSM to an orbit of approximately , and later a plane change maneuver to compensate for the rotation of the Moon. 
Roosa took pictures from lunar orbit. The Lunar Topographic Camera, also known as the Hycon camera, was supposed to be used to image the surface, including the Descartes Highlands site being considered for Apollo 16, but it quickly developed a fault with the shutter that Roosa could not fix despite considerable help from Houston. Although about half of the photographic targets had to be scrubbed, Roosa was able to obtain photographs of Descartes with a Hasselblad camera and confirm that it was a suitable landing point. Roosa also used the Hasselblad to take photographs of the impact point of Apollo 13's S-IVB near Lansburg B crater. After the mission, troubleshooting found a tiny piece of aluminum contaminating the shutter control circuit, which caused the shutter to operate continuously.
Roosa was able to see the sun glinting off "Antares" and view its lengthy shadow on the lunar surface on Orbit 17; on Orbit 29 he could see the sun reflecting off the ALSEP. He also took astronomical photographs, of the Gegenschein, and of the Lagrangian point of the Sun-Earth system that lies beyond the Earth (L), testing the theory that the Gegenschein is generated by reflections off particles at L. Performing the bistatic radar experiment, he also focused "Kitty Hawk"'s VHF and S-band transmitters at the Moon so that they would bounce off and be detected on Earth in an effort to learn more about the depth of the lunar regolith.
"Antares" lifted off from the Moon at 1:48:42 pm EST (18:48:42 UTC) on February 6, 1971. Following the first direct (first orbit) rendezvous on a lunar landing mission, docking took place an hour and 47 minutes later. Despite concerns based on the docking problems early in the mission, the docking was successful on the first attempt, though the LM's Abort Guidance System, used for navigation, failed just before the two craft docked. After crew, equipment and lunar samples were transferred to "Kitty Hawk", the ascent stage was jettisoned, and impacted the Moon, setting off waves registered by the seismometers from Apollo 12 and 14.
A transearth injection burn took place on February 6 at 8:39:04 pm (February 7 at 01:39:04 UTC) taking 350.8 seconds, during "Kitty Hawk"'s 34th lunar revolution. During the transearth coast, two tests of the oxygen system were performed, one to ensure the system would operate properly with low densities of oxygen in the tanks, the second to operate the system at a high flow rate, as would be necessary for the in-flight EVAs scheduled for Apollo 15 and later. Additionally, a navigation exercise was done to simulate a return to Earth following a loss of communications. All were successful. During his rest periods on the voyage, Mitchell conducted ESP experiments without NASA's knowledge or sanction, attempting by prearrangement to send images of cards he had brought with him to four people on Earth. He stated after the mission that two of the four had gotten 51 out of 200 correct (the others were less successful), whereas random chance would have dictated 40.
On the final evening in space, the crew conducted a press conference, with the questions submitted to NASA in advance and read to the astronauts by the CAPCOM.
The command module "Kitty Hawk" splashed down in the South Pacific Ocean on February 9, 1971, at 21:05 [UTC], approximately south of American Samoa. After recovery by the ship USS "New Orleans", the crew was flown to Pago Pago International Airport in Tafuna, then to Honolulu, then to Ellington Air Force Base near Houston in a plane containing a Mobile Quarantine Facility trailer before they continued their quarantine in the Lunar Receiving Laboratory. They remained there until their release from quarantine on February 27, 1971. The Apollo 14 astronauts were the last lunar explorers to be quarantined on their return from the Moon. They were the only Apollo crew to be quarantined both before and after the flight.
Roosa, who worked in forestry in his youth, took several hundred tree seeds on the flight. These were germinated after the return to Earth, and were widely distributed around the world as commemorative Moon trees. Some seedlings were given to state forestry associations in 1975 and 1976 to mark the United States Bicentennial.
The mission insignia is an oval depicting the Earth and the Moon, and an astronaut pin drawn with a comet trail. The pin is leaving Earth and is approaching the Moon. A gold band around the edge includes the mission and astronaut names. The designer was Jean Beaulieu, who based it on a sketch by Shepard, who had been head of the Astronaut Office and meant the pin to symbolize that through him, the entire corps was in spirit flying to the Moon.
The backup crew spoofed the patch with its own version, with revised artwork showing a Wile E. Coyote cartoon character depicted as gray-bearded (for Shepard, who was 47 at the time of the mission and the oldest man on the Moon), pot-bellied (for Mitchell, who had a pudgy appearance) and red furred (for Roosa's red hair), still on the way to the Moon, while Road Runner (for the backup crew) is already on the Moon, holding a U.S. flag and a flag labeled "1st Team". The flight name is replaced by "BEEP BEEP" and the backup crew's names are given. Several of these patches were hidden by the backup crew and found during the flight by the crew in notebooks and storage lockers in both the CSM "Kitty Hawk" and the LM "Antares", and one patch was stored in the MET lunar hand cart. One patch, attached to Shepard's PLSS, was worn on the lunar surface, and, mounted on a plaque, was presented by him to Cernan after the mission.
The Apollo 14 command module "Kitty Hawk" is on display at the Apollo/Saturn V Center at the Kennedy Space Center Visitor Complex after being on display at the United States Astronaut Hall of Fame near Titusville, Florida, for several years. The SM reentered Earth's atmosphere and was destroyed, though there was no tracking or sightings of it.
The S-IVB booster impacted the Moon on February4 at . The ascent stage of lunar module "Antares" impacted the Moon on February7, 1971, at 00:45:25.7 UT (February 6, 7:45 p.m. EST), at . "Antares"' descent stage and the mission's other equipment remain at Fra Mauro at .
Photographs taken in 2009 by the Lunar Reconnaissance Orbiter were released on July 17, and the Fra Mauro equipment was the most visible Apollo hardware at that time, owing to particularly good lighting conditions. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
NASA reports
Multimedia

</doc>
<doc id="1969" url="https://en.wikipedia.org/wiki?curid=1969" title="Apollo 15">
Apollo 15

Apollo 15 was the ninth crewed mission in the United States' Apollo program and the fourth to land on the Moon. It was the first J mission, with a longer stay on the Moon and a greater focus on science than earlier landings. Apollo 15 had the first use of the Lunar Roving Vehicle.
The 1971 mission began on July 26 and ended on August 7, with the lunar surface exploration taking place between July 30 and August 2. Commander David Scott and Lunar Module Pilot James Irwin landed near Hadley Rille and explored the local area using the rover, allowing them to travel further from the lunar module than had been possible on previous missions. They spent 18 hours on the Moon's surface on extravehicular activity (EVA), and collected of surface material.
At the same time, Command Module Pilot Alfred Worden orbited the Moon, operating the sensors in the SIM bay of the service module. This suite of instruments collected data on the Moon and its environment using a panoramic camera, a gamma-ray spectrometer, a mapping camera, a laser altimeter, a mass spectrometer, and a lunar subsatellite deployed at the end of the moonwalks. The lunar module returned safely to the command module and, at the end of Apollo 15's 74th lunar orbit the engine was fired for the journey home. During the return trip Worden performed the first spacewalk in deep space. The Apollo 15 mission splashed down safely on August7 despite the loss of one of its three parachutes.
The mission accomplished its goals but was marred by negative publicity the following year when it emerged that the crew had carried unauthorized postal covers to the lunar surface, some of which were sold by a West German stamp dealer. The members of the crew were reprimanded for poor judgment, and did not fly in space again. Apollo 15 is also remembered for the discovery of the Genesis Rock, and for Scott's use of a hammer and a feather to validate Galileo's theory that absent air resistance, objects drop at the same rate due to gravity.
In 1962, NASA contracted for fifteen Saturn V rockets to achieve the Apollo program's goal of a crewed landing on the Moon by 1970; at the time no one knew how many missions this would require. Since success was obtained in 1969 with the sixth SaturnV on Apollo 11, nine rockets remained available for a hoped-for total of ten landings. These plans included a heavier, extended version of the Apollo spacecraft to be used in the last five missions (Apollo 16 through 20). The revamped lunar module would be capable of up to a 75-hour stay, and would carry a Lunar Roving Vehicle to the Moon's surface. The service module would house a package of orbital experiments to gather data on the Moon. In the original plan, Apollo 15 was to be the last of the non-extended missions, to land in Censorinus crater. But in anticipation of budget cuts, NASA cancelled three landing missions by September 1970. Apollo 15 became the first of three extended missions, known as J missions, and the landing site was moved to Hadley Rille, originally planned for Apollo 19.
Scott was born in 1932 in San Antonio, Texas, and had graduated from the United States Military Academy in 1954. Serving in the Air Force, Scott had received two advanced degrees from MIT in 1962 before being selected as one of the third group of astronauts the following year. He flew in Gemini 8 in 1966 alongside Neil Armstrong and as command module pilot of Apollo 9 in 1969. Worden was born in 1932 in Jackson, Michigan, and like his commander, had attended West Point (class of 1955) and served in the Air Force. Worden earned two master's degrees in engineering from the University of Michigan in 1963. Irwin had been born in 1930 in Pittsburgh, and had attended the United States Naval Academy, graduating in 1951 and serving in the Air Force, receiving a master's degree from Michigan in 1957. Both Worden and Irwin were selected in the fifth group of astronauts (1966), and Apollo 15 would be their only spaceflight.
The backup crew was Richard F. Gordon Jr. as commander, Vance D. Brand as command module pilot and Harrison H. Schmitt as lunar module pilot. By the usual rotation of crews, the three would most likely have flown Apollo 18, which was canceled. Brand flew later on the Apollo-Soyuz Test Project and on STS-5, the first operational Space Shuttle mission. With NASA under intense pressure to send a professional scientist to the Moon, Schmitt, a geologist, was selected as LMP of Apollo 17 instead of Joe Engle.
Apollo 15's support crew consisted of astronauts Joseph P. Allen, Robert A. Parker and Karl G. Henize. All three were scientist-astronauts, selected in 1967, as the prime crew felt they needed more assistance with the science than with the piloting. None of the support crew would fly during the Apollo program, waiting until the Space Shuttle program to go into space.
The flight directors for Apollo 15 were as follows:
During a mission the capsule communicators (CAPCOMs), always fellow astronauts, were the only people who normally would speak to the crew. For Apollo 15, the CAPCOMs were Allen, Brand, C. Gordon Fullerton, Gordon, Henize, Edgar D. Mitchell, Parker, Schmitt and Alan B. Shepard.
Schmitt and other scientist-astronauts advocated for a greater place for science on the early Apollo missions. They were often met with disinterest from other astronauts, or found science displaced by higher priorities. Schmitt realized that what was needed was an expert teacher who could fire the astronauts' enthusiasm, and contacted Caltech geologist Lee Silver, whom Schmitt introduced to Apollo 13's commander, Jim Lovell, and to its lunar module pilot, Fred Haise, then in training for their mission. Lovell and Haise were willing to go on a field expedition with Silver, and geology became a significant part of their training. Geologist Farouk El-Baz trained the prime crew's command module pilot, Ken Mattingly to inform his planned observations from lunar orbit. The crew's newly acquired skills mostly went unused, due to the explosion that damaged the Apollo 13 spacecraft, and caused an abort of the mission. Apollo 14's CMP, Stuart Roosa, was enthusiastic about geology, but the mission commander, Shepard, less so.
Already familiar with the spacecraft as the backup crew for Apollo 12, Scott, Worden and Irwin could devote more of their training time as prime crew for Apollo 15 to geology and sampling techniques. Scott was determined that his crew bring back the maximum amount of scientific data possible, and met with Silver in April 1970 to begin planning the geological training. Schmitt's assignment as Apollo 15's backup LMP made him an insider, and allowed him to spark competition between the prime and backup crews. The cancellation of two Apollo missions in September 1970 transformed Apollo 15 into a J mission, with a longer stay on the lunar surface, and the first Lunar Roving Vehicle (LRV). This change was welcomed by Scott, who according to David West Reynolds in his account of the Apollo program, was "something more than a hotshot pilot. Scott had the spirit of a true explorer", one determined to get the most from the J mission. The additional need for communications, including from planned experiments and the rover, required the near-rebuilding of the Honeysuckle Creek Tracking Station in Australia.
Geology field trips took place about once a month throughout the crew's 20 months of training. At first Silver would take the commanders and LMPs from the prime and backup crews to geological sites in Arizona and New Mexico as if for a normal field geology lesson, but closer to launch, these trips became more realistic. Crews began to wear mock-ups of the backpacks they would carry, and communicate using walkie-talkies to a CAPCOM in a tent. The CAPCOM was accompanied by a geologist unfamiliar with the area who would rely on the astronauts' descriptions to interpret the findings, and familiarized the crew members with describing landscapes to people who could not see them. Considering himself a serious amateur, Scott came to enjoy field geology.
The decision to land at Hadley came in September 1970. The Site Selection Committee had narrowed the field down to two sites—Hadley Rille, a deep channel on the edge of Mare Imbrium close to the Apennine mountains or the crater Marius, near which were a group of low, possibly volcanic, domes. Although not ultimately his decision, the commander of a mission always held great sway. To David Scott the choice was clear, as Hadley "had more variety. There is a certain intangible quality which drives the spirit of exploration and I felt that Hadley had it. Besides it looked beautiful and usually when things look good they are good." The selection of Hadley was made although NASA lacked high resolution images of the landing site; none had been made as the site was considered too rough to risk one of the earlier Apollo missions. The proximity of the Apennine mountains to the Hadley site required a landing approach trajectory of 26 degrees, far steeper than the 15 degrees in earlier Apollo landings.
The expanded mission meant that Worden spent much of his time at North American Rockwell's facilities at Downey, California, where the command and service module (CSM) was being built. He undertook a different kind of geology training. Working with El-Baz, he studied maps and photographs of the craters he would pass over while orbiting alone in the CSM. As El-Baz listened and gave feedback, Worden learned how to describe lunar features in a way that would be useful to the scientists who would listen to his transmissions back on Earth. Worden found El-Baz to be an enjoyable and inspiring teacher. Worden usually accompanied his crewmates on their geology field trips, though he was often in an airplane overhead, describing features of the landscape as the plane simulated the speed at which the lunar landscape would pass below the CSM.
The demands of the training strained both Worden's and Irwin's marriages; each sought Scott's advice, fearing a divorce might endanger their places on the mission as not projecting the image NASA wanted for the astronauts. Scott consulted Director of Flight Crew Operations Deke Slayton, their boss, who stated what was important was that the astronauts do their jobs. Although the Irwins overcame their marital difficulties, the Wordens divorced before the mission.
Apollo 15 used command and service module CSM-112, which was given the call sign "Endeavour", named after HMS "Endeavour", and lunar module LM-10, call sign "Falcon", named after the United States Air Force Academy mascot. Scott explained the choice of the name "Endeavour" on the grounds that its captain, James Cook had commanded the first purely scientific sea voyage, and Apollo 15 was the first lunar landing mission on which there was a heavy emphasis on science. Apollo 15 took with it a small piece of wood from Cook's ship while "Falcon" carried two falcon feathers to the Moon in recognition of the crew's service in the Air Force.
Technicians at the Kennedy Space Center had some problems with the instruments in the service module's scientific instrument module (SIM) bay. Some instruments were late in arriving, and principal investigators or representatives of NASA contractors sought further testing or to make small changes. Mechanical problems came from the fact the instruments were designed to operate in space, but had to be tested on the surface of the Earth. As such, things like the 7.5 m (24 ft) booms for the mass and gamma ray spectrometers could be tested only using equipment that tried to mimic the space environment, and, in space, the mass spectrometer boom several times did not fully retract.
On the lunar module, the fuel and oxidizer tanks were enlarged on both the descent and ascent stages, and the engine bell on the descent stage was extended. Batteries and solar cells were added for increased electrical power. In all this increased the weight of the lunar module to , heavier than previous models.
If Apollo 15 had flown as an H mission, it would have been with CSM-111 and LM-9. That CSM was used by the Apollo–Soyuz Test Project in 1975, but the lunar module went unused and is now at the Kennedy Space Center Visitor Complex. "Endeavour" is on display at the National Museum of the United States Air Force at Wright-Patterson Air Force Base in Dayton, Ohio.
The Saturn V that launched Apollo 15 was designated SA-510, the tenth flight-ready model of the rocket. As the payload of the rocket was greater, changes were made to the rocket and to its launch trajectory. It was launched in a more southerly direction (80–100 degrees azimuth) than previous missions, and the Earth parking orbit was lowered to . These two changes meant more could be launched. The propellant reserves were reduced and the number of retrorockets on the S-IC first stage (used to separate the spent first stage from the S-II second stage) reduced from eight to four. The four outboard engines of the S-IC would be burned longer and the center engine would also burn longer. Changes were also made to the S-II to dampen pogo oscillations.
Once all major systems were installed in the SaturnV, it was moved from the Vehicle Assembly Building to the launch site, Launch Complex 39A. During late June and early July 1971, the rocket and Launch Umbilical Tower (LUT) were struck by lightning at least four times. There was no damage to the vehicle, and only minor damage to ground support equipment.
The Apollo 15 astronauts wore redesigned space suits. On all previous Apollo flights, including the non-lunar flights, the commander and lunar module pilot had worn suits with the life support, liquid cooling, and communications connections in two parallel rows of three. On Apollo 15, the new suits, dubbed the "A7LB", had the connectors situated in triangular pairs. This new arrangement, along with the relocation of the entry zipper (which went in an up-down motion on the old suits), to run diagonally from the right shoulder to the left hip, aided in suiting and unsuiting in the cramped confines of the spacecraft. It also allowed for a new waist joint, letting the astronauts bend completely over, and also sit on the rover. Upgraded backpacks allowed for longer-duration moonwalks. As in all missions from and after Apollo 13, the commander's suit bore a red stripe on the helmet, arms and legs.
Worden wore a suit similar to those worn by the Apollo 14 astronauts, but modified to interface with Apollo 15's equipment. Gear needed only for lunar surface EVAs, such as the liquid cooling garment, was not included with Worden's suit, as the only EVA he was expected to do was one to retrieve film cartridges from the SIM bay on the flight home.
A vehicle that could operate on the surface of the Moon had been considered by NASA since the early 1960s. An early version was called MOLAB, which had a closed cabin and would have massed about ; some scaled-down prototypes were tested in Arizona. As it became clear NASA would not soon establish a lunar base, such a large vehicle seemed unnecessary. Still, a rover would enhance the J missions, which were to concentrate on science, though its mass was limited to about and it was not then clear that so light a vehicle could be useful. NASA did not decide to proceed with a rover until May 1969, as Apollo 10, the dress rehearsal for the Moon landing, made its way home from lunar orbit. Boeing got the contract for three rovers on a cost plus basis; overruns (especially in the navigation system) meant the three vehicles eventually cost a total of $40 million. These cost overruns gained considerable media attention at a time of greater public weariness with the space program, when NASA's budget was being cut.
The Lunar Roving Vehicle could be folded into a space 5 ft by 20 in (1.5 m by 0.5 m). Unloaded, it weighed 460 lb (209 kg) and when carrying two astronauts and their equipment, 1500 lb (700 kg). Each wheel was independently driven by a ¼ horsepower (200 W) electric motor. Although it could be driven by either astronaut, the commander always drove. Travelling at speeds up to 6to 8mph (10to 12km/h), it meant that for the first time the astronauts could travel far afield from their lander and still have enough time to do some scientific experiments. The Apollo 15 rover bore a plaque, reading: "Man's First Wheels on the Moon, Delivered by Falcon, July 30, 1971". During pre-launch testing, the LRV was given additional bracing, lest it collapse if someone sat on it under Earth conditions.
The Apollo 15 Particles and Fields Subsatellite (PFS-1) was a small satellite released into lunar orbit from the SIM bay just before the mission left orbit to return to Earth. Its main objectives were to study the plasma, particle, and magnetic field environment of the Moon and map the lunar gravity field. Specifically, it measured plasma and energetic particle intensities and vector magnetic fields, and facilitated tracking of the satellite velocity to high precision. A basic requirement was that the satellite acquire fields and particle data everywhere on the orbit around the Moon. As well as measuring magnetic fields, the satellite contained sensors to study the Moon's mass concentrations, or mascons. The satellite orbited the Moon and returned data from August 4, 1971, until January 1973, when, following multiple failures of the subsatellite's electronics, ground support was terminated. It is believed to have crashed into the Moon sometime thereafter.
Apollo 15 was launched on July 26, 1971, at 9:34am EDT from the Kennedy Space Center at Merritt Island, Florida. The time of launch was at the very start of the two-hour, 37 minute launch window, which would allow Apollo 15 to arrive at the Moon with the proper lighting conditions at Hadley Rille; had the mission been postponed beyond another window on July 27, it could not have been rescheduled until late August. The astronauts had been wakened five and a quarter hours before launch by Slayton, and after breakfast and suiting up, had been taken to Pad 39A, launch site of all seven attempts at crewed lunar landing, and entered the spacecraft about three hours before launch. There were no unplanned delays in the countdown.
At 000:11:36 into the mission, the S-IVB engine shut down, leaving Apollo 15 in its planned parking orbit in low Earth orbit. The mission remained there for 2hours and 40 minutes, allowing the crew (and Houston, via telemetry) to check the spacecraft's systems. At 002:50.02.6 into the mission, the S-IVB was restarted for trans-lunar injection (TLI), placing the craft on a path to the Moon. Before TLI, the craft had completed 1.5 orbits around the Earth.
The command and service module (CSM) and the lunar module remained attached to the nearly-exhausted S-IVB booster. Once trans-lunar injection had been achieved, placing the spacecraft on a trajectory towards the Moon, explosive cords separated the CSM from the booster as Worden operated the CSM's thrusters to push it away. Worden then maneuvered the CSM to dock with the LM (mounted on the end of the S-IVB), and the combined craft was then separated from the S-IVB by explosives. After Apollo 15 separated from the booster, the S-IVB maneuvered away, and, as planned, impacted the Moon about an hour after the crewed spacecraft entered lunar orbit, though due to an error the impact was away from the intended target. The booster's impact was detected by the seismometers left on the Moon by Apollo 12 and Apollo 14, providing useful scientific data.
There was a malfunctioning light on the craft's service propulsion system (SPS); after considerable troubleshooting, the astronauts did a test burn of the system that also served as a midcourse correction. This occurred about 028:40:00 into the mission. Fearing that the light meant the SPS might unexpectedly fire, the astronauts avoided using the control bank with the faulty light, bringing it online only for major burns, and controlling it manually. After the mission returned, the malfunction proved to be caused by a tiny bit of wire trapped within the switch.
After purging and renewing the LM's atmosphere to eliminate any contamination, the astronauts entered the LM about 34 hours into the mission, needing to check the condition of its equipment and move in items that would be required on the Moon. Much of this work was televised back to Earth, the camera operated by Worden. The crew discovered a broken outer cover on the Range/Range Rate tapemeter. This was a concern not only because an important piece of equipment, providing information on distance and rate of approach, might not work properly, but because bits of the glass cover were floating around "Falcon"'s interior. The tapemeter was supposed to be in a helium atmosphere, but due to the breakage, it was in the LM's oxygen atmosphere. Testing on the ground verified the tapemeter would still work properly, and the crew removed most of the glass using a vacuum cleaner and adhesive tape.
As yet, there had been only minor problems, but at about 61:15:00 mission time (the evening of July 28 in Houston), Scott discovered a leak in the water system while preparing to chlorinate the water supply. The crew could not tell where it was coming from, and the issue had the potential to become serious. The experts in Houston found a solution, which was successfully implemented by the crew. The water was mopped up with towels, which were then put out to dry in the tunnel between the command module (CM) and lunar module—Scott stated it looked like someone's laundry.
At 073:31:14 into the mission, a second midcourse correction, with less than a second of burn, was made. Although there were four opportunities to make midcourse corrections following TLI, only two were needed. Apollo 15 approached the Moon on July 29, and the lunar orbit insertion (LOI) burn had to be made using the SPS, on the far side of the Moon, out of radio contact with Earth. If no burn occurred, Apollo 15 would emerge from the lunar shadow and come back in radio contact faster than expected; the continued lack of communication allowed Mission Control to conclude that the burn had taken place. When contact resumed, Scott did not immediately give the particulars of the burn, but spoke admiringly of the beauty of the Moon, causing Alan Shepard, the Apollo 14 commander, who was awaiting a television interview, to grumble, "To hell with that shit, give us details of the burn." The 398.36-second burn took place at 078:31:46.7 into the mission at an altitude of above the Moon, and placed Apollo 15 in an elliptical lunar orbit of .
On Apollo 11 and 12, the lunar module decoupled from the CSM and descended to a much lower orbit from which the lunar landing attempt commenced; to save fuel in an increasingly heavy lander, beginning with Apollo 14, the SPS in the service module made that burn, known as descent orbit insertion (DOI), with the lunar module still attached to the CSM. The initial orbit Apollo 15 was in had its apocynthion, or high point, over the landing site at Hadley; a burn at the opposite point in the orbit was performed, with the result that Hadley would now be under the craft's pericynthion, or low point. The DOI burn was performed at 082:39:49.09 and took 24.53 seconds; the result was an orbit with apocynthion of and pericynthion of . Overnight between July 29 and 30, as the crew rested, it became apparent to Mission Control that mass concentrations in the Moon were making Apollo 15's orbit increasingly elliptical—pericynthion was by the time the crew was awakened on July 30. This, and uncertainty as to the exact altitude of the landing site, made it desirable that the orbit be modified, or trimmed. Using the craft's RCS thrusters, this took place at 095:56:44.70, lasting 30.40 seconds, and raised the pericynthion to and the apocynthion to .
As well as preparing the lunar module for its descent, the crew continued observations of the Moon (including of the landing site at Hadley) and provided television footage of the surface. Then, Scott and Irwin entered the lunar module in preparation for the landing attempt. Undocking was planned for 100:13:56, over the far side of the Moon, but nothing happened when separation was attempted. After analyzing the problem, the crew and Houston decided the probe instrumentation umbilical was likely loose or disconnected; Worden went into the tunnel connecting the command and lunar modules and determined this was so, seating it more firmly. With the problem resolved, "Falcon" separated from "Endeavour" at 100:39:16.2, about 25 minutes late, at an altitude of . Worden in "Endeavour" executed a SPS burn at 101:38:58.98 to send "Endeavour" to an orbit of by in preparation for his scientific work.
Aboard "Falcon", Scott and Irwin prepared for powered descent initiation (PDI), the burn that was to place them on the lunar surface, and, after Mission Control gave them permission, they initiated PDI at 104:30:09.4 at an altitude of , slightly higher than planned. During the first part of the descent, "Falcon" was aligned so the astronauts were on their backs and thus could not see the lunar surface below them, but after the craft made a pitchover maneuver, they were upright and could see the surface in front of them. Scott, who as commander performed the landing, was confronted with a landscape that did not at first seem to resemble what he had seen during simulations. Part of this was due to an error in the landing path of some , of which CAPCOM Ed Mitchell informed the crew prior to pitchover; part because the craters Scott had relied on in the simulator were difficult to make out under lunar conditions, and he initially could not see Hadley Rille. He concluded that they were likely to overshoot the planned landing site, and, once he could see the rille, started maneuvering the vehicle to move the computer's landing target back towards the planned spot, and looked for a relatively smooth place to land.
Below about , Scott could see nothing of the surface because of the quantities of lunar dust being displaced by "Falcon"'s exhaust. "Falcon" had a larger engine bell than previous LMs, in part to accommodate a heavier load, and the importance of shutting down the engine at initial contact rather than risk "blowback", the exhaust reflecting off the lunar surface and going back into the engine (possibly causing an explosion) had been impressed on the astronauts by mission planners. Thus, when Irwin called "Contact", indicating that one of the probes on the landing leg extensions had touched the surface, Scott immediately shut off the engine, letting the lander fall the remaining distance to the surface. Already moving downward at about per second, "Falcon" dropped from a height of . Scott's speed resulted in what was likely the hardest lunar landing of any of the crewed missions, at about per second, causing a startled Irwin to yell "Bam!" Scott had landed "Falcon" on the rim of a small crater he could not see, and the lander settled back at an angle of 6.9 degrees and to the left of 8.6 degrees. Irwin described it in his autobiography as the hardest landing he had ever been in, and he feared that the craft would keep tipping over, forcing an immediate abort.
"Falcon" landed at 104:42:29.3 (22:16:29 GMT on July 30), with approximately 103 seconds of fuel remaining, about from the planned landing site. After Irwin's exclamation, Scott reported, "Okay, Houston. The "Falcon" is on the Plain at Hadley." Once within the planned landing zone, the increased mobility provided by the Lunar Roving Vehicle made unnecessary any further maneuvering.
With "Falcon" due to remain on the lunar surface for almost three days, Scott deemed it important to maintain the circadian rhythm they were used to, and as they had landed in the late afternoon, Houston time, the two astronauts were to sleep before going onto the surface. But the time schedule allowed Scott to open the lander's top hatch (usually used for docking) and spend a half hour looking at their surroundings, describing them, and taking photographs. Lee Silver had taught him the importance of going to a high place to survey a new field site, and the top hatch served that purpose. Deke Slayton and other managers were initially opposed due to the oxygen that would be lost, but Scott got his way. During the only stand-up extravehicular activity (EVA) ever performed through the LM's top hatch on the lunar surface, Scott was able to make plans for the following day's EVA. He offered Irwin a chance to look out as well, but this would have required rearranging the umbilicals connecting Irwin to "Falcon"'s life support system, and he declined. After repressurizing the spacecraft, Scott and Irwin removed their space suits for sleep, becoming the first astronauts to doff their suits while on the Moon.
Throughout the sleep period Mission Control in Houston monitored a slow but steady oxygen loss. Scott and Irwin eventually were awakened an hour early, and the source of the problem was found to be an open valve on the urine transfer device. In post-mission debriefing, Scott recommended that future crews be woken at once under similar circumstances. After the problem was solved, the crew began preparation for the first Moon walk.
After donning their suits and depressurizing the cabin, Scott and Irwin began their first full EVA, becoming the seventh and eighth humans, respectively, to walk on the Moon. They began deploying the lunar rover, stored folded up in a compartment of "Falcon"'s descent stage, but this proved troublesome due to the slant of the lander. The experts in Houston suggested lifting the front end of the rover as the astronauts pulled it out, and this worked. Scott began a system checkout. One of the batteries gave a zero voltage reading, but this was only an instrumentation problem. A greater concern was that the front wheel steering would not work. However the rear wheel steering was sufficient to maneuver the vehicle. Completing his checkout, Scott said "Okay. Out of detent; we're moving", maneuvering the rover away from "Falcon" in mid-sentence. These were the first words uttered by a human while driving a vehicle on the Moon. The rover carried a television camera, controlled remotely from Houston by NASA's Ed Fendell. The resolution was not high compared to the still photographs that would be taken, but the camera allowed the geologists on Earth to indirectly participate in Scott and Irwin's activities.
The rille was not visible from the landing site, but as Scott and Irwin drove over the rolling terrain, it came into view. They were able to see Elbow crater, and they began to drive in that direction. Reaching Elbow, a known location, allowed Mission Control to backtrack and get closer to pinpointing the location of the lander. The astronauts took samples there, and then drove to another crater on the flank of Mons Hadley Delta, where they took more. After concluding this stop, they returned to the lander to drop off their samples and prepare to set up the Apollo Lunar Surface Experiments Package (ALSEP), the scientific instruments that would remain when they left. Scott had difficulty drilling the holes required for the heat flow experiment, and the work was not completed when they had to return to the lander. The first EVA lasted 6hours and 32 minutes.
The rover's front steering, inoperative during the first EVA, worked during the second and third ones. The target of the second EVA, on August 1, was the slope of Mons Hadley Delta, where the pair sampled boulders and craters along the Apennine Front. They spent an hour at Spur crater, during which the astronauts secured what came to be one of the more famous lunar samples, #15415, more commonly known as the "Genesis Rock". This rock, an anorthosite, is believed to be part of the early lunar crust—the hope of finding such a specimen had been one reason the Hadley area had been chosen. Once back at the landing site, Scott continued to try to drill holes for experiments at the ALSEP site, with which he had struggled the day before. After conducting soil-mechanics experiments and raising the U.S. flag, Scott and Irwin returned to the LM. EVA2 lasted 7hours and 12 minutes.
Although Scott had eventually been successful at drilling the holes, he and Irwin had been unable to retrieve a core sample, and this was an early order of business during EVA 3, their third and final moonwalk. Time that could have been devoted to geology ticked away as Scott and Irwin attempted to pull it out. Once it had been retrieved, more time passed as they attempted to break the core into pieces for transport to Earth. Hampered by an incorrectly-mounted vise on the rover, they eventually gave up on this—the core would be transported home with one segment longer than planned. Scott wondered if the core was worth the amount of time and effort invested, and the CAPCOM, Joe Allen, assured him it was. The core proved one of the most important items brought back from the Moon, revealing much about its history, but the expended time meant the planned visit to a group of hills known as the North Complex had to be scrubbed. Instead, the crew again ventured to the edge of Hadley Rille, this time to the northwest of the immediate landing site.
Once the astronauts were beside the LM, Scott used a kit provided by the Postal Service to cancel a first day cover of two stamps being issued on August 2, the current date. Scott then performed an experiment in view of the television camera, using a feather and hammer to demonstrate Galileo's theory that all objects in a given gravity field fall at the same rate, regardless of mass, in the absence of aerodynamic drag. He dropped the hammer and feather at the same time; because of the negligible lunar atmosphere, there was no drag on the feather, which hit the ground at the same time as the hammer. This was Joe Allen's idea (he also served as CAPCOM during it) and was part of an effort to find a memorable popular science experiment to do on the Moon along the lines of Shepard's hitting of golf balls. The feather was most likely from a female gyrfalcon (a type of falcon), a mascot at the United States Air Force Academy.
Scott then drove the rover to a position away from the LM, where the television camera could be used to observe the lunar liftoff. Near the rover, he left a small aluminum statuette called "Fallen Astronaut", along with a plaque bearing the names of 14 known American astronauts and Soviet cosmonauts who had died in the furtherance of space exploration. The memorial was left while the television camera was turned away; he told Mission Control he was doing some cleanup activities around the rover. Scott disclosed the memorial in a post-flight news conference. He also placed a Bible on the control panel of the rover before leaving it for the last time to enter the LM.
The EVA lasted 4 hours, 49 minutes and 50 seconds. In total, the two astronauts spent 18 hours outside the LM and collected approximately of lunar samples.
After the departure of "Falcon", Worden in "Endeavour" executed a burn to take the CSM to a higher orbit. While "Falcon" was on the Moon, the mission effectively split, Worden and the CSM being assigned their own CAPCOM and flight support team.
Worden got busy with the tasks that were to occupy him for much of the time he spent in space alone: photography and operating the instruments in the SIM bay. The door to the SIM bay had been explosively jettisoned during the translunar coast. Filling previously-unused space in the service module, the SIM bay contained a gamma-ray spectrometer, mounted on the end of a boom, an X-ray spectrometer and a laser altimeter, which failed part way through the mission. Two cameras, a stellar camera and a metric camera, together comprised the mapping camera, which was complemented by a panoramic camera, derived from spy technology. The altimeter and cameras permitted the exact time and location from which pictures were taken to be determined. Also present were an alpha particle spectrometer, which could be used to detect evidence of lunar volcanism, and a mass spectrometer, also on a boom in the hope it would be unaffected by contamination from the ship. The boom would prove troublesome, as Worden would not always be able to get it to retract.
"Endeavour" was slated to pass over the landing site at the moment of planned landing, but Worden could not see "Falcon" and did not spot it until a subsequent orbit. He also exercised to avoid muscle atrophy, and Houston kept him up to date on Scott and Irwin's activities on the lunar surface. The panoramic camera did not operate perfectly, but provided enough images that no special adjustment was made. Worden took many photographs through the command module's windows, often with shots taken at regular intervals. His task was complicated by the lack of a working mission timer in the Lower Equipment Bay of the command module, as its circuit breaker had popped en route to the Moon. Worden's observations and photographs would inform the decision to send Apollo 17 to Taurus-Littrow to search for evidence of volcanic activity. There was a communications blackout when the CSM passed over the far side of the Moon from Earth; Worden greeted each resumption of contact with the words, "Hello, Earth. Greetings from "Endeavour"", expressed in different languages. Worden and El-Baz had come up with the idea, and the geology instructor had aided the astronaut in accumulating translations.
Results from the SIM bay experiments would include the conclusion, from data gathered by the X-ray spectrometer, that there was greater fluorescent X-ray flux than anticipated, and that the lunar highlands were richer in aluminum than were the mares. "Endeavour" was in a more inclined orbit than previous crewed missions, and Worden saw features that were not known previously, supplementing photographs with thorough descriptions.
By the time Scott and Irwin were ready to take off from the lunar surface and return to "Endeavour", the CSM's orbit had drifted due to the rotation of the Moon, and a plane change burn was required to ensure that the CSM's orbit would be in the same plane as that of the LM once it took off from the Moon. Worden accomplished the 18-second burn with the SPS.
"Falcon" lifted off the Moon at 17:11:22 GMT on August2 after 66 hours and 55 minutes on the lunar surface. Docking with the CSM took place just under two hours later. After the astronauts transferred samples and other items from the LM to the CSM, the LM was sealed off, jettisoned, and intentionally crashed into the lunar surface, an impact registered by the seismometers left by Apollo 12, 14 and 15. The jettison proved difficult because of problems getting airtight seals, requiring a delay in discarding the LM. After the jettison, Slayton came on the loop to recommend the astronauts take sleeping pills, or at least that Scott and Irwin do so. Scott as mission commander refused to allow it, feeling there was no need. During the EVAs, the doctors had noticed irregularities in both Scott's and Irwin's heartbeats, but the crew were not informed during the flight. Irwin had heart problems after retiring as an astronaut and died in 1991 of a heart attack; Scott felt that he as commander should have been informed of the biomedical readings. NASA doctors at the time theorized the heart readings were due to potassium deficiency, due to their hard work on the surface and inadequate resupply through liquids.
The crew spent the next two days working on orbital science experiments, including more observations of the Moon from orbit and releasing the subsatellite. "Endeavour" departed lunar orbit with another burn of the SPS engine of 2minutes 21 seconds at 21:22:45 GMT on August4. The next day, during the return to Earth, Worden performed a 39-minute EVA to retrieve film cassettes from the service module's scientific instrument module (SIM) bay, with assistance from Irwin who remained at the command module's hatch. At approximately 171,000 nautical miles (197,000 mi; 317,000 km) from Earth, it was the first "deep space" EVA in history, performed at great distance from any planetary body. As of , it remains one of only three such EVAs, all performed during Apollo's J-missions under similar circumstances. Later that day, the crew set a record for the longest Apollo flight to that point.
On approach to Earth on August7, the service module was jettisoned, and the command module reentered the Earth's atmosphere. Although one of the three parachutes on the CM failed after deploying, likely due to damage as the spacecraft vented fuel, only two were required for a safe landing (one extra for redundancy). Upon landing in the North Pacific Ocean, the CM and crew were recovered and taken aboard the recovery ship, , after a mission lasting 12 days, 7hours, 11 minutes and 53 seconds.
The mission objectives for Apollo 15 were to "perform selenological inspection, survey, and sampling of materials and surface features in a pre-selected area of the Hadley–Apennine region. Emplace and activate surface experiments. Evaluate the capability of the Apollo equipment to provide extended lunar surface stay time, increased extravehicular operations, and surface mobility. [and] Conduct inflight experiments and photographic tasks from lunar orbit." It achieved all those objectives. The mission also completed a long list of other tasks, including experiments. One of the photographic objectives, to obtain images of the gegenschein from lunar orbit, was not completed, as the camera was not pointed at the proper spot in the sky. According to the conclusions in the "Apollo 15 Mission Report", the journey "was the fourth lunar landing and resulted in the collection of a wealth of scientific information. The Apollo system, in addition to providing a means of transportation, excelled as an operational scientific facility."
Apollo 15 saw an increase in public interest in the Apollo program, in part due to fascination with the LRV, as well as the attractiveness of the Hadley Rille site and the increased television coverage.
According to David Woods in the "Apollo Lunar Flight Journal",
Despite the successful mission, the careers of the crew were tarnished by a deal they had made before the flight to carry postal covers to the Moon in exchange for about $7,000 each, which they planned to set aside for their children. Walter Eiermann, who had many professional and social contacts with NASA employees and the astronaut corps, served as intermediary between the astronauts and a West German stamp dealer, Hermann Sieger, and Scott carried about 400 covers onto the spacecraft; they were subsequently transferred into "Falcon" and remained inside the lander during the astronauts' activities on the surface of the Moon. After the return to Earth, 100 of the covers were given to Eiermann, who passed them on to Sieger, receiving a commission. No permission had been received from Slayton to carry the covers, as required.
The 100 covers were put on sale to Sieger's customers in late 1971 at a price of about $1,500 each. After receiving the agreed payments, the astronauts returned them, and accepted no compensation. In April 1972, Slayton learned that unauthorized covers had been carried, and removed the three as the backup crew for Apollo 17. The matter became public in June 1972 and the three astronauts were reprimanded for poor judgment; none ever flew in space again. During the investigation, the astronauts had surrendered those covers still in their possession; after Worden filed suit, they were returned in 1983, something "Slate" magazine deemed an exoneration.
Another controversy surrounding the "Fallen Astronaut" statuette that Scott had left on the Moon, arose later. Before the mission, Scott had made a verbal agreement with Belgian artist Paul Van Hoeydonck to sculpt the statuette. Scott's intent, in keeping with NASA's strict policy against commercial exploitation of the US government's space program, was for a simple memorial with a minimum of publicity, keeping the artist anonymous, no commercial replicas being made except for a single copy for public exhibit at the National Air and Space Museum commissioned after the sculpture's public disclosure during the post-flight press conference. Van Hoeydonck claims to have had a different understanding of the agreement, by which he would have received recognition as the creator of a tribute to human space exploration, with rights to sell replicas to the public. Under pressure from NASA, Van Hoeydonck canceled a plan to publicly sell 950 signed copies.
The Apollo 15 mission patch carries Air Force motifs, a nod to the crew's service there, just as the Apollo 12 all-Navy crew's patch had featured a sailing ship. The circular patch features stylized red, white and blue birds flying over Hadley Rille. Immediately behind the birds, a line of craters form the Roman numeral XV. The Roman numerals were hidden in emphasized outlines of some craters after NASA insisted that the mission number be displayed in Arabic numerals. The artwork is circled in red, with a white band giving the mission and crew names and a blue border. Scott contacted fashion designer Emilio Pucci to design the patch, who came up with the basic idea of the three-bird motif on a square patch.
The crew changed the shape to round and the colors from blues and greens to a patriotic red, white and blue. Worden stated that each bird also represented an astronaut, white being his own color (and as Command Module Pilot, uppermost), Scott being the blue bird and Irwin the red. The colors also matched Chevrolet Corvettes driven by the astronauts at KSC; they were photographed with the cars and the training LRV for the June 11, 1971, edition of "Life" magazine.
The halo area of the Apollo 15 landing site, created by the LM's exhaust plume, was observed by a camera aboard the Japanese lunar orbiter SELENE and confirmed by comparative analysis of photographs in May 2008. This corresponds well to photographs taken from the Apollo 15 command module showing a change in surface reflectivity due to the plume, and was the first visible trace of crewed landings on the Moon seen from space since the close of the Apollo program.

</doc>
<doc id="1970" url="https://en.wikipedia.org/wiki?curid=1970" title="Apollo 16">
Apollo 16

Apollo 16 was the tenth crewed mission in the United States Apollo space program, the fifth and penultimate to land on the Moon, and the second to land in the lunar highlands. The second of Apollo's "J missions," it was crewed by Commander John Young, Lunar Module Pilot Charles Duke and Command Module Pilot Ken Mattingly. Launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972, the mission lasted 11 days, 1hour, and 51 minutes, and concluded at 2:45 p.m. EST on April 27.
Young and Duke spent 71 hours—just under three days—on the lunar surface, during which they conducted three extra-vehicular activities or moonwalks, totaling 20 hours and 14 minutes. The pair drove the Lunar Roving Vehicle (LRV), the second produced and used on the Moon, for . On the surface, Young and Duke collected of lunar samples for return to Earth, while Command Module Pilot Ken Mattingly orbited in the command and service module (CSM) above to perform observations. Mattingly, staying with the command module, spent 126 hours and 64 revolutions in lunar orbit. After Young and Duke rejoined Mattingly in lunar orbit, the crew released a subsatellite from the service module (SM). During the return trip to Earth, Mattingly performed a one-hour spacewalk to retrieve several film cassettes from the exterior of the service module.
Apollo 16's landing spot in the highlands was chosen to allow the astronauts to gather geologically older lunar material than the samples obtained in three of the first four Moon landings, which were in or near lunar maria (Apollo 14 landed in the Fra Mauro Highlands). Samples from the Descartes Formation and the Cayley Formation disproved a hypothesis that the formations were volcanic in origin.
Mattingly had originally been assigned to the prime crew of Apollo 13, but was exposed to rubella through Duke, at that time on the back-up crew for Apollo 13, who had caught it from one of his children. He never contracted the illness, but was nevertheless removed from the crew and replaced by his backup, Jack Swigert, three days before the launch. Young, a captain in the United States Navy, had flown on three spaceflights prior to Apollo 16: Gemini 3, Gemini 10 and Apollo 10, which orbited the Moon. One of 19 astronauts selected by NASA in April 1966, Duke had never flown in space before Apollo 16. He served on the support crew of Apollo 10 and was a capsule communicator (CAPCOM) for Apollo 11.
Although not officially announced, the original backup crew consisted of Fred W. Haise (CDR), William R. Pogue (CMP) and Gerald P. Carr (LMP), who were targeted for the prime crew assignment on Apollo 19. However, after the cancellations of Apollos 18 and 19 were finalized in September 1970 this crew would not rotate to a lunar mission as planned. Subsequently, Roosa and Mitchell were recycled to serve as members of the backup crew after returning from Apollo 14, while Pogue and Carr were reassigned to the Skylab program where they flew on Skylab 4.
The insignia of Apollo 16 is dominated by a rendering of an American eagle and a red, white and blue shield, representing the people of the United States, over a gray background representing the lunar surface. Overlaying the shield is a gold NASA vector, orbiting the Moon. On its gold-outlined blue border, there are 16 stars, representing the mission number, and the names of the crew members: Young, Mattingly, Duke. The insignia was designed from ideas originally submitted by the crew of the mission.
Apollo 16 was the second of the Apollo type J missions, featuring the use of the Lunar Roving Vehicle, increased scientific capability, and lunar surface stays of three days. As Apollo 16 was the penultimate mission in the Apollo program and there was no new hardware or procedures to test on the lunar surface, the last two missions (the other being Apollo 17) presented opportunities for astronauts to clear up some uncertainties in understanding the Moon's properties. Although previous Apollo expeditions, including Apollo 14 and Apollo 15, obtained samples of pre-mare lunar material, before lava began to upwell from the Moon's interior and flood the low areas and basins, none had actually visited the lunar highlands.
Apollo 14 had visited and sampled a ridge of material ejected by the impact that created the Mare Imbrium impact basin. Likewise, Apollo 15 had also sampled material in the region of Imbrium, visiting the basin's edge. There remained the possibility, because the Apollo 14 and Apollo 15 landing sites were closely associated with the Imbrium basin, that different geologic processes were prevalent in areas of the lunar highlands far from Mare Imbrium. Several members of the scientific community remarked that the central lunar highlands resembled regions on Earth that were created by volcanic processes and hypothesized the same might be true on the Moon. They hoped scientific output from the Apollo 16 mission would provide an answer.
Two locations on the Moon were given primary consideration for exploration by the Apollo 16 expedition: the Descartes Highlands region west of Mare Nectaris and the crater Alphonsus. At Descartes, the Cayley and Descartes formations were the primary areas of interest in that scientists suspected, based on telescopic and orbital imagery, that the terrain found there was formed by magma more viscous than what had formed the lunar maria. The Cayley Formation's age was approximated to be about the same as Mare Imbrium based on the local frequency of impact craters. The considerable distance between the Descartes site and previous Apollo landing sites would be beneficial for the network of geophysical instruments, portions of which were deployed on each Apollo expedition beginning with Apollo 12.
At the Alphonsus, three scientific objectives were determined to be of primary interest and paramount importance: the possibility of old, pre-Imbrium impact material from within the crater's wall, the composition of the crater's interior and the possibility of past volcanic activity on the floor of the crater at several smaller "dark halo" craters. Geologists feared, however, that samples obtained from the crater might have been contaminated by the Imbrium impact, thus preventing Apollo 16 from obtaining samples of pre-Imbrium material. There also remained the distinct possibility that this objective had already been satisfied by the Apollo 14 and Apollo 15 missions, as the Apollo 14 samples had not yet been completely analyzed and samples from Apollo 15 had not yet been obtained.
It was decided to target the Apollo 16 mission for the Descartes site. Following the decision, the Alphonsus site was considered the most likely candidate for Apollo 17, but was eventually rejected. With the assistance of orbital photography obtained on the Apollo 14 mission, the Descartes site was determined to be safe enough for a crewed landing. The specific landing site was between two young impact craters, North Ray and South Ray craters – in diameter, respectively – which provided "natural drill holes" which penetrated through the lunar regolith at the site, thus leaving exposed bedrock that could be sampled by the crew.
After selecting the landing site for Apollo 16, sampling the Descartes and Cayley formations, two geologic units of the lunar highlands, was determined by mission planners to be the primary sampling interest of the mission. It was these formations that the scientific community widely suspected were formed by lunar volcanism, but this hypothesis was proven incorrect by the composition of lunar samples from the mission.
In preparing for their mission, in addition to the usual Apollo spacecraft training, Young and Duke, along with backup commander Fred Haise, underwent an extensive geological training program that included several field trips to introduce them to concepts and techniques they would use in analyzing features and collecting samples on the lunar surface. During these trips, they visited and provided scientific descriptions of geologic features they were likely to encounter. In July 1971, they visited Sudbury, Ontario, Canada for geology training exercises, the first time U.S. astronauts did so. Geologists chose the area because of a wide crater created about 1.8 billion years ago by a large meteorite. The Sudbury Basin shows evidence of shatter cone geology familiarizing the Apollo crew with geologic evidence of a meteorite impact. During the training exercises the astronauts did not wear space suits, but carried radio equipment to converse with each other and scientist-astronaut Anthony W. England, practicing procedures they would use on the lunar surface.
In addition to the field geology training, Young and Duke also trained to use their EVA space suits, adapt to the reduced lunar gravity, collect samples, and drive the Lunar Roving Vehicle. They also received survival training and preparation for other technical aspects of the mission.
Command Module Pilot Mattingly also received training in recognizing geological features from orbit by flying over the field areas in an airplane, and trained to operate the Scientific Instrument Module from lunar orbit.
The launch of Apollo 16 was delayed one month from March 17 to April 16. This was the first launch delay in the Apollo program due to a technical problem. During the delay, the space suits, a spacecraft separation mechanism and batteries in the lunar module (LM) were modified and tested. There were concerns that the explosive mechanism designed to separate the docking ring from the command module (CM) would not create enough pressure to completely sever the ring. This, along with a dexterity issue in Young's space suit and fluctuations in the capacity of the lunar module batteries, required investigation and trouble-shooting. In January 1972, three months before the planned April launch date, a fuel tank in the command module was accidentally damaged during a routine test. The rocket was returned to the Vertical Assembly Building (VAB) and the fuel tank replaced, and the rocket returned to the launch pad in February in time for the scheduled launch.
The official mission countdown began on Monday, April 10, 1972, at 8:30 AM, six days before the launch. At this point the SaturnV rocket's three stages were powered up and drinking water was pumped into the spacecraft. As the countdown began, the crew of Apollo 16 was participating in final training exercises in anticipation of a launch on April 16. The astronauts underwent their final preflight physical examination on April 11. On April 15, liquid hydrogen and liquid oxygen propellants were pumped into the spacecraft, while the astronauts rested in anticipation of their launch the next day.
The Apollo 16 mission launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972. The launch was nominal; the crew experienced vibration similar to that of previous crews. The first and second stages of the SaturnV performed nominally; the spacecraft entered orbit around Earth just under 12 minutes after lift-off. After reaching orbit, the crew spent time adapting to the zero-gravity environment and preparing the spacecraft for Trans Lunar Injection (TLI), the burn of the third-stage rocket that would propel them to the Moon. In Earth orbit, the crew faced minor technical issues, including a potential problem with the environmental control system and the S-IVB third stage's attitude control system, but eventually resolved or compensated for them as they prepared to depart towards the Moon. After two orbits, the rocket's third stage reignited for just over five minutes, propelling the craft towards the Moon at about . Six minutes after the burn of the S-IVB, the command and service module, containing the crew, separated from the rocket and traveled for before turning around and retrieving the lunar module from inside the expended rocket stage. The maneuver, known as transposition, docking, and extraction, went smoothly. Following transposition and docking, the crew noticed the exterior surface of the lunar module was giving off particles from a spot where the LM's skin appeared torn or shredded; at one point, Duke estimated they were seeing about five to ten particles per second. The crew entered the lunar module through the docking tunnel connecting it with the command module to inspect its systems, at which time they did not spot any major issues. Once on course towards the Moon, the crew put the spacecraft into a rotisserie "barbecue" mode in which the craft rotated along its long axis three times per hour to ensure even heat distribution about the spacecraft from the Sun. After further preparing the craft for the voyage, the crew began the first sleep period of the mission just under 15 hours after launch.
By the time Mission Control issued the wake-up call to the crew for flight day two, the spacecraft was about away from the Earth, traveling at about . As it was not due to arrive in lunar orbit until flight day four, flight days two and three were largely preparatory days, consisting of spacecraft maintenance and scientific research. On day two, the crew performed an electrophoresis experiment, also performed on Apollo 14, in which they attempted to prove the higher purity of particle migrations in the zero-gravity environment. The remainder of day two included a two-second mid-course correction burn performed by the CSM's service propulsion system engine to tweak the spacecraft's trajectory. Later in the day, the astronauts entered the lunar module for the second time in the mission to further inspect the landing craft's systems. The crew reported they had observed additional paint peeling from a portion of the LM's outer aluminum skin. Despite this, the crew discovered that the spacecraft's systems were performing nominally. Following the LM inspection, the crew reviewed checklists and procedures for the following days in anticipation of their arrival and the Lunar Orbit Insertion burn. Command Module Pilot Mattingly reported a "gimbal lock" warning light, indicating the craft was not reporting an attitude. Mattingly alleviated this by realigning the guidance system using the Sun and Moon. At the end of day two, Apollo 16 was about away from Earth.
At the beginning of day three, the spacecraft was about away from the Earth. The velocity of the craft steadily decreased, as it had not yet reached the lunar sphere of gravitational influence. The early part of day three was largely housekeeping, spacecraft maintenance and exchanging status reports with Mission Control in Houston. The crew performed the Apollo light flash experiment, or ALFMED, to investigate "light flashes" that were seen by the astronauts when the spacecraft was dark, regardless of whether or not their eyes were open, on Apollo lunar flights. This was thought to be caused by the penetration of the eye by cosmic ray particles. During the second half of the day, Young and Duke again entered the lunar module to power it up and check its systems, and perform housekeeping tasks in preparation for lunar landing. The systems were found to be functioning as expected. Following this, the crew donned their space suits and rehearsed procedures that would be used on landing day. Just before the end of flight day three at 59 hours, 19 minutes, 45 seconds after liftoff, while from the Earth and from the Moon, the spacecraft's velocity began increasing as it accelerated towards the Moon after entering the lunar sphere of influence.
After waking up on flight day four, the crew began preparations for the maneuver that would brake them into orbit. At an altitude of the scientific instrument module (SIM) bay cover was jettisoned. At just over 74 hours into the mission, the spacecraft passed behind the Moon, losing direct contact with Mission Control. While over the far side, the CSM's service propulsion system engine burned for 6minutes and 15 seconds, braking the spacecraft into an orbit with a low point (pericynthion) of 58.3 and a high point (apocynthion) of 170.4 nautical miles (108.0 and 315.6 km, respectively). After entering lunar orbit, the crew began preparations for the Descent Orbit Insertion (DOI) maneuver to further modify the spacecraft's orbital trajectory. The maneuver was successful, decreasing the craft's pericynthion to . The remainder of flight day four was spent making observations and preparing for activation of the lunar module, undocking, and landing the next day.
The crew continued preparing for lunar module activation and undocking shortly after waking up to begin flight day five. The boom that extended the mass spectrometer out from the CSM's scientific instruments bay was stuck, semi-deployed. It was decided that Young and Duke would visually inspect the boom after undocking from the CSM in the LM. They entered the LM for activation and checkout of the spacecraft's systems. Despite entering the LM 40 minutes ahead of schedule, they completed preparations only 10 minutes early due to numerous delays in the process. With the preparations finished, they undocked in the LM "Orion" from Mattingly in the CSM "Casper" 96 hours, 13 minutes, 31 seconds into the mission. For the rest of the two crafts' passes over the near side of the Moon, Mattingly prepared to shift "Casper" to a circular orbit while Young and Duke prepared "Orion" for the descent to the lunar surface. At this point, during tests of the CSM's steerable rocket engine in preparation for the burn to modify the craft's orbit, a malfunction occurred in the engine's backup system. According to mission rules, "Orion" would have then re-docked with "Casper", in case Mission Control decided to abort the landing and use the lunar module's engines for the return trip to Earth. After several hours of analysis, however, mission controllers determined that the malfunction could be worked around and Young and Duke could proceed with the landing. As a result of this, powered descent to the lunar surface began about six hours behind schedule. Because of the delay, Young and Duke began their descent to the surface at an altitude higher than that of any previous mission, at . At an altitude of about , Young was able to view the landing site in its entirety. Throttle-down of the LM's landing engine occurred on time and the spacecraft tilted forward to its landing orientation at an altitude of . The LM landed north and west of the planned landing site at 104 hours, 29 minutes, and 35 seconds into the mission, at 2:23:35 UTC on April 21.
After landing, Young and Duke began powering down some of the LM's systems to conserve battery power. Upon completing their initial adjustments, the pair configured "Orion" for their three-day stay on the lunar surface, removed their space suits and took initial geological observations of the immediate landing site. They then settled down for their first meal on the surface. After eating, they configured the cabin for their first sleep period on the Moon. The landing delay caused by the malfunction in the CSM's main engine necessitated significant modifications to the mission schedule. Apollo 16 would spend one less day in lunar orbit after surface exploration had been completed to afford the crew contingency time to compensate for any further problems and to conserve expendables. In order to improve Young's and Duke's sleep schedule, the third and final moonwalk of the mission was trimmed from seven hours to five.
The next morning, flight day five, Young and Duke ate breakfast and began preparations for the first extra-vehicular activity (EVA), or moonwalk. After the pair donned and pressurized their space suits and depressurized the lunar module cabin, Young climbed out onto the "porch" of the LM, a small platform above the ladder. Duke handed Young a jettison bag full of trash to dispose of on the surface. Young then lowered the equipment transfer bag (ETB), containing equipment for use during the EVA, to the surface. Young descended the ladder and, upon setting foot on the lunar surface, became the ninth human to walk on the Moon. Upon stepping onto the surface, Young expressed his sentiments about being there: "There you are: Mysterious and Unknown Descartes. Highland plains. Apollo 16 is gonna change your image. I'm sure glad they got ol' Brer Rabbit, here, back in the briar patch where he belongs." Duke soon descended the ladder and joined Young on the surface, becoming the tenth and youngest human to walk on the Moon, at age 36. After setting foot on the lunar surface, Duke expressed his excitement, commenting: "Fantastic! Oh, that first foot on the lunar surface is super, Tony!" The pair's first task of the moonwalk was to unload the Lunar Roving Vehicle, the Far Ultraviolet Camera/Spectrograph (UVC), and other equipment, from the lunar module. This was done without problems. On first driving the lunar rover, Young discovered that the rear steering was not working. He alerted Mission Control to the problem before setting up the television camera and planting the United States flag with Duke. During lunar surface operations, Commander Young always drove the rover, while Lunar Module Pilot Duke was a passenger who assisted with navigation. This division of responsibilities between the two crew positions was used consistently throughout Apollo's J missions. At a 2019 reunion, when asked to describe driving the rover, Duke replied that he never drove the vehicle, but was instead a navigator.
The day's next task was to deploy the Apollo Lunar Surface Experiments Package (ALSEP); while they were parking the lunar rover, on which the TV camera was mounted, to observe the deployment, the rear steering began functioning without explanation. While deploying a heat-flow experiment (that had burned up with the lunar module "Aquarius" on Apollo 13 and had been attempted with limited success on Apollo 15), a cable was inadvertently snapped after getting caught around Young's foot. After ALSEP deployment, they collected samples in the vicinity. About four hours after the beginning of EVA-1, they mounted the lunar rover and drove to the first geologic stop, Plum Crater, a crater on the rim of Flag crater, about across. There, at a distance of from the LM, they sampled material from the vicinity of Flag Crater, which scientists believed penetrated through the upper regolith layer to the underlying Cayley Formation. It was there that Duke retrieved, at the request of Mission Control, the largest rock returned by an Apollo mission, a breccia nicknamed Big Muley after mission geology principal investigator William R. Muehlberger. The next stop of the day was Buster Crater, about from the LM. There, Duke took pictures of Stone Mountain and South Ray Crater while Young deployed a magnetic field experiment. At that point, scientists began to reconsider their pre-mission hypothesis that Descartes had been the setting of ancient volcanic activity, as the two astronauts had yet to find any volcanic material. Following their stop at Buster, Young did a demonstration drive of the lunar rover while Duke filmed with a 16 mm movie camera. After completing more tasks at the ALSEP, they returned to the LM to close out the moonwalk. They reentered the LM 7hours, 6minutes, and 56 seconds after the start of the EVA. Once inside, they pressurized the LM cabin, went through a half-hour briefing with scientists in Mission Control, and configured the cabin for the sleep period.
Shortly after waking up on the morning of flight day six three and a half minutes early, they discussed with Mission Control in Houston the day's timeline of events. The second lunar excursion's primary objective was to visit Stone Mountain to climb up the slope of about 20 degrees to reach a cluster of five craters known as "Cinco Craters". After preparations for the day's moonwalk were completed, the astronauts climbed out of the lunar module. After departing the immediate landing site in the lunar rover, they arrived at the day's first destination, the Cinco craters, from the LM. At above the valley floor, the pair were at the highest elevation above the LM of any Apollo mission. After marveling at the view (including South Ray) from the side of Stone Mountain, which Duke described as "spectacular," the astronauts gathered samples in the vicinity. After spending 54 minutes on the slope, they climbed aboard the lunar rover en route to the day's second stop, station five, a crater across. There, they hoped to find Descartes material that had not been contaminated by ejecta from South Ray Crater, a large crater south of the landing site. The samples they collected there, although their origin is still not certain, are, according to geologist Don Wilhelms, "a reasonable bet to be Descartes". The next stop, station six, was a blocky crater, where the astronauts believed they could sample the Cayley Formation as evidenced by the firmer soil found there. Bypassing station seven to save time, they arrived at station eight on the lower flank of Stone Mountain, where they sampled material on a ray from South Ray Crater for about an hour. There, they collected black and white breccias and smaller, crystalline rocks rich in plagioclase. At station nine, an area known as the "Vacant Lot," which was believed to be free of ejecta from South Ray, they spent about 40 minutes gathering samples. Twenty-five minutes after departing station nine, they arrived at the final stop of the day, halfway between the ALSEP site and the LM. There, they dug a double core and conducted several penetrometer tests along a line stretching east of the ALSEP. At the request of Young and Duke, the moonwalk was extended by ten minutes. After returning to the LM to wrap up the second lunar excursion, they climbed back inside the landing craft's cabin, sealing and pressurizing the interior after 7hours, 23 minutes, and 26 seconds of EVA time, breaking a record that had been set on Apollo 15. After eating a meal and proceeding with a debriefing on the day's activities with Mission Control, they reconfigured the LM cabin and prepared for the sleep period.
Flight day seven was their third and final day on the lunar surface, returning to orbit to rejoin Mattingly in the CSM following the day's moonwalk. During the third and final lunar excursion, they were to explore North Ray Crater, the largest of any of the craters any Apollo expedition had visited. After exiting "Orion", the pair drove the lunar rover away from the LM before adjusting their heading to travel to North Ray Crater. The drive was smoother than that of the previous day, as the craters were shallower and boulders were less abundant north of the immediate landing site. After passing Palmetto crater, boulders gradually became larger and more abundant as they approached North Ray in the lunar rover. Upon arriving at the rim of North Ray crater, they were away from the LM. After their arrival, the duo took photographs of the wide and deep crater. They visited a large boulder, taller than a four-story building, which became known as 'House Rock'. Samples obtained from this boulder delivered the final blow to the pre-mission volcanic hypothesis, proving it incorrect. House Rock had numerous bullet hole-like marks where micrometeoroids from space had impacted the rock. About 1hour and 22 minutes after arriving, they departed for station 13, a large boulder field about from North Ray. On the way, they set a lunar speed record, traveling at an estimated downhill. They arrived at a high boulder, which they called 'Shadow Rock'. Here, they sampled permanently shadowed soil. During this time, Mattingly was preparing the CSM in anticipation of their return approximately six hours later. After three hours and six minutes, they returned to the LM, where they completed several experiments and offloaded the rover. A short distance from the LM, Duke placed a photograph of his family and a United States Air Force commemorative medallion on the surface. Young drove the rover to a point about east of the LM, known as the 'VIP site,' so its television camera, controlled remotely by Mission Control, could observe Apollo 16's liftoff from the Moon. They then reentered the LM after a 5-hour and 40 minute final excursion. After pressurizing the LM cabin, the crew began preparing to return to lunar orbit.
Eight minutes before departing the lunar surface, CAPCOM James Irwin notified Young and Duke from Mission Control that they were go for liftoff. Two minutes before launch, they activated the "Master Arm" switch and then the "Abort Stage" button, after which they awaited ignition of "Orion"s ascent stage engine. When the ascent stage ignited, small explosive charges severed the ascent stage from the descent stage and cables connecting the two were severed by a guillotine-like mechanism. Six minutes after liftoff, at a speed of about , Young and Duke reached lunar orbit. Young and Duke successfully rendezvoused and re-docked with Mattingly in the CSM. To minimize the transfer of lunar dust from the LM cabin into the CSM, Young and Duke cleaned the cabin before opening the hatch separating the two spacecraft. After opening the hatch and reuniting with Mattingly, the crew transferred the samples Young and Duke had collected on the surface into the CSM for transfer to Earth. After transfers were completed, the crew would sleep before jettisoning the empty lunar module ascent stage the next day, when it was to be crashed intentionally into the lunar surface.
The next day, after final checks were completed, the expended LM ascent stage was jettisoned. Because of a failure by the crew to activate a certain switch in the LM before sealing it off, it initially tumbled after separation and did not execute the rocket burn necessary for the craft's intentional de-orbit. The ascent stage eventually crashed into the lunar surface nearly a year after the mission. The crew's next task, after jettisoning the lunar module ascent stage, was to release a subsatellite into lunar orbit from the CSM's scientific instrument bay. The burn to alter the CSM's orbit to that desired for the subsatellite had been cancelled; as a result, the subsatellite lasted half of its anticipated lifetime. Just under five hours later, on the CSM's 65th orbit around the Moon, its service propulsion system main engine was reignited to propel the craft on a trajectory that would return it to Earth. The SPS engine performed the burn flawlessly despite the malfunction that had delayed their landing several days before.
During the return to Earth, Mattingly performed an 83-minute EVA to retrieve film cassettes from the service module's scientific instrument module (SIM) bay, with assistance from Duke who remained at the command module's hatch. At approximately 173,000 nautical miles (199,000 mi; 320,000 km) from Earth, it was the second "deep space" EVA in history, performed at great distance from any planetary body. As of , it remains one of only three such EVAs, all performed during Apollo's J-missions under similar circumstances. Additionally, Mattingly set up a biological experiment, the Microbial Ecology Evaluation Device (MEED), an experiment unique to Apollo 16. The crew carried out various housekeeping and maintenance tasks aboard the spacecraft and ate a meal before concluding the day.
The penultimate day of the flight was largely spent performing experiments, aside from a twenty-minute press conference during the second half of the day. During the press conference, the astronauts answered questions pertaining to several technical and non-technical aspects of the mission prepared and listed by priority at the Manned Spacecraft Center in Houston by journalists covering the flight. In addition to numerous housekeeping tasks, the astronauts prepared the spacecraft for its atmospheric reentry the next day. At the end of the crew's final full day in space, the spacecraft was approximately from Earth and closing at a rate of about .
When the wake-up call was issued to the crew for their final day in space by CAPCOM Tony England, it was about out from Earth, traveling just over . Just over three hours before splashdown in the Pacific Ocean, the crew performed a final course correction burn, changing their velocity by . Approximately ten minutes before reentry into Earth's atmosphere, the cone-shaped command module containing the three crewmembers separated from the service module, which would burn up during reentry. At 265 hours and 37 minutes into the mission, at a velocity of about , Apollo 16 began atmospheric reentry. At its maximum, the temperature of the heat shield was between . After successful parachute deployment and less than 14 minutes after reentry began, the command module splashed down in the Pacific Ocean southeast of the island of Kiritimati 265 hours, 51 minutes, 5seconds after liftoff. The spacecraft and its crew was retrieved by . They were safely aboard the "Ticonderoga" 37 minutes after splashdown.
The Apollo 16 Particles and Fields Subsatellite (PFS-2) was a small satellite released into lunar orbit from the service module. Its principal objective was to measure charged particles and magnetic fields all around the Moon as the Moon orbited Earth, similar to its sister spacecraft, PFS-1, released eight months earlier by Apollo 15. "The low orbits of both subsatellites were to be similar ellipses, ranging from above the lunar surface."
Instead, something unexpected happened. "The orbit of PFS-2 rapidly changed shape and distance from the Moon. In 2-1/2 weeks the satellite was swooping to within a hair-raising of the lunar surface at closest approach. As the orbit kept changing, PFS-2 backed off again, until it seemed to be a safe 30 miles away. But not for long: inexorably, the subsatellite's orbit carried it back toward the Moon. And on May 29, 1972—only 35 days and 425 orbits after its release"—PFS-2 crashed into the Lunar surface.
The aircraft carrier USS "Ticonderoga" delivered the Apollo 16 command module to the North Island Naval Air Station, near San Diego, California, on Friday, May 5, 1972. On Monday, May 8, 1972, ground service equipment being used to empty the residual toxic reaction control system fuel in the command module tanks exploded in a Naval Air Station hangar. Forty-six people were sent to the hospital for 24 to 48 hours' observation, most suffering from inhalation of toxic fumes. Most seriously injured was a technician who suffered a fractured kneecap when the GSE cart overturned on him. A hole was blown in the hangar roof 250 feet above; about 40 windows in the hangar were shattered. The command module suffered a three-inch gash in one panel.
The Apollo 16 command module "Casper" is on display at the U.S. Space & Rocket Center in Huntsville, Alabama. The lunar module ascent stage separated 24 April 1972 but a loss of attitude control rendered it out of control. It orbited the Moon for about a year. Its impact site remains unknown. The S-IVB was deliberately crashed into the Moon. However, due to a communication failure before impact the exact location was unknown until January 2016, when it was discovered within Mare Insularum by the Lunar Reconnaissance Orbiter, approximately southwest of Copernicus Crater.
Duke donated some flown items, including a lunar map, to Kennesaw State University in Kennesaw, Georgia. He left two items on the Moon, both of which he photographed. The most famous is a plastic-encased photo portrait of his family (NASA Photo AS16-117-18841). The reverse of the photo is signed by Duke's family and bears this message: "This is the family of Astronaut Duke from Planet Earth. Landed on the Moon, April 1972." The other item was a commemorative medal issued by the United States Air Force, which was celebrating its 25th anniversary in 1972. He took two medals, leaving one on the Moon and donating the other to the Wright-Patterson Air Force Base museum.
In 2006, shortly after Hurricane Ernesto affected Bath, North Carolina, eleven-year-old Kevin Schanze discovered a piece of metal debris on the ground near his beach home. Schanze and a friend discovered a "stamp" on the flat metal sheet, which upon further inspection turned out to be a faded copy of the Apollo 16 mission insignia. NASA later confirmed the object to be a piece of the first stage of the SaturnV that had launched Apollo 16 into space. In July 2011, after returning the piece of debris at NASA's request, 16-year-old Schanze was given an all-access tour of the Kennedy Space Center and VIP seating for the launch of STS-135, the final mission of the Space Shuttle program.

</doc>
<doc id="1971" url="https://en.wikipedia.org/wiki?curid=1971" title="Apollo 17">
Apollo 17

Apollo 17 (December 7–19, 1972) was the final Moon landing mission of NASA's Apollo program, and remains the most recent time humans have travelled beyond low Earth orbit. Its crew consisted of Commander Eugene Cernan, Lunar Module Pilot Harrison Schmitt, and Command Module Pilot Ronald Evans, and it carried a biological experiment containing five mice.
Launched at 12:33 a.m. Eastern Standard Time (EST) on December 7, 1972, Apollo 17 was a "J-type mission" that included three days on the lunar surface, extended scientific capability, and the use of the third Lunar Roving Vehicle (LRV).
Cernan and Schmitt landed in the Taurus–Littrow valley and completed three moonwalks, taking lunar samples and deploying scientific instruments. The landing site had been chosen to further the mission's main goals: to sample lunar highland material older than Mare Imbrium, and to investigate the possibility of relatively recent volcanic activity. Evans remained in lunar orbit in the command and service module (CSM), taking scientific measurements and photographs.
Cernan, Evans, Schmitt, and the mice returned to Earth on December 19.
Apollo 17 was the first mission to have no one on board who had been a test pilot; X-15 test pilot Joe Engle lost the lunar module pilot assignment to Schmitt, a geologist. The mission included the first night launch of a U.S. human spaceflight and the final crewed launch of a SaturnV rocket. It was also the final use of Apollo hardware for its original purpose (extra Apollo spacecraft were later used in the Skylab and Apollo–Soyuz programs).
The mission broke several crewed spaceflight records: the longest Moon landing, longest total extravehicular activities (moonwalks), largest lunar sample, longest time in lunar orbit, and, at 75, most lunar orbits.
In 1969, NASA announced that the backup crew of Apollo 14, slated to fly in 1971, would be Eugene Cernan, Ronald Evans, and former X-15 pilot Joe Engle (whose 16 flights in the X-15 had thrice taken him past the border of space). Because the Apollo program generally slated a backup crew to fly as prime crew three missions later, Cernan, Evans, and Engle were in line to be prime crew of Apollo 17. Meanwhile, Harrison Schmitt—a professional geologist—was assigned to the backup crew of Apollo 15 and slated to fly as Lunar Module Pilot on Apollo 18.
However, Apollo 18 was cancelled in September 1970. The scientific community subsequently pressed NASA to find a way to assign a geologist—and not just a pilot with geology training—to an Apollo landing. So NASA assigned Schmitt as the Lunar Module Pilot of Apollo 17, bumping astronaut Curt Michel, who had a Ph.D in physics.
That opened the question of who would fill the other two Apollo 17 slots: the rest of the Apollo 15 backup crew (Dick Gordon and Vance Brand) or the Apollo 14 backup crew (minus Engle)? NASA Director of Flight Crew Operations Deke Slayton ultimately chose Cernan and Evans.
The Apollo 15 prime crew received the backup assignment since this was to be the last lunar mission and the backup crew would not rotate to another mission. However, when the Apollo 15 postage stamp incident became public in early 1972 the crew was reprimanded by NASA and the United States Air Force (they were active duty officers). Director of Flight Crew Operations Deke Slayton removed them from flight status and replaced them with Young and Duke from the Apollo 16 prime crew and Roosa from the Apollo 14 prime and Apollo 16 backup crews.
The insignia's most prominent feature is an image of the Greek sun god Apollo backdropped by a rendering of an American eagle, the red bars on the eagle mirroring those on the flag of the United States. Three white stars above the red bars represent the three crewmen of the mission. The background includes the Moon, the planet Saturn, and a galaxy or nebula. The wing of the eagle partially overlays the Moon, suggesting man's established presence there. The gaze of Apollo and the direction of the eagle's motion embody man's intention to explore further destinations in space.
The patch includes, along with the colors of the U.S. flag (red, white, and blue), the color gold, representative of a "golden age" of spaceflight that was to begin with Apollo 17. The image of Apollo in the mission insignia is a rendering of the "Apollo Belvedere" sculpture. The insignia was designed by Robert McCall, with input from the crew.
Like Apollo 15 and Apollo 16, Apollo 17 was slated to be a "J-mission", an Apollo mission type that featured lunar surface stays of three days, higher scientific capability, and the usage of the Lunar Roving Vehicle. Since Apollo 17 was to be the final lunar landing of the Apollo program, high-priority landing sites that had not been visited previously were given consideration for potential exploration. A landing in the crater Copernicus was considered, but was ultimately rejected because Apollo 12 had already obtained samples from that impact, and three other Apollo expeditions had already visited the vicinity of Mare Imbrium. A landing in the lunar highlands near the crater Tycho was also considered, but was rejected because of the rough terrain found there and a landing on the lunar far side in the crater Tsiolkovskiy was rejected due to technical considerations and the operational costs of maintaining communication during surface operations. A landing in a region southwest of Mare Crisium was also considered, but rejected on the grounds that a Soviet spacecraft could easily access the site; Luna 21 eventually did so shortly after the Apollo 17 site selection was made.
After the elimination of several sites, three sites made the final consideration for Apollo 17: Alphonsus crater, Gassendi crater, and the Taurus-Littrow valley. In making the final landing site decision, mission planners took into consideration the primary objectives for Apollo 17: obtaining old highlands material from a substantial distance from Mare Imbrium, sampling material from young volcanic activity (i.e., less than three billion years), and having minimal ground overlap with the orbital ground tracks of Apollo 15 and Apollo 16 to maximize the amount of new data obtained.
The Taurus-Littrow site was selected with the prediction that the crew would be able to obtain samples of old highland material from the remnants of a landslide event that occurred on the south wall of the valley and the possibility of relatively young, explosive volcanic activity in the area. Although the valley is similar to the landing site of Apollo 15 in that it is on the border of a lunar mare, the advantages of Taurus-Littrow were believed to outweigh the drawbacks, thus leading to its selection as the Apollo 17 landing site.
As with previous lunar landings, the Apollo 17 astronauts underwent an extensive training program that included training to collect samples on the surface, usage of the spacesuits, navigation in the Lunar Roving Vehicle, field geology training, survival training, splashdown and recovery training, and equipment training.
Apollo 17 was the last crewed SaturnV launch and the only night launch. The launch was delayed by two hours and forty minutes due to an automatic cutoff in the launch sequencer at the T-30 second mark in the countdown. The issue was quickly determined to be a minor technical error. The clock was reset and held at the T-22 minute mark while technicians worked around the malfunction in order to continue with the launch. This pause was the only launch delay in the Apollo program caused by this type of hardware failure. The countdown then resumed, and the liftoff occurred at 12:33 am EST.
Approximately 500,000 people were estimated to have observed the launch in the immediate vicinity of Kennedy Space Center, despite the early morning hour. The launch was visible as far away as ; observers in Miami, Florida, saw a "red streak" crossing the northern sky.
At 3:46 am EST, the S-IVB third stage was re-ignited to propel the spacecraft towards the Moon.
At approximately 2:47 pm EST on December 10, the service propulsion system engine on the CSM ignited to slow down the CSM/LM stack into lunar orbit. Following orbit insertion and orbital stabilization, the crew began preparations for landing in the Taurus-Littrow valley.
After separating from the CSM, the LM "Challenger" and its crew of two, Eugene Cernan and Harrison Schmitt, adjusted their orbit and began preparations for the descent to Taurus-Littrow. While Cernan and Schmitt prepared for landing, Command Module Pilot Ron Evans remained in orbit to take observations, perform experiments and await the return of his crew-mates a few days later.
Soon after completing their preparations for landing, Cernan and Schmitt began their descent to the Taurus-Littrow valley on the lunar surface. Several minutes after the descent phase was initiated, the LM pitched over, giving the crew their first look at the landing site during the descent phase and allowing Cernan to guide the spacecraft to a desirable landing target while Schmitt provided data from the flight computer essential for landing. The LM touched down on the lunar surface at 2:55 pm EST on December 11. Shortly thereafter, the two astronauts began re-configuring the LM for their stay on the surface and began preparations for the first moonwalk of the mission, or EVA-1.
Over three moonwalks (EVAs), Cernan and Schmitt deployed the LRV, the Apollo Lunar Surface Experiments Package (ALSEP) and seismic explosive charges. They parked the rover at nine planned geological survey stations to collect samples and make observations. Additionally, twelve short sampling stops were made at Schmitt's discretion while riding the rover, during which the astronauts rapidly collected lunar material without dismounting. During lunar surface operations, Commander Cernan always drove the rover, while Lunar Module Pilot Schmitt was a passenger who assisted with navigation. This division of responsibilities between the two crew positions was used consistently throughout Apollo's J missions.
The first lunar excursion began four hours after landing, at 6:54 p.m. EST on December 11. The first task was to offload the rover and other equipment from the LM. While working near the rover, Cernan caught his hammer under the right-rear fender extension, accidentally breaking it off. A similar incident occurred on Apollo 16 as John Young maneuvered around the rover. Although this was not a mission-critical issue, the loss of the part caused Cernan and Schmitt to be covered with dust thrown up when the rover was in motion. The crew attempted a short-lived fix using duct tape, attaching a map to the damaged fender. However lunar dust stuck to the tape's surface, preventing it from adhering properly. The crew deployed the ALSEP just west of the landing site. This task done, they departed for the first geological survey station: Steno crater to the south of the landing site. The astronauts gathered of samples, took seven gravimeter measurements, and deployed two explosive packages. The latter were detonated remotely to test geophones placed by the astronauts, and also seismometers left during previous missions. The EVA ended after seven hours and twelve minutes.
On December 12, awakened by "Ride of the Valkyries", Cernan and Schmitt began their second lunar excursion. First, the rover's fender needed a better fix. Overnight, the flight controllers devised a procedure communicated by John Young: taping four cronopaque maps together and clamping the "replacement fender extension" onto the fender. The astronauts carried out the new fix which did its job, lasting the remainder of the exploration. Cernan and Schmitt then departed for station 2—Nansen Crater, at the foot of the South Massif. Upon arrival, Cernan reported their range as 7.6 km (4.7 mi, 25,029 ft) away from the lunar module; it was the furthest distance traveled away from a spacecraft during the Apollo program. The astronauts were at the extremity of their "walkback limit", a safety constraint meant to ensure that they could walk back to the LM if for whatever reason the rover failed. They began a return trip, traveling northeast. Stopping at station 4—Shorty crater—the astronauts discovered orange soil, which proved to be very small beads of volcanic glass formed over 3.5 billion years ago. The final stop before returning to the LM was Camelot crater; throughout the sojourn, the astronauts collected of samples, took another seven gravimeter measurements, and deployed three more explosive packages. Concluding the EVA at seven hours and thirty-seven minutes, Cernan and Schmitt had completed the longest-duration EVA in history to-date, traveling further away from a spacecraft and covering more ground on a planetary body during a single EVA than any other spacefarers. Once the LM was repressurized, CAPCOM Bob Parker was particularly impressed, saying: "Absolutely outstanding. I can't say more than that. And I mean it from the bottom of my heart or the bottom of my soul or something, my conscience."
The third moonwalk, the last of the Apollo program, began at 5:25 pm EST on December 13. Cernan and Schmitt rode the rover northeast of the landing site, exploring the base of the North Massif and the Sculptured Hills. Stopping at station 6, they examined a house-sized split boulder dubbed Tracy's Rock (or Split Rock), after Cernan's daughter. The ninth and final planned station was conducted at Van Serg crater. The crew collected of lunar samples and took another nine gravimeter measurements. Before concluding the moonwalk, the crew collected a breccia rock, dedicating it to the nations of Earth, several of which were represented in Mission Control Center in Houston, Texas, at the time. A plaque located on the LM, commemorating the achievements made during the Apollo program, was then unveiled. Before reentering the LM for the final time, Gene Cernan expressed his thoughts:
Cernan then followed Schmitt into the LM; the final lunar excursion had a duration of seven hours and fifteen minutes.
Eugene Cernan and Harrison Schmitt successfully lifted off from the lunar surface in the ascent stage of the LM on December 14, at 5:55 pm EST. After a successful rendezvous and docking with Ron Evans in the CSM in orbit, the crew transferred equipment and lunar samples between the LM and the CSM for return to Earth. Following this, the LM ascent stage was sealed off and jettisoned at 1:31 am on December 15. The ascent stage was then deliberately crashed into the Moon in a collision recorded by seismometers deployed on Apollo 17 and previous Apollo expeditions.
During the return to Earth, Evans performed a 65-minute EVA to retrieve film cassettes from the service module's scientific instrument module (SIM) bay, with assistance from Schmitt who remained at the command module's hatch. At approximately 160,000 nautical miles (184,000 mi; 296,000 km) from Earth, it was the third "deep space" EVA in history, performed at great distance from any planetary body. As of , it remains one of only three such EVAs, all performed during Apollo's J-missions under similar circumstances. It was the last EVA of the Apollo program.
On December 19, the crew jettisoned the no-longer-needed SM, leaving only the CM for return to Earth. The Apollo 17 spacecraft reentered Earth's atmosphere and landed safely in the Pacific Ocean at 2:25 p.m., from the recovery ship, . Cernan, Evans, and Schmitt were then retrieved by a recovery helicopter and were safely aboard the recovery ship 52 minutes after landing.
Apollo 17 was the third mission (the others being Apollo 15 and Apollo 16) to make use of a Lunar Roving Vehicle. The LRV, in addition to being used by the astronauts for transport from station to station on the mission's three moonwalks, was used to transport the astronauts' tools, communications equipment, and samples. The Apollo 17 LRV was also used to carry experiments unique to the mission, such as the Traverse Gravimeter and Surface Electrical Properties experiment. The Apollo 17 LRV traveled a cumulative distance of approximately in a total drive time of about four hours and twenty-six minutes; the greatest distance Eugene Cernan and Harrison Schmitt traveled from the lunar module was about .
Apollo 17 included a biological cosmic ray experiment (BIOCORE), carrying five mice that had been implanted with radiation monitors to see whether they suffered damage from cosmic rays.
The five pocket mice ("Perognathus longimembris") were implanted with radiation monitors under their scalps and flown on the mission. The species was chosen because it was well-documented, small, easy to maintain in an isolated state (not requiring drinking water for the duration of the mission and with highly concentrated waste), and for its ability to withstand environmental stress. Four of the five mice survived the flight; the cause of death of the fifth mouse was not determined.
The study found lesions in the scalp itself and liver. The scalp lesions and liver lesions appeared to be unrelated to one another, and were not thought to be the result of cosmic rays. No damage was found in the mice's retinas or viscera. At the time of the publication of the Apollo 17 Preliminary Science Report, the mouse brains had not yet been examined. However, subsequent studies showed no significant effect on the brains.
Officially, the mice—four male and one female—were assigned the identification numbers A3326, A3400, A3305, A3356 and A3352. Unofficially, according to Cernan, the Apollo 17 crew dubbed them Fe, Fi, Fo, Fum, and Phooey.
Sector one of the Apollo 17 SM contained the scientific instrument module (SIM) bay. The SIM bay housed three experiments for use in lunar orbit: a lunar sounder, an infrared scanning radiometer, and a far-ultraviolet spectrometer. A mapping camera, panoramic camera, and a laser altimeter were also included in the SIM bay.
The lunar sounder beamed electromagnetic impulses toward the lunar surface, which were designed with the objective of obtaining data to assist in developing a geological model of the interior of the Moon to an approximate depth of .
The infrared scanning radiometer was designed with the objective of generating a temperature map of the lunar surface to aid in locating surface features such as rock fields, structural differences in the lunar crust, and volcanic activity.
The far-ultraviolet spectrometer was to be used to obtain data pertaining to the composition, density, and constituency of the lunar atmosphere. The spectrometer was also designed to detect far-UV radiation emitted by the Sun that has been reflected off the lunar surface.
The laser altimeter was designed with the intention of measuring the altitude of the spacecraft above the lunar surface within approximately , and providing altitude information to the panoramic and mapping cameras.
Throughout the Apollo lunar missions, the crew members observed light flashes that penetrated closed eyelids. These flashes, described as "streaks" or "specks" of light, were usually observed by astronauts while the spacecraft was darkened during a sleep period. These flashes, while not observed on the lunar surface, would average about two per minute and were observed by the crew members during the trip out to the Moon, back to Earth, and in lunar orbit.
The Apollo 17 crew conducted an experiment, also conducted on Apollo 16, with the objective of linking these light flashes with cosmic rays. As part of an experiment conducted by NASA and the University of Houston, one astronaut wore a device that recorded the time, strength, and path of high-energy atomic particles that penetrated the device. Evidence supports the hypothesis that these flashes occur when charged particles travel through the retina in the eye.
Apollo 17 was the only Apollo lunar landing mission to carry the Traverse Gravimeter Experiment (TGE), built by Draper Laboratory at the Massachusetts Institute of Technology. As gravimeters had proven to be useful in the geologic investigation of the Earth, the objective of this experiment was to determine the feasibility of using the same techniques on the Moon to learn about its internal structure. The gravimeter was used to obtain relative gravity measurements at the landing site in the immediate vicinity of the lunar module, as well as various locations on the mission's traverse routes. Scientists would then use this data to help determine the geological substructure of the landing site and the surrounding vicinity.
The TGE was carried on the Lunar Roving Vehicle; measurements were taken by the astronauts while the LRV was not in motion or after the gravimeter was placed on the surface. A total of twenty-six measurements were taken with the TGE during the mission's three moonwalks, with productive results. As part of the Apollo Lunar Surface Experiments Package (ALSEP), the astronauts also deployed the Lunar Surface Gravimeter, a similar experiment, which ultimately failed to function properly.
Apollo 17 was the only lunar surface expedition to include the surface electrical properties (SEP) experiment. The experiment included two major components: a transmitting antenna deployed near the lunar module and a receiving antenna located on the Lunar Roving Vehicle. At different stops during the mission's traverses, electrical signals traveled from the transmitting device, through the ground, and were received at the LRV. The electrical properties of the lunar soil could be determined by comparison of the transmitted and received electrical signals. The results of this experiment, which are consistent with lunar rock composition, show that the top of the Moon are extremely dry.
The Command Module "America" is currently on display at Space Center Houston at the Lyndon B. Johnson Space Center in Houston, Texas.
The ascent stage of Lunar Module "Challenger" impacted the Moon December 15, 1972, at 06:50:20.8 UT (1:50 am EST), at . The descent stage remains on the Moon at the landing site, .
In 2009 and again in 2011, the Lunar Reconnaissance Orbiter photographed the landing site from increasingly low orbits.
The German space company PTScientists is planning to land two lunar rovers near the landing site in 2020 or later.
The crew of Apollo 17 carried a small Panamanian flag to the Moon during the mission. In 1973, U.S. President Richard Nixon gave the flag and a Moon rock to the government of Panama.
Portions of the Apollo 17 mission are dramatized in the 1998 HBO miniseries "From the Earth to the Moon" episode entitled "Le Voyage dans la Lune."
The prologue to the 1999 novel "Back to the Moon", by Homer Hickam, begins with a dramatized depiction of the end of the second Apollo 17 EVA. The orange soil then becomes the major driver of the plot of the rest of the story.
The 2005 novel "Tyrannosaur Canyon" by Douglas Preston opens with a depiction of the Apollo 17 moonwalks using quotes taken from the official mission transcript.
Additionally, there have been fictional astronauts in film, literature and television who have been described as "the last man to walk on the Moon", implying they were crew members on Apollo 17 or an analogue mission. One such character was Steve Austin in the television series "The Six Million Dollar Man". In the 1972 novel "Cyborg", upon which the series was based, Austin remembers watching the Earth "fall away during Apollo XVII." In the 1998 film "Deep Impact" fictional astronaut Spurgeon "Fish" Tanner, portrayed by Robert Duvall, was described at a Presidential press conference as the "last man to walk on the Moon" by the President of the United States, portrayed by Morgan Freeman.
The 2013 song "Contact" from Daft Punk includes audio from the Apollo 17 mission, courtesy of NASA and Captain Eugene Cernan.
In the 2014 anime "Aldnoah.Zero", the Apollo 17 mission locates an ancient transporter gate leading to Mars left by an unknown, extinct alien race. This discovery is the divergence point for the story's alternative history.
The song "Tomorrow" by Public Service Broadcasting also includes audio of Commander Eugene A. Cernan and Lunar Module Pilot Harrison Schmitt from the mission.

</doc>
<doc id="1973" url="https://en.wikipedia.org/wiki?curid=1973" title="American Revolution">
American Revolution

The American Revolution was an ideological and political revolution which occurred in colonial North America between 1765 and 1783. The American Patriots in the Thirteen Colonies defeated the British in the American Revolutionary War (1775–1783), gaining independence from the British Crown and establishing the United States of America, the first modern democracy and history's first nation explicitly founded as a democracy.
The American colonials proclaimed "no taxation without representation" starting with the Stamp Act Congress in 1765. They had no representatives in the British Parliament and so rejected Parliament's authority to tax them. Protests steadily escalated to the Boston Massacre in 1770 and the burning of the "Gaspee" in Rhode Island in 1772, followed by the Boston Tea Party in December 1773. The British responded by closing Boston Harbor and enacting a series of punitive laws which effectively rescinded Massachusetts Bay Colony's rights of self-government. The other colonies rallied behind Massachusetts, and a group of American Patriot leaders set up their own government in late 1774 at the Continental Congress to coordinate their resistance of Britain; other colonists retained their allegiance to the Crown and were known as "Loyalists" or "Tories".
Tensions erupted into battle between Patriot militia and British regulars when King George's forces attempted to destroy American military supplies at Lexington and Concord on April 19, 1775. The conflict quickly escalated into war, during which the Patriots (and later their French allies) fought the British and Loyalists in the Revolutionary War. Each colony formed a Provincial Congress which assumed power from the former colonial governments, suppressed Loyalism, and recruited a Continental Army led by General George Washington. The Continental Congress declared King George a tyrant who trampled the colonists' rights as Englishmen, and they declared the colonies free and independent states on July 2, 1776. The Patriot leadership professed the political philosophies of liberalism and republicanism to reject monarchy and aristocracy, and they proclaimed that all men are created equal.
The Patriots unsuccessfully attempted to invade Quebec during the winter of 1775–76. The newly created Continental Army forced the British military out of Boston in March 1776, but the British captured New York City and its strategic harbor that summer, which they held for the duration of the war. The Royal Navy blockaded ports and captured other cities for brief periods, but they failed to destroy Washington's forces. The Continental Army captured a British army at the Battle of Saratoga in October 1777, and France then entered the war as an ally of the United States. Britain then refocused its war to make France the main enemy. Britain also attempted to hold the Southern states with the anticipated aid of Loyalists, and the war moved south. Charles Cornwallis captured an army at Charleston, South Carolina in early 1780, but he failed to enlist enough volunteers from Loyalist civilians to take effective control of the territory. Finally, a combined American and French force captured a second British army at Yorktown in the fall of 1781, effectively ending the war. The Treaty of Paris was signed on September 3, 1783, formally ending the conflict and confirming the new nation's complete separation from the British Empire. The United States took possession of nearly all the territory east of the Mississippi River and south of the Great Lakes, with the British retaining control of northern Canada, and Spain taking Florida.
Among the significant results of the Revolution were American independence and friendly economic trade with Britain. The Americans adopted the United States Constitution, establishing a strong national government which included an elected executive, a national judiciary, and an elected bicameral Congress representing states in the Senate and the population in the House of Representatives. Around 60,000 Loyalists migrated to other British territories, particularly to British North America (Canada), but the great majority remained in the United States.
As early as 1651, the English government had sought to regulate trade in the American colonies, and Parliament passed the Navigation Acts on October 9 to provide the plantation colonies of the south with a profitable export market. The Acts prohibited British producers from growing tobacco and also encouraged shipbuilding, particularly in the New England colonies. Some argue that the economic impact was minimal on the colonists, but the political friction which the acts triggered was more serious, as the merchants most directly affected were also the most politically active.
King Philip's War ended in 1678, which the New England colonies fought without any military assistance from England, and this contributed to the development of a unique identity separate from that of the British people. But King Charles II determined to bring the New England colonies under a more centralized administration in the 1680s to regulate trade to more effectively benefit the homeland. The New England colonists fiercely opposed his efforts, and the Crown nullified their colonial charters in response. Charles' successor James II finalized these efforts in 1686, establishing the consolidated Dominion of New England. Dominion rule triggered bitter resentment throughout New England; the enforcement of the unpopular Navigation Acts and the curtailing of local democracy angered the colonists. New Englanders were encouraged, however, by a change of government in England which saw James II effectively abdicate, and a populist uprising in New England overthrew Dominion rule on April 18, 1689. Colonial governments reasserted their control after the revolt, and successive governments made no more attempts to restore the Dominion.
Subsequent English governments continued in their efforts to tax certain goods, passing acts regulating the trade of wool, hats, and molasses. The Molasses Act of 1733 was particularly egregious to the colonists, as a significant part of colonial trade relied on molasses. The taxes severely damaged the New England economy and resulted in a surge of smuggling, bribery, and intimidation of customs officials. Colonial wars fought in America were also a source of considerable tension. The British captured the fortress of Louisbourg during King George's War but then ceded it back to France in 1748. New England colonists resented their losses of lives, as well as the effort and expenditure involved in subduing the fortress, only to have it returned to their erstwhile enemy.
Some writers begin their histories of the American Revolution with the British coalition victory in the Seven Years' War in 1763, viewing the French and Indian War as though it were the American theater of the Seven Years' War. Lawrence Henry Gipson writes:
The Royal Proclamation of 1763 redrew boundaries of the lands west of Quebec and west of a line running along the crest of the Allegheny Mountains, making them indigenous territory and barred to colonial settlement for two years. The colonists protested, and the boundary line was adjusted in a series of treaties with indigenous tribes. In 1768, the Iroquois agreed to the Treaty of Fort Stanwix, and the Cherokee agreed to the Treaty of Hard Labour followed in 1770 by the Treaty of Lochaber. The treaties opened most of Kentucky and West Virginia to colonial settlement. The new map was drawn up at the Treaty of Fort Stanwix in 1768 which moved the line much farther to the west, from the green line to the red line on the map at right.
Prime Minister George Grenville asserted in 1762 that the whole revenue of the custom houses in America amounted to one or two thousand pounds a year, and that the English exchequer was paying between seven and eight thousand pounds a year to collect . Adam Smith wrote in "The Wealth of Nations" that Parliament "has never hitherto demanded of [the American colonies] anything which even approached to a just proportion to what was paid by their fellow subjects at home."
As early as 1651, the English government had sought to regulate trade in the American colonies. On October 9, 1651, they passed the Navigation Acts to pursue a mercantilist policy intended to ensure that trade enriched Great Britain but prohibited trade with any other nations. Parliament also passed the Sugar Act, decreasing the existing customs duties on sugar and molasses but providing stricter measures of enforcement and collection. That same year, Grenville proposed direct taxes on the colonies to raise revenue, but he delayed action to see whether the colonies would propose some way to raise the revenue themselves.
Parliament finally passed the Stamp Act in March 1765, which imposed direct taxes on the colonies for the first time. All official documents, newspapers, almanacs, and pamphlets were required to have the stamps—even decks of playing cards. The colonists did not object that the taxes were high; they were actually low. They objected to their lack of representation in the Parliament, which gave them no voice concerning legislation that affected them. Benjamin Franklin testified in Parliament in 1766 that Americans already contributed heavily to the defense of the Empire. He said that local governments had raised, outfitted, and paid 25,000 soldiers to fight France—as many as Britain itself sent—and spent many millions from American treasuries doing so in the French and Indian War alone. London had to deal with 1,500 politically well-connected British Army soldiers. The decision was to keep them on active duty with full pay, but they had to be stationed somewhere. Stationing a standing army in Great Britain during peacetime was politically unacceptable, so the decision was made to station them in America and have the Americans pay them. The soldiers had no military mission; they were not there to defend the colonies because there was no threat to the colonies.
The Sons of Liberty formed that same year in 1765, and they used public demonstrations, boycotts, and threats of violence to ensure that the British tax laws were unenforceable. In Boston, the Sons of Liberty burned the records of the vice admiralty court and looted the home of chief justice Thomas Hutchinson. Several legislatures called for united action, and nine colonies sent delegates to the Stamp Act Congress in New York City in October. Moderates led by John Dickinson drew up a "Declaration of Rights and Grievances" stating that taxes passed without representation violated their rights as Englishmen, and colonists emphasized their determination by boycotting imports of British merchandise.
The Parliament at Westminster saw itself as the supreme lawmaking authority throughout all British possessions and thus entitled to levy any tax without colonial approval. They argued that the colonies were legally British corporations subordinate to the British parliament, and they pointed to numerous instances where Parliament had made laws in the past that were binding on the colonies. Parliament insisted that the colonies effectively enjoyed a "virtual representation" as most British people did, as only a small minority of the British population elected representatives to Parliament, but Americans such as James Otis maintained that they were not "virtually represented" at all.
The Rockingham government came to power in July 1765, and Parliament debated whether to repeal the stamp tax or to send an army to enforce it. Benjamin Franklin made the case for repeal, explaining that the colonies had spent heavily in manpower, money, and blood defending the empire in a series of wars against the French and indigenous people, and that further taxes to pay for those wars were unjust and might bring about a rebellion. Parliament agreed and repealed the tax on February 21, 1766, but they insisted in the Declaratory Act of March 1766 that they retained full power to make laws for the colonies "in all cases whatsoever". The repeal nonetheless caused widespread celebrations in the colonies.
In 1767, the Parliament passed the Townshend Acts which placed duties on a number of staple goods, including paper, glass, and tea, and established a Board of Customs in Boston to more rigorously execute trade regulations. The new taxes were enacted on the belief that Americans only objected to internal taxes and not to external taxes such as custom duties. The Americans, however, argued against the constitutionality of the act because its purpose was to raise revenue and not regulate trade. Colonists responded by organizing new boycotts of British goods. These boycotts were less effective, however, as the Townshend goods were widely used.
In February 1768, the Assembly of Massachusetts Bay issued a circular letter to the other colonies urging them to coordinate resistance. The governor dissolved the assembly when it refused to rescind the letter. Meanwhile, a riot broke out in Boston in June 1768 over the seizure of the sloop "Liberty", owned by John Hancock, for alleged smuggling. Customs officials were forced to flee, prompting the British to deploy troops to Boston. A Boston town meeting declared that no obedience was due to parliamentary laws and called for the convening of a convention. A convention assembled but only issued a mild protest before dissolving itself. In January 1769, Parliament responded to the unrest by reactivating the Treason Act 1543 which called for subjects outside the realm to face trials for treason in England. The governor of Massachusetts was instructed to collect evidence of said treason, and the threat caused widespread outrage, though it was not carried out.
On March 5, 1770, a large crowd gathered around a group of British soldiers. The crowd grew threatening, throwing snowballs, rocks, and debris at them. One soldier was clubbed and fell. There was no order to fire, but the soldiers fired into the crowd anyway. They hit 11 people; three civilians died at the scene of the shooting, and two died after the incident. The event quickly came to be called the Boston Massacre. The soldiers were tried and acquitted (defended by John Adams), but the widespread descriptions soon began to turn colonial sentiment against the British. This began a downward spiral in the relationship between Britain and the Province of Massachusetts.
A new ministry under Lord North came to power in 1770, and Parliament withdrew all taxes except the tax on tea, giving up its efforts to raise revenue while maintaining the right to tax. This temporarily resolved the crisis, and the boycott of British goods largely ceased, with only the more radical patriots such as Samuel Adams continuing to agitate.
In June 1772, American patriots, including John Brown, burned a British warship that had been vigorously enforcing unpopular trade regulations in what became known as the "Gaspee" Affair. The affair was investigated for possible treason, but no action was taken.
In 1772, it became known that the Crown intended to pay fixed salaries to the governors and judges in Massachusetts, which had been paid by local authorities. This would reduce the influence of colonial representatives over their government. Samuel Adams in Boston set about creating new Committees of Correspondence, which linked Patriots in all 13 colonies and eventually provided the framework for a rebel government. Virginia, the largest colony, set up its Committee of Correspondence in early 1773, on which Patrick Henry and Thomas Jefferson served.
A total of about 7,000 to 8,000 Patriots served on "Committees of Correspondence" at the colonial and local levels, comprising most of the leadership in their communities. Loyalists were excluded. The committees became the leaders of the American resistance to British actions, and largely determined the war effort at the state and local level. When the First Continental Congress decided to boycott British products, the colonial and local Committees took charge, examining merchant records and publishing the names of merchants who attempted to defy the boycott by importing British goods.
In 1773, private letters were published in which Massachusetts Governor Thomas Hutchinson claimed that the colonists could not enjoy all English liberties, and Lieutenant Governor Andrew Oliver called for the direct payment of colonial officials. The letters' contents were used as evidence of a systematic plot against American rights, and discredited Hutchinson in the eyes of the people; the Assembly petitioned for his recall. Benjamin Franklin, postmaster general for the colonies, acknowledged that he leaked the letters, which led to him being berated by British officials and fired from his job.
Meanwhile, Parliament passed the Tea Act to lower the price of taxed tea exported to the colonies to help the East India Company undersell smuggled Dutch tea. Special consignees were appointed to sell the tea to bypass colonial merchants. The act was opposed by those who resisted the taxes and also by smugglers who stood to lose business. In most instances, the consignees were forced to resign and the tea was turned back, but Massachusetts governor Hutchinson refused to allow Boston merchants to give in to pressure. A town meeting in Boston determined that the tea would not be landed, and ignored a demand from the governor to disperse. On December 16, 1773, a group of men, led by Samuel Adams and dressed to evoke the appearance of indigenous people, boarded the ships of the British East India Company and dumped £10,000 worth of tea from their holds (approximately £636,000 in 2008) into Boston Harbor. Decades later, this event became known as the Boston Tea Party and remains a significant part of American patriotic lore.
The British government responded by passing several Acts which came to be known as the Intolerable Acts, which further darkened colonial opinion towards the British. They consisted of four laws enacted by the British parliament. The first was the Massachusetts Government Act which altered the Massachusetts charter and restricted town meetings. The second act was the Administration of Justice Act which ordered that all British soldiers to be tried were to be arraigned in Britain, not in the colonies. The third Act was the Boston Port Act, which closed the port of Boston until the British had been compensated for the tea lost in the Boston Tea Party. The fourth Act was the Quartering Act of 1774, which allowed royal governors to house British troops in the homes of citizens without requiring permission of the owner.
In response, Massachusetts patriots issued the Suffolk Resolves and formed an alternative shadow government known as the "Provincial Congress" which began training militia outside British-occupied Boston. In September 1774, the First Continental Congress convened, consisting of representatives from each colony, to serve as a vehicle for deliberation and collective action. During secret debates, conservative Joseph Galloway proposed the creation of a colonial Parliament that would be able to approve or disapprove of acts of the British Parliament, but his idea was not accepted. The Congress instead endorsed the proposal of John Adams that Americans would obey Parliament voluntarily but would resist all taxes in disguise. Congress called for a boycott beginning on 1 December 1774 of all British goods; it was enforced by new committees authorized by the Congress.
Massachusetts was declared in a state of rebellion in February 1775 and the British garrison received orders to disarm the rebels and arrest their leaders, leading to the Battles of Lexington and Concord on 19 April 1775. The Patriots laid siege to Boston, expelled royal officials from all the colonies, and took control through the establishment of Provincial Congresses. The Battle of Bunker Hill followed on June 17, 1775. It was a British victory—but at a great cost: about 1,000 British casualties from a garrison of about 6,000, as compared to 500 American casualties from a much larger force. The Second Continental Congress was divided on the best course of action, but eventually produced the Olive Branch Petition, in which they attempted to come to an accord with King George. The king, however, issued a Proclamation of Rebellion which stated that the states were "in rebellion" and the members of Congress were traitors.
The war that arose was in some ways a classic insurgency. As Benjamin Franklin wrote to Joseph Priestley in October 1775: "Britain, at the expense of three millions, has killed 150 Yankees this campaign, which is £20,000 a head ... During the same time, 60,000 children have been born in America. From these data his mathematical head will easily calculate the time and expense necessary to kill us all.".
In the winter of 1775, the Americans invaded northern Canada under generals Benedict Arnold and Richard Montgomery, expecting to rally sympathetic colonists there. The attack was a failure; many Americans who weren't killed were either captured or died of smallpox.
In March 1776, the Continental Army forced the British to evacuate Boston, with George Washington as the commander of the new army. The revolutionaries now fully controlled all thirteen colonies and were ready to declare independence. There still were many Loyalists, but they were no longer in control anywhere by July 1776, and all of the Royal officials had fled.
Following the Battle of Bunker Hill in June 1775, the Patriots had control of Massachusetts outside the Boston city limits, and the Loyalists suddenly found themselves on the defensive with no protection from the British army. In all 13 colonies, Patriots had overthrown their existing governments, closing courts and driving away British officials. They had elected conventions and "legislatures" that existed outside any legal framework; new constitutions were drawn up in each state to supersede royal charters. They declared that they were states, not colonies.
On January 5, 1776, New Hampshire ratified the first state constitution. In May 1776, Congress voted to suppress all forms of crown authority, to be replaced by locally created authority. Virginia, South Carolina, and New Jersey created their constitutions before July 4. Rhode Island and Connecticut simply took their existing royal charters and deleted all references to the crown. The new states were all committed to republicanism, with no inherited offices. They decided what form of government to create, and also how to select those who would craft the constitutions and how the resulting document would be ratified. On 26 May 1776, John Adams wrote James Sullivan from Philadelphia:
The resulting constitutions in states such as Maryland, Virginia, Delaware, New York, and Massachusetts featured:
In Pennsylvania, New Jersey, and New Hampshire, the resulting constitutions embodied:
The radical provisions of Pennsylvania's constitution lasted only 14 years. In 1790, conservatives gained power in the state legislature, called a new constitutional convention, and rewrote the constitution. The new constitution substantially reduced universal male suffrage, gave the governor veto power and patronage appointment authority, and added an upper house with substantial wealth qualifications to the unicameral legislature. Thomas Paine called it a constitution unworthy of America.
In April 1776, the North Carolina Provincial Congress issued the Halifax Resolves explicitly authorizing its delegates to vote for independence. By June, nine Provincial Congresses were ready for independence; one by one, the last four fell into line: Pennsylvania, Delaware, Maryland, and New York. Richard Henry Lee was instructed by the Virginia legislature to propose independence, and he did so on June 7, 1776. On June 11, a committee was created to draft a document explaining the justifications for separation from Britain. After securing enough votes for passage, independence was voted for on July 2.
The Declaration of Independence was drafted largely by Thomas Jefferson and presented by the committee; it was unanimously adopted by the entire Congress on July 4, and each colony became independent and autonomous. The next step was to form a union to facilitate international relations and alliances.
The Second Continental Congress approved the "Articles of Confederation" for ratification by the states on November 15, 1777; the Congress immediately began operating under the Articles' terms, providing a structure of shared sovereignty during prosecution of the war and facilitating international relations and alliances with France and Spain. The articles were ratified on March 1, 1781. At that point, the Continental Congress was dissolved and a new government of the United States in Congress Assembled took its place on the following day, with Samuel Huntington as presiding officer.
According to British historian Jeremy Black, the British had significant advantages, including a highly trained army, the world's largest navy, and an efficient system of public finance that could easily fund the war. However, they seriously misunderstood the depth of support for the American Patriot position and ignored the advice of General Gage, misinterpreting the situation as merely a large-scale riot. The British government believed that they could overawe the Americans by sending a large military and naval force, forcing them to be loyal again:
Washington forced the British out of Boston in the spring of 1776, and neither the British nor the Loyalists controlled any significant areas. The British, however, were massing forces at their naval base at Halifax, Nova Scotia. They returned in force in July 1776, landing in New York and defeating Washington's Continental Army in August at the Battle of Brooklyn. Following that victory, they requested a meeting with representatives from Congress to negotiate an end to hostilities.
A delegation including John Adams and Benjamin Franklin met British admiral Richard Howe on Staten Island in New York Harbor on September 11 in what became known as the Staten Island Peace Conference. Howe demanded that the Americans retract the Declaration of Independence, which they refused to do, and negotiations ended. The British then seized New York City and nearly captured Washington's army. They made New York their main political and military base of operations, holding it until November 1783. The city became the destination for Loyalist refugees and a focal point of Washington's intelligence network.
The British also took New Jersey, pushing the Continental Army into Pennsylvania. Washington crossed the Delaware River back into New Jersey in a surprise attack in late December 1776 and defeated the Hessian and British armies at Trenton and Princeton, thereby regaining control of most of New Jersey. The victories gave an important boost to Patriots at a time when morale was flagging, and they have become iconic events of the war.
In 1777, the British sent Burgoyne's invasion force from Canada south to New York to seal off New England. Their aim was to isolate New England, which the British perceived as the primary source of agitation. Rather than move north to support Burgoyne, the British army in New York City went to Philadelphia in a major case of mis-coordination, capturing it from Washington. The invasion army under Burgoyne was much too slow and became trapped in northern New York state. It surrendered after the Battles of Saratoga in October 1777. From early October 1777 until November 15, a siege distracted British troops at Fort Mifflin, Philadelphia, Pennsylvania, and allowed Washington time to preserve the Continental Army by safely leading his troops to harsh winter quarters at Valley Forge.
On August 23, 1775, George III declared Americans to be traitors to the Crown if they took up arms against royal authority. There were thousands of British and Hessian soldiers in American hands following their surrender at the Battles of Saratoga in October 1777. Lord Germain took a hard line, but the British generals on American soil never held treason trials and treated captured American soldiers as prisoners of war. The dilemma was that tens of thousands of Loyalists were under American control and American retaliation would have been easy. The British built much of their strategy around using these Loyalists. The British maltreated the prisoners whom they held, resulting in more deaths to American prisoners of war than from combat operations. At the end of the war, both sides released their surviving prisoners.
The capture of a British army at Saratoga encouraged the French to formally enter the war in support of Congress, and Benjamin Franklin negotiated a permanent military alliance in early 1778; France thus became the first foreign nation to officially recognize the Declaration of Independence. On February 6, 1778, the United States and France signed the Treaty of Amity and Commerce and the Treaty of Alliance. William Pitt spoke out in Parliament urging Britain to make peace in America and to unite with America against France, while British politicians who had sympathized with colonial grievances now turned against the Americans for allying with Britain's rival and enemy.
The Spanish and the Dutch became allies of the French in 1779 and 1780 respectively, forcing the British to fight a global war without major allies and requiring it to slip through a combined blockade of the Atlantic. Britain began to view the American war for independence as merely one front in a wider war, and the British chose to withdraw troops from America to reinforce the British colonies in the Caribbean, which were under threat of Spanish or French invasion. British commander Sir Henry Clinton evacuated Philadelphia and returned to New York City. General Washington intercepted him in the Battle of Monmouth Court House, the last major battle fought in the north. After an inconclusive engagement, the British retreated to New York City. The northern war subsequently became a stalemate, as the focus of attention shifted to the smaller southern theater.
The British strategy in America now concentrated on a campaign in the southern states. With fewer regular troops at their disposal, the British commanders saw the "southern strategy" as a more viable plan, as they perceived the south as strongly Loyalist with a large population of recent immigrants and large numbers of slaves who might be tempted to run away from their masters to join the British.
Beginning in late December 1778, they captured Savannah and controlled the Georgia coastline. In 1780, they launched a fresh invasion and took Charleston, as well. A significant victory at the Battle of Camden meant that royal forces soon controlled most of Georgia and South Carolina. The British set up a network of forts inland, hoping that the Loyalists would rally to the flag. Not enough Loyalists turned out, however, and the British had to fight their way north into North Carolina and Virginia with a severely weakened army. Behind them, much of the territory that they had already captured dissolved into a chaotic guerrilla war, fought predominantly between bands of Loyalists and American militia, which negated many of the gains that the British had previously made.
The British army under Cornwallis marched to Yorktown, Virginia, where they expected to be rescued by a British fleet. The fleet did arrive, but so did a larger French fleet. The French were victorious in the Battle of the Chesapeake, and the British fleet returned to New York for reinforcements, leaving Cornwallis trapped. In October 1781, the British surrendered their second invading army of the war under a siege by the combined French and Continental armies commanded by Washington.
Historians continue to debate whether the odds were long or short for American victory. John E. Ferling says that the odds were so long that the American victory was "almost a miracle". On the other hand, Joseph Ellis says that the odds favored the Americans, and asks whether there ever was any realistic chance for the British to win. He argues that this opportunity came only once, in the summer of 1776, and the British failed that test. Admiral Howe and his brother General Howe "missed several opportunities to destroy the Continental Army ... Chance, luck, and even the vagaries of the weather played crucial roles." Ellis's point is that the strategic and tactical decisions of the Howes were fatally flawed because they underestimated the challenges posed by the Patriots. Ellis concludes that, once the Howe brothers failed, the opportunity "would never come again" for a British victory.
Support for the conflict had never been strong in Britain, where many sympathized with the Americans, but now it reached a new low. King George wanted to fight on, but his supporters lost control of Parliament and they launched no further offensives in America. War erupted between America and Britain three decades later with the War of 1812, which firmly established the permanence of the United States and its complete autonomy.
Washington did not know whether the British might reopen hostilities after Yorktown. They still had 26,000 troops occupying New York City, Charleston, and Savannah, together with a powerful fleet. The French army and navy departed, so the Americans were on their own in 1782–83. The treasury was empty, and the unpaid soldiers were growing restive, almost to the point of mutiny or possible "coup d'état". Washington dispelled the unrest among officers of the Newburgh Conspiracy in 1783, and Congress subsequently created the promise of a five years bonus for all officers.
During negotiations in Paris, the American delegation discovered that France supported American independence but no territorial gains, hoping to confine the new nation to the area east of the Appalachian Mountains. The Americans opened direct secret negotiations with London, cutting out the French. British Prime Minister Lord Shelburne was in charge of the British negotiations, and he saw a chance to make the United States a valuable economic partner. The US obtained all the land east of the Mississippi River, including southern Canada, but Spain took control of Florida from the British. It gained fishing rights off Canadian coasts, and agreed to allow British merchants and Loyalists to recover their property. Prime Minister Shelburne foresaw highly profitable two-way trade between Britain and the rapidly growing United States, which did come to pass. The blockade was lifted and all British interference had been driven out, and American merchants were free to trade with any nation anywhere in the world.
The British largely abandoned their indigenous allies, who were not a party to this treaty and did not recognize it until they were defeated militarily by the United States. However, the British did sell them munitions and maintain forts in American territory until the Jay Treaty of 1795.
Losing the war and the Thirteen Colonies was a shock to Britain. The war revealed the limitations of Britain's fiscal-military state when they discovered that they suddenly faced powerful enemies with no allies, and they were dependent on extended and vulnerable transatlantic lines of communication. The defeat heightened dissension and escalated political antagonism to the King's ministers. Inside Parliament, the primary concern changed from fears of an over-mighty monarch to the issues of representation, parliamentary reform, and government retrenchment. Reformers sought to destroy what they saw as widespread institutional corruption, and the result was a crisis from 1776 to 1783. The peace in 1783 left France financially prostrate, while the British economy boomed thanks to the return of American business. The crisis ended after 1784 thanks to the King's shrewdness in outwitting Charles James Fox (the leader of the Fox-North Coalition), and renewed confidence in the system engendered by the leadership of Prime Minister William Pitt. Some historians suggest that loss of the American colonies enabled Britain to deal with the French Revolution with more unity and better organization than would otherwise have been the case. Britain turned towards Asia, the Pacific, and later Africa with subsequent exploration leading to the rise of the Second British Empire.
Britain's war against the Americans, the French, and the Spanish cost about £100 million, and the Treasury borrowed 40-percent of the money that it needed. Heavy spending brought France to the verge of bankruptcy and revolution, while the British had relatively little difficulty financing their war, keeping their suppliers and soldiers paid, and hiring tens of thousands of German soldiers. Britain had a sophisticated financial system based on the wealth of thousands of landowners who supported the government, together with banks and financiers in London. The British tax system collected about 12 percent of the GDP in taxes during the 1770s.
In sharp contrast, Congress and the American states had no end of difficulty financing the war. In 1775, there was at most 12 million dollars in gold in the colonies, not nearly enough to cover current transactions, let alone finance a major war. The British made the situation much worse by imposing a tight blockade on every American port, which cut off almost all imports and exports. One partial solution was to rely on volunteer support from militiamen and donations from patriotic citizens. Another was to delay actual payments, pay soldiers and suppliers in depreciated currency, and promise that it would be made good after the war. Indeed, the soldiers and officers were given land grants in 1783 to cover the wages that they had earned but had not been paid during the war. The national government did not have a strong leader in financial matters until 1781, when Robert Morris was named Superintendent of Finance of the United States. Morris used a French loan in 1782 to set up the private Bank of North America to finance the war. He reduced the civil list, saved money by using competitive bidding for contracts, tightened accounting procedures, and demanded the national government's full share of money and supplies from the individual states.
Congress used four main methods to cover the cost of the war, which cost about 66 million dollars in specie (gold and silver). Congress made issues of paper money in 1775–1780 and in 1780–81. The first issue amounted to 242 million dollars. This paper money would supposedly be redeemed for state taxes, but the holders were eventually paid off in 1791 at the rate of one cent on the dollar. By 1780, the paper money was "not worth a Continental", as people said. The skyrocketing inflation was a hardship on the few people who had fixed incomes, but 90 percent of the people were farmers and were not directly affected by it. Debtors benefited by paying off their debts with depreciated paper. The greatest burden was borne by the soldiers of the Continental Army whose wages were usually paid late and declined in value every month, weakening their morale and adding to the hardships of their families.
Beginning in 1777, Congress repeatedly asked the states to provide money, but the states had no system of taxation and were of little help. By 1780, Congress was making requisitions for specific supplies of corn, beef, pork, and other necessities, an inefficient system which barely kept the army alive. Starting in 1776, the Congress sought to raise money by loans from wealthy individuals, promising to redeem the bonds after the war. The bonds were redeemed in 1791 at face value, but the scheme raised little money because Americans had little specie, and many of the rich merchants were supporters of the Crown. The French secretly supplied the Americans with money, gunpowder, and munitions to weaken Great Britain; the subsidies continued when France entered the war in 1778, and the French government and Paris bankers lent large sums to the American war effort. The Americans struggled to pay off the loans; they ceased making interest payments to France in 1785 and defaulted on installments due in 1787. In 1790, however, they resumed regular payments on their debts to the French, and settled their accounts with the French government in 1795 by selling the debt to James Swan, an American banker.
The war ended in 1783 and was followed by a period of prosperity. The national government was still operating under the Articles of Confederation and settled the issue of the western territories, which the states ceded to Congress. American settlers moved rapidly into those areas, with Vermont, Kentucky, and Tennessee becoming states in the 1790s.
However, the national government had no money either to pay the war debts owed to European nations and the private banks, or to pay Americans who had been given millions of dollars of promissory notes for supplies during the war. Nationalists led by Washington, Alexander Hamilton, and other veterans feared that the new nation was too fragile to withstand an international war, or even internal revolts such as the Shays' Rebellion of 1786 in Massachusetts. They convinced Congress to call the Philadelphia Convention in 1787 and named their party the Federalist party. The Convention adopted a new Constitution which provided for a much stronger federal government, including an effective executive in a check-and-balance system with the judiciary and legislature. The Constitution was ratified in 1788, after a fierce debate in the states over the proposed new government. The new government under President George Washington took office in New York in March 1789. James Madison spearheaded Congressional amendments to the Constitution as assurances to those cautious about federal power, guaranteeing many of the inalienable rights that formed a foundation for the revolution, and Rhode Island was the final state to ratify the Constitution in 1791.
The national debt fell into three categories after the American Revolution. The first was the $12 million owed to foreigners, mostly money borrowed from France. There was general agreement to pay the foreign debts at full value. The national government owed $40 million and state governments owed $25 million to Americans who had sold food, horses, and supplies to the Patriot forces. There were also other debts which consisted of promissory notes issued during the war to soldiers, merchants, and farmers who accepted these payments on the premise that the new Constitution would create a government that would pay these debts eventually.
The war expenses of the individual states added up to $114 million, compared to $37 million by the central government. In 1790, Congress combined the remaining state debts with the foreign and domestic debts into one national debt totaling $80 million at the recommendation of first Secretary of the Treasury Alexander Hamilton. Everyone received face value for wartime certificates, so that the national honor would be sustained and the national credit established.
The population of the Thirteen States was not homogeneous in political views and attitudes. Loyalties and allegiances varied widely within regions and communities and even within families, and sometimes shifted during the Revolution.
The American Enlightenment was a critical precursor of the American Revolution. Chief among the ideas of the American Enlightenment were the concepts of natural law, natural rights, consent of the governed, individualism, property rights, self-ownership, self-determination, liberalism, republicanism, and defense against corruption. A growing number of American colonists embraced these views and fostered an intellectual environment which led to a new sense of political and social identity.
John Locke's (1632–1704) ideas on liberty influenced the political thinking behind the revolution, especially through his indirect influence on English writers such as John Trenchard, Thomas Gordon, and Benjamin Hoadly, whose political ideas had a strong influence on the American Patriots. Locke is often referred to as "the philosopher of the American Revolution" due to his work in the Social Contract and Natural Rights theories that underpinned the Revolution's political ideology. Locke's Two Treatises of Government published in 1689 was especially influential. He argued that all humans were created equally free, and governments therefore needed the "consent of the governed". In late eighteenth-century America, belief was still widespread in "equality by creation" and "rights by creation".
The theory of the "social contract" influenced the belief among many of the Founders that the right of the people to overthrow their leaders was one of the "natural rights" of man, should those leaders betray the historic rights of Englishmen. The Americans heavily used Montesquieu's analysis of the wisdom of the "balanced" British Constitution (mixed government) in writing the state and national constitutions.
The American ideology called "republicanism" was inspired by the Whig party in Great Britain which openly criticized the corruption within the British government. Americans were increasingly embracing republican values, seeing Britain as corrupt and hostile to American interests. The colonists associated political corruption with luxury and inherited aristocracy, which they condemned.
The Founding Fathers were strong advocates of republican values, particularly Samuel Adams, Patrick Henry, John Adams, Benjamin Franklin, Thomas Jefferson, Thomas Paine, George Washington, James Madison, and Alexander Hamilton, which required men to put civic duty ahead of their personal desires. Men had a civic duty to be prepared and willing to fight for the rights and liberties of their countrymen. John Adams wrote to Mercy Otis Warren in 1776, agreeing with some classical Greek and Roman thinkers: "Public Virtue cannot exist without private, and public Virtue is the only Foundation of Republics." He continued:
"Republican motherhood" became the ideal for American women, exemplified by Abigail Adams and Mercy Otis Warren; the first duty of the republican woman was to instill republican values in her children and to avoid luxury and ostentation.
Thomas Paine published his pamphlet "Common Sense" in January 1776, after the Revolution had started. It was widely distributed and often read aloud in taverns, contributing significantly to spreading the ideas of republicanism and liberalism together, bolstering enthusiasm for separation from Great Britain and encouraging recruitment for the Continental Army. Paine offered a solution for Americans alarmed by the threat of tyranny.
Protestant churches that had separated from the Church of England (called "dissenters") were the "school of democracy", in the words of historian Patricia Bonomi. Before the Revolution, the Southern Colonies and three of the New England Colonies had officially established churches: Congregational in Massachusetts Bay, Connecticut, and New Hampshire, and Anglican in Maryland, Virginia, North-Carolina, South Carolina, and Georgia. New York, New Jersey, Pennsylvania, Delaware, and the Colony of Rhode Island and Providence Plantations had no officially established churches. Church membership statistics from the period are unreliable and scarce, but what little data exists indicates that Anglicans were not in the majority, not even in the colonies where the Church of England was the established church, and they probably did not comprise even 30 percent of the population (with the possible exception of Virginia).
President John Witherspoon of the College of New Jersey (now Princeton University) wrote widely circulated sermons linking the American Revolution to the teachings of the Bible. Throughout the colonies, dissenting Protestant ministers (Congregational, Baptist, and Presbyterian) preached Revolutionary themes in their sermons, while most Church of England clergymen preached loyalty to the king, the titular head of the English state church. Religious motivation for fighting tyranny transcended socioeconomic lines to encompass rich and poor, men and women, frontiersmen and townsmen, farmers and merchants. The Declaration of Independence also referred to the "Laws of Nature and of Nature's God" as justification for the Americans' separation from the British monarchy. Most eighteenth-century Americans believed that the entire universe ("nature") was God's creation and he was "Nature's God". Everything was part of the "universal order of things" which began with God and was directed by his providence. Accordingly, the signers of the Declaration professed their "firm reliance on the Protection of divine Providence", and they appealed to "the Supreme Judge for the rectitude of our intentions". George Washington was firmly convinced that he was an instrument of providence, to the benefit of the American people and of all humanity.
Historian Bernard Bailyn argues that the evangelicalism of the era challenged traditional notions of natural hierarchy by preaching that the Bible teaches that all men are equal, so that the true value of a man lies in his moral behavior, not in his class. Kidd argues that religious disestablishment, belief in God as the source of human rights, and shared convictions about sin, virtue, and divine providence worked together to unite rationalists and evangelicals and thus encouraged a large proportion of Americans to fight for independence from the Empire. Bailyn, on the other hand, denies that religion played such a critical role. Alan Heimert argues that New Light anti-authoritarianism was essential to furthering democracy in colonial American society, and set the stage for a confrontation with British monarchical and aristocratic rule.
John Adams concluded in 1818:
In the mid-20th century, historian Leonard Woods Labaree identified eight characteristics of the Loyalists that made them essentially conservative, opposite to the characteristics of the Patriots. Loyalists tended to feel that resistance to the Crown was morally wrong, while the Patriots thought that morality was on their side. Loyalists were alienated when the Patriots resorted to violence, such as burning houses and tarring and feathering. Loyalists wanted to take a centrist position and resisted the Patriots' demand to declare their opposition to the Crown. Many Loyalists had maintained strong and long-standing relations with Britain, especially merchants in port cities such as New York and Boston. Many Loyalists felt that independence was bound to come eventually, but they were fearful that revolution might lead to anarchy, tyranny, or mob rule. In contrast, the prevailing attitude among Patriots was a desire to seize the initiative. Labaree also wrote that Loyalists were pessimists who lacked the confidence in the future displayed by the Patriots.
Historians in the early 20th century such as J. Franklin Jameson examined the class composition of the Patriot cause, looking for evidence of a class war inside the revolution. More recent historians have largely abandoned that interpretation, emphasizing instead the high level of ideological unity. Both Loyalists and Patriots were a "mixed lot", but ideological demands always came first. The Patriots viewed independence as a means to gain freedom from British oppression and taxation and to reassert their basic rights. Most yeomen farmers, craftsmen, and small merchants joined the Patriot cause to demand more political equality. They were especially successful in Pennsylvania but less so in New England, where John Adams attacked Thomas Paine's "Common Sense" for the "absurd democratical notions" that it proposed.
The war became a personal issue for the king, fueled by his growing belief that British leniency would be taken as weakness by the Americans. He also sincerely believed that he was defending Britain's constitution against usurpers, rather than opposing patriots fighting for their natural rights.
Those who fought for independence were called "Patriots", "Whigs", "Congress-men", or "Americans" during and after the war. They included a full range of social and economic classes but were unanimous regarding the need to defend the rights of Americans and uphold the principles of republicanism in rejecting monarchy and aristocracy, while emphasizing civic virtue by citizens. Newspapers were strongholds of patriotism (although there were a few Loyalist papers) and printed many pamphlets, announcements, patriotic letters, and pronouncements.
According to historian Robert Calhoon, 40– to 45-percent of the white population in the Thirteen Colonies supported the Patriots' cause, 15– to 20-percent supported the Loyalists, and the remainder were neutral or kept a low profile. Mark Lender analyzes why ordinary people became insurgents against the British, even if they were unfamiliar with the ideological reasons behind the war. He concludes that such people held a sense of rights which the British were violating, rights that stressed local autonomy, fair dealing, and government by consent. They were highly sensitive to the issue of tyranny, which they saw manifested in the British response to the Boston Tea Party. The arrival in Boston of the British Army heightened their sense of violated rights, leading to rage and demands for revenge. They had faith that God was on their side. The signers of the Declaration of Independence were mostly well-educated, of British stock, and of the Protestant faith.
The consensus of scholars is that about 15– to 20-percent of the white population remained loyal to the British Crown. Those who actively supported the king were known at the time as "Loyalists", "Tories", or "King's men". The Loyalists never controlled territory unless the British Army occupied it. They were typically older, less willing to break with old loyalties, and often connected to the Church of England; they included many established merchants with strong business connections throughout the Empire, as well as royal officials such as Thomas Hutchinson of Boston. There were 500 to 1,000 black loyalists, slaves who escaped to British lines and joined the British army. Many succumbed to various diseases, but Britain took the survivors to Canada as free men.
The revolution could divide families, such as William Franklin, son of Benjamin Franklin and royal governor of the Province of New Jersey who remained loyal to the Crown throughout the war. He and his father never spoke again. Recent immigrants who had not been fully Americanized were also inclined to support the King, such as Flora MacDonald, a Scottish settler in the backcountry.
After the war, the most of the approximately 500,000 Loyalists remained in America and resumed normal lives. Some became prominent American leaders, such as Samuel Seabury. Approximately 46,000 Loyalists relocated to Canada; others moved to Britain (7,000), Florida, or the West Indies (9,000). The exiles represented approximately two percent of the total population of the colonies. Nearly all black loyalists left for Nova Scotia, Florida, or England, where they could remain free. Loyalists who left the South in 1783 took thousands of their slaves with them as they fled to British colonies in the West Indies.
A minority of uncertain size tried to stay neutral in the war. Most kept a low profile, but the Quakers were the most important group to speak out for neutrality, especially in Pennsylvania. The Quakers continued to do business with the British even after the war began, and they were accused of supporting British rule, "contrivers and authors of seditious publications" critical of the revolutionary cause. Most Quakers remained neutral, although a sizeable number nevertheless participated to some degree.
Women contributed to the American Revolution in many ways and were involved on both sides. Formal politics did not include women, but ordinary domestic behaviors became charged with political significance as Patriot women confronted a war which permeated all aspects of political, civil, and domestic life. They participated by boycotting British goods, spying on the British, following armies as they marched, washing, cooking, and mending for soldiers, delivering secret messages, and even fighting disguised as men in a few cases, such as Deborah Samson. Mercy Otis Warren held meetings in her house and cleverly attacked Loyalists with her creative plays and histories. Many women also acted as nurses and helpers, tending to the soldiers' wounds and buying and selling goods for them. Some of these camp followers even participated in combat, such as Madam John Turchin who led her husband's regiment into battle. Above all, women continued the agricultural work at home to feed their families and the armies. They maintained their families during their husbands' absences and sometimes after their deaths.
American women were integral to the success of the boycott of British goods, as the boycotted items were largely household articles such as tea and cloth. Women had to return to knitting goods and to spinning and weaving their own cloth—skills that had fallen into disuse. In 1769, the women of Boston produced 40,000 skeins of yarn, and 180 women in Middletown, Massachusetts wove of cloth. Many women gathered food, money, clothes, and other supplies during the war to help the soldiers. A woman's loyalty to her husband could become an open political act, especially for women in America committed to men who remained loyal to the King. Legal divorce, usually rare, was granted to Patriot women whose husbands supported the King.
In early 1776, France set up a major program of aid to the Americans, and the Spanish secretly added funds. Each country spent one million "livres tournaises" to buy munitions. A dummy corporation run by Pierre Beaumarchais concealed their activities. American Patriots obtained some munitions through the Dutch Republic, as well as French and Spanish ports in the West Indies. Heavy expenditures and a weak taxation system pushed France toward bankruptcy.
Spain did not officially recognize the U.S. but it separately declared war on Britain on June 21, 1779. Bernardo de Gálvez y Madrid, general of the Spanish forces in New Spain, also served as governor of Louisiana. He led an expedition of colonial troops to capture Florida from the British and to keep open a vital conduit for supplies.
Most indigenous people rejected pleas that they remain neutral and instead supported the British Crown. The great majority of the 200,000 indigenous people east of the Mississippi distrusted the colonists and supported the British cause, hoping to forestall continued colonial expansion into their territories. Those tribes closely involved in trade tended to side with the Patriots, although political factors were important, as well.
Most indigenous people did not participate directly in the war, except for warriors and bands associated with four of the Iroquois tribes in New York and Pennsylvania which allied with the British. The British did have other allies, especially in the upper Midwest. They provided indigenous people with funding and weapons to attack Continental Army outposts. Some indigenous people tried to remain neutral, seeing little value in joining what they perceived to be a "white man's war", and fearing reprisals from whichever side they opposed. The Oneida and Tuscarora tribes among the Iroquois of central and western New York supported the American cause. The British provided arms to indigenous people who were led by Loyalists in war parties to raid frontier settlements from the Carolinas to New York. These war parties managed to kill many settlers on the frontier, especially in Pennsylvania and New York's Mohawk Valley.
In 1776, Cherokee war parties attacked American Colonists all along the southern frontier of the uplands throughout the Washington District, North Carolina (now Tennessee) and the Kentucky wilderness area. They would launch raids with roughly 200 warriors, as seen in the Cherokee–American wars; they could not mobilize enough forces to invade Colonial areas without the help of allies, most often the Creek. The Chickamauga Cherokee under Dragging Canoe allied themselves closely with the British, and fought on for an additional decade after the Treaty of Paris was signed. Joseph Brant of the powerful Mohawk tribe in New York was the most prominent indigenous leader against the Patriot forces. In 1778 and 1780, he led 300 Iroquois warriors and 100 white Loyalists in multiple attacks on small frontier settlements in New York and Pennsylvania, killing many settlers and destroying villages, crops, and stores. The Seneca, Onondaga, and Cayuga of the Iroquois Confederacy also allied with the British against the Americans.
In 1779, the Americans forced the hostile indigenous people out of upstate New York when Washington sent an army under John Sullivan which destroyed 40 empty Iroquois villages in central and western New York. The Battle of Newtown proved decisive, as the Patriots had an advantage of three-to-one, and it ended significant resistance; there was little combat otherwise. Sullivan systematically burned the empty villages and destroyed about 160,000 bushels of corn that composed the winter food supply. Facing starvation and homeless for the winter, the Iroquois fled to Canada. The British resettled them in Ontario, providing land grants as compensation for some of their losses.
At the peace conference following the war, the British ceded lands which they did not really control, and they did not consult their indigenous allies during the treaty. They transferred control to the United States of all the land east of the Mississippi and north of Florida. Calloway concludes:
The British did not give up their forts until 1796 in the eastern Midwest, stretching from Ohio to Wisconsin; they kept alive the dream of forming an allied indigenous nation there, which they referred to a "Indian barrier state". That goal was one of the causes of the War of 1812.
Free blacks in the North and South fought on both sides of the Revolution, but the majority fought for the Patriots. Gary Nash reports that there were about 9,000 black Patriots, counting the Continental Army and Navy, state militia units, privateers, wagoneers in the Army, servants to officers, and spies. Ray Raphael notes that thousands did join the Loyalist cause, but "a far larger number, free as well as slave, tried to further their interests by siding with the patriots." Crispus Attucks was one of the five people killed in the Boston Massacre in 1770 and is considered the first American casualty for the cause of independence.
Many black slaves sided with the Loyalists. Tens of thousands in the South used the turmoil of war to escape, and the southern plantation economies of South Carolina and Georgia were disrupted in particular. During the Revolution, the British commanders attempted to weaken the Patriots by issuing proclamations of freedom to their slaves. Historian David Brion Davis explains the difficulties with a policy of wholesale arming of the slaves:
Davis underscores the British dilemma: "Britain, when confronted by the rebellious American colonists, hoped to exploit their fear of slave revolts while also reassuring the large number of slave-holding Loyalists and wealthy Caribbean planters and merchants that their slave property would be secure". The Americans, however, accused the British of encouraging slave revolts, with the issue becoming one of the 27 colonial grievances.
American advocates of independence were commonly lampooned in Great Britain for what was termed their hypocritical calls for freedom, while many of their leaders were planters who held hundreds of slaves. Samuel Johnson snapped, "how is it we hear the loudest yelps for liberty among the drivers of the Negroes?" Benjamin Franklin countered by criticizing the British self-congratulation about "the freeing of one Negro" named Somersett while they allowed slave trade to continue unabated. Phyllis Wheatley was a black poet who popularized the image of Columbia to represent America. She came to public attention when her "Poems on Various Subjects, Religious and Moral" appeared in 1773.
The effects of the war were more dramatic in the South. In Virginia, royal governor Lord Dunmore recruited black men into the British forces with the promise of freedom, protection for their families, and land grants. Tens of thousands of slaves escaped to British lines throughout the South, causing dramatic losses to slaveholders and disrupting cultivation and harvesting of crops. For instance, South Carolina was estimated to have lost about 25,000 slaves to flight, migration, or death—amounting to a third of its slave population. From 1770 to 1790, the black proportion of the population (mostly slaves) in South Carolina dropped from 60.5 percent to 43.8 percent, and from 45.2 percent to 36.1 percent in Georgia.
British forces gave transportation to 10,000 slaves when they evacuated Savannah and Charleston, carrying through on their promise. They evacuated and resettled more than 3,000 Black Loyalists from New York to Nova Scotia, Upper Canada, and Lower Canada. Others sailed with the British to England or were resettled as freedmen in the West Indies of the Caribbean. But slaves carried to the Caribbean under control of Loyalist masters generally remained slaves until British abolition of slavery in its colonies in 1833-38. More than 1,200 of the Black Loyalists of Nova Scotia later resettled in the British colony of Sierra Leone, where they became leaders of the Krio ethnic group of Freetown and the later national government. Many of their descendants still live in Sierra Leone, as well as other African countries.
Tens of thousands of Loyalists left the United States following the war, and Maya Jasanoff estimates as many as 70,000. Some migrated to Britain, but the great majority received land and subsidies for resettlement in British colonies in North America, especially Quebec (concentrating in the Eastern Townships), Prince Edward Island, and Nova Scotia. Britain created the colonies of Upper Canada (Ontario) and New Brunswick expressly for their benefit, and the Crown awarded land to Loyalists as compensation for losses in the United States. Nevertheless, approximately eighty-five percent of the Loyalists stayed in the United States as American citizens, and some of the exiles later returned to the U.S. Patrick Henry spoke of the issue of allowing Loyalists to return as such: "Shall we, who have laid the proud British lion at our feet, be frightened of its whelps?" His actions helped secure return of the Loyalists to American soil.
Interpretations vary concerning the effect of the Revolution. Historians such as Bernard Bailyn, Gordon Wood, and Edmund Morgan view it as a unique and radical event which produced deep changes and had a profound effect on world affairs, such as an increasing belief in the principles of the Enlightenment. These were demonstrated by a leadership and government that espoused protection of natural rights, and a system of laws chosen by the people. John Murrin, by contrast, argues that the definition of "the people" at that time was mostly restricted to free men who passed a property qualification. This view argues that any significant gain of the revolution was irrelevant in the short term to women, black Americans and slaves, poor white men, youth, and native Americans.
Gordon Wood states: 
Edmund Morgan has argued that, in terms of long-term impact on American society and values:
After the Revolution, genuinely democratic politics became possible in the former colonies. The rights of the people were incorporated into state constitutions. Concepts of liberty, individual rights, equality among men and hostility toward corruption became incorporated as core values of liberal republicanism. The greatest challenge to the old order in Europe was the challenge to inherited political power and the democratic idea that government rests on the consent of the governed. The example of the first successful revolution against a European empire, and the first successful establishment of a republican form of democratically elected government, provided a model for many other colonial peoples who realized that they too could break away and become self-governing nations with directly elected representative government.
The Dutch Republic, also at war with Britain, was the next country to sign a treaty with the United States, on October 8, 1782. On April 3, 1783, Ambassador Extraordinary Gustaf Philip Creutz, representing King Gustav III of Sweden, and Benjamin Franklin, signed a Treaty of Amity and Commerce with the U.S.
The American Revolution was the first wave of the Atlantic Revolutions: the French Revolution, the Haitian Revolution, and the Latin American wars of independence. Aftershocks reached Ireland in the Irish Rebellion of 1798, in the Polish–Lithuanian Commonwealth, and in the Netherlands.
The Revolution had a strong, immediate influence in Great Britain, Ireland, the Netherlands, and France. Many British and Irish Whigs spoke glowingly in favor of the American cause. In Ireland, the Protestants who controlled Ireland demanded self-rule. Under the leadership of Henry Grattan, the so-called "Patriots" forced the reversal of mercantilist prohibitions against trade with other British colonies. The King and his cabinet in London could not risk another rebellion on the American model, and made a series of concessions to the Patriot faction in Dublin. Armed Protestant volunteer units were set up to protect against an invasion from France. As in America, so too in Ireland the King no longer had a monopoly of lethal force.
The Revolution, along with the Dutch Revolt (end of the 16th century) and the 17th century English Civil War, was among the examples of overthrowing an old regime for many Europeans who later were active during the era of the French Revolution, such as the Marquis de Lafayette. The American Declaration of Independence influenced the French Declaration of the Rights of Man and of the Citizen of 1789. The spirit of the Declaration of Independence led to laws ending slavery in all the Northern states and the Northwest Territory, with New Jersey the last in 1804. States such as New Jersey and New York adopted gradual emancipation, which kept some people as slaves for more than two decades longer.
The democratic ideals of the Revolution inspired changes in the roles of women.
The concept of republican motherhood was inspired by this period and reflects the importance of Republicanism as the dominant American ideology. It assumed that a successful republic rested upon the virtue of its citizens. Women were considered to have the essential role of instilling their children with values conducive to a healthy republic. During this period, the wife's relationship with her husband also became more liberal, as love and affection instead of obedience and subservience began to characterize the ideal marital relationship. In addition, many women contributed to the war effort through fundraising and running family businesses without their husbands.
The traditional constraints gave way to more liberal conditions for women. Patriarchy faded as an ideal; young people had more freedom to choose their spouses and more often used birth control to regulate the size of their families. Society emphasized the role of mothers in child rearing, especially the patriotic goal of raising republican children rather than those locked into aristocratic value systems. There was more permissiveness in child-rearing. Patriot women married to Loyalists who left the state could get a divorce and obtain control of the ex-husband's property.
Whatever gains they had made, however, women still found themselves subordinated, legally and socially, to their husbands, disfranchised and usually with only the role of mother open to them. But, some women earned livelihoods as midwives and in other roles in the community not originally recognized as significant by men.
Abigail Adams expressed to her husband, the president, the desire of women to have a place in the new republic: "I desire you would remember the Ladies, and be more generous and favourable to them than your ancestors. Do not put such unlimited power into the hands of the Husbands."
The Revolution sparked a discussion on the rights of woman and an environment favorable to women's participation in politics. Briefly the possibilities for women's rights were highly favorable, but a backlash led to a greater rigidity that excluded women from politics.
For more than thirty years, however, the 1776 New Jersey State Constitution gave the vote to "all inhabitants" who had a certain level of wealth, including unmarried women and blacks (not married women because they could not own property separately from their husbands), until in 1807, when that state legislature passed a bill interpreting the constitution to mean universal "white male" suffrage, excluding paupers.
In the first two decades after the American Revolution, state legislatures and individuals took actions to free numerous slaves, in part based on revolutionary ideals. Northern states passed new constitutions that contained language about equal rights or specifically abolished slavery; some states, such as New York and New Jersey, where slavery was more widespread, passed laws by the end of the 18th century to abolish slavery by a gradual method; in New York, the last slaves were freed in 1827.
While no southern state abolished slavery, for a period individual owners could free their slaves by personal decision, often providing for manumission in wills but sometimes filing deeds or court papers to free individuals. Numerous slaveholders who freed their slaves cited revolutionary ideals in their documents; others freed slaves as a reward for service. Records also suggest that some slaveholders were freeing their own mixed-race children, born into slavery to slave mothers.
The American Revolution has a central place in the American memory as the story of the nation's founding. It is covered in the schools, memorialized by a national holiday, and commemorated in innumerable monuments. George Washington's estate at Mount Vernon was one of the first national pilgrimages for tourists and attracted 10,000 visitors a year by the 1850s.
The Revolution became a matter of contention in the 1850s in the debates leading to the American Civil War (1861–1865), as spokesmen of both the Northern United States and the Southern United States claimed that their region was the true custodian of the legacy of 1776. The United States Bicentennial in 1976 came a year after the American withdrawal from the Vietnam War, and speakers stressed the themes of renewal and rebirth based on a restoration of traditional values.
Today, more than 100 are protected and maintained by the government. The National Park Service alone owns and maintains more than 50 battlefield parks and sites related to the Revolution. The American Battlefield Trust preserves almost 700 acres of battlefield land in six states.
online

</doc>
<doc id="1974" url="https://en.wikipedia.org/wiki?curid=1974" title="April 17">
April 17


</doc>
<doc id="1975" url="https://en.wikipedia.org/wiki?curid=1975" title="Alan Ayckbourn">
Alan Ayckbourn

Sir Alan Ayckbourn (born 12 April 1939) is a prolific British playwright and director. He has written and produced more than seventy full-length plays in Scarborough and London and was, between 1972 and 2009, the artistic director of the Stephen Joseph Theatre in Scarborough, where all but four of his plays have received their first performance. More than 40 have subsequently been produced in the West End, at the Royal National Theatre or by the Royal Shakespeare Company since his first hit "Relatively Speaking" opened at the Duke of York's Theatre in 1969.
Major successes include "Absurd Person Singular" (1975), "The Norman Conquests" trilogy (1973), "Bedroom Farce" (1975), "Just Between Ourselves" (1976), "A Chorus of Disapproval" (1984), "Woman in Mind" (1985), "A Small Family Business" (1987), "Man of the Moment" (1988), "House" & "Garden" (1999) and "Private Fears in Public Places" (2004). His plays have won numerous awards, including seven London "Evening Standard" Awards. They have been translated into over 35 languages and are performed on stage and television throughout the world. Ten of his plays have been staged on Broadway, attracting two Tony nominations, and one Tony award.
Ayckbourn was born in Hampstead, London. His mother Irene Worley ("Lolly") (1906–1998) was a writer of short stories who published under the name "Mary James". His father, Horace Ayckbourn (1904–1965), was an orchestral violinist, at one time deputy leader of the London Symphony Orchestra. His parents, who separated shortly after World War II, never married, and Ayckbourn's mother divorced her first husband to marry again in 1948.
Ayckbourn wrote his first play at Wisborough Lodge (a preparatory school in the village of Wisborough Green) when he was about 10. Whilst at prep school as a boarder, his mother wrote to tell him she was marrying Cecil Pye, a bank manager. When he went home for the holidays, his new family consisted of his mother, his stepfather and Christopher, his stepfather's son by an earlier marriage. This relationship too, reportedly ran into difficulties early on.
Ayckbourn attended Haileybury and Imperial Service College, in the village of Hertford Heath, and whilst there toured Europe and America with the school's Shakespeare company.
After leaving school at 17, Ayckbourn's career took several temporary jobs in various places before starting a temporary job at the Scarborough Library Theatre, where he was introduced to the artistic director, Stephen Joseph. It is said that Joseph became both a mentor and father figure for Ayckbourn until his untimely death in 1967, and Ayckbourn has consistently spoken highly of him.
Ayckbourn's career was briefly interrupted when he was called for National Service. He was swiftly discharged, officially on medical grounds, but it is suggested that a doctor who noticed his reluctance to join the Armed Forces deliberately failed the medical as a favour. Although Ayckbourn continued to move where his career took him, he settled in Scarborough, eventually buying Longwestgate House, the house formerly owned by Stephen Joseph.
In 1957, Ayckbourn married Christine Roland, another member of the Library Theatre company, and indeed Ayckbourn's first two plays were written jointly with her under the pseudonym of "Roland Allen". They had two sons, Steven and Philip. However, the marriage had difficulties which eventually led to their separation in 1971. Ayckbourn said that his relationship with Roland became easy once they agreed their marriage was over. Around this time, he started to share a home with Heather Stoney, an actress he had first met ten years earlier. Like his mother, neither he nor Roland sought a divorce for the next thirty years and it was only in 1997 that they formally divorced; Ayckbourn married Stoney. One side-effect of the timing is that, as Ayckbourn was awarded a knighthood a few months before the divorce, both his first and second wife were entitled to take the title of Lady Ayckbourn.
In February 2006, he suffered a stroke in Scarborough, and stated: "I hope to be back on my feet, or should I say my left leg, as soon as possible, but I know it is going to take some time. In the meantime I am in excellent hands and so is the Stephen Joseph Theatre." He left hospital after eight weeks and returned to directing after six months, but the following year he announced he would step down as artistic director of the Stephen Joseph Theatre. Ayckbourn, however, continues to write and direct his own work at the theatre.
Since Ayckbourn's plays started becoming established in the West End, interviewers have raised the question of whether his work is autobiographical. There is no clear answer to this question. There has only been one biography, written by Paul Allen, and this primarily covers his career in the theatre. Ayckbourn has frequently said he sees aspects of himself in all his characters. For example, in "Bedroom Farce" (1975), he admitted to being, in some respects, all four of the men in the play. It has been suggested that, after Ayckbourn himself, the person who is used the most in his plays is his mother, particularly as Susan in "Woman in Mind" (1985).
What is less clear is how much influence events in Ayckbourn's life have had on his writing. It is true that the theme of marriages in various difficulties was heavily present throughout his plays in the early seventies, around the time his own marriage was coming to an end. However, by this time, he had also witnessed the failures of his parents' relationships as well as those of some of his friends. Which relationships, if any, he drew on for his plays, is unclear. In Paul Allen's biography, Ayckbourn is briefly compared to Dafydd and Guy in "A Chorus of Disapproval" (1984). Both characters feel themselves in trouble, and there was speculation that Ayckbourn himself may have felt himself to be in trouble. At the time, he had reportedly become seriously involved with another actress, which threatened his relationship with Stoney. But again, it is unclear whether this had any effect on the writing, and Paul Allen's view is that it is not current experience that Ayckbourn uses for his plays.
It could be that Ayckbourn had written plays with himself and his own issues in mind, but as Ayckbourn is portrayed as a guarded and private man, it is hard to imagine him exposing his own life in his plays to any great degree. In the biography, Paul Allen wrote, regarding a suggestion in "Cosmopolitan" that his plays were becoming autobiographical: "If we take that to mean that his plays tell his own life story, he still hasn't started."
On leaving school his theatrical career started immediately, with an introduction to Sir Donald Wolfit by his French master. Ayckbourn joined Wolfit on tour to the Edinburgh Festival Fringe as an acting assistant stage manager (meaning a role that involved both acting and stage management) for three weeks, with his first role on the professional stage being various parts in "The Strong are Lonely" by Fritz Hochwälder. In the following year, Ayckbourn appeared in six other plays at the Connaught Theatre, Worthing, and the Thorndyke theatre, Leatherhead.
In 1957, Ayckbourn was employed by the director Stephen Joseph at the Library Theatre, Scarborough, the predecessor to the modern Stephen Joseph Theatre. His role, again, was initially an acting stage manager. This employment led to Ayckbourn's first professional script commission, in 1958. When he complained about the quality of a script he was performing, Joseph challenged him to write a better one. The result was "The Square Cat", written under the pseudonym Roland Allen and first performed in 1959. In this play, Ayckbourn himself played the character Jerry Watiss.
After thirty-four appearances in plays at the Library Theatre, including four of his own, in 1962 Ayckbourn moved to Stoke-on-Trent to help set up the Victoria Theatre, (now the New Vic), where he appeared in a further eighteen plays. His final appearance in one of his own plays was as the Crimson Gollywog in the disastrous children's play "Christmas v Mastermind". He left the Stoke company in 1964, officially to commit his time to the London production of "Mr. Whatnot", but reportedly because was having trouble working with the artistic director, Peter Cheeseman. By now, his career as a writer was coming to fruition, and his acting career was sidelined.
His final role on stage was as Jerry in "Two for the Seesaw" by William Gibson, at the Civic Theatre in Rotherham. He was left stranded on stage because Heather Stoney was unable to re-appear because the props had been left unpacked, and this led him to decide acting was more trouble than it was worth. The assistant stage manager on the production, Bill Kenwright, would become one of the UK's most successful producers.
Ayckbourn's earliest plays were written and produced at a time when the Scarborough Library theatre, like most regional theatres, regularly commissioned work from their own actors to keep costs down (the other notable actor whose work was being commissioned being David Campton). His first play, "The Square Cat", was sufficiently popular locally to secure further commissions although not this or the following three plays had much impact beyond Scarborough. But, after his transfer to Victoria Theatre in Stoke-on-Trent, there came "Christmas v Mastermind", which flopped and is now universally regarded as Ayckbourn's greatest disaster.
His fortunes began to revive in 1963 with "Mr. Whatnot", again premiering at the Victoria Theatre. This was the first play that Ayckbourn was sufficiently happy with to allow performances today, and the first play to receive a West End performance. However, the West End production flopped, in part down to misguided casting. After this, Ayckbourn experimented by collaborating with comedians, first writing a monologue for Tommy Cooper, and later with Ronnie Barker, who played Lord Slingsby-Craddock in the London production of "Mr Whatnot" in 1964, for the scripts of for LWT's "Hark at Barker". Ayckbourn used the pseudonym 'Peter Caulfield' because he was under exclusive contract to the BBC at the time.
Then, in 1965, back at the Scarborough Library Theatre, "Meet my Father" was produced, later retitled "Relatively Speaking". This time, the play was a massive success, both in Scarborough and the West End, making Alan Ayckbourn rich and earning him a congratulatory telegram from Noël Coward. This was not quite the end of Ayckbourn's hit-and-miss record, because his following play, "The Sparrow" only ran for three weeks at Scarborough. However, the following play, "How the Other Half Loves", secured his runaway success as a playwright.
The height of Ayckbourn's commercial success included "Absurd Person Singular" (1975), "The Norman Conquests" trilogy (1973), "Bedroom Farce" (1975) and "Just Between Ourselves" (1976), all plays that focused heavily on marriage in the British middle classes. The only failure during this period was a 1975 musical with Andrew Lloyd Webber, "Jeeves", and even this did little to dent Ayckbourn's popularity. Although his plays have received major West End productions almost from the beginning of his writing career, and hence have been reviewed in British newspapers, Ayckbourn's work was for years routinely dismissed as being too slight for serious study. Recently, scholars have begun to view Ayckbourn as an important commentator on the lifestyles of the British suburban middle class, and as a stylistic innovator who experiments with theatrical styles within the boundaries set by popular tastes.
From the 1980s, Ayckbourn began to move away from the recurring themes of marriage and explore other contemporary themes, one example being "Woman in Mind", a play performed entirely from the perspective of a Woman going through a nervous breakdown. He also experimented with several more unconventional ways of writing plays, such as "Intimate Exchanges", which has one beginning and sixteen possible endings, and "House & Garden", where two plays take place simultaneously of two different stages, as well as diversifying into children's theatre (such as "Mr A's Amazing Maze Plays" and musical plays, such as "By Jeeves" (a more successful rewrite of the original "Jeeves").
With a résumé of over seventy plays, of which more than forty have played at the National Theatre or in the West End, Alan Ayckbourn is one of England's most successful living playwrights. Despite his success, honours and awards (which include a prestigious Laurence Olivier Award), Alan Ayckbourn remains a relatively anonymous figure dedicated to regional theatre. Throughout his writing career, all but four of his plays were premiered at the Stephen Joseph Theatre in Scarborough in its three different locations.
Ayckbourn received the CBE in 1987 and was knighted in the 1997 New Year Honours. It is frequently claimed (but not proven) that Alan Ayckbourn is the most performed living English playwright, and the second most performed of all time after Shakespeare.
Although Ayckbourn's plays no longer dominate the theatrical scene on the scale of his earlier works, he continues to write, his most recent major success being "Private Fears in Public Places" that had a hugely successful Off-Broadway run at 59E59 Theaters, and in 2006 was made into a film "Cœurs", directed by Alain Resnais. After suffering a stroke, there was uncertainty as to whether he could continue to write (the Ayckbourn play premiered immediately after the stroke, "If I Were You", was written before his illness), but his first play written afterwards, "Life and Beth", was premiered in the summer of 2008. Ayckbourn continues to write for the Stephen Joseph Theatre on invitation of his successor as artistic director, Chris Monks, with the first new play under this arrangement, "My Wonderful Day", performed in October 2009. His play "Roundelay" opened in September 2014; the order in which each of the five acts is played in each performance is to be left to chance (allowing 120 possible permutations), with members of the audience being invited to extract five coloured ping pong balls from a bag beforehand.
Many of Ayckbourn's plays have had their New York premiere at 59E59 Theaters as part of their annual Brits Off Broadway Festitval including "Private Fears in Public Places", "Intimate Exchanges", "My Wonderful Day" and "Neighbourhood Watch" among others.
Although Ayckbourn is best known as a writer, it is said that he only spends 10% of his time writing plays. Most of the rest of his time is spent directing.
Ayckbourn began directing at the Scarborough Library Theatre in 1961, with a production of "Gaslight" by Patrick Hamilton. He directed five other plays that year and the following year in Scarborough, and after transferring to the Victoria Theatre, directed a further six plays in 1963. Between 1964 and 1967 (when much of his time was taken up by various productions of his early successes "Mr. Whatnot" and "Relatively Speaking") he only directed one play ("The Sparrow", written by himself, later withdrawn), but in 1968 he resumed regularly directing plays, mostly at Scarborough. At this time he also worked as a radio drama producer for the BBC, based in Leeds.
At first, his directing career was separate from his writing career. It was not until 1963 that Ayckbourn directed a play of his own (a revival of "Standing Room Only"), 1967 that Ayckbourn directed a premiere of his own ("The Sparrow"). The London premieres remained in the hands of other directors for longer, with the first play of his both written and directed by him in London ("Bedroom Farce") waiting until 1977.
After the death of Stephen Joseph in 1967, the position of Director of Productions was appointed on an annual basis. Ayckbourn was offered this position in 1969 and 1970, succeeding Rodney Wood, but he handed the position over to Caroline Smith in 1971 (having spent most of his time that year in the US with "How the Other Half Loves"). He became Director of Productions again in 1972, and this time, on 12 November that same year, he was made the permanent artistic director of the theatre.
In mid-1986, Ayckbourn accepted an invitation to work as a visiting director for two years at the Royal National Theatre in London, form his own company, and perform a play in each of the three auditoria provided at least one was a new play of his own. Using a stock company that included established performers like Michael Gambon, Polly Adams and Simon Cadell. The three plays became four, and were: "Tons of Money" by Will Evans and Valentine, with adaptations by Ayckbourn (Lyttelton), Arthur Miller's "A View From the Bridge" (Cottesloe), his own "A Small Family Business" (Olivier) and John Ford's "'Tis Pity She's a Whore" (Olivier again). During this time, Ayckbourn shared his role of artistic director of the Stephen Joseph Theatre with Robin Herford and returned in 1987 to direct the premiere of "Henceforward...".
He announced in 1999 that he would step back from directing the work of other playwrights, to concentrate on his own plays, the last one being Rob Shearman's "Knights in Plastic Armour" in 1999; the exception being in 2002 when he directed the world premiere of Tim Firth's "The Safari Party".
In 2002, following a dispute over the Duchess Theatre's handling of "Damsels in Distress", Ayckbourn sharply criticised both this and the West End's treatment of theatre in general, in particular their casting of celebrities. Although he did not explicitly say he would boycott the West End, he did not return to direct in the West End again until 2009 with a revival of "Woman in Mind" (although he did allow other West End producers to revive "Absurd Person Singular" in 2007 and "The Norman Conquests" in 2008).
After Ayckbourn suffered a stroke in February 2006, he returned to work in September and premiered his 70th play "If I Were You" at the Stephen Joseph Theatre the following month.
He announced in June 2007 that he would retire as artistic director of the Stephen Joseph Theatre after the 2008 season. His successor, Chris Monks, took over at the start of the 2009–2010 season, but Ayckbourn remained to direct premieres and revivals of his work at the theatre, beginning with "How the Other Half Loves" in June 2009.
In March 2010 he directed an in-the-round revival of his play "Taking Steps" at the Orange Tree Theatre, winning universal press acclaim.
In July 2014, Ayckbourn directed a musical adaptation of "The Boy Who Fell into A Book", with musical adaptation and lyrics by Paul James and music by Eric Angus and Cathy Shostak. The show ran in The Stephen Joseph Theatre and received critical acclaim.
Ayckbourn also sits on the Council of the Society of Authors.
There are eight one-act plays written by Alan Ayckbourn. Five of them ("Mother Figure", "Drinking Companion", "Between Mouthfuls", "Gosforth's Fete" and "Widows Might") were written for "Confusions", first performed in 1974.
The other three one-act plays were:
Plays adapted as films include:

</doc>
<doc id="1979" url="https://en.wikipedia.org/wiki?curid=1979" title="Alpha Centauri">
Alpha Centauri

Alpha Centauri (Latinized from α Centauri, abbreviated Alpha Cen or α Cen) is the closest star system and closest planetary system to Earth's Solar System at 4.37 light-years (1.34 parsecs) from the Sun. It is a triple star system, consisting of three stars: α Centauri A (officially Rigil Kentaurus), α Centauri B (officially Toliman), and α Centauri C (officially Proxima Centauri).
Alpha Centauri A and B are Sun-like stars (Class G and K), and together they form the binary star Alpha Centauri AB. To the naked eye, the two main components appear to be a single star with an apparent magnitude of −0.27, the brightest star in the southern constellation of Centaurus and the third-brightest in the night sky, outshone only by Sirius and Canopus.
Alpha Centauri A has 1.1 times the mass and 1.519 times the luminosity of the Sun, while Alpha Centauri B is smaller and cooler, at 0.907 times the Sun's mass and 0.445 times its luminosity. The pair orbit around a common centre with an orbital period of 79.91 years. Their elliptical orbit is eccentric, so that the distance between A and B varies from 35.6 AU (astronomical units), or about the distance between Pluto and the Sun, to 11.2 AU, or about the distance between Saturn and the Sun.
Alpha Centauri C, or Proxima Centauri, is a small and faint red dwarf (Class M). Though not visible to the naked eye, Proxima Centauri is the closest star to the Sun at a distance of , slightly closer than Alpha Centauri AB. Currently, the distance between Proxima Centauri and Alpha Centauri AB is about , equivalent to about 430 times the radius of Neptune's orbit.
Proxima Centauri has two planets: Proxima b, an Earth-sized exoplanet in the habitable zone discovered in 2016; and Proxima c, a ringed super-Earth 1.5 AU away, discovered in 2019. Alpha Centauri B has one retracted planet (Alpha Centauri Bb), and one potential candidate (Bc).
"α Centauri" (Latinised to "Alpha Centauri") is the system's designation given by Johann Bayer in 1603. It bears the traditional name "Rigil Kentaurus", which is a Latinisation of the Arabic name "Rijl al-Qinṭūrus," meaning 'the Foot of the Centaur'.
The name is frequently abbreviated to "Rigil Kent" or even "Rigil", though the latter name is better known for Beta Orionis (Rigel).
An alternative name found in European sources, "Toliman", is an approximation of the Arabic "aẓ-Ẓalīmān" (in older transcription, "aṭ-Ṭhalīmān"), meaning 'the (two male) Ostriches', an appellation Zakariya al-Qazwini had applied to Lambda and Mu Sagittarii, also in the southern hemisphere.
A third name that has been applied is "Bungula" (), of obscure origin. Allen can only surmise it may have been coined from the Greek letter beta (β) and Latin 'hoof'.
Alpha Centauri C was discovered in 1915 by Robert T. A. Innes, who suggested that it be named "Proxima Centaurus", . The name "Proxima Centauri" later became more widely used and is now listed by the IAU as the approved proper name.
In 2016, the Working Group on Star Names of the International Astronomical Union (IAU), having decided to attribute proper names to individual component stars rather than to multiple systems, approved the name "Rigil Kentaurus" () as being restricted to "Alpha Centauri A" and the name "Proxima Centauri" () for "Alpha Centauri C". 
On 10 August 2018, the IAU approved the name "Toliman" () for "Alpha Centauri B".
Alpha Centauri is a triple star system, with its two main stars, Alpha Centauri A and Alpha Centauri B, being a binary component. The "AB" designation, or older "A×B", denotes the mass centre of a main binary system relative to companion star(s) in a multiple star system. "AB-C" refers to the component of Proxima Centauri in relation to the central binary, being the distance between the centre of mass and the outlying companion. Because the distance between Proxima (C) and either of Alpha Centauri A or B is similar, the AB binary system is sometimes treated as a single gravitational object.
The A and B components of Alpha Centauri have an orbital period of 79.91 years. Their orbit is moderately eccentric, "e" = 0.5179; their closest approach or periastron is , or about the distance between the Sun and Saturn; and their furthest separation or apastron is , about the distance between the Sun and Pluto. The most recent periastron was in August 1955 and the next will occur in May 2035; the most recent apastron was in May 1995 and will next occur in 2075.
Viewed from Earth, the "apparent orbit" of A and B means that their separation and position angle (PA) are in continuous change throughout their projected orbit. Observed stellar positions in 2019 are separated by 4.92 arcsec through the PA of 337.1°, increasing to 5.49 arcsec through 345.3° in 2020. The closest recent approach was in February 2016, at 4.0 arcsec through the PA of 300°. The observed maximum separation of these stars is about 22 arcsec, while the minimum distance is 1.7 arcsec. The widest separation occurred during February 1976, and the next will be in January 2056.
Alpha Centauri C is about 13,000 AU away from Alpha Centauri AB. This is equivalent to —about 5% the distance between Alpha Centauri AB and the Sun. Until 2017, measurements of its small speed and its trajectory were of too little accuracy and duration in years to determine whether it is bound to Alpha Centauri AB or unrelated.
Radial velocity measurements made in 2017 were precise enough to show that Proxima Centauri and Alpha Centauri AB are gravitationally bound. The orbital period of Proxima Centauri is approximately years, with an eccentricity of 0.50 ± 0.08, much more eccentric than Mercury's. Proxima Centauri comes within of AB at periastron, and its apastron occurs at .
Asteroseismic studies, chromospheric activity, and stellar rotation (gyrochronology) are all consistent with the Alpha Centauri system being similar in age to, or slightly older than, the Sun. Asteroseismic analyses that incorporate tight observational constraints on the stellar parameters for the Alpha Centauri stars have yielded age estimates of  Gyr,  Gyr, 5.2 ± 1.9 Gyr, 6.4 Gyr, and  Gyr. Age estimates for the stars based on chromospheric activity (Calcium H & K emission) yield 4.4 ± 2.1 Gyr, whereas gyrochronology yields  Gyr. Stellar evolution theory implies both stars are slightly older than the Sun at 5 to 6 billion years, as derived by their mass and spectral characteristics.
From the orbital elements, the total mass of Alpha Centauri AB is about —or twice that of the Sun. The average individual stellar masses are and , respectively, though slightly higher masses have been quoted in recent years, such as and , or totalling . Alpha Centauri A and B have absolute magnitudes of +4.38 and +5.71, respectively.
Alpha Centauri A, also known as Rigil Kentaurus, is the principal member, or primary, of the binary system. It is a solar-like main-sequence star with a similar yellowish colour, whose stellar classification is spectral type G2 V; it is slightly larger and more luminous than the Sun. Alpha Centauri A is about 10 percent more massive than the Sun, with a radius about 22 percent larger. When considered among the individual brightest stars in the sky (excluding the Sun), it is the fourth brightest at an apparent magnitude of −0.01, being slightly fainter than Arcturus at an apparent magnitude of −0.05.
The type of magnetic activity on Alpha Centauri A is comparable to that of the Sun, showing coronal variability due to star spots, as modulated by the rotation of the star. However, since 2005 the activity level has fallen into a deep minimum that might be similar to the Sun's historical Maunder Minimum. Alternatively, it may have a very long stellar activity cycle and is slowly recovering from a minimum phase.
Alpha Centauri B, also known as Toliman, is the secondary star of the binary system. It is a main-sequence star of spectral type K1 V, making it more an orange colour than Alpha Centauri A; it has around 90 percent the mass of the Sun and a 14 percent smaller diameter. Although it has a lower luminosity than A, Alpha Centauri B emits more energy in the X-ray band. Its light curve varies on a short time scale, and there has been at least one observed flare. It is more magnetically active than Alpha Centauri A, showing a cycle of compared to 11 years for the Sun, and about half the minimum-to-peak variation in coronal luminosity of the Sun. Alpha Centauri B has an apparent magnitude of +1.35, slightly dimmer than Mimosa.
Alpha Centauri C, better known as Proxima Centauri, is a small main-sequence red dwarf of spectral class M6 Ve. It has an absolute magnitude of +15.60, over 20,000 times fainter than the Sun. Its mass is calculated to be . It is the closest star to the Sun but is too faint to be visible to the naked eye.
To the naked eye, Alpha Centauri AB appears to be a single star, the brightest in the southern constellation of Centaurus. Their apparent angular separation varies over about 80 years between 2 and 22 arcsec (the naked eye has a resolution of 60 arcsec), but through much of the orbit, both are easily resolved in binoculars or small telescopes. At −0.27 apparent magnitude (combined for A and B magnitudes), Alpha Centauri is fainter only than Sirius and Canopus. It is the outer star of "The Pointers" or "The Southern Pointers", so called because the line through Beta Centauri (Hadar/Agena),
some 4.5° west, points to the constellation Crux—the Southern Cross. The Pointers easily distinguish the true Southern Cross from the fainter asterism known as the False Cross.
South of about 29° S latitude, Alpha Centauri is circumpolar and never sets below the horizon. North of about 29° N latitude, Alpha Centauri never rises. Alpha Centauri lies close to the southern horizon when viewed from the 29° N latitude to the equator (close to Hermosillo, Chihuahua City in Mexico, Galveston, Texas, Ocala, Florida and Lanzarote, the Canary Islands of Spain), but only for a short time around its culmination. The star culminates each year at local midnight on 24 April and at local 9 p.m. on 8 June.
As seen from Earth, Proxima Centauri is 2.2° southwest from Alpha Centauri AB, about four times the angular diameter of the Moon. Proxima Centauri appears as a deep-red star of a typical apparent magnitude of 11.1 in a sparsely populated star field, requiring moderately sized telescopes to be seen. Listed as V645 Cen in the "General Catalogue of Variable Stars Version 4.2", this UV Ceti-type flare star can unexpectedly brighten rapidly by as much as 0.6 magnitudes at visual wavelengths, then fade after only a few minutes. Some amateur and professional astronomers regularly monitor for outbursts using either optical or radio telescopes. In August 2015, the largest recorded flares of the star occurred, with the star becoming 8.3 times brighter than normal on 13 August, in the B band (blue light region).
Alpha Centauri is inside the G-cloud, and its nearest known system is the binary brown dwarf system Luhman 16 at .
Alpha Centauri is listed in the 2nd-century star catalog of Ptolemy. He gave its ecliptic coordinates, but texts differ as to whether the ecliptic latitude reads or . (Presently the ecliptic latitude is , but it has decreased by a fraction of a degree since Ptolemy's time due to proper motion.) In Ptolemy's time, Alpha Centauri was visible from Alexandria, Egypt, at but, due to precession, its declination is now , and it can no longer be seen at that latitude. English explorer Robert Hues brought Alpha Centauri to the attention of European observers in his 1592 work "Tractatus de Globis", along with Canopus and Achernar, noting:
The binary nature of Alpha Centauri AB was recognised in December 1689 by Jean Richaud, while observing a passing comet from his station in Puducherry. Alpha Centauri was only the second binary star to be discovered, preceded by Acrux.
The large proper motion of Alpha Centauri AB was discovered by Manuel John Johnson, observing from Saint Helena, who informed Thomas Henderson at the Royal Observatory, Cape of Good Hope of it. The parallax of Alpha Centauri was subsequently determined by Henderson from many exacting positional observations of the AB system between April 1832 and May 1833. He withheld his results, however, because he suspected they were too large to be true, but eventually published them in 1839 after Friedrich Wilhelm Bessel released his own accurately determined parallax for 61 Cygni in 1838. For this reason, Alpha Centauri is sometimes considered as the second star to have its distance measured because Henderson's work was not fully acknowledged at first. (The distance of Alpha Centauri from the Earth is now reckoned at .)
Later, John Herschel made the first micrometrical observations in 1834. Since the early 20th century, measures have been made with photographic plates.
By 1926, William Stephen Finsen calculated the approximate orbit elements close to those now accepted for this system. All future positions are now sufficiently accurate for visual observers to determine the relative places of the stars from a binary star ephemeris. Others, like D. Pourbaix (2002), have regularly refined the precision of new published orbital elements.
Robert T. A. Innes discovered Proxima Centauri in 1915 by blinking photographic plates taken at different times during a proper motion survey. These showed large proper motion and parallax similar in both size and direction to those of Alpha Centauri AB, suggesting that Proxima Centauri is part of the Alpha Centauri system and slightly closer to Earth than Alpha Centauri AB. Lying away, Proxima Centauri is the nearest star to the Sun.
All components of Alpha Centauri display significant proper motion against the background sky. Over centuries, this causes their apparent positions to slowly change. Proper motion was unknown to ancient astronomers. Most assumed that the stars are permanently fixed on the celestial sphere, as stated in the works of the philosopher Aristotle. In 1718, Edmond Halley found that some stars had significantly moved from their ancient astrometric positions.
In the 1830s, Thomas Henderson discovered the true distance to Alpha Centauri by analysing his many astrometric mural circle observations. He then realised this system also likely had a high proper motion. In this case, the apparent stellar motion was found using Nicolas Louis de Lacaille's astrometric observations of 1751–1752, by the observed differences between the two measured positions in different epochs.
Calculated proper motion of the centre of mass for Alpha Centauri AB is about 3620 mas (milli-arcseconds) per year toward the west and 694 mas/y toward the north, giving an overall motion of 3686 mas/y in a direction 11° north of west. The motion of the centre of mass is about 6.1 arcmin each century, or 1.02° each millennium. The velocity in the western direction is 23.0 km/s and in the northerly direction 4.4 km/s. Using spectroscopy the mean radial velocity has been determined to be around 22.4 km/s towards the Solar System.
Since Alpha Centauri AB is almost exactly in the plane of the Milky Way as viewed from Earth, there are many stars behind them. In early May 2028, Alpha Centauri A will pass between the Earth and a distant red star, when there will be a 45% probability that an Einstein ring will be observed. Other conjunctions will also occur in the coming decades, allowing accurate measurement of proper motions and possibly giving information on planets.
Based on the system's common known proper motion and radial velocities, Alpha Centauri will continue to change its position in the sky significantly and will gradually brighten. For example, in about 6,200 AD, α Centauri's true motion will cause an extremely rare first-magnitude stellar conjunction with Beta Centauri, forming a brilliant optical double star in the southern sky. It will then pass just north of the Southern Cross or Crux, before moving northwest and up towards the present celestial equator and away from the galactic plane. By about 26,700 AD, in the present-day constellation of Hydra, Alpha Centauri will reach perihelion at away, though later calculations suggest that this will occur in 27,000 AD. At nearest approach, Alpha Centauri will attain a maximum apparent magnitude of −0.86, comparable to present-day magnitude of Canopus, but it will still not surpass that of Sirius, which will brighten incrementally over the next 60,000 years, and will continue to be the brightest star as seen from Earth (other than the Sun) for the next 210,000 years.
Proxima Centauri b is a terrestrial planet discovered in 2016 by astronomers at the European Southern Observatory. It has a minimum mass of 1.17 (Earth masses) and orbits approximately 0.049 AU from Proxima Centauri, placing it in the star's habitable zone.
Proxima Centauri c was formally discovered and confirmed in 2020 and is a likely super-Earth or mini-Neptune. It has a mass of roughly 7 and orbits about 1.49 AU from Proxima Centauri with a period of . In June 2020, a large ring system encircling the planet was possibly detected.
In 2012, a planet around Alpha Centauri B was announced, Alpha Centauri Bb, but in 2015 a new analysis concluded that it almost certainly does not exist and was just a spurious artefact of the data analysis.
Whilst ruling out the existence of Alpha Centauri Bb, a possible transit of a separate exoplanet in 2013 was observed. The transit event could correspond to a planetary body with a radius around . This planet would most likely orbit Alpha Centauri B with an orbital period of 20.4 days or less, with only a 5 percent chance of it having a longer orbit. The median of the likely orbits is 12.4 days with an impact parameter of around 0–0.3. Its orbit would likely have an eccentricity of 0.24 or less. Like the probably spurious Alpha Centauri Bb, it likely has lakes of molten lava and would be far too close to Alpha Centauri B to harbour life.
Around Proxima Centauri, a small spike with a periodicity of 5.15 days was found while revisiting Proxima Centauri b. Were it a planetary companion, it would have to be at most 0.29 Earth masses, but it could also be random noise found in the data.
Additional planets may exist in the Alpha Centauri system, either orbiting Alpha Centauri A or Alpha Centauri B individually, or in large orbits around Alpha Centauri AB. Because both stars are fairly similar to the Sun (for example, in age and metallicity), astronomers have been especially interested in making detailed searches for planets in the Alpha Centauri system. Several established planet-hunting teams have used various radial velocity or star transit methods in their searches around these two bright stars. All the observational studies have so far failed to find evidence for brown dwarfs or gas giants.
In 2009, computer simulations showed that a planet might have been able to form near the inner edge of Alpha Centauri B's habitable zone, which extends from 0.5 to 0.9 AU from the star. Certain special assumptions, such as considering that the Alpha Centauri pair may have initially formed with a wider separation and later moved closer to each other (as might be possible if they formed in a dense star cluster), would permit an accretion-friendly environment farther from the star. Bodies around Alpha Centauri A would be able to orbit at slightly farther distances due to its stronger gravity. In addition, the lack of any brown dwarfs or gas giants in close orbits around Alpha Centauri make the likelihood of terrestrial planets greater than otherwise. A theoretical study indicates that a radial velocity analysis might detect a hypothetical planet of in Alpha Centauri B's habitable zone.
Radial velocity measurements of Alpha Centauri B made with the High Accuracy Radial Velocity Planet Searcher spectrograph were sufficiently sensitive to detect a planet within the habitable zone of the star (i.e. with an orbital period P = 200 days), but no planets were detected.
Current estimates place the probability of finding an Earth-like planet around Alpha Centauri at roughly 75%. The observational thresholds for planet detection in the habitable zones by the radial velocity method are currently (2017) estimated to be about for Alpha Centauri A, for Alpha Centauri B, and for Proxima Centauri.
Early computer-generated models of planetary formation predicted the existence of terrestrial planets around both Alpha Centauri A and B, but most recent numerical investigations have shown that the gravitational pull of the companion star renders the accretion of planets difficult. Despite these difficulties, given the similarities to the Sun in spectral types, star type, age and probable stability of the orbits, it has been suggested that this stellar system could hold one of the best possibilities for harbouring extraterrestrial life on a potential planet.
In the Solar System, Jupiter and Saturn were probably crucial in perturbing comets into the inner Solar System, providing the inner planets with a source of water and various other ices. In the Alpha Centauri system, Proxima Centauri may have influenced the planetary disk as the Alpha Centauri system was forming, enriching the area around Alpha Centauri with volatile materials. This would be discounted if, for example, Alpha Centauri B happened to have gas giants orbiting Alpha Centauri A (or vice versa), or if Alpha Centauri A and B themselves were able to perturb comets into each other's inner system as Jupiter and Saturn presumably have done in the Solar System. Such icy bodies probably also reside in Oort clouds of other planetary systems. When they are influenced gravitationally by either the gas giants or disruptions by passing nearby stars, many of these icy bodies then travel star-wards. Such ideas also apply to the close approach of Alpha Centauri or other stars to the Solar System, when, in the distant future, the Oort Cloud might be disrupted enough to increase the number of active comets.
To be in the habitable zone, a planet around Alpha Centauri A would have an orbital radius of between about 1 and so as to have similar planetary temperatures and conditions for liquid water to exist. For the slightly less luminous and cooler Alpha Centauri B, the habitable zone is between about 0.7 and .
With the goal of finding evidence of such planets, both Proxima Centauri and Alpha Centauri AB were among the listed "Tier 1" target stars for NASA's Space Interferometry Mission (SIM). Detecting planets as small as three Earth-masses or smaller within two AU of a "Tier 1" target would have been possible with this new instrument. The SIM mission, however, was cancelled due to financial issues in 2010.
Based on observations between 2007 and 2012, a study found a slight excess of emissions in the 24 µm (mid/far-infrared) band surrounding , which may be interpreted as evidence for a sparse circumstellar disc or dense interplanetary dust. The total mass was estimated to be between to the mass of the Moon, or 10–100 times the mass of the Solar System's zodiacal cloud. If such a disc existed around both stars, disc would likely be stable to 2.8 AU, and would likely be stable to 2.5 AU. This would put A's disc entirely within the frost line, and a small part of B's outer disc just outside.
The sky from Alpha Centauri AB would appear much as it does from the Earth, except that Centaurus would be missing its brightest star. The Sun would appear as a yellow star of apparent magnitude +0.47, roughly the same as the average brightness of Betelgeuse from Earth. It would be at the antipodal point of Alpha Centauri AB's current right ascension and declination, at (2000), in eastern Cassiopeia, easily outshining all the rest of the stars in the constellation. With the placement of the Sun east of the magnitude 3.4 star Epsilon Cassiopeiae, nearly in front of the Heart Nebula, the "W" line of stars of Cassiopeia would have a "/W" shape.
In modern literature, "Rigil Kent" (also "Rigel Kent" and variants; ) and "Toliman", are used as colloquial alternative names of Alpha Centauri (then became the proper name of Alpha Centauri B on 10 August 2018 by approval of IAU).
"Rigil Kent" is short for "Rigil Kentaurus", which is sometimes further abbreviated to "Rigil" or "Rigel", though that is ambiguous with Beta Orionis, which is also called Rigel.
The name "Toliman" originates with Jacobus Golius' 1669 edition of Al-Farghani's "Compendium". "Tolimân" is Golius' latinisation of the Arabic name "the ostriches", the name of an asterism of which Alpha Centauri formed the main star.
During the 19th century, the northern amateur popularist Elijah H. Burritt used the now-obscure name "Bungula", possibly coined from "β" and the Latin "ungula" ("hoof").
Together, Alpha and Beta Centauri form the "Southern Pointers" or "The Pointers", as they point towards the Southern Cross, the asterism of the constellation of Crux.
In Chinese astronomy, "Nán Mén", meaning "Southern Gate", refers to an asterism consisting of Alpha Centauri and Epsilon Centauri. Consequently, the Chinese name for Alpha Centauri itself is "Nán Mén Èr", the Second Star of the Southern Gate.
To the Australian aboriginal Boorong people of northwestern Victoria, Alpha Centauri and Beta Centauri are "Bermbermgle", two brothers noted for their courage and destructiveness, who speared and killed "Tchingal" "The Emu" (the Coalsack Nebula). The form in Wotjobaluk is "Bram-bram-bult".
Alpha Centauri is a likely first target for crewed or robotic interstellar exploration. Using current spacecraft technologies, crossing the distance between the Sun and Alpha Centauri would take several millennia, though the possibility of nuclear pulse propulsion or laser light sail technology, as considered in the Breakthrough Starshot program, could reduce the journey time to decades. An objective of such a mission would be to make a fly-by of, and possibly photograph, planets that might exist in the system. The existence of Proxima Centauri b, announced by the European Southern Observatory (ESO) in August 2016, would be a target for the Starshot program.
In January 2017, Breakthrough Initiatives and the ESO entered a collaboration to search for habitable planets in the Alpha Centauri system. The agreement involves Breakthrough Initiatives providing funding for an upgrade to the VISIR (VLT Imager and Spectrometer for mid-Infrared) instrument on ESO's Very Large Telescope (VLT) in Chile. This upgrade will greatly increase the likelihood of planet detection in the system.

</doc>
<doc id="1980" url="https://en.wikipedia.org/wiki?curid=1980" title="Amiga">
Amiga

The Amiga is a family of personal computers introduced by Commodore in 1985. The original model was one of a number of 16/32- and 32-bit computers that featured 256 KB or more of RAM, mouse-based GUIs, and significantly improved graphics and audio over 8-bit systems. This wave included the Atari ST—released the same year—Apple's Macintosh, and later the Apple IIGS. Based on the Motorola 68000 microprocessor, the Amiga differed from its contemporaries through the inclusion of custom hardware to accelerate graphics and sound, including sprites and a blitter, and a pre-emptive multitasking operating system called AmigaOS.
The Amiga 1000 was released in July 1985, but a series of production problems kept it from becoming widely available until early 1986. The best selling model, the Amiga 500, introduced in 1987 (along with the more expandable / professional oriented Amiga 2000) became one of the leading home computers of the late 1980s and early 1990s with four to six million sold. The A3000 was introduced in 1990, followed by the A500+, and the A600 in March 1992. Finally, the A1200 and the A4000 were released in late 1992. The platform became particularly popular for gaming and programming demos. It also found a prominent role in the desktop video, video production, and show control business, leading to video editing systems such as the Video Toaster. The Amiga's native ability to simultaneously play back multiple digital sound samples made it a popular platform for early tracker music software. The relatively powerful processor and ability to access several megabytes of memory enabled the development of 3D rendering packages, including LightWave 3D, Imagine, Aladdin4D, TurboSilver and Traces, a predecessor to Blender.
Although early Commodore advertisements attempt to cast the computer as an all-purpose business machine, especially when outfitted with the Amiga Sidecar PC compatibility add-on, the Amiga was most commercially successful as a home computer, with a wide range of games and creative software. Poor marketing and the failure of the later models to repeat the technological advances of the first systems meant that the Amiga quickly lost its market share to competing platforms, such as the fourth generation game consoles, Macintosh, and the rapidly dropping prices of IBM PC compatibles, which gained 256-color VGA graphics in 1987. Commodore ultimately went bankrupt in April 1994 after a version of the Amiga packaged as a game console, the Amiga CD32, failed in the marketplace.
Since the demise of Commodore, various groups have marketed successors to the original Amiga line, including Genesi, Eyetech, ACube Systems Srl and A-EON Technology. Likewise, AmigaOS has influenced replacements, clones and compatible systems such as MorphOS, AmigaOS 4 and AROS.
Jay Miner joined Atari in the 1970s to develop custom integrated circuits, and led development of the Atari 2600's TIA. Almost as soon as its development was complete, the team began developing a much more sophisticated set of chips, CTIA, ANTIC and POKEY, that formed the basis of the Atari 8-bit family.
With the 8-bit line's launch in 1979, the team once again started looking at a next generation chipset. Nolan Bushnell had sold the company to Warner Communications in 1978, and the new management was much more interested in the existing lines than development of new products that might cut into their sales. Miner wanted to start work with the new Motorola 68000, but management was only interested in another 6502 based system. Miner left the company, and, for a time, the industry.
In 1979, Larry Kaplan left Atari and founded Activision. In 1982, Kaplan was approached by a number of investors who wanted to develop a new game platform. Kaplan hired Miner to run the hardware side of the newly formed company, "Hi-Toro". The system was code-named "Lorraine" in keeping with Miner's policy of giving systems female names, in this case the company president's wife, Lorraine Morse. When Kaplan left the company late in 1982, Miner was promoted to head engineer and the company relaunched as Amiga Corporation.
A breadboard prototype was largely completed by late 1983, and shown at the January 1984 Consumer Electronics Show (CES). At the time, the operating system was not ready, so the machine was demonstrated with the Boing Ball demo. A further developed version of the system was demonstrated at the June 1984 CES and shown to many companies in hopes of garnering further funding, but found little interest in a market that was in the final stages of the North American video game crash of 1983.
In March, Atari expressed a tepid interest in Lorraine for its potential use in a games console or home computer tentatively known as the 1850XLD. But the talks were progressing slowly, and Amiga was running out of money. A temporary arrangement in June led to a $500,000 loan from Atari to Amiga to keep the company going. The terms required the loan to be repaid at the end of the month, otherwise Amiga would forfeit the Lorraine design to Atari.
During 1983, Atari lost over $1 million a week, due to the combined effects of the crash and the ongoing price war in the home computer market. By the end of the year, Warner was desperate to sell the company. In January 1984, Jack Tramiel resigned from Commodore due to internal battles over the future direction of the company. A number of Commodore employees followed him to his new company, Tramel Technology. This included a number of the senior technical staff, where they began development of a 68000-based machine of their own. In June, Tramiel arranged a no-cash deal to take over Atari, reforming it as Atari Corporation.
As many Commodore technical staff had moved to Atari, Commodore was left with no workable path to design their own next-generation computer. The company approached Amiga offering to fund development as a home computer system. They quickly arranged to repay the Atari loan, ending that threat. The two companies were initially arranging a $4 million license agreement before Commodore offered $24 million to purchase Amiga outright.
By late 1984 the prototype breadboard chipset had successfully been turned into integrated circuits, and the system hardware was being readied for production. At this time the operating system (OS) was not as ready, and led to a deal to port an OS known as TRIPOS to the platform. TRIPOS was a multitasking system that had been written in BCPL during the 1970s for minicomputer systems like the PDP-11, but later experimentally ported to the 68000. This early version was known as AmigaDOS and the GUI as Workbench. The BCPL parts were later rewritten in the C language, and the entire system became AmigaOS.
The system was enclosed in a pizza box form factor case; a late change was the introduction of vertical supports on either side of the case to provide a "garage" under the main section of the system where the keyboard could be stored.
The first model was announced in 1985 as simply "The Amiga from Commodore", later to be retroactively dubbed the Amiga 1000. They were first offered for sale in August, but by October only 50 had been built, all of which were used by Commodore. Machines only began to arrive in quantity in mid-November, meaning they missed the Christmas buying rush. By the end of the year, they had sold 35,000 machines, and severe cashflow problems made the company pull out of the January 1986 CES. Bad or entirely missing marketing, forcing the development team to move to the east coast, notorious stability problems and other blunders limited sales in early 1986 to between 10,000 and 15,000 units a month.
In late 1985 Thomas Rattigan was promoted to COO of Commodore, and then to CEO in February 1986. He immediately implemented an ambitious plan that covered almost all of the company's operations. Among these were the long overdue cancelation of the now outdated PET and VIC-20 lines, as well as a variety of poorly selling Commodore 64 offshoots and the Commodore 900 workstation effort.
Another one of the changes was to split the Amiga into two products, a new high-end version of the Amiga aimed at the creative market, and a cost-reduced version that would take over for the Commodore 64 in the low-end market. These new designs were released in 1987 as the Amiga 2000 and Amiga 500, the latter of which went on to widespread success and became their best selling model.
Similar high-end/low-end models would make up the Amiga line for the rest of its history; follow-on designs included the Amiga 3000/Amiga 500 Plus/Amiga 600, and the Amiga 4000/Amiga 1200. These models incorporated a series of technical upgrades known as the ECS and AGA, which added higher resolution displays among many other improvements and simplifications.
Ultimately the Amiga line would sell an estimated 4,850,000 machines over its lifetime. The machines were most popular in the UK and Germany, with about 1.5 million sold in each country, and sales in the high hundreds of thousands in other European nations. The machine was less popular in North America, where an estimated 700,000 were sold. In particular, in the U.S. the Amiga did not achieve any success outside of Commodore's traditional enthusiast market except in vertical markets for video processing and editing.
In spite of his successes in making the company profitable and bringing the Amiga line to market, Rattigan was soon forced out in a power struggle with majority shareholder, Irving Gould. This is widely regarded as the turning point, as further improvements to the Amiga were eroded by rapid improvements in other platforms.
On April 29, 1994, Commodore filed for bankruptcy and its assets were purchased by Escom, a German PC manufacturer, who created the subsidiary company Amiga Technologies. They re-released the A1200 and A4000T, and introduced a new 68060 version of the A4000T. Amiga Technologies researched and developed the Amiga Walker prototype. They presented the machine publicly at CeBit. Escom, in turn, went bankrupt in 1997.
The Amiga brand was then sold to a U.S. Wintel PC manufacturer, Gateway 2000, which had announced grand plans for it. In 2000, however, Gateway sold the Amiga brand without having released any products. The current owner of the trademark, Amiga, Inc., licensed the rights to sell hardware using either the Amiga or AmigaOne brand to Eyetech Group, Hyperion Entertainment and Commodore USA.
At its core, the Amiga has a custom chipset consisting of several coprocessors, which handle audio, video and direct memory access independently of the Central Processing Unit (CPU). This architecture freed up the Amiga's processor for other tasks and gave the Amiga a performance edge over its competitors, particularly in terms of video-intensive applications and games.
The general Amiga architecture uses two distinct bus subsystems, namely, the chipset bus and the CPU bus. The chipset bus allows the custom coprocessors and CPU to address "Chip RAM". The CPU bus provides addressing to other subsystems, such as conventional RAM, ROM and the Zorro II or Zorro III expansion subsystems. This architecture enables independent operation of the subsystems; the CPU "Fast" bus can be much faster than the chipset bus. CPU expansion boards may provide additional custom buses. Additionally, "busboards" or "bridgeboards" may provide ISA or PCI buses.
The Motorola 68000 series of microprocessors was used in all Amiga models from Commodore. While all CPU in the 68000 family have a 32-bit ISA design (programmer uses and sees a 32-bit model), the MC68000 used in the most popular models is a 16-bit (or 16/32-bit) processor because its ALU operates in 16-bit (32-bit operations require additional clock cycles, consuming more time). The MC68000 has a 16-bit external data bus so 32-bits of data is transferred in two consecutive steps, a technique called multiplexing. This is transparent to the software, which was 32-bit from the beginning. The MC68000 can address 16 MB of physical memory. Later Amiga models featured higher-speed, full 32-bit CPUs with a larger address space and instruction pipeline facilities.
CPU upgrades were offered by both Commodore and third-party manufacturers. Most Amiga models can be upgraded either by direct CPU replacement or through expansion boards. Such boards often featured faster and higher capacity memory interfaces and hard disk controllers.
Towards the end of Commodore's time in charge of Amiga development, there were suggestions that Commodore intended to move away from the 68000 series to higher performance RISC processors, such as the PA-RISC. Those ideas were never developed before Commodore filed for bankruptcy. Despite this, third-party manufacturers designed upgrades featuring a combination of 68000 series and PowerPC processors along with a PowerPC native microkernel and software. Later Amiga clones featured PowerPC processors only.
The custom chipset at the core of the Amiga design appeared in three distinct generations, with a large degree of backward-compatibility. The Original Chip Set (OCS) appeared with the launch of the A1000 in 1985. OCS was eventually followed by the modestly improved Enhanced Chip Set (ECS) in 1990 and finally by the partly 32-bit Advanced Graphics Architecture (AGA) in 1992. Each chipset consists of several coprocessors that handle graphics acceleration, digital audio, direct memory access and communication between various peripherals (e.g., CPU, memory and floppy disks). In addition, some models featured auxiliary custom chips that performed tasks such as SCSI control and display de-interlacing.
All Amiga systems can display full-screen animated graphics with 2, 4, 8, 16, 32, 64 (EHB Mode), or 4096 colors (HAM Mode). Models with the AGA chipset (A1200 and A4000) also have non-EHB 64, 128, 256, and (HAM8 Mode) color modes and a palette expanded from 4096 to 16.8 million colors.
The Amiga chipset can "genlock", which is the ability to adjust its own screen refresh timing to match an incoming NTSC or PAL video signal. When combined with setting transparency, this allows an Amiga to overlay an external video source with graphics. This ability made the Amiga popular for many applications, and provides the ability to do character generation and CGI effects far more cheaply than earlier systems. This ability has been frequently utilized by wedding videographers, TV stations and their weather forecasting divisions (for weather graphics and radar), advertising channels, music video production, and desktop videographers. The NewTek Video Toaster was made possible by the genlock ability of the Amiga.
In 1988, the release of the Amiga A2024 fixed-frequency monochrome monitor with built-in framebuffer and flicker fixer hardware provided the Amiga with a choice of high-resolution graphic modes (1024×800 for NTSC and 1024×1024 for PAL).
ReTargetable Graphics is an API for device drivers mainly used by 3rd party graphics hardware to interface with AmigaOS via a set of libraries. The software libraries may include software tools to adjust resolution, screen colors, pointers and screenmodes. The standard Intuition interface is limited to display depths of 8 bits, while RTG makes it possible to handle higher depths like 24-bits.
The sound chip, named Paula, supports four PCM-sample-based sound channels (two for the left speaker and two for the right) with 8-bit resolution for each channel and a 6-bit volume control per channel. The analog output is connected to a low-pass filter, which filters out high-frequency aliases when the Amiga is using a lower sampling rate (see Nyquist frequency). The brightness of the Amiga's power LED is used to indicate the status of the Amiga's low-pass filter. The filter is active when the LED is at normal brightness, and deactivated when dimmed (or off on older A500 Amigas). On Amiga 1000 (and first Amiga 500 and Amiga 2000 model), the power LED had no relation to the filter's status, and a wire needed to be manually soldered between pins on the sound chip to disable the filter. Paula can read directly from the system's RAM, using direct memory access (DMA), making sound playback without CPU intervention possible.
Although the hardware is limited to four separate sound channels, software such as "OctaMED" uses software mixing to allow eight or more virtual channels, and it was possible for software to mix two hardware channels to achieve a single 14-bit resolution channel by playing with the volumes of the channels in such a way that one of the source channels contributes the most significant bits and the other the least.
The quality of the Amiga's sound output, and the fact that the hardware is ubiquitous and easily addressed by software, were standout features of Amiga hardware unavailable on PC platforms for years. Third-party sound cards exist that provide DSP functions, multi-track direct-to-disk recording, multiple hardware sound channels and 16-bit and beyond resolutions. A retargetable sound API called AHI was developed allowing these cards to be used transparently by the OS and software.
Kickstart is the firmware upon which AmigaOS is bootstrapped. Its purpose is to initialize the Amiga hardware and core components of AmigaOS and then attempt to boot from a bootable volume, such as a floppy disk or hard disk drive. Most models (excluding the Amiga 1000) come equipped with Kickstart on an embedded ROM-chip.
The keyboard on Amiga computers is similar to that found on a mid 80s IBM PC: Ten function keys, a numeric keypad, and four separate directional arrow keys. Caps Lock and Control share space to the left of A. Absent are Home, End, Page Up, and Page Down keys: These functions are accomplished on Amigas by pressing shift and the appropriate arrow key. The Amiga keyboard adds a Help key, which a function key usually acts as on PCs (usually F1). In addition to the Control and Alt modifier keys, the Amiga has 2 'Amiga' keys, rendered as 'Open Amiga' and 'Closed Amiga' similar to the Open/Closed Apple logo keys on Apple II keyboards. The left is used to manipulate the operating system (moving screens and the like) and the right delivers commands to the application. The absence of Num lock frees space for more mathematical symbols around the numeric pad. Contemporary Macintosh computers, for comparison, lack function keys completely.
The mouse has two buttons like Windows, but unlike Windows pressing and holding the right button replaces the system status line at the top of the screen with a Maclike menu bar. As with Apple's Mac OS prior to Mac OS 8, menu options are selected by releasing the button over that option, not by left clicking. Menu items that have a boolean toggle state can be left clicked whilst the menu is kept open with the right button, which allows the user – for example – to set some selected text to bold, underline and italics all at once.
The mouse plugs into one of two Atari joystick ports used for joysticks, game paddles, and graphics tablets. Although compatible with analog joysticks, Atari-style digital joysticks became standard. Unusually, two independent mice can be connected to the joystick ports; some games, such as Lemmings, were designed to take advantage of this.
The Amiga was one of the first computers for which inexpensive sound sampling and video digitization accessories were available. As a result of this and the Amiga's audio and video capabilities, the Amiga became a popular system for editing and producing both music and video.
Many expansion boards were produced for Amiga computers to improve the performance and capability of the hardware, such as memory expansions, SCSI controllers, CPU boards, and graphics boards. Other upgrades include genlocks, network cards for Ethernet, modems, sound cards and samplers, video digitizers, extra serial ports, and IDE controllers. Additions after the demise of Commodore company are USB cards. The most popular upgrades were memory, SCSI controllers and CPU accelerator cards. These were sometimes combined into the one device.
Early CPU accelerator cards feature full 32-bit CPUs of the 68000 family such as the Motorola 68020 and Motorola 68030, almost always with 32-bit memory and usually with FPUs and MMUs or the facility to add them. Later designs feature the Motorola 68040 or Motorola 68060. Both CPUs feature integrated FPUs and MMUs. Many CPU accelerator cards also had integrated SCSI controllers.
Phase5 designed the PowerUP boards (Blizzard PPC and CyberStorm PPC) featuring both a 68k (a 68040 or 68060) and a PowerPC (603 or 604) CPU, which are able to run the two CPUs at the same time and share the system memory. The PowerPC CPU on PowerUP boards is usually used as a coprocessor for heavy computations; a powerful CPU is needed to run MAME for example, but even decoding JPEG pictures and MP3 audio was considered heavy computation at the time. It is also possible to ignore the 68k CPU and run Linux on the PPC via project Linux APUS, but a PowerPC-native AmigaOS promised by Amiga Technologies GmbH was not available when the PowerUP boards first appeared.
24-bit graphics cards and video cards were also available. Graphics cards were designed primarily for 2D artwork production, workstation use, and later, gaming. Video cards are designed for inputting and outputting video signals, and processing and manipulating video.
In the North American market, the "NewTek Video Toaster" was a video effects board that turned the Amiga into an affordable video processing computer that found its way into many professional video environments. One well-known use was to create the special effects in early series of "Babylon 5". Due to its NTSC-only design, it did not find a market in countries that used the PAL standard, such as in Europe. In those countries, the "OpalVision" card was popular, although less featured and supported than the Video Toaster. Low-cost time base correctors (TBC) specifically designed to work with the Toaster quickly came to market, most of which were designed as standard Amiga bus cards.
Various manufacturers started producing PCI busboards for the A1200, A3000 and A4000, allowing standard Amiga computers to use PCI cards such as graphics cards, Sound Blaster sound cards, 10/100 Ethernet cards, USB cards, and television tuner cards. Other manufacturers produced hybrid boards that contained an Intel x86 series chip, allowing the Amiga to emulate a PC.
PowerPC upgrades with Wide SCSI controllers, PCI busboards with Ethernet, sound and 3D graphics cards, and tower cases allowed the A1200 and A4000 to survive well into the late nineties.
Expansion boards were made by Richmond Sound Design that allow their show control and sound design software to communicate with their custom hardware frames either by either ribbon cable or fiber optic cable for long distances, allowing the Amiga to control up to eight million digitally controlled external audio, lighting, automation, relay and voltage control channels spread around a large theme park, for example. See Amiga software for more information on these applications.
Other devices included the following:
The Commodore A2232 board provides serial ports in addition to the Amiga's built-in serial port. Each port can be driven independently at speeds of There is, however, a driver available on Aminet that allows two of the serial ports to be driven at The serial card used the 65CE02 CPU clocked at . This CPU was also part of the CSG 4510 CPU core that was used in the Commodore 65 computer.
Amiga has three networking interface APIs:
Different network media were used:
The original Amiga models were produced from 1985 to 1996. They are, in order of production: 1000, 2000, 500, 1500, 2500, 3000, 3000UX, 3000T, CDTV, 500+, 600, 4000, 1200, CD32, and 4000T. The PowerPC based AmigaOne computers were later marketed since 2002. Several companies and private persons have also released Amiga clones and still do so today.
The first Amiga model, the Amiga 1000, was launched in 1985. In 2006, PC World rated the Amiga 1000 as the seventh greatest PC of all time, stating "Years ahead of its time, the Amiga was the world's first multimedia, multitasking personal computer".
Commodore updated the desktop line of Amiga computers with the Amiga 2000 in 1987, the Amiga 3000 in 1990, and the Amiga 4000 in 1992, each offering improved capabilities and expansion options. The best selling models were the budget models, however, particularly the highly successful Amiga 500 (1987) and the Amiga 1200 (1992). The Amiga 500+ (1991) was the shortest lived model, replacing the Amiga 500 and lasting only six months until it was phased out and replaced with the Amiga 600 (1992), which in turn was also quickly replaced by the Amiga 1200.
The CDTV, launched in 1991, was a CD-ROM based all-in-one multimedia system. It was an early attempt at a multi-purpose multimedia appliance in an era before multimedia consoles and CD-ROM drives were common. Unfortunately for Commodore, the system never achieved any real commercial success. Like the Commodore 64GS that was a video game console based on a computer, the CDTV was designed as a video game console and multimedia platform. It had existed before the Sony PlayStation and Sega Saturn, but had influenced them. It competed with the Turbo-Grafx CD and Sega CD system add ons when it was being sold.
Commodore's last Amiga offering before filing for bankruptcy was an attempt to capture a portion of the highly competitive 1990s console market with the Amiga CD32 (1993), a 32-bit CD-ROM games console. Although discontinued after Commodore's demise it met with moderate commercial success in Europe. The CD32 was a next generation CDTV, and it was designed to save Commodore by entering the growing video game console market.
Following purchase of Commodore's assets by Escom in 1995, the A1200 and A4000T continued to be sold in small quantities until 1996, though the ground lost since the initial launch and the prohibitive expense of these units meant that the Amiga line never regained any real popularity.
Several Amiga models contained references to songs by the rock band The B-52's. Early A500 units had the words "B52/ROCK LOBSTER" silk-screen printed onto their printed circuit board, a reference to the song "Rock Lobster" The Amiga 600 referenced "JUNE BUG" (after the song "Junebug") and the Amiga 1200 had "CHANNEL Z" (after "Channel Z")., and the CD-32 had "Spellbound."
AmigaOS 4 is designed for PowerPC Amiga systems. It is mainly based on AmigaOS 3.1 source code, with some parts of version 3.9. Currently runs on both Amigas equipped with CyberstormPPC or BlizzardPPC accelerator boards, on the Teron series based AmigaOne computers built by Eyetech under license by Amiga, Inc., on the Pegasos II from Genesi/bPlan GmbH, on the ACube Systems Srl Sam440ep / Sam460ex / AmigaOne 500 systems and on the A-EON AmigaOne X1000.
AmigaOS 4.0 had been available only in developer pre-releases for numerous years until it was officially released in December 2006. Due to the nature of some provisions of the contract between Amiga Inc. and Hyperion Entertainment (the Belgian company that is developing the OS), the commercial AmigaOS 4 had been available only to licensed buyers of AmigaOne motherboards.
AmigaOS 4.0 for Amigas equipped with PowerUP accelerator boards was released in November 2007. Version 4.1 was released in August 2008 for AmigaOne systems, and in May 2011 for Amigas equipped with PowerUP accelerator boards. The most recent release of AmigaOS for all supported platforms is 4.1 update 5. Starting with release 4.1 update 4 there is an Emulation drawer containing official AmigaOS 3.x ROMs (all classic Amiga models including CD32) and relative Workbench files.
Acube Systems entered an agreement with Hyperion under which it has ported AmigaOS 4 to its Sam440ep and Sam460ex line of PowerPC-based motherboards. In 2009 a version for Pegasos II was released in co-operation with Acube Systems. In 2012, A-EON Technology Ltd manufactured and released the AmigaOne X1000 to consumers through their partner, Amiga Kit who provided end-user support, assembly and worldwide distribution of the new system.
Long-time Amiga developer MacroSystem entered the Amiga-clone market with their DraCo non-linear video editing system. It appears in two versions, initially a tower model and later a cube. DraCo expanded upon and combined a number of earlier expansion cards developed for Amiga (VLabMotion, Toccata, WarpEngine, RetinaIII) into a true Amiga-clone powered by the Motorola 68060 processor. The DraCo can run AmigaOS 3.1 up through AmigaOS 3.9. It is the only Amiga-based system to support FireWire for video I/O. DraCo also offers an Amiga-compatible Zorro-II expansion bus and introduced a faster custom DraCoBus, capable of transfer rates (faster than Commodore's Zorro-III). The technology was later used in the Casablanca system, a set-top-box also designed for non-linear video editing.
In 1998, Index Information released the Access, an Amiga-clone similar to the Amiga 1200, but on a motherboard that could fit into a standard 5¼" drive bay. It features either a 68020 or 68030 CPU, with a redesigned AGA chipset, and runs AmigaOS 3.1.
In 1998, former Amiga employees (John Smith, Peter Kittel, Dave Haynie and Andy Finkel to mention few) formed a new company called PIOS. Their hardware platform, PIOS One, was aimed at Amiga, Atari and Macintosh users. The company was renamed to Met@box in 1999 until it folded.
The NatAmi (short for "Native Amiga") hardware project began in 2005 with the aim of designing and building an Amiga clone motherboard that is enhanced with modern features. The NatAmi motherboard is a standard Mini-ITX-compatible form factor computer motherboard, powered by a Motorola/Freescale 68060 and its chipset. It is compatible with the original Amiga chipset, which has been inscribed on a programmable FPGA Altera chip on the board. The NatAmi is the second Amiga clone project after the Minimig motherboard, and its history is very similar to that of the C-One mainboard developed by Jeri Ellsworth and Jens Schönfeld. From a commercial point of view, Natami's circuitry and design are currently closed source. One goal of the NatAmi project is to design an Amiga-compatible motherboard that includes up-to-date features but that does not rely on emulation (as in WinUAE), modern PC Intel components, or a modern PowerPC mainboard. As such, NatAmi is not intended to become another evolutionary heir to classic Amigas, such as with AmigaOne or Pegasos computers. This "purist" philosophy essentially limits the resulting processor speed but puts the focus on bandwidth and low latencies. The developers also recreated the entire Amiga chipset, freeing it from legacy Amiga limitations such as two megabytes of audio and video graphics RAM as in the AGA chipset, and rebuilt this new chipset by programming a modern FPGA Altera Cyclone IV chip. Later, the developers decided to create from scratch a new software-form processor chip, codenamed "N68050" that resides in the physical Altera FPGA programmable chip.
In 2006, two new Amiga clones were announced, both using FPGA based hardware synthesis to replace the Amiga OCS custom chipset. The first, the Minimig, is a personal project of Dutch engineer Dennis van Weeren. Referred to as "new Amiga hardware", the original model was built on a Xilinx Spartan-3 development board, but soon a dedicated board was developed. The minimig uses the FPGA to reproduce the custom Denise, Agnus, Paula and Gary chips as well as both 8520 CIAs and implements a simple version of Amber. The rest of the chips are an actual 68000 CPU, ram chips, and a PIC microcontroller for BIOS control. The design for Minimig was released as open-source on July 25, 2007. In February 2008, an Italian company Acube Systems began selling Minimig boards. A third party upgrade replaces the PIC microcontroller with a more powerful ARM processor, providing more functionality such as write access and support for hard disk images. The Minimig core has been ported to the FPGArcade "Replay" board. The Replay uses an FPGA with about more capacity and that does support the AGA chipset and a 68020 soft core with 68030 capabilities. The Replay board is designed to implement many older computers and classic arcade machines.
The second is the Clone-A system announced by Individual Computers. As of mid 2007 it has been shown in its development form, with FPGA-based boards replacing the Amiga chipset and mounted on an Amiga 500 motherboard.
Like many popular but discontinued platforms, the Amiga has been emulated so that software developed for the Amiga can be run on other computer platforms without the original hardware. Such emulators attempt to replicate the functionality of the Amiga architecture in software. As mentioned above, attempts have also been made to replicate the Amiga chipset in FPGA chips.
One of the most challenging aspects of emulation is the design of the Amiga chipset, which relies on cycle-critical timings. As a result, early emulators did not always achieve the intended results though later emulator versions can now accurately reproduce the behavior of Amiga systems.
AmigaOS is a single-user multitasking operating system. It was developed first by Commodore International, and initially introduced in 1985 with the Amiga 1000. Original versions run on the Motorola 68000 series of microprocessors, while AmigaOS 4 runs only on PowerPC microprocessors. At the time of release AmigaOS put an operating system that was well ahead of its time into the hands of the average consumer. It was one of the first commercially available consumer operating systems for personal computers to implement preemptive multitasking.
Another notable feature was the combined use of both a command-line interface and graphical user interface. AmigaDOS was the disk operating system and command line portion of the OS and Workbench the native graphical windowing, icons, menu and pointer environment for file management and launching applications. Notably, AmigaDOS allowed long filenames (up to 107 characters) with whitespace and did not require filename extensions. The windowing system and user interface engine that handles all input events is called Intuition.
The multi-tasking kernel is called Exec. It acts as a scheduler for tasks running on the system, providing pre-emptive multitasking with prioritised round-robin scheduling. It enabled true pre-emptive multitasking in as little as 256 KB of free memory.
AmigaOS does not implement memory protection, because the 68000 CPU does not include a memory management unit. Although this speeds and eases inter-process communication because programs can communicate by simply passing a pointer back and forth, the lack of memory protection made the AmigaOS more vulnerable to crashes from badly behaving programs than other multitasking systems that did implement memory protection, and Amiga OS is fundamentally incapable of enforcing any form of security model since any program had full access to the system. A co-operational memory protection feature was implemented in AmigaOS 4 and could be retrofitted to old AmigaOS systems using Enforcer or CyberGuard tools.
The problem was somewhat exacerbated by Commodore's initial decision to release documentation relating not only to the OS's underlying software routines, but also to the hardware itself, enabling intrepid programmers who had developed their skills on the Commodore 64 to POKE the hardware directly, as was done on the older platform. While the decision to release the documentation was a popular one and allowed the creation of fast, sophisticated sound and graphics routines in games and demos, it also contributed to system instabilityas some programmers lacked the expertise to program at this level. For this reason, when the new AGA chipset was released, Commodore declined to release low-level documentation in an attempt to force developers into using the approved software routines.
AmigaOS directly or indirectly inspired the development of various operating systems. MorphOS and AROS clearly inherit heavily from the structure of AmigaOS as explained directly in articles regarding these two operating systems. AmigaOS also influenced BeOS, which featured a centralized system of Datatypes, similar to that present in AmigaOS. Likewise, DragonFly BSD was also inspired by AmigaOS as stated by Dragonfly developer Matthew Dillon who is a former Amiga developer. WindowLab and amiwm are among several window managers for the X Window System seek to mimic the Workbench interface. IBM licensed the Amiga GUI from Commodore in exchange for the REXX language license. This allowed OS/2 to have the WPS (Workplace Shell) GUI shell for OS/2 2.0, a 32-bit operating system.
Commodore-Amiga produced Amiga Unix, informally known as Amix, based on AT&T SVR4. It supports the Amiga 2500 and Amiga 3000 and is included with the Amiga 3000UX. Among other unusual features of Amix is a hardware-accelerated windowing system that can scroll windows without copying data. Amix is not supported on the later Amiga systems based on 68040 or 68060 processors.
Other, still maintained, operating systems are available for the classic Amiga platform, including Linux and NetBSD. Both require a CPU with MMU such as the 68020 with 68851 or full versions of the 68030, 68040 or 68060. There is also a version of Linux for Amigas with PowerPC accelerator cards. Debian and Yellow Dog Linux can run on the AmigaOne.
There is an official, older version of OpenBSD. The last Amiga release is 3.2. MINIX 1.5.10 also runs on Amiga.
The Amiga is able to emulate other computer platforms ranging from many 8-bit systems such as the ZX Spectrum, Commodore 64, Nintendo Game Boy, Nintendo Entertainment System, Apple II and the TRS-80. The Commodore PC-Transformer software emulated an IBM 5150 at 1 MHz in Monochrome mode. Later PC-Bridgecards were a full hardware PC on a card with 8086/80286/80386 Intel chips running MS-DOS and Windows in an Amiga window. A-Max emulated an Apple Macintosh using a serial port dongle that had a Macintosh ROM on it. The Amiga had the same 68000 CPU as the Macintosh and, using a Macintosh emulator, could run Mac 68K operating systems and programs. The Amiga could not directly read Macintosh 3.5" floppies, however, due to their proprietary format. Further, it required a compatible Macintosh for a copy of its ROM. The Atari ST was also emulated. MAME (the arcade machine emulator) is also available for Amiga systems with PPC accelerator card upgrades.
In the late 1980s and early 1990s the platform became particularly popular for gaming, demoscene activities and creative software uses. During this time commercial developers marketed a wide range of games and creative software, often developing titles simultaneously for the Atari ST due to the similar hardware architecture. Popular creative software included 3D rendering (ray-tracing) packages, bitmap graphics editors, desktop video software, software development packages and "tracker" music editors.
Until the late 1990s the Amiga remained a popular platform for non-commercial software, often developed by enthusiasts, and much of which was freely redistributable. An on-line archive, Aminet, was created in 1992 and until around 1996 was the largest public archive of software, art and documents for any platform.
The name "Amiga" was chosen by the developers from the Spanish word for a female friend, because they knew Spanish, and because it occurred before Apple and Atari alphabetically. It also conveyed the message that the Amiga computer line was "user friendly" as a pun or play on words.
The first official Amiga logo was a rainbow-colored double check mark. In later marketing material Commodore largely dropped the checkmark and used logos styled with various typefaces. Although it was never adopted as a trademark by Commodore, the "Boing Ball" has been synonymous with Amiga since its launch. It became an unofficial and enduring theme after a visually impressive animated demonstration at the 1984 Winter Consumer Electronics Show in January 1984 showing a checkered ball bouncing and rotating. Following Escom's purchase of Commodore in 1996, the Boing Ball theme was incorporated into a new logo.
Early Commodore advertisements attempted to cast the computer as an all-purpose business machine, though the Amiga was most commercially successful as a home computer. Throughout the 1980s and early 1990s Commodore primarily placed advertising in computer magazines and occasionally in national newspapers and on television.
Since the demise of Commodore, various groups have marketed successors to the original Amiga line:
AmigaOS and MorphOS are commercial proprietary operating systems. AmigaOS 4, based on AmigaOS 3.1 source code with some parts of version 3.9, is developed by Hyperion Entertainment and runs on PowerPC based hardware. MorphOS, based on some parts of AROS source code, is developed by MorphOS Team and is continued on Apple and other PowerPC based hardware.
There is also AROS, a free and open source operating system (re-implementation of the AmigaOS 3.1 APIs), for Amiga 68k, x86 and ARM hardware (one version runs Linux-hosted on the Raspberry Pi). In particular, AROS for Amiga 68k hardware aims to create an open source Kickstart ROM replacement for emulation purpose and/or for use on real "classic" hardware.
After Commodore went bankrupt in 1994, an active Amiga community continued to support the platform long after mainstream commercial vendors abandoned it. The most popular Amiga magazine, "Amiga Format", continued to publish editions until 2000, some six years after Commodore filed for bankruptcy. Another magazine, "Amiga Active", was launched in 1999 and was published until 2001. In spite of declining interest in the platform, there was a bi-weekly specialist column in the UK weekly magazine "Micro Mart".
Several notable magazines are in publication today: "Amiga Future", which is available in both English and German; "Bitplane.it", a bi-monthly magazine in Italian; and "AmigaPower", a long-running French magazine.
Online Amiga magazines still exists like Obligement, a French publication available since 1997.
The Amiga series of computers found a place in early computer graphic design and television presentation. Below are some examples of notable uses and users:
In addition, many other celebrities and notable individuals have made use of the Amiga:
The Amiga was also used in a number of special purpose applications:

</doc>
<doc id="1985" url="https://en.wikipedia.org/wiki?curid=1985" title="Absorption">
Absorption

Absorption may refer to:

</doc>
<doc id="1986" url="https://en.wikipedia.org/wiki?curid=1986" title="Actinophryid">
Actinophryid

The actinophryids are an order of heliozoa. They are the most common heliozoa in fresh water and can also be found in marine and soil habitats. Actinophryids are unicellular and roughly spherical in shape, with many axopodia that radiate outward from the cell body. Axopodia are a type of pseudopodia that are supported by hundreds of microtubules arranged in a needle-like internal structure. These axopods adhere to passing prey and assist with cell movement, as well as playing a part in cell division and cell fusion.
Actinophryids are largely aquatic protozoa with a spherical cell body and many needle-like axopodia. They resemble the shape of a sun due to this structure, which is the inspiration for their common name: heliozoa, or "sun-animalcules". They range in size from a few micrometers to a full millimeter across.
The cell body is largely vacuolated, with the ectoplasm consisting almost entirely of these structures. The endoplasm of actinophryids is often darker and denser than the outer layer, and can sometimes be seen as a sharp boundary under a light microscope. The organisms can be either mononucleate, with a single, well defined nucleus in the center of the cell body, or multinucleate, with 10 or more nuclei dispersed throughout the organism. The cytoplasm of actinophryids is often granular, similar to that of "Amoeba".
Contractile vacuoles are common in these organisms, who use them to maintain homeostasis and control buoyancy. These are visible as clear bulges from the surface of the cell body that slowly fill then rapidly deflate, expelling the contents into the environment.
The most distinctive characteristic of the actinophryids is their axopodia. These axopodia consist of a central, rigid rod which is coated in a thin layer of ectoplasm. These axonemes are rooted in the endoplasm and terminate there, sometimes close to a nucleus. The axonemes are composed microtubules arranged in a double spiral pattern characteristic of the order. Due to their long, parallel construction these microtubules demonstrate strong birefringence.
These axopodia are used for prey capture, mobility, and cell fusion and division. They can be flexible, especially when the organisms are starved, and are highly dynamic, undergoing frequent construction and destruction. When used to collect prey items, two methods of capture have been noted, termed axopodial flow and rapid axopodial contraction. Axopodial flow involves the slow movement of a prey item along the surface of the axopod as the ectoplasm itself moves, while rapid axopodial contraction involves the collapse of the axoneme's microtubule structure. This behavior has been documented in many species, including "Actinosphaerium nucleofilum", "Actinophrys sol", and "Raphidiophrys contractilis". The rapid axopodial contraction occurs at high speed, often in excess of 5mm/s or tens of body lengths per second.
The axopodial contractions have been shown to be highly sensitive to environmental factors such as temperature and pressure as well as chemical signals like Ca and colchicine. They may also be triggered by mechanical or electrical stimulation.
Reproduction in actinophryids generally takes place via fission, where one parent cell divides into two or more daughter cells. For multinucleate heliozoa, this process is plasmotomic as the nuclei are not duplicated prior to division. It has been observed that reproduction appears to be a response to food scarcity, with an increased number of divisions following the removal of food and larger organisms during times of food excess.
Actinophryids also undergo autogamy during times of food scarcity. This is better described as genetic reorganization than reproduction, as the number of individuals produced is the same as the initial number. Nonetheless, it serves as a way to increase genetic diversity within an individual which may improve the likelihood of expressing favorable genetic traits.
Plastogamy has also been extensively documented in actinophryids, especially in multinucleate ones. "Actinosphaerium" were observed to combine freely without the combination of nuclei, and this process sometimes resulted in more or less individuals than originally combined. This process is not caused merely by contact between two individuals but can be caused by damage to the cell body.
Under unfavourable conditions, some species will form a cyst. This is often the product of autogamy, in which case the cysts produced are zygotes. Cells undergoing this process withdraw their axopodia, adhere to the substrate, and take on an opaque and grayish appearance. This cyst then divides until only uninucleate cells remain. The cyst wall is thickly layered 7-8 times and includes gelatinous layers, layers of silica plates, and iron.
Originally placed in Heliozoa (Sarcodina), the group's current location within the larger tree of life is debated. It may belong to either the Actinochrysophyceae (Axodines), or to Raphidomonadea.
There are several genera included within this classification. "Actinophrys" are smaller and have a single, central nucleus. Most have a cell body 40-50 micrometer in diameter with axopods around 100 μm in length, though this varies significantly. "Actinosphaerium" are several times larger, from 200-1000 μm in diameter, with many nuclei and are found exclusively in fresh water. A third genus, "Camptonema", was named as a junior subjective synonym of "Actinosphaerium" by Mikrjukov & Patterson in 2001, but Cavalier-Smith & Scoble (2013) preserve the genus. "Heliorapha" was also added to this classification by Cavalier & Smith (2013), which was previously the genus "Ciliophrys".
Classification based on Cavalier-Smith and Scoble 2013

</doc>
