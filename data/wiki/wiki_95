<doc id="11772" url="https://en.wikipedia.org/wiki?curid=11772" title="Finnish Civil War">
Finnish Civil War

The Finnish Civil War was a civil war in Finland in 1918 fought for the leadership and control of Finland between White Finland and Finnish Socialist Workers' Republic (Red Finland) during the country's transition from a Grand Duchy of the Russian Empire to an independent state. The clashes took place in the context of the national, political, and social turmoil caused by World War I (Eastern Front) in Europe. The war was fought between the "Reds", led by a section of the Social Democratic Party, and the "Whites", conducted by the conservative-based Senate and the German Imperial Army. The paramilitary Red Guards, composed of industrial and agrarian workers, controlled the cities and industrial centres of southern Finland. The paramilitary White Guards, composed of farmers, along with middle-class and upper-class social strata, controlled rural central and northern Finland led by General C. G. E. Mannerheim.
In the years before the conflict, Finnish society had experienced rapid population growth, industrialisation, pre-urbanisation and the rise of a comprehensive labour movement. The country's political and governmental systems were in an unstable phase of democratisation and modernisation. The socio-economic condition and education of the population had gradually improved, as well as national thinking and cultural life had awakened.
World War I led to the collapse of the Russian Empire, causing a power vacuum in Finland, and a subsequent struggle for dominance led to militarisation and an escalating crisis between the left-leaning labour movement and the conservatives. The Reds carried out an unsuccessful general offensive in February 1918, supplied with weapons by Soviet Russia. A counteroffensive by the Whites began in March, reinforced by the German Empire's military detachments in April. The decisive engagements were the Battles of Tampere and Vyborg (; ), won by the Whites, and the Battles of Helsinki and Lahti, won by German troops, leading to overall victory for the Whites and the German forces. Political violence became a part of this warfare. Around 12,500 Red prisoners died of malnutrition and disease in camps. About 39,000 people, of whom 36,000 were Finns, perished in the conflict.
In the aftermath, the Finns passed from Russian governance to the German sphere of influence with a plan to establish a German-led Finnish monarchy. The scheme was cancelled with the defeat of Germany in World War I and Finland instead emerged as an independent, democratic republic. The Civil War divided the nation for decades. Finnish society was reunited through social compromises based on a long-term culture of moderate politics and religion and the post-war economic recovery.
The main factor behind the Finnish Civil War was a political crisis arising out of World War I. Under the pressures of the Great War, the Russian Empire collapsed, leading to the February and October Revolutions in 1917. This breakdown caused a power vacuum and a subsequent struggle for power in Eastern Europe. Russia's Grand Duchy of Finland (1809–1917), became embroiled in the turmoil. Geopolitically less important than the continental Moscow–Warsaw gateway, Finland, isolated by the Baltic Sea was a peaceful side front until early 1918. The war between the German Empire and Russia had only indirect effects on the Finns. Since the end of the 19th century, the Grand Duchy had become a vital source of raw materials, industrial products, food and labour for the growing Imperial Russian capital Petrograd (modern Saint Petersburg), and World War I emphasised that role. Strategically, the Finnish territory was the less important northern section of the Estonian–Finnish gateway and a buffer zone to and from Petrograd through the Narva area, the Gulf of Finland and the Karelian Isthmus.
The German Empire saw Eastern Europe—primarily Russia—as a major source of vital products and raw materials, both during World War I and for the future. Her resources overstretched by the two-front war, Germany attempted to divide Russia by providing financial support to revolutionary groups, such as the Bolsheviks and the Socialist Revolutionary Party, and to radical, separatist factions, such as the Finnish national activist movement leaning toward Germanism. Between 30 and 40 million marks were spent on this endeavour. Controlling the Finnish area would allow the Imperial German Army to penetrate Petrograd and the Kola Peninsula, an area rich in raw materials for the mining industry. Finland possessed large ore reserves and a well-developed forest industry.
From 1809 to 1898, a period called "Pax Russica", the peripheral authority of the Finns gradually increased, and Russo-Finnish relations were exceptionally peaceful in comparison with other parts of the Russian Empire. Russia's defeat in the Crimean War in the 1850s led to attempts to speed up the modernisation of the country. This caused more than 50 years of economic, industrial, cultural and educational progress in the Grand Duchy of Finland, including an improvement in the status of the Finnish language. All this encouraged Finnish nationalism and cultural unity through the birth of the Fennoman movement, which bound the Finns to the domestic administration and led to the idea that the Grand Duchy was an increasingly autonomous state of the Russian Empire.
In 1899, the Russian Empire initiated a policy of integration through the Russification of Finland. The strengthened, pan-slavist central power tried to unite the "Russian Multinational Dynastic Union" as the military and strategic situation of Russia became more perilous due to the rise of Germany and Japan. Finns called the increased military and administrative control, "the First Period of Oppression", and for the first time Finnish politicians drew up plans for disengagement from Russia or sovereignty for Finland. In the struggle against integration, activists drawn from sections of the working class and the Swedish-speaking intelligentsia carried out terrorist acts. During World War I and the rise of Germanism, the pro-Swedish Svecomans began their covert collaboration with Imperial Germany and, from 1915 to 1917, a Jäger (; ) battalion consisting of 1,900 Finnish volunteers was trained in Germany.
The major reasons for rising political tensions among Finns were the autocratic rule of the Russian czar and the undemocratic class system of the estates of the realm. The latter system originated in the regime of the Swedish Empire that preceded Russian governance and divided the Finnish people economically, socially and politically. Finland's population grew rapidly in the nineteenth century (from 860,000 in 1810 to 3,130,000 in 1917), and a class of agrarian and industrial workers, as well as crofters, emerged over the period. The Industrial Revolution was rapid in Finland, though it started later than in the rest of Western Europe. Industrialisation was financed by the state and some of the social problems associated with the industrial process were diminished by the administration's actions. Among urban workers, socio-economic problems steepened during periods of industrial depression. The position of rural workers worsened after the end of the nineteenth century, as farming became more efficient and market-oriented, and the development of industry was insufficiently vigorous to fully utilise the rapid population growth of the countryside.
The difference between Scandinavian-Finnish (Finno-Ugric peoples) and Russian-Slavic culture affected the nature of Finnish national integration. The upper social strata took the lead and gained domestic authority from the Russian czar in 1809. The estates planned to build an increasingly autonomous Finnish state, led by the elite and the intelligentsia. The Fennoman movement aimed to include the common people in a non-political role; the labour movement, youth associations and the temperance movement were initially led "from above".
Between 1870 and 1916 industrialisation gradually improved social conditions and the self-confidence of workers, but while the standard of living of the common people rose in absolute terms, the rift between rich and poor deepened markedly. The commoners' rising awareness of socio-economic and political questions interacted with the ideas of socialism, social liberalism and nationalism. The workers' initiatives and the corresponding responses of the dominant authorities intensified social conflict in Finland.The Finnish labour movement, which emerged at the end of the nineteenth century from temperance, religious movements and Fennomania, had a Finnish nationalist, working-class character. From 1899 to 1906, the movement became conclusively independent, shedding the paternalistic thinking of the Fennoman estates, and it was represented by the Finnish Social Democratic Party, established in 1899. Workers' activism was directed both toward opposing Russification and in developing a domestic policy that tackled social problems and responded to the demand for democracy. This was a reaction to the domestic dispute, ongoing since the 1880s, between the Finnish nobility-bourgeoisie and the labour movement concerning voting rights for the common people.
Despite their obligations as obedient, peaceful and non-political inhabitants of the Grand Duchy (who had, only a few decades earlier, accepted the class system as the natural order of their life), the commoners began to demand their civil rights and citizenship in Finnish society. The power struggle between the Finnish estates and the Russian administration gave a concrete role model and free space for the labour movement. On the other side, due to an at-least century-long tradition and experience of administrative authority, the Finnish elite saw itself as the inherent natural leader of the nation. The political struggle for democracy was solved outside Finland, in international politics: the Russian Empire's failed 1904–1905 war against Japan led to the 1905 Revolution in Russia and to a general strike in Finland. In an attempt to quell the general unrest, the system of estates was abolished in the Parliamentary Reform of 1906. The general strike increased support for the social democrats substantially. The party encompassed a higher proportion of the population than any other socialist movement in the world.
The Reform of 1906 was a giant leap towards the political and social liberalisation of the common Finnish people because the Russian House of Romanov had been the most autocratic and conservative ruler in Europe. The Finns adopted a unicameral parliamentary system, the Parliament of Finland (; ) with universal suffrage. The number of voters increased from 126,000 to 1,273,000, including female citizens. The reform led to the social democrats obtaining about fifty percent of the popular vote, but the Czar regained his authority after the crisis of 1905. Subsequently, during the more severe programme of Russification, called "the Second Period of Oppression" by the Finns, the Czar neutralised the power of the Finnish Parliament between 1908 and 1917. He dissolved the assembly, ordered parliamentary elections almost annually, and determined the composition of the Finnish Senate, which did not correlate with the Parliament.
The capacity of the Finnish Parliament to solve socio-economic problems was stymied by confrontations between the largely uneducated commoners and the former estates. Another conflict festered as employers denied collective bargaining and the right of the labour unions to represent workers. The parliamentary process disappointed the labour movement, but as dominance in the Parliament and legislation was the workers' most likely way to obtain a more balanced society, they identified themselves with the state. Overall domestic politics led to a contest for leadership of the Finnish state during the ten years before the collapse of the Russian Empire.
The Second Period of Russification was halted on 15 March 1917 by the February Revolution, which removed the czar, Nicholas II. The collapse of Russia was caused by military defeats, war-weariness against the duration and hardships of the Great War, and the collision between the most conservative regime in Europe and a Russian people desiring modernisation. The Czar's power was transferred to the State Duma (Russian Parliament) and the right-wing Provisional Government, but this new authority was challenged by the Petrograd Soviet (city council), leading to dual power in the country.
The autonomous status of 1809–1899 was returned to the Finns by the March 1917 manifesto of the Russian Provisional Government. For the first time in history, "de facto" political power existed in the Parliament of Finland. The political left, consisting mainly of social democrats, covered a wide spectrum from moderate to revolutionary socialists. The political right was even more diverse, ranging from social liberals and moderate conservatives to rightist conservative elements. The four main parties were:
During 1917, a power struggle and social disintegration interacted. The collapse of Russia induced a chain reaction of disintegration, starting from the government, military and economy, and spreading to all fields of society, such as local administration, workplaces and to individual citizens. The social democrats wanted to retain the civil rights already achieved and to increase the socialists' power over society. The conservatives feared the loss of their long-held socio-economic dominance. Both factions collaborated with their equivalents in Russia, deepening the split in the nation.
The Social Democratic Party gained an absolute majority in the parliamentary elections of 1916. A new Senate was formed in March 1917 by Oskari Tokoi, but it did not reflect the socialists' large parliamentary majority: it comprised six social democrats and six non-socialists. In theory, the Senate consisted of a broad national coalition, but in practice (with the main political groups unwilling to compromise and top politicians remaining outside of it), it proved unable to solve any major Finnish problem. After the February Revolution, political authority descended to the street level: mass meetings, strike organisations and worker-soldier councils on the left and to active organisations of employers on the right, all serving to undermine the authority of the state.
The February Revolution halted the Finnish economic boom caused by the Russian war-economy. The collapse in business led to unemployment and high inflation, but the employed workers gained an opportunity to resolve workplace problems. The commoners' call for the eight-hour working day, better working conditions and higher wages led to demonstrations and large-scale strikes in industry and agriculture.
While the Finns had specialised in milk and butter production, the bulk of the food supply for the country depended on cereals produced in southern Russia. The cessation of cereal imports from disintegrating Russia led to food shortages in Finland. The Senate responded by introducing rationing and price controls. The farmers resisted the state control and thus a black market, accompanied by sharply rising food prices, formed. As a consequence, export to the free market of the Petrograd area increased. Food supply, prices and, in the end, the fear of starvation became emotional political issues between farmers and urban workers, especially those who were unemployed. Common people, their fears exploited by politicians and an incendiary, polarised political media, took to the streets. Despite the food shortages, no actual large-scale starvation hit southern Finland before the civil war and the food market remained a secondary stimulator in the power struggle of the Finnish state.
The passing of the Tokoi Senate bill called the "Law of Supreme Power" (, more commonly known as "valtalaki"; ) in July 1917, triggered one of the key crises in the power struggle between the social democrats and the conservatives. The fall of the Russian Empire opened the question of who would hold sovereign political authority in the former Grand Duchy. After decades of political disappointment, the February Revolution offered the Finnish social democrats an opportunity to govern; they held the absolute majority in Parliament. The conservatives were alarmed by the continuous increase of the socialists' influence since 1899, which reached a climax in 1917.
The "Law of Supreme Power" incorporated a plan by the socialists to substantially increase the authority of Parliament, as a reaction to the non-parliamentary and conservative leadership of the Finnish Senate between 1906 and 1916. The bill furthered Finnish autonomy in domestic affairs: the Russian Provisional Government was only allowed the right to control Finnish foreign and military policies. The Act was adopted with the support of the Social Democratic Party, the Agrarian League, part of the Young Finnish Party and some activists eager for Finnish sovereignty. The conservatives opposed the bill and some of the most right-wing representatives resigned from Parliament.
In Petrograd, the social democrats' plan had the backing of the Bolsheviks. They had been plotting a revolt against the Provisional Government since April 1917, and pro-Soviet demonstrations during the July Days brought matters to a head. The Helsinki Soviet and the Regional Committee of the Finnish Soviets, led by the Bolshevik Ivar Smilga, both pledged to defend the Finnish Parliament, were it threatened with attack. However, the Provisional Government still had sufficient support in the Russian army to survive and as the street movement waned, Vladimir Lenin fled to Karelia. In the aftermath of these events, the "Law of Supreme Power" was overruled and the social democrats eventually backed down; more Russian troops were sent to Finland and, with the co-operation and insistence of the Finnish conservatives, Parliament was dissolved and new elections announced.
In the October 1917 elections, the social democrats lost their absolute majority, which radicalised the labour movement and decreased support for moderate politics. The crisis of July 1917 did not bring about the Red Revolution of January 1918 on its own, but together with political developments based on the commoners' interpretation of the ideas of Fennomania and socialism, the events favoured a Finnish revolution. In order to win power, the socialists had to overcome Parliament.
The February Revolution resulted in a loss of institutional authority in Finland and the dissolution of the police force, creating fear and uncertainty. In response, both the right and left assembled their own security groups, which were initially local and largely unarmed. By late 1917, following the dissolution of Parliament, in the absence of a strong government and national armed forces, the security groups began assuming a broader and more paramilitary character. The Civil Guards (; ; ) and the later White Guards (; ) were organised by local men of influence: conservative academics, industrialists, major landowners, and activists. The Workers' Order Guards (; ) and the Red Guards (; ) were recruited through the local social democratic party sections and from the labour unions.
The Bolsheviks' and Vladimir Lenin's October Revolution of 7 November 1917 transferred political power in Petrograd to the radical, left-wing socialists. The German government's decision to arrange safe-conduct for Lenin and his comrades from exile in Switzerland to Petrograd in April 1917, was a success. An armistice between Germany and the Bolshevik regime came into force on 6 December and peace negotiations began on 22 December 1917 at Brest-Litovsk.
November 1917 became another watershed in the 1917–1918 rivalry for the leadership of Finland. After the dissolution of the Finnish Parliament, polarisation between the social democrats and the conservatives increased markedly and the period witnessed the appearance of political violence. An agricultural worker was shot during a local strike on 9 August 1917 at Ypäjä and a Civil Guard member was killed in a local political crisis at Malmi on 24 September. The October Revolution disrupted the informal truce between the Finnish non-socialists and the Russian Provisional Government. After political wrangling over how to react to the revolt, the majority of the politicians accepted a compromise proposal by Santeri Alkio, the leader of the Agrarian League. Parliament seized the sovereign power in Finland on 15 November 1917 based on the socialists' "Law of Supreme Power" and ratified their proposals of an eight-hour working day and universal suffrage in local elections, from July 1917.
The purely non-socialist, conservative-led government of Pehr Evind Svinhufvud was appointed on 27 November. This nomination was both a long-term aim of the conservatives and a response to the challenges of the labour movement during November 1917. Svinhufvud's main aspirations were to separate Finland from Russia, to strengthen the Civil Guards, and to return a part of Parliament's new authority to the Senate. There were 149 Civil Guards on 31 August 1917 in Finland, counting local units and subsidiary White Guards in towns and rural communes; 251 on 30 September; 315 on 31 October; 380 on 30 November and 408 on 26 January 1918. The first attempt at serious military training among the Guards was the establishment of a 200-strong cavalry school at the Saksanniemi estate in the vicinity of the town of Porvoo, in September 1917. The vanguard of the Finnish Jägers and German weaponry arrived in Finland during October–November 1917 on the ' freighter and the German U-boat '; around 50 Jägers had returned by the end of 1917.
After political defeats in July and October 1917, the social democrats put forward an uncompromising program called "We Demand" (; ) on 1 November, in order to push for political concessions. They insisted upon a return to the political status before the dissolution of Parliament in July 1917, disbandment of the Civil Guards and elections to establish a Finnish Constituent Assembly. The program failed and the socialists initiated a general strike during 14–19 November to increase political pressure on the conservatives, who had opposed the "Law of Supreme Power" and the parliamentary proclamation of sovereign power on 15 November.
Revolution became the goal of the radicalised socialists after the loss of political control, and events in November 1917 offered momentum for a socialist uprising. In this phase, Lenin and Joseph Stalin, under threat in Petrograd, urged the social democrats to take power in Finland. The majority of Finnish socialists were moderate and preferred parliamentary methods, prompting the Bolsheviks to label them "reluctant revolutionaries". The reluctance diminished as the general strike appeared to offer a major channel of influence for the workers in southern Finland. The strike leadership voted by a narrow majority to start a revolution on 16 November, but the uprising had to be called off the same day due to the lack of active revolutionaries to execute it.
At the end of November 1917, the moderate socialists among the social democrats won a second vote over the radicals in a debate over revolutionary versus parliamentary means, but when they tried to pass a resolution to completely abandon the idea of a socialist revolution, the party representatives and several influential leaders voted it down. The Finnish labour movement wanted to sustain a military force of its own and to keep the revolutionary road open, too. The wavering Finnish socialists disappointed V. I. Lenin and in turn, he began to encourage the Finnish Bolsheviks in Petrograd.
Among the labour movement, a more marked consequence of the events of 1917 was the rise of the Workers' Order Guards. There were 20–60 separate guards between 31 August and 30 September 1917, but on 20 October, after defeat in parliamentary elections, the Finnish labour movement proclaimed the need to establish more worker units. The announcement led to a rush of recruits: on 31 October the number of guards was 100–150; 342 on 30 November 1917 and 375 on 26 January 1918. Since May 1917, the paramilitary organisations of the left had grown in two phases, the majority of them as Workers' Order Guards. The minority were Red Guards, these were partly underground groups formed in industrialised towns and industrial centres, such as Helsinki, Kotka and Tampere, based on the original Red Guards that had been formed during 1905–1906 in Finland.
The presence of the two opposing armed forces created a state of dual power and divided sovereignty on Finnish society. The decisive rift between the guards broke out during the general strike: the Reds executed several political opponents in southern Finland and the first armed clashes between the Whites and Reds took place. In total, 34 casualties were reported. Eventually, the political rivalries of 1917 led to an arms race and an escalation towards civil war.
The disintegration of Russia offered Finns an historic opportunity to gain national independence. After the October Revolution, the conservatives were eager for secession from Russia in order to control the left and minimise the influence of the Bolsheviks. The socialists were skeptical about sovereignty under conservative rule, but they feared a loss of support among nationalistic workers, particularly after having promised increased national liberty through the "Law of Supreme Power". Eventually, both political factions supported an independent Finland, despite strong disagreement over the composition of the nation's leadership.
Nationalism had become a "civic religion" in Finland by the end of nineteenth century, but the goal during the general strike of 1905 was a return to the autonomy of 1809–1898, not full independence. In comparison to the unitary Swedish regime, the domestic power of Finns had increased under the less uniform Russian rule. Economically, the Grand Duchy of Finland benefited from having an independent domestic state budget, a central bank with national currency, the markka (deployed 1860), and customs organisation and the industrial progress of 1860–1916. The economy was dependent on the huge Russian market and separation would disrupt the profitable Finnish financial zone. The economic collapse of Russia and the power struggle of the Finnish state in 1917 were among the key factors that brought sovereignty to the fore in Finland.
Svinhufvud's Senate introduced Finland's Declaration of Independence on 4 December 1917 and Parliament adopted it on 6 December. The social democrats voted against the Senate's proposal, while presenting an alternative declaration of sovereignty. The establishment of an independent state was not a guaranteed conclusion for the small Finnish nation. Recognition by Russia and other great powers was essential; Svinhufvud accepted that he had to negotiate with Lenin for the acknowledgement. The socialists, having been reluctant to enter talks with the Russian leadership in July 1917, sent two delegations to Petrograd to request that Lenin approve Finnish sovereignty.
In December 1917, Lenin was under intense pressure from the Germans to conclude peace negotiations at Brest-Litovsk and the Bolsheviks' rule was in crisis, with an inexperienced administration and the demoralised army facing powerful political and military opponents. Lenin calculated that the Bolsheviks could fight for central parts of Russia but had to give up some peripheral territories, including Finland in the geopolitically less important north-western corner. As a result, Svinhufvud's delegation won Lenin's concession of sovereignty on 31 December 1917.
By the beginning of the Civil War, Austria-Hungary, Denmark, France, Germany, Greece, Norway, Sweden and Switzerland had recognised Finnish independence. The United Kingdom and United States did not approve it; they waited and monitored the relations between Finland and Germany (the main enemy of the Allies), hoping to override Lenin's regime and to get Russia back into the war against the German Empire. In turn, the Germans hastened Finland's separation from Russia so as to move the country to within their sphere of influence.
The final escalation towards war began in early January 1918, as each military or political action of the Reds or the Whites resulted in a corresponding counteraction by the other. Both sides justified their activities as defensive measures, particularly to their own supporters. On the left, the vanguard of the movement was the urban Red Guards from Helsinki, Kotka and Turku; they led the rural Reds and convinced the socialist leaders who wavered between peace and war to support the revolution. On the right, the vanguard was the Jägers, who had transferred to Finland, and the volunteer Civil Guards of southwestern Finland, southern Ostrobothnia and Vyborg province in the southeastern corner of Finland. The first local battles were fought during 9–21 January 1918 in southern and southeastern Finland, mainly to win the arms race and to control Vyborg (; ).
On 12 January 1918, Parliament authorised the Svinhufvud Senate to establish internal order and discipline on behalf of the state. On 15 January, Carl Gustaf Emil Mannerheim, a former Finnish general of the Imperial Russian Army, was appointed the commander-in-chief of the Civil Guards. The Senate appointed the Guards, henceforth called the White Guards, as the White Army of Finland. Mannerheim placed his Headquarters of the White Army in the Vaasa–Seinäjoki area. The White Order to engage was issued on 25 January. The Whites gained weaponry by disarming Russian garrisons during 21–28 January, in particular in southern Ostrobothnia.
The Red Guards, led by Ali Aaltonen, refused to recognise the Whites' hegemony and established a military authority of their own. Aaltonen installed his headquarters in Helsinki and nicknamed it Smolna echoing the Smolny Institute, the Bolsheviks' headquarters in Petrograd. The Red Order of Revolution was issued on 26 January, and a red lantern, a symbolic indicator of the uprising, was lit in the tower of the Helsinki Workers' House. A large-scale mobilisation of the Reds began late in the evening of 27 January, with the Helsinki Red Guard and some of the Guards located along the Vyborg-Tampere railway having been activated between 23 and 26 January, in order to safeguard vital positions and escort a heavy railroad shipment of Bolshevik weapons from Petrograd to Finland. White troops tried to capture the shipment: 20–30 Finns, Red and White, died in the Battle of Kämärä at the Karelian Isthmus on 27 January 1918. The Finnish rivalry for power had culminated.
At the beginning of the war, a discontinuous front line ran through southern Finland from west to east, dividing the country into White Finland and Red Finland. The Red Guards controlled the area to the south, including nearly all the major towns and industrial centres, along with the largest estates and farms with the highest numbers of crofters and tenant farmers. The White Army controlled the area to the north, which was predominantly agrarian and contained small or medium-sized farms and tenant farmers. The number of crofters was lower and they held a better social status than those in the south. Enclaves of the opposing forces existed on both sides of the front line: within the White area lay the industrial towns of Varkaus, Kuopio, Oulu, Raahe, Kemi and Tornio; within the Red area lay Porvoo, Kirkkonummi and Uusikaupunki. The elimination of these strongholds was a priority for both armies in February 1918.
Red Finland was led by the People's Delegation (; ), established on 28 January 1918 in Helsinki. The delegation sought democratic socialism based on the Finnish Social Democratic Party's ethos; their visions differed from Lenin's dictatorship of the proletariat. Otto Ville Kuusinen formulated a proposal for a new constitution, influenced by those of Switzerland and the United States. With it, political power was to be concentrated to Parliament, with a lesser role for a government. The proposal included a multi-party system; freedom of assembly, speech and press; and the use of referenda in political decision-making. In order to ensure the authority of the labour movement, the common people would have a right to permanent revolution. The socialists planned to transfer a substantial part of property rights to the state and local administrations.
In foreign policy, Red Finland leaned on Bolshevist Russia. A Red-initiated Finno–Russian treaty and peace agreement was signed on 1 March 1918, where Red Finland was called the Finnish Socialist Workers' Republic (; ). The negotiations for the treaty implied that –as in World War I in general– nationalism was more important for both sides than the principles of international socialism. The Red Finns did not simply accept an alliance with the Bolsheviks and major disputes appeared, for example, over the demarcation of the border between Red Finland and Soviet Russia. The significance of the Russo–Finnish Treaty evaporated quickly due to the signing of the Treaty of Brest-Litovsk between the Bolsheviks and the German Empire on 3 March 1918.
Lenin's policy on the right of nations to self-determination aimed at preventing the disintegration of Russia during the period of military weakness. He assumed that in war-torn, splintering Europe, the proletariat of free nations would carry out socialist revolutions and unite with Soviet Russia later. The majority of the Finnish labour movement supported Finland's independence. The Finnish Bolsheviks, influential, though few in number, favoured annexation of Finland by Russia.
The government of White Finland, Pehr Evind Svinhufvud's first senate, was called the Vaasa Senate after its relocation to the safer west-coast city of Vaasa, which acted as the capital of the Whites from 29 January to 3 May 1918. In domestic policy, the White Senate's main goal was to return the political right to power in Finland. The conservatives planned a monarchist political system, with a lesser role for Parliament. A section of the conservatives had always supported monarchy and opposed democracy; others had approved of parliamentarianism since the revolutionary reform of 1906, but after the crisis of 1917–1918, concluded that empowering the common people would not work. Social liberals and reformist non-socialists opposed any restriction of parliamentarianism. They initially resisted German military help, but the prolonged warfare changed their stance.
In foreign policy, the Vaasa Senate relied on the German Empire for military and political aid. Their objective was to defeat the Finnish Reds; end the influence of Bolshevist Russia in Finland and expand Finnish territory to East Karelia, a geopolitically significant home to people speaking Finno-Ugric languages. The weakness of Russia inspired an idea of Greater Finland among the expansionist factions of both the right and left: the Reds had claims concerning the same areas. General Mannerheim agreed on the need to take over East Karelia and to request German weapons, but opposed actual German intervention in Finland. Mannerheim recognised the Red Guards' lack of combat skill and trusted in the abilities of the German-trained Finnish Jägers. As a former Russian army officer, Mannerheim was well aware of the demoralisation of the Russian army. He co-operated with White-aligned Russian officers in Finland and Russia.
The number of Finnish troops on each side varied from 70,000 to 90,000 and both had around 100,000 rifles, 300–400 machine guns and a few hundred cannons. While the Red Guards consisted mostly of volunteers, with wages paid at the beginning of the war, the White Army consisted predominantly of conscripts with 11,000–15,000 volunteers. The main motives for volunteering were socio-economic factors, such as salary and food, as well as idealism and peer pressure. The Red Guards included 2,600 women, mostly girls recruited from the industrial centres and cities of southern Finland. Urban and agricultural workers constituted the majority of the Red Guards, whereas land-owning farmers and well-educated people formed the backbone of the White Army. Both armies used child soldiers, mainly between 14 and 17 years of age. The use of juvenile soldiers was not rare in World War I; children of the time were under the absolute authority of adults and were not shielded against exploitation.
Rifles and machine guns from Imperial Russia were the main armaments of the Reds and the Whites. The most commonly used rifle was the Russian Mosin–Nagant Model 1891. In total, around ten different rifle models were in service, causing problems for ammunition supply. The Maxim gun was the most-used machine gun, along with the less-used M1895 Colt–Browning, Lewis and Madsen guns. The machine guns caused a substantial part of the casualties in combat. Russian field guns were mostly used with direct fire.
The Civil War was fought primarily along railways; vital means for transporting troops and supplies, as well for using armoured trains, equipped with light cannons and heavy machine guns. The strategically most important railway junction was Haapamäki, approximately northeast of Tampere, connecting eastern and western Finland and as well as southern and northern Finland. Other critical junctions included Kouvola, Riihimäki, Tampere, Toijala and Vyborg. The Whites captured Haapamäki at the end of January 1918, leading to the Battle of Vilppula.
The Finnish Red Guards seized the early initiative in the war by taking control of Helsinki on 28 January 1918 and by undertaking a general offensive lasting from February till early March 1918. The Reds were relatively well-armed, but a chronic shortage of skilled leaders, both at the command level and in the field, left them unable to capitalise on this momentum, and most of the offensives came to nothing. The military chain of command functioned relatively well at company and platoon level, but leadership and authority remained weak as most of the field commanders were chosen by the vote of the troops. The common troops were more or less armed civilians, whose military training, discipline and combat morale were both inadequate and low.
Ali Aaltonen was replaced on 28 January 1918 by Eero Haapalainen as commander-in-chief. He, in turn, was displaced by the Bolshevik triumvirate of Eino Rahja, Adolf Taimi and Evert Eloranta on 20 March. The last commander-in-chief of the Red Guard was Kullervo Manner, from 10 April until the last period of the war when the Reds no longer had a named leader. Some talented local commanders, such as Hugo Salmela in the Battle of Tampere, provided successful leadership, but could not change the course of the war. The Reds achieved some local victories as they retreated from southern Finland toward Russia, such as against German troops in the Battle of Syrjäntaka on 28–29 April in Tuulos.
The revolutions in Russia divided the Soviet army officers politically and their attitude towards the Finnish Civil War varied. Mikhail Svechnikov led Finnish Red troops in western Finland in February and Konstantin Yeremejev Soviet forces on the Karelian Isthmus, while other officers were mistrustful of their revolutionary peers and instead co-operated with General Mannerheim, in disarming Soviet garrisons in Finland. On 30 January 1918, Mannerheim proclaimed to Russian soldiers in Finland that the White Army did not fight against Russia, but that the objective of the White campaign was to beat the Finnish Reds and the Soviet troops supporting them.
The number of Soviet soldiers active in the civil war declined markedly once Germany attacked Russia on 18 February 1918. The German-Soviet Treaty of Brest-Litovsk of 3 March restricted the Bolsheviks' support for the Finnish Reds to weapons and supplies. The Soviets remained active on the south-eastern front, mainly in the Battle of Rautu on the Karelian Isthmus between February and April 1918, where they defended the approaches to Petrograd.
While the conflict has been called by some, "The War of Amateurs", the White Army had two major advantages over the Red Guards: the professional military leadership of Gustaf Mannerheim and his staff, which included 84 Swedish volunteer officers and former Finnish officers of the czar's army; and 1,450 soldiers of the 1,900-strong, Jäger battalion. The majority of the unit arrived in Vaasa on 25 February 1918. On the battlefield, the Jägers, battle-hardened on the Eastern Front, provided strong leadership that made disciplined combat of the common White troopers possible. The soldiers were similar to those of the Reds, having brief and inadequate training. At the beginning of the war, the White Guards' top leadership had little authority over volunteer White units, which obeyed only their local leaders. At the end of February, the Jägers started a rapid training of six conscript regiments.
The Jäger battalion was politically divided, too. Four-hundred-and-fifty –mostly socialist– Jägers remained stationed in Germany, as it was feared they were likely to side with the Reds. White Guard leaders faced a similar problem when drafting young men to the army in February 1918: 30,000 obvious supporters of the Finnish labour movement never showed up. It was also uncertain whether common troops drafted from the small-sized and poor farms of central and northern Finland had strong enough motivation to fight the Finnish Reds. The Whites' propaganda promoted the idea that they were fighting a defensive war against Bolshevist Russians, and belittled the role of the Red Finns among their enemies. Social divisions appeared both between southern and northern Finland and within rural Finland. The economy and society of the north had modernised more slowly than that of the south. There was a more pronounced conflict between Christianity and socialism in the north, and the ownership of farmland conferred major social status, motivating the farmers to fight against the Reds.
Sweden declared neutrality both during World War I and the Finnish Civil War. General opinion, in particular among the Swedish elite, was divided between supporters of the Allies and the Central powers, Germanism being somewhat more popular. Three war-time priorities determined the pragmatic policy of the Swedish liberal-social democratic government: sound economics, with export of iron-ore and foodstuff to Germany; sustaining the tranquility of Swedish society; and geopolitics. The government accepted the participation of Swedish volunteer officers and soldiers in the Finnish White Army in order to block expansion of revolutionary unrest to Scandinavia.
A 1,000-strong paramilitary Swedish Brigade, led by Hjalmar Frisell, took part in the Battle of Tampere and in the fighting south of the town. In February 1918, the Swedish Navy escorted the German naval squadron transporting Finnish Jägers and German weapons and allowed it to pass through Swedish territorial waters. The Swedish socialists tried to open peace negotiations between the Whites and the Reds. The weakness of Finland offered Sweden a chance to take over the geopolitically vital Finnish Åland Islands, east of Stockholm, but the German army's Finland operation stalled this plan.
In March 1918, the German Empire intervened in the Finnish Civil War on the side of the White Army. Finnish activists leaning on Germanism had been seeking German aid in freeing Finland from Soviet hegemony since late 1917, but because of the pressure they were facing at the Western Front, the Germans did not want to jeopardise their armistice and peace negotiations with the Soviet Union. The German stance changed after 10 February when Leon Trotsky, despite the weakness of the Bolsheviks' position, broke off negotiations, hoping revolutions would break out in the German Empire and change everything. On 13 February, the German leadership decided to retaliate and send military detachments to Finland too. As a pretext for aggression, the Germans invited "requests for help" from the western neighbouring countries of Russia. Representatives of White Finland in Berlin duly requested help on 14 February.
The Imperial German Army attacked Russia on 18 February. The offensive led to a rapid collapse of the Soviet forces and to the signing of the first Treaty of Brest-Litovsk by the Bolsheviks on 3 March 1918. Finland, the Baltic countries, Poland and Ukraine were transferred to the German sphere of influence. The Finnish Civil War opened a low-cost access route to Fennoscandia, where the geopolitical status was altered as a British Naval squadron invaded the Soviet harbour of Murmansk by the Arctic Ocean on 9 March 1918. The leader of the German war effort, General Erich Ludendorff, wanted to keep Petrograd under threat of attack via the Vyborg-Narva area and to install a German-led monarchy in Finland.
On 5 March 1918, a German naval squadron landed on the Åland Islands (in mid-February 1918, the islands had been occupied by a Swedish military expedition, which departed from there in May). On 3 April 1918, the 10,000-strong Baltic Sea Division (), led by General Rüdiger von der Goltz, launched the main attack at Hanko, west of Helsinki. It was followed on 7 April by Colonel Otto von Brandenstein's 3,000-strong Detachment Brandenstein () taking the town of Loviisa east of Helsinki. The larger German formations advanced eastwards from Hanko and took Helsinki on 12–13 April, while Detachment Brandenstein overran the town of Lahti on 19 April. The main German detachment proceeded northwards from Helsinki and took Hyvinkää and Riihimäki on 21–22 April, followed by Hämeenlinna on 26 April. The final blow to the cause of the Finnish Reds was dealt when the Bolsheviks broke off the peace negotiations at Brest-Litovsk, leading to the German eastern offensive in February 1918.
In February 1918, General Mannerheim deliberated on where to focus the general offensive of the Whites. There were two strategically vital enemy strongholds: Tampere, Finland's major industrial town in the south-west, and Vyborg, Karelia's main city. Although seizing Vyborg offered many advantages, his army's lack of combat skills and the potential for a major counterattack by the Reds in the area or in the south-west made it too risky.
Mannerheim decided to strike first at Tampere. He launched the main assault on 16 March 1918, at Längelmäki north-east of the town, through the right flank of the Reds' defence. At the same time, the Whites attacked through the north-western frontline Vilppula–Kuru–Kyröskoski–Suodenniemi. Although the Whites were unaccustomed to offensive warfare, some Red Guard units collapsed and retreated in panic under the weight of the offensive, while other Red detachments defended their posts to the last and were able to slow the advance of the White troops. Eventually, the Whites lay siege to Tampere. They cut off the Reds' southward connection at Lempäälä on 24 March and westward ones at Siuro, Nokia, and Ylöjärvi on 25 March.
The Battle for Tampere was fought between 16,000 White and 14,000 Red soldiers. It was Finland's first large-scale urban battle and one of the four most decisive military engagements of the war. The fight for the area of Tampere began on 28 March, on the eve of Easter 1918, later called "Bloody Maundy Thursday", in the Kalevankangas cemetery. The White Army did not achieve a decisive victory in the fierce combat, suffering more than 50 percent losses in some of their units. The Whites had to re-organise their troops and battle plans, managing to raid the town centre in the early hours of 3 April.
After a heavy, concentrated artillery barrage, the White Guards advanced from house to house and street to street, as the Red Guards retreated. In the late evening of 3 April, the Whites reached the eastern banks of the Tammerkoski rapids. The Reds' attempts to break the siege of Tampere from the outside along the Helsinki-Tampere railway failed. The Red Guards lost the western parts of the town between 4 and 5 April. The Tampere City Hall was among the last strongholds of the Reds. The battle ended 6 April 1918 with the surrender of Red forces in the Pyynikki and Pispala sections of Tampere.
The Reds, now on the defensive, showed increased motivation to fight during the battle. General Mannerheim was compelled to deploy some of the best-trained Jäger detachments, initially meant to be conserved for later use in the Vyborg area. The Battle of Tampere was the bloodiest action of the Civil War. The White Army lost 700–900 men, including 50 Jägers, the highest number of deaths the Jäger battalion suffered in a single battle of the 1918 war. The Red Guards lost 1,000–1,500 soldiers, with a further 11,000–12,000 captured. 71 civilians died, mainly due to artillery fire. The eastern parts of the city, consisting mostly of wooden buildings, were completely destroyed.
After peace talks between Germans and the Finnish Reds were broken off on 11 April 1918, the battle for the capital of Finland began. At 05:00 on 12 April, around 2,000–3,000 German Baltic Sea Division soldiers, led by Colonel Hans von Tschirsky und von Bögendorff, attacked the city from the north-west, supported via the Helsinki-Turku railway. The Germans broke through the area between Munkkiniemi and Pasila, and advanced on the central-western parts of the town. The German naval squadron led by Vice Admiral Hugo Meurer blocked the city harbour, bombarded the southern town area, and landed "Seebataillon" marines at Katajanokka.
Around 7,000 Finnish Reds defended Helsinki, but their best troops fought on other fronts of the war. The main strongholds of the Red defence were the Workers' Hall, the Helsinki railway station, the Red Headquarters at Smolna, the Senate Palace–Helsinki University area and the former Russian garrisons. By the late evening of 12 April, most of the southern parts and all of the western area of the city had been occupied by the Germans. Local Helsinki White Guards, having hidden in the city during the war, joined the battle as the Germans advanced through the town.
On 13 April, German troops took over the Market Square, the Smolna, the Presidential Palace and the Senate-Ritarihuone area. Toward the end, a German brigade with 2,000–3,000 soldiers, led by Colonel Kondrad Wolf joined the battle. The unit rushed from north to the eastern parts of Helsinki, pushing into the working-class neighborhoods of Hermanni, Kallio and Sörnäinen. German artillery bombarded and destroyed the Workers' Hall and put out the red lantern of the Finnish revolution. The eastern parts of the town surrendered around 14:00 on 13 April, when a white flag was raised in the tower of the Kallio Church. Sporadic fighting lasted until the evening. In total, 60 Germans, 300–400 Reds and 23 White Guard troopers were killed in the battle. Around 7,000 Reds were captured. The German army celebrated the victory with a military parade in the centre of Helsinki on 14 April 1918.
On 19 April 1918, Detachment Brandenstein took over the town of Lahti. The German troops advanced from the east-southeast via Nastola, through the Mustankallio graveyard in Salpausselkä and the Russian garrisons at Hennala. The battle was minor but strategically important as it cut the connection between the western and eastern Red Guards. Local engagements broke out in the town and the surrounding area between 22 April and 1 May 1918 as several thousand western Red Guards and Red civilian refugees tried to push through on their way to Russia. The German troops were able to hold major parts of the town and halt the Red advance. In total, 600 Reds and 80 German soldiers perished, and 30,000 Reds were captured in and around Lahti.
After the defeat in Tampere, the Red Guards began a slow retreat eastwards. As the German army seized Helsinki, the White Army shifted the military focus to Vyborg area, where 18,500 Whites advanced against 15,000 defending Reds. General Mannerheim's war plan had been revised as a result of the Battle for Tampere, a civilian, industrial town. He aimed to avoid new, complex city combat in Vyborg, an old military fortress. The Jäger detachments tried to tie down and destroy the Red force outside the town. The Whites were able to cut the Reds' connection to Petrograd and weaken the troops on the Karelian Isthmus on 20–26 April, but the decisive blow remained to be dealt in Vyborg. The final attack began on late 27 April with a heavy Jäger artillery barrage. The Reds' defence collapsed gradually, and eventually the Whites conquered Patterinmäki—the Reds' symbolic last stand of the 1918 uprising—in the early hours of 29 April 1918. In total, 400 Whites died, and 500–600 Reds perished and 12,000–15,000 were captured.
Both Whites and Reds carried out political violence through executions, respectively termed White Terror (; ) and Red Terror (; ). The threshold of political violence had already been crossed by the Finnish activists during the First Period of Russification. Large-scale terror operations were born and bred in Europe during World War I, the first total war. The February and October Revolutions initiated similar violence in Finland: at first by Russian army troops executing their officers, later between the Finnish Reds and Whites.
The terror consisted of a calculated aspect of general warfare and, on the other hand, the local, personal murders and corresponding acts of revenge. In the former, the commanding staff planned and organised the actions and gave orders to the lower ranks. At least a third of the Red terror and most of the White terror was centrally led. In February 1918, a "Desk of Securing Occupied Areas" was implemented by the highest-ranking White staff, and the White troops were given "Instructions for Wartime Judicature", later called the Shoot on the Spot Declaration. This order authorised field commanders to execute essentially anyone they saw fit. No order by the less-organised, highest Red Guard leadership authorising Red Terror has been found. The paper was "burned" or the command was oral.
The main goals of the terror were to destroy the command structure of the enemy; to clear and secure the areas governed and occupied by armies; and to create shock and fear among the civil population and the enemy soldiers. Additionally, the common troops' paramilitary nature and their lack of combat skills drove them to use political violence as a military weapon. Most of the executions were carried out by cavalry units called Flying Patrols, consisting of 10 to 80 soldiers aged 15 to 20 and led by an experienced, adult leader with absolute authority. The patrols, specialised in search and destroy operations and death squad tactics, were similar to German Sturmbattalions and Russian Assault units organized during World War I. The terror achieved some of its objectives but also gave additional motivation to fight against an enemy perceived to be inhuman and cruel. Both Red and White propaganda made effective use of their opponents' actions, increasing the spiral of revenge.
The Red Guards executed influential Whites, including politicians, major landowners, industrialists, police officers, civil servants and teachers as well as White Guards. Ten priests of the Evangelical Lutheran Church and 90 moderate socialists were killed. The number of executions varied over the war months, peaking in February as the Reds secured power, but March saw low counts because the Reds could not seize new areas outside of the original frontlines. The numbers rose again in April as the Reds aimed to leave Finland. The two major centres for Red Terror were Toijala and Kouvola, where 300–350 Whites were executed between February and April 1918.
The White Guards executed Red Guard and party leaders, Red troops, socialist members of the Finnish Parliament and local Red administrators, and those active in implementing Red Terror. The numbers varied over the months as the Whites conquered southern Finland. Comprehensive White Terror started with their general offensive in March 1918 and increased constantly. It peaked at the end of the war and declined and ceased after the enemy troops had been transferred to prison camps. During the high point of the executions, between the end of April and the beginning of May, 200 Reds were shot per day. White Terror was decisive against Russian soldiers who assisted the Finnish Reds, and several Russian non-socialist civilians were killed in the Vyborg massacre, the aftermath of the Battle of Vyborg.
In total, 1,650 Whites died as a result of Red Terror, while around 10,000 Reds perished by White Terror, which turned into political cleansing. White victims have been recorded exactly, while the number of Red troops executed immediately after battles remains unclear. Together with the harsh prison-camp treatment of the Reds during 1918, the executions inflicted the deepest mental scars on the Finns, regardless of their political allegiance. Some of those who carried out the killings were traumatised, a phenomenon that was later documented.
On 8 April 1918, after the defeat in Tampere and the German army intervention, the People's Delegation retreated from Helsinki to Vyborg. The loss of Helsinki pushed them to Petrograd on 25 April. The escape of the leadership embittered many Reds, and thousands of them tried to flee to Russia, but most of the refugees were encircled by White and German troops. In the Lahti area they surrendered on 1–2 May. The long Red caravans included women and children, who experienced a desperate, chaotic escape with severe losses due to White attacks. The scene was described as a "road of tears" for the Reds, but for the Whites, the sight of long, enemy caravans heading east was a victorious moment. The Red Guards' last strongholds between the Kouvola and Kotka area fell by 5 May, after the Battle of Ahvenkoski. The war of 1918 ended on 15 May 1918, when the Whites took over Fort Ino, a Russian coastal artillery base on the Karelian Isthmus, from the Russian troops. White Finland and General Mannerheim celebrated the victory with a large military parade in Helsinki on 16 May 1918.
The Red Guards had been defeated. The initially pacifist Finnish labour movement had lost the Civil War, several military leaders committed suicide and a majority of the Reds were sent to prison camps. The Vaasa Senate returned to Helsinki on 4 May 1918, but the capital was under the control of the German army. White Finland had become a protectorate of the German Empire and General Rüdiger von der Goltz was called "the true Regent of Finland". No armistice or peace negotiations were carried out between the Whites and Reds and an official peace treaty to end the Finnish Civil War was never signed.
The White Army and German troops captured around 80,000 Red prisoners, including 5,000 women, 1,500 children and 8,000 Russians. The largest prison camps were Suomenlinna (an island facing Helsinki), Hämeenlinna, Lahti, Riihimäki, Tammisaari, Tampere and Vyborg. The Senate decided to keep the POWs detained until each individual's role in the Civil War had been investigated. Legislation making provision for a Treason Court (; ) was enacted on 29 May 1918. The judicature of the 145 inferior courts led by the Supreme Treason Court (; ) did not meet the standards of impartiality, due to the condemnatory atmosphere of White Finland. In total 76,000 cases were examined and 68,000 Reds were convicted, primarily for treason; 39,000 were released on parole while the mean-length of punishment for the rest was two to four years in jail. 555 people were sentenced to death, of whom 113 were executed. The trials revealed that some innocent adults had been imprisoned.
Combined with the severe food shortages caused by the Civil War, mass imprisonment led to high mortality rates in the prison camps, and the catastrophe was compounded by the angry, punitive and uncaring mentality of the victors. Many prisoners felt that they had been abandoned by their own leaders, who had fled to Russia. The physical and mental condition of the POWs declined in May 1918. Many prisoners had been sent to the camps in Tampere and Helsinki in the first half of April and food supplies were disrupted during the Reds' eastward retreat. Consequently, in June 2,900 prisoners starved to death, or died as a result of diseases caused by malnutrition or the Spanish flu: 5,000 in July; 2,200 in August; and 1,000 in September. The mortality rate was highest in the Tammisaari camp at 34 percent, while the rate varied between 5 percent and 20 percent in the others. In total, around 12,500 Finns perished (3,000–4,000 due to the Spanish flu) while detained. The dead were buried in mass graves near the camps. Moreover, 700 severely weakened POWs died soon after release from the camps.
Most POWs were paroled or pardoned by the end of 1918, after a shift in the political situation. There were 6,100 Red prisoners left at the end of the year and 4,000 at the end of 1919. In January 1920, 3,000 POWs were pardoned and civil rights were returned to 40,000 former Reds. In 1927, the Social Democratic Party government led by Väinö Tanner pardoned the last 50 prisoners. The Finnish government paid reparations to 11,600 POWs in 1973. The traumatic hardships of the prison camps increased support for communism in Finland.
The Civil War was a catastrophe for Finland: around 36,000 people – 1.2 percent of the population – perished. The war left approximately 15,000 children orphaned. Most of the casualties occurred outside the battlefields: in the prison camps and the terror campaigns. Many Reds fled to Russia at the end of the war and during the period that followed. The fear, bitterness and trauma caused by the war deepened the divisions within Finnish society and many moderate Finns identified themselves as "citizens of two nations."
The conflict caused disintegration within both socialist and non-socialist factions. The rightward shift of power caused a dispute between conservatives and liberals on the best system of government for Finland to adopt: the former demanded monarchy and restricted parliamentarianism; the latter demanded a democratic republic. Both sides justified their views on political and legal grounds. The monarchists leaned on the Swedish regime's 1772 monarchist constitution (accepted by Russia in 1809), belittled the Declaration of Independence of 1917, and proposed a modernised, monarchist constitution for Finland. The republicans argued that the 1772 law lost validity in the February Revolution, that the authority of the Russian czar was assumed by the Finnish Parliament on 15 November 1917, and that the Republic of Finland had been adopted on 6 December that year. The republicans were able to halt the passage of the monarchists' proposal in Parliament. The royalists responded by applying the 1772 law to select a new monarch for the country without reference to Parliament.
The Finnish labour movement was divided into three parts: moderate social democrats in Finland; radical socialists in Finland; and communists in Soviet Russia. The Social Democratic Party had its first official party meeting after the Civil War on 25 December 1918, at which the party proclaimed a commitment to parliamentary means and disavowed Bolshevism and communism. The leaders of Red Finland, who had fled to Russia, established the Communist Party of Finland in Moscow on 29 August 1918. After the power struggle of 1917 and the bloody civil war, the former Fennomans and the social democrats who had supported "ultra-democratic" means in Red Finland declared a commitment to revolutionary Bolshevism–communism and to the dictatorship of the proletariat, under the control of Lenin.
In May 1918, a conservative-monarchist Senate was formed by J. K. Paasikivi, and the Senate asked the German troops to remain in Finland. 3 March 1918 Treaty of Brest-Litovsk and 7 March German-Finnish agreements bound White Finland to the German Empire's sphere of influence. General Mannerheim resigned his post on 25 May after disagreements with the Senate about German hegemony over Finland, and about his planned attack on Petrograd to repulse the Bolsheviks and capture Russian Karelia. The Germans opposed these plans due to their peace treaties with Lenin. The Civil War weakened the Finnish Parliament; it became a Rump Parliament that included only three socialist representatives.
On 9 October 1918, under pressure from Germany, the Senate and Parliament elected a German prince, Friedrich Karl, the brother-in-law of German Emperor William II, to become the King of Finland. The German leadership was able to utilise the breakdown of Russia for the geopolitical benefit of the German Empire in Fennoscandia also. The Civil War and the aftermath diminished independence of Finland, compared to the status it had held at the turn of the year 1917–1918.
The economic condition of Finland deteriorated drastically from 1918; recovery to pre-conflict levels was achieved only in 1925. The most acute crisis was in food supply, already deficient in 1917, though large-scale starvation had been avoided that year. The Civil War caused marked starvation in southern Finland. Late in 1918, Finnish politician Rudolf Holsti appealed for relief to Herbert Hoover, the American chairman of the Committee for Relief in Belgium. Hoover arranged for the delivery of food shipments and persuaded the Allies to relax their blockade of the Baltic Sea, which had obstructed food supplies to Finland, and to allow food into the country.
On 15 March 1917, the fate of Finns had been decided outside Finland, in Petrograd. On 11 November 1918, the future of the nation was determined in Berlin, as a result of Germany's surrender to end World War I. The German Empire collapsed in the German Revolution of 1918–19, caused by lack of food, war-weariness and defeat in the battles of the Western Front. General Rüdiger von der Goltz and his division left Helsinki on 16 December 1918, and Prince Friedrich Karl, who had not yet been crowned, abandoned his role four days later. Finland's status shifted from a monarchist protectorate of the German Empire to an independent republic. The new system of government was confirmed by the Constitution Act (; ) on 17 July 1919.
The first local elections based on universal suffrage in Finland were held during 17–28 December 1918, and the first free parliamentary election took place after the Civil War on 3 March 1919. The United States and the United Kingdom recognised Finnish sovereignty on 6–7 May 1919. The Western powers demanded the establishment of democratic republics in post-war Europe, to lure the masses away from widespread revolutionary movements. The Finno–Russian Treaty of Tartu was signed on 14 October 1920, with the aim of stabilizing political relations between Finland and Russia and settling the border question.
In April 1918, the leading Finnish social liberal and the eventual first President of Finland, Kaarlo Juho Ståhlberg wrote: "It is urgent to get the life and development in this country back on the path that we had already reached in 1906 and which the turmoil of war turned us away from." Moderate social democrat Väinö Voionmaa agonised in 1919: "Those who still trust in the future of this nation must have an exceptionally strong faith. This young independent country has lost almost everything due to the war." Voionmaa was a vital companion for the leader of the reformed Social Democratic Party, Väinö Tanner.
Santeri Alkio supported moderate politics. His party colleague, Kyösti Kallio urged in his Nivala address of 5 May 1918: "We must rebuild a Finnish nation, which is not divided into the Reds and Whites. We have to establish a democratic Finnish republic, where all the Finns can feel that we are true citizens and members of this society." In the end, many of the moderate Finnish conservatives followed the thinking of National Coalition Party member Lauri Ingman, who wrote in early 1918: "A political turn more to the right will not help us now, instead it would strengthen the support of socialism in this country."
Together with other broad-minded Finns, the new partnership constructed a Finnish compromise which eventually delivered a stable and broad parliamentary democracy. The compromise was based both on the defeat of the Reds in the Civil War and the fact that most of the Whites' political goals had not been achieved. After foreign forces left Finland, the militant factions of the Reds and the Whites lost their backing, while the pre-1918 cultural and national integrity and the legacy of Fennomania stood out among the Finns.
The weakness of both Germany and Russia after World War I empowered Finland and made a peaceful, domestic Finnish social and political settlement possible. A reconciliation process led to a slow and painful, but steady, national unification. In the end, the power vacuum and interregnum of 1917–1919 gave way to the Finnish compromise. From 1919 to 1991, the democracy and sovereignty of the Finns withstood challenges from right-wing and left-wing political radicalism, the crisis of World War II and pressure from the Soviet Union during the Cold War.
Despite the fact that the Civil War was one of the most sensitive and controversial topics more than a hundred years later in Finland, still between 1918 and the 1950s, mainstream literature and poetry presented the 1918 war from the White victors' point of view, with works such as the "Psalm of the Cannons" () by Arvi Järventaus in 1918. In poetry, Bertel Gripenberg, who had volunteered for the White Army, celebrated its cause in "The Great Age" () in 1928 and V. A. Koskenniemi in "Young Anthony" () in 1918. The war tales of the Reds were kept silent.
The first neutrally critical books were written soon after the war, notably, "Devout Misery" () written by the Nobel Prize laureate Frans Emil Sillanpää in 1919; "Dead Apple Trees" () by Joel Lehtonen in 1918; and "Homecoming" () by Runar Schildt in 1919. These were followed by Jarl Hemmer in 1931 with the book "A Man and His Conscience" () and Oiva Paloheimo in 1942 with "Restless Childhood" (). Lauri Viita's book "Scrambled Ground" () from 1950 presented the life and experiences of a worker family in the Tampere of 1918, including a point of view from outsiders to the Civil War.
Between 1959 and 1962, Väinö Linna described in his trilogy "Under the North Star" () the Civil War and World War II from the viewpoint of the common people. Part II of Linna's work opened a larger view of these events and included tales of the Reds in the 1918 war. At the same time, a new outlook on the war was opened by Paavo Haavikko's book "Private Matters" (), Veijo Meri's "The Events of 1918" () and Paavo Rintala's "My Grandmother and Mannerheim" (), all published in 1960. In poetry, Viljo Kajava, who had experienced the Battle of Tampere at the age of nine, presented a pacifist view of the Civil War in his "Poems of Tampere" () in 1966. The same battle is described in the novel "Corpse Bearer" () by Antti Tuuri from 2007. Jenni Linturi's multilayered "Malmi 1917" (2013) describes contradictory emotions and attitudes in a village drifting towards civil war.
Väinö Linna's trilogy turned the general tide, and after it, several books were written mainly from the Red viewpoint: The Tampere-trilogy by Erkki Lepokorpi in 1977; Juhani Syrjä's "Juho 18" in 1998; "The Command" () by Leena Lander in 2003; and "Sandra" by Heidi Köngäs in 2017. Kjell Westö's epic novel "Where We Once Went" (), published in 2006, deals with the period of 1915–1930 from both the Red and the White sides. Westö's book "Mirage 38" () from 2013, describes post-war traumas of the 1918 war and Finnish mentality in the 1930s. Many of the stories have been utilised in motion pictures and in theatre.
The Civil War and the literature about it has inspired many Finnish filmmakers to take it the subject for the film and television adaptations. As early as 1957, "1918", a film directed by Toivo Särkkä and based on Jarl Hemmer's play and novel "A Man and His Conscience", was screened at the 7th Berlin International Film Festival. The most recent films about the civil war include the 2007 film "The Border", directed by Lauri Törhönen, and the 2008 film "Tears of April", directed by Aku Louhimies and based on Leena Lander's novel "The Command". However, perhaps the most famous film about the Finnish Civil War is the 1968 film "Here, Beneath the North Star", directed by Edvin Laine and based on the first two books of Väinö Linna's "Under the North Star" trilogy.
In 2012, the dramatized documentary "Dead or Alive 1918" (or "The Battle of Näsilinna 1918"; ) was made, which tells the story of the Battle of Tampere during the Civil War. Other noteworthy documentary-styled films about the Finnish Civil War include "" from 1973, "Trust" from 1976, and "Flame Top" from 1980.

</doc>
<doc id="11773" url="https://en.wikipedia.org/wiki?curid=11773" title="Flynn effect">
Flynn effect

The Flynn effect is the substantial and long-sustained increase in both fluid and crystallized intelligence test scores that were measured in many parts of the world over the 20th century. When intelligence quotient (IQ) tests are initially standardized using a sample of test-takers, by convention the average of the test results is set to 100 and their standard deviation is set to 15 or 16 IQ points. When IQ tests are revised, they are again standardized using a new sample of test-takers, usually born more recently than the first. Again, the average result is set to 100. However, when the new test subjects take the older tests, in almost every case their average scores are significantly above 100.
Test score increases have been continuous and approximately linear from the earliest years of testing to the present. For the Raven's Progressive Matrices test, a study published in the year 2009 found that British children's average scores rose by 14 IQ points from 1942 to 2008. Similar gains have been observed in many other countries in which IQ testing has long been widely used, including other Western European countries, Japan, and South Korea.
There are numerous proposed explanations of the Flynn effect, as well as some skepticism about its implications. Similar improvements have been reported for other cognitions such as semantic and episodic memory. Research suggests that there is an ongoing reversed Flynn effect, i.e. a decline in IQ scores, in Norway, Denmark, Australia, Britain, the Netherlands, Sweden, Finland, France and German-speaking countries, a development which appears to have started in the 1990s.
The Flynn effect is named for James R. Flynn, who did much to document it and promote awareness of its implications. The term itself was coined by Richard Herrnstein and Charles Murray, authors of "The Bell Curve". Although the general term for the phenomenon—referring to no researcher in particular—continues to be "secular rise in IQ scores", many textbooks on psychology and IQ testing have now followed the lead of Herrnstein and Murray in calling the phenomenon the Flynn effect.
IQ tests are updated periodically. For example, the Wechsler Intelligence Scale for Children (WISC), originally developed in 1949, was updated in 1974, 1991, 2003 and again in 2014. The revised versions are standardized based on the performance of test-takers in standardization samples. A standard score of IQ 100 is defined as the median performance of the standardization sample. Thus one way to see changes in norms over time is to conduct a study in which the same test-takers take both an old and new version of the same test. Doing so confirms IQ gains over time. Some IQ tests, for example tests used for military draftees in NATO countries in Europe, report raw scores, and those also confirm a trend of rising scores over time. The average rate of increase seems to be about three IQ points per decade in the United States, as scaled by the Wechsler tests. The increasing test performance over time appears on every major test, in every age range, at every ability level, and in every modern industrialized country, although not necessarily at the same rate as in the United States. The increase was continuous and roughly linear from the earliest days of testing to the mid-1990s. Though the effect is most associated with IQ increases, a similar effect has been found with increases in attention and of semantic and episodic memory.
Ulric Neisser estimated that using the IQ values of 1997, the average IQ of the United States in 1932, according to the first Stanford–Binet Intelligence Scales standardization sample, was 80. Neisser states that "Hardly any of them would have scored 'very superior', but nearly one-quarter would have appeared to be 'deficient.'" He also wrote that "Test scores are certainly going up all over the world, but whether intelligence itself has risen remains controversial."
Trahan et al. (2014) found that the effect was about 2.93 points per decade, based on both Stanford–Binet and Wechsler tests; they also found no evidence the effect was diminishing. In contrast, Pietschnig and Voracek (2015) reported, in their meta-analysis of studies involving nearly 4 million participants, that the Flynn effect had decreased in recent decades. They also reported that the magnitude of the effect was different for different types of intelligence ("0.41, 0.30, 0.28, and 0.21 IQ points annually for fluid, spatial, full-scale, and crystallized IQ test performance, respectively"), and that the effect was stronger for adults than for children.
Raven (2000) found that, as Flynn suggested, data interpreted as showing a decrease in many abilities with increasing age must be re-interpreted as showing that there has been a dramatic increase of these abilities with date of birth. On many tests this occurs at all levels of ability.
Some studies have found the gains of the Flynn effect to be particularly concentrated at the lower end of the distribution. Teasdale and Owen (1989), for example, found the effect primarily reduced the number of low-end scores, resulting in an increased number of moderately high scores, with no increase in very high scores. In another study, two large samples of Spanish children were assessed with a 30-year gap. Comparison of the IQ distributions indicated that the mean IQ scores on the test had increased by 9.7 points (the Flynn effect), the gains were concentrated in the lower half of the distribution and negligible in the top half, and the gains gradually decreased as the IQ of the individuals increased. Some studies have found a reverse Flynn effect with declining scores for those with high IQ.
In 1987, Flynn took the position that the very large increase indicates that IQ tests do not measure intelligence but only a minor sort of "abstract problem-solving ability" with little practical significance. He argued that if IQ gains do reflect intelligence increases, there would have been consequent changes of our society that have not been observed (a presumed non-occurrence of a "cultural renaissance"). Flynn no longer endorses this view of intelligence and has since elaborated and refined his view of what rising IQ scores mean.
Earlier investigators had discovered rises in raw IQ test scores in some study populations, but had not published general investigations of that issue in particular. Historian Daniel C. Calhoun cited earlier psychology literature on IQ score trends in his book "The Intelligence of a People" (1973). R. L. Thorndike drew attention to rises in Stanford-Binet scores in a 1975 review of the history of intelligence testing.
There is debate about whether the rise in IQ scores also corresponds to a rise in general intelligence, or only a rise in special skills related to taking IQ tests. Because children attend school longer now and have become much more familiar with the testing of school-related material, one might expect the greatest gains to occur on such school content-related tests as vocabulary, arithmetic or general information. Just the opposite is the case: abilities such as these have experienced relatively small gains and even occasional decreases over the years. Meta-analytic findings indicate that Flynn effects occur for tests assessing both fluid and crystallized abilities. For example, Dutch conscripts gained 21 points during only 30 years, or 7 points per decade, between 1952 and 1982. But this rise in IQ test scores is not wholly explained by an increase in general intelligence. Studies have shown that while test scores have improved over time, the improvement is not fully correlated with latent factors related to intelligence. Rushton has shown that the gains in IQ over time (the Lynn-Flynn effect) are unrelated to "g". Other researchers have shown that the IQ gains described by the Flynn effect are due in part to increasing intelligence, and in part to increases in test-specific skills. In parallel with the measured gains in IQ scores, secular declines have been found for "mental speed, digit span backwards, the use of difficult words, and color acuity, all of which are related to intelligence".
A 2017 survey of 75 experts in the field of intelligence research suggested four key causes of the Flynn effect: Better health, better nutrition, more and better education, and rising standards of living. Genetic changes were seen as not important. The experts' views agreed with an independently performed meta-analysis on published Flynn effect data, except that the latter found life history speed to be the most important factor.
The expert survey explained the possible end or decline in the Flynn effect by asymmetric fertility by means of genetic effects, migration, asymmetric fertility by means of socialization effects, declines in education, and the influence of media.
Duration of average schooling has increased steadily. One problem with this explanation is that if in the US comparing older and more recent subjects with similar educational levels, then the IQ gains appear almost undiminished in each such group considered individually.
Many studies find that children who do not attend school score drastically lower on the tests than their regularly attending peers. During the 1960s, when some Virginia counties closed their public schools to avoid racial integration, compensatory private schooling was available only for Caucasian children. On average, the scores of African-American children who received no formal education during that period decreased at a rate of about six IQ points per year.
Another explanation is an increased familiarity of the general population with tests and testing. For example, children who take the very same IQ test a second time usually gain five or six points. However, this seems to set an upper limit on the effects of test sophistication. One problem with this explanation and others related to schooling is that in the US, the groups with greater test familiarity show smaller IQ increases.
Early intervention programs have shown mixed results. Some preschool (ages 3–4) intervention programs like "Head Start" do not produce lasting changes of IQ, although they may confer other benefits. The "Abecedarian Early Intervention Project", an all-day program that provided various forms of environmental enrichment to children from infancy onward, showed IQ gains that did not diminish over time. The IQ difference between the groups, although only five points, was still present at age 12. Not all such projects have been successful. Also, such IQ gains can diminish until age 18.
Citing a high correlation between rising literacy rates and gains in IQ, David Marks has argued that the Flynn effect is caused by changes in literacy rates.
Still another theory is that the general environment today is much more complex and stimulating. One of the most striking 20th-century changes of the human intellectual environment has come from the increase of exposure to many types of visual media. From pictures on the wall to movies to television to video games to computers, each successive generation has been exposed to richer optical displays than the one before and may have become more adept at visual analysis. This would explain why visual tests like the Raven's have shown the greatest increases. An increase only of particular forms of intelligence would explain why the Flynn effect has not caused a "cultural renaissance too great to be overlooked."
In 2001, Dickens and Flynn presented a model for resolving several contradictory findings regarding IQ. They argue that the measure "heritability" includes both a direct effect of the genotype on IQ and also indirect effects such that the genotype changes the environment, thereby affecting IQ. That is, those with a greater IQ tend to seek stimulating environments that further increase IQ. These reciprocal effects result in gene environment correlation. The direct effect could initially have been very small, but feedback can create large differences of IQ. In their model, an environmental stimulus can have a very great effect on IQ, even for adults, but this effect also decays over time unless the stimulus continues (the model could be adapted to include possible factors, like nutrition during early childhood, that may cause permanent effects). The Flynn effect can be explained by a generally more stimulating environment for all people. The authors suggest that any program designed to increase IQ may produce long-term IQ gains if that program teaches children how to replicate the types of cognitively demanding experiences that produce IQ gains outside the program. To maximize lifetime IQ, the programs should also motivate them to continue searching for cognitively demanding experiences after they have left the program.
Flynn in his 2007 book "What Is Intelligence?" further expanded on this theory. Environmental changes resulting from modernization—such as more intellectually demanding work, greater use of technology and smaller families—have meant that a much larger proportion of people are more accustomed to manipulating abstract concepts such as hypotheses and categories than a century ago. Substantial portions of IQ tests deal with these abilities. Flynn gives, as an example, the question 'What do a dog and a rabbit have in common?' A modern respondent might say they are both mammals (an abstract, or "a priori" answer, which depends only on the meanings of the words "dog" and "rabbit"), whereas someone a century ago might have said that humans catch rabbits with dogs (a concrete, or "a posteriori" answer, which depended on what happened to be the case at that time).
Improved nutrition is another possible explanation. Today's average adult from an industrialized nation is taller than a comparable adult of a century ago. That increase of stature, likely the result of general improvements of nutrition and health, has been at a rate of more than a centimeter per decade. Available data suggest that these gains have been accompanied by analogous increases of head size, and by an increase in the average size of the brain. This argument had been thought to suffer the difficulty that groups who tend to be of smaller overall body size (e.g. women, or people of Asian ancestry) do not have lower average IQs.
A 2005 study presented data supporting the nutrition hypothesis, which predicts that gains will occur predominantly at the low end of the IQ distribution, where nutritional deprivation is probably most severe. An alternative interpretation of skewed IQ gains could be that improved education has been particularly important for this group. Richard Lynn makes the case for nutrition, arguing that cultural factors cannot typically explain the Flynn effect because its gains are observed even at infant and preschool levels, with rates of IQ test score increase about equal to those of school students and adults. Lynn states that "This rules out improvements in education, greater test sophistication, etc. and most of the other factors that have been proposed to explain the Flynn effect. He proposes that the most probable factor has been improvements in pre-natal and early post-natal nutrition."
A century ago, nutritional deficiencies may have limited body and organ functionality, including skull volume. The first two years of life is a critical time for nutrition. The consequences of malnutrition can be irreversible and may include poor cognitive development, educability, and future economic productivity. On the other hand, Flynn has pointed to 20-point gains on Dutch military (Raven's type) IQ tests between 1952, 1962, 1972, and 1982. He observes that the Dutch 18-year-olds of 1962 had a major nutritional handicap. They were either in the womb, or were recently born, during the great Dutch famine of 1944—when German troops monopolized food and 18,000 people died of starvation. Yet, concludes Flynn, "they do not show up even as a blip in the pattern of Dutch IQ gains. It is as if the famine had never occurred." It appears that the effects of diet are gradual, taking effect over decades (affecting mother as well as child) rather than a few months.
In support of the nutritional hypothesis, it is known that, in the United States, the average height before 1900 was about 10 cm (∼4 inches) shorter than it is today. Possibly related to the Flynn effect is a similar change of skull size and shape during the last 150 years. Though the idea that brain size is unrelated to race and intelligence was popularized in the 1980s, studies continue to show significant correlations.
A Norwegian study found that height gains were strongly correlated with intelligence gains until the cessation of height gains in military conscript cohorts towards the end of the 1980s.<ref name="doi10.1016/j.intell.2004.06.004"></ref> Both height and skull size increases probably result from a combination of phenotypic plasticity and genetic selection over this period. With only five or six human generations in 150 years, time for natural selection has been very limited, suggesting that increased skeletal size resulting from changes in population phenotypes is more likely than recent genetic evolution.
It is well known that micronutrient deficiencies change the development of intelligence. For instance, one study has found that iodine deficiency causes a fall, on average, of 12 IQ points in China.
Scientists James Feyrer, Dimitra Politi, and David N. Weil have found in the U.S. that the proliferation of iodized salt increased IQ by 15 points in some areas. Journalist Max Nisen has stated that, with this type of salt becoming popular, that "the aggregate effect has been extremely positive."
Daley et al. (2003) found a significant Flynn effect among children in rural Kenya, and concluded that nutrition was one of the hypothesized explanations that best explained their results (the others were parental literacy and family structure).
Eppig, Fincher, and Thornhill (2009) argue that "From an energetics standpoint, a developing human will have difficulty building a brain and fighting off infectious diseases at the same time, as both are very metabolically costly tasks" and that "the Flynn effect may be caused in part by the decrease in the intensity of infectious diseases as nations develop." They suggest that improvements in gross domestic product (GDP), education, literacy, and nutrition may have an effect on IQ mainly through reducing the intensity of infectious diseases.
Eppig, Fincher, and Thornhill (2011) in a similar study instead looking at different US states found that states with a higher prevalence of infectious diseases had lower average IQ. The effect remained after controlling for the effects of wealth and educational variation.
Atheendar Venkataramani (2010) studied the effect of malaria on IQ in a sample of Mexicans. Malaria eradication during the birth year was associated with increases in IQ. It also increased the probability of employment in a skilled occupation. The author suggests that this may be one explanation for the Flynn effect and that this may be an important explanation for the link between national malaria burden and economic development. A literature review of 44 papers states that cognitive abilities and school performance were shown to be impaired in sub-groups of patients (with either cerebral malaria or uncomplicated malaria) when compared with healthy controls. Studies comparing cognitive functions before and after treatment for acute malarial illness continued to show significantly impaired school performance and cognitive abilities even after recovery. Malaria prophylaxis was shown to improve cognitive function and school performance in clinical trials when compared to placebo groups.
Heterosis, or hybrid vigor associated with historical reductions of the levels of inbreeding, has been proposed by Michael Mingroni as an alternative explanation of the Flynn effect. However, James Flynn has pointed out that even if everyone mated with a sibling in 1900, subsequent increases in heterosis would not be a sufficient explanation of the observed IQ gains.
Jon Martin Sundet and colleagues (2004) examined scores on intelligence tests given to Norwegian conscripts between the 1950s and 2002. They found that the increase of scores of general intelligence stopped after the mid-1990s and declined in numerical reasoning sub-tests.
Teasdale and Owen (2005) examined the results of IQ tests given to Danish male conscripts. Between 1959 and 1979 the gains were 3 points per decade. Between 1979 and 1989 the increase approached 2 IQ points. Between 1989 and 1998 the gain was about 1.3 points. Between 1998 and 2004 IQ declined by about the same amount as it gained between 1989 and 1998. They speculate that "a contributing factor in this recent fall could be a simultaneous decline in proportions of students entering 3-year advanced-level school programs for 16–18-year-olds." The same authors in a more comprehensive 2008 study, again on Danish male conscripts, found that there was a 1.5-point increase between 1988 and 1998, but a 1.5-point decrease between 1998 and 2003/2004. A possible contributing factor to the more recent decline may be changes in the Danish educational system. Another may be the rising proportion of immigrants or their immediate descendants in Denmark. This is supported by data on Danish draftees where first or second generation immigrants with Danish nationality score below average.
In Australia, the IQ of 6–12 year olds as measured by the Colored Progressive Matrices has shown no increase from 1975–2003.
In the United Kingdom, a study by Flynn (2009) found that tests carried out in 1980 and again in 2008 show that the IQ score of an average 14-year-old dropped by more than two points over the period. For the upper half of the results the performance was even worse. Average IQ scores declined by six points. However, children aged between five and 10 saw their IQs increase by up to half a point a year over the three decades. Flynn argues that the abnormal drop in British teenage IQ could be due to youth culture having "stagnated" or even dumbed down. He also states that the youth culture is more oriented towards computer games than towards reading and holding conversations. Researcher Richard Gray, commenting on the study, also mentions the computer culture diminishing reading books as well as a tendency towards teaching to the test.
Lynn and Harvey argued in 2008 that the causes of the above are difficult to interpret since these countries had had significant recent immigration from countries with lower average national IQs. Nevertheless, they expect that similar patterns will occur, or have occurred, first in other developed nations and then in the developing world as there is a limit to how much environmental factors can improve intelligence. Furthermore, during the last century there is a negative correlation between fertility and intelligence although there is not yet any conclusive evidence of the association between the two. They estimate that there has been a dysgenic decline in the world's genotypic IQ (masked by the Flynn effect for the phenotype) of 0.86 IQ points per decade for the years 1950–2000. 
Stefansson et al. (2017) similarly argue for a decline in polygenic scores pertaining to educational attainment in Icelandic individuals born from 1910 to 1990, stating that it could be declining at a rate that is two to three times faster that the captured rate of 0.30 IQ points per decade. 
Bratsberg & Rogeberg (2018) present evidence that the Flynn effect in Norway has reversed, and that both the original rise in mean IQ scores and their subsequent decline were caused by environmental factors. They conclude that environmental factors explain all or almost all of the decline, and the hypothesised declines in genotypic IQ is negligible, although they "cannot rule out the theoretical possibility of negative selection on a genetic component that is masked when assessed using environmentally influenced measures", not being able to rule out the decline posited by Stefansson et al.
One possible explanation of a worldwide decline in intelligence, suggested by the World Health Organization and the Forum of International Respiratory Societies' Environmental Committee, is an increase in air pollution, which now affects over 90% of the world's population.
If the Flynn effect has ended in developed nations, then this may possibly allow national differences in IQ scores to diminish if the Flynn effect continues in nations with lower average national IQs.
Also, if the Flynn effect has ended for the majority in developed nations, it may still continue for minorities, especially for groups like immigrants where many may have received poor nutrition during early childhood or have had other disadvantages. A study in the Netherlands found that children of non-Western immigrants had improvements for "g", educational achievements, and work proficiency compared to their parents, although there were still remaining differences compared to ethnic Dutch.
There is a controversy as to whether the US racial gap in IQ scores is diminishing. If that is the case then this may or may not be related to the Flynn effect. Flynn has commented that he never claimed that the Flynn effect has the same causes as the black-white gap, but that it shows that environmental factors can create IQ differences of a magnitude similar to the gap. Research that has examined whether g factor and IQ gains from the Flynn effect are related have found there is a negative correlation between the two, which may indicate that group differences and the Flynn effect are possibly due to differing causes.
The Flynn effect has also been part of the discussions regarding Spearman's hypothesis, which states that differences in the g factor are the major source of differences between blacks and whites observed in many studies of race and intelligence.

</doc>
<doc id="11774" url="https://en.wikipedia.org/wiki?curid=11774" title="Field ion microscope">
Field ion microscope

The Field ion microscope (FIM) was invented by Müller in 1951. It is a type of microscope that can be used to image the arrangement of atoms at the surface of a sharp metal tip.
On October 11, 1955, Erwin Müller and his Ph.D. student, Kanwar Bahadur (Pennsylvania State University) observed individual tungsten atoms on the surface of a sharply pointed tungsten tip by cooling it to 21 K and employing helium as the imaging gas. Müller & Bahadur were the first persons to observe individual atoms directly.
In FIM, a sharp (<50 nm tip radius) metal tip is produced and placed in an ultra high vacuum chamber, which is backfilled with an imaging gas such as helium or neon. The tip is cooled to cryogenic temperatures (20–100 K). A positive voltage of 5 to 10 kilovolts is applied to the tip. Gas atoms adsorbed on the tip are ionized by the strong electric field in the vicinity of the tip (thus, "field ionization"), becoming positively charged and being repelled from the tip. The curvature of the surface near the tip causes a natural magnification — ions are repelled in a direction roughly perpendicular to the surface (a "point projection" effect). A detector is placed so as to collect these repelled ions; the image formed from all the collected ions can be of sufficient resolution to image individual atoms on the tip surface.
Unlike conventional microscopes, where the spatial resolution is limited by the wavelength of the particles which are used for imaging, the FIM is a projection type microscope with atomic resolution and an approximate magnification of a few million times.
FIM like Field Emission Microscopy (FEM) consists of a sharp sample tip and a fluorescent screen (now replaced by a multichannel plate) as the key elements. However, there are some essential differences as follows:
Like FEM, the field strength at the tip apex is typically a few V/Å. The experimental set-up and image formation in FIM is illustrated in the accompanying figures.
In FIM the presence of a strong field is critical. The imaging gas atoms (He, Ne) near the tip are polarized by the field and since the field is non-uniform the polarized atoms are attracted towards the tip surface. The imaging atoms then lose their kinetic energy performing a series of hops and accommodate to the tip temperature. Eventually, the imaging atoms are ionized by tunneling electrons into the surface and the resulting positive ions are accelerated along the field lines to the screen to form a highly magnified image of the sample tip.
In FIM, the ionization takes place close to the tip, where the field is strongest. The electron that tunnels from the atom is picked up by the tip. There is a critical distance, xc, at which the tunneling probability is a maximum. This distance is typically about 0.4 nm. The very high spatial resolution and high contrast for features on the atomic scale arises from the fact that the electric field is enhanced in the vicinity of the surface atoms because of the higher local curvature. The resolution of FIM is limited by the thermal velocity of the imaging ion. Resolution of the order of 1Å (atomic resolution) can be achieved by effective cooling of the tip.
Application of FIM, like FEM, is limited by the materials which can be fabricated in the shape of a sharp tip, can be used in an ultra high vacuum (UHV) environment, and can tolerate the high electrostatic fields. For these reasons, refractory metals with high melting temperature (e.g. W, Mo, Pt, Ir) are conventional objects for FIM experiments. Metal tips for FEM and FIM are prepared by electropolishing (electrochemical polishing) of thin wires. However, these tips usually contain many asperities. The final preparation procedure involves the in situ removal of these asperities by field evaporation just by raising the tip voltage. Field evaporation is a field induced process which involves the removal of atoms from the surface itself at very high field strengths and typically occurs in the range 2-5 V/Å. The effect of the field in this case is to reduce the effective binding energy of the atom to the surface and to give, in effect, a greatly increased evaporation rate relative to that expected at that temperature at zero fields. This process is self-regulating since the atoms that are at positions of high local curvature, such as adatoms or ledge atoms, are removed preferentially. The tips used in FIM is sharper (tip radius is 100~300 Å) compared to those used in FEM experiments (tip radius ~1000 Å).
FIM has been used to study dynamical behavior of surfaces and the behavior of adatoms on surfaces. The problems studied include adsorption-desorption phenomena, surface diffusion of adatoms and clusters, adatom-adatom interactions, step motion, equilibrium crystal shape, etc. However, there is the possibility of the results being affected by the limited surface area (i.e. edge effects) and by the presence of large electric field.

</doc>
<doc id="11775" url="https://en.wikipedia.org/wiki?curid=11775" title="First Battle of El Alamein">
First Battle of El Alamein

The First Battle of El Alamein (1–27 July 1942) was a battle of the Western Desert Campaign of the Second World War, fought in Egypt between Axis forces (Germany and Italy) of the Panzer Army Africa () (which included the under Field Marshal () Erwin Rommel) and Allied (British Imperial and Commonwealth) forces (Britain, British India, Australia, South Africa and New Zealand) of the Eighth Army (General Claude Auchinleck).
The British prevented a second advance by the Axis forces into Egypt. Axis positions near El Alamein, only from Alexandria, were dangerously close to the ports and cities of Egypt, the base facilities of the Commonwealth forces and the Suez Canal. However, the Axis forces were too far from their base at Tripoli in Libya to remain at El Alamein indefinitely, which led both sides to accumulate supplies for more offensives, against the constraints of time and distance.
Following their defeat at the Battle of Gazala in Eastern Libya in June 1942, the British Eighth Army, commanded by Lieutenant-General Neil Ritchie, had retreated east from the Gazala line into north-western Egypt as far as Mersa Matruh, roughly inside the border. Ritchie had decided not to hold the defences on the Egyptian border, because the defensive plan there was for infantry to hold defended localities and a strong armoured force behind them to meet any attempts to penetrate or outflank the fixed defences. Since General Ritchie had virtually no armoured units left fit to fight, the infantry positions would be defeated in detail. The Mersa defence plan also included an armoured reserve but in its absence Ritchie believed he could organise his infantry to cover the minefields between the defended localities to prevent Axis engineers from having undisturbed access.
To defend the Matruh line, Ritchie placed 10th Indian Infantry Division (in Matruh itself) and 50th (Northumbrian) Infantry Division (some down the coast at Gerawla) under X Corps HQ, newly arrived from Syria. Inland from X Corps would be XIII Corps with 5th Indian Infantry Division (with only one infantry brigade, 29th Indian, and two artillery regiments) around Sidi Hamza about inland, and the newly arrived 2nd New Zealand Division (short one brigade, the 6th, which had been left out of combat in case the division was captured and it would be needed to serve as the nucleus of a new division) at Minqar Qaim (on the escarpment inland) and 1st Armored Division in the open desert to the south. The 1st Armored Division had taken over 4th and 22nd Armoured Brigades from 7th Armoured Division which by this time had only three tank regiments (battalions) between them.
On 25 June, General Claude Auchinleck—Commander-in-Chief (C-in-C) Middle East Command—relieved Ritchie and assumed direct command of the Eighth Army himself. He decided not to seek a decisive confrontation at the Mersa Matruh position. He concluded that his inferiority in armour after the Gazala defeat, meant he would be unable to prevent Rommel either breaking through his centre or enveloping his open left flank to the south in the same way he had at Gazala. He decided instead to employ delaying tactics while withdrawing a further or more east to a more defensible position near El Alamein on the Mediterranean coast. Only to the south of El Alamein, the steep slopes of the Qattara Depression ruled out the possibility of Axis armour moving around the southern flank of his defences and limited the width of the front he had to defend.
While preparing the Alamein positions, Auchinleck fought strong delaying actions, first at Mersa Matruh on 26–27 June and then Fuka on 28 June. The late change of orders resulted in some confusion in the forward formations (X Corps and XIII Corps) between the desire to inflict damage on the enemy and the intention not to get trapped in the Matruh position but retreat in good order. The result was poor co-ordination between the two forward Corps and units within them. Late on 26 June, the German 90th Light and 21st "Panzer" Divisions managed to find their way through the minefields in the centre of the front. Early on 27 June, resuming its advance, the 90th Light was checked by British 50th Division's artillery. Meanwhile, the 15th and 21st "Panzer" Divisions advanced east above and below the escarpment. The 15th "Panzer" were blocked by 4th Armoured and 7th Motor Brigades, but the 21st "Panzer" were ordered on to attack Minqar Qaim. Rommel ordered 90th Light to resume its advance, requiring it to cut the coast road behind 50th Division by the evening. As the 21st "Panzer" moved on Minqar Qaim, the 2nd New Zealand Division found itself surrounded but broke out on the night of 27/28 June without serious losses and withdrew east.
Auchinleck had planned a second delaying position at Fuka, some east of Matruh, and at 21:20 he issued the orders for a withdrawal to Fuka. Confusion in communication led the division withdrawing immediately to the El Alamein position. X Corps, having made an unsuccessful attempt to secure a position on the escarpment, were out of touch with Eighth Army from 19:30 until 04:30 the next morning. Only then did they discover that the withdrawal order had been given. The withdrawal of XIII Corps had left the southern flank of X Corps on the coast at Matruh exposed and their line of retreat compromised by the cutting of the coastal road east of Matruh. They were ordered to break out southwards into the desert and then make their way east. Auchinleck ordered XIII Corps to provide support but they were in no position to do so. At 21:00 on 28 June, X Corps—organised into brigade groups—headed south. In the darkness, there was considerable confusion as they came across enemy units laagered for the night. In the process, 5th Indian Division in particular sustained heavy casualties, including the destruction of the 29th Indian Infantry Brigade at Fuka. Axis forces captured more than 6,000 prisoners, in addition to 40 tanks and an enormous quantity of supplies.
Alamein itself was an inconsequential railway station on the coast. Some to the south lay the Ruweisat Ridge, a low stony prominence that gave excellent observation for many miles over the surrounding desert; to the south was the Qattara Depression. The line the British chose to defend stretched between the sea and the Depression, which meant that Rommel could outflank it only by taking a significant detour to the south and crossing the Sahara Desert. The British Army in Egypt recognised this before the war and had the Eighth Army begin construction of several "boxes" (localities with dug-outs and surrounded by minefields and barbed wire) the most developed being around the railway station at Alamein. Most of the "line" was open, empty desert. Lieutenant-General William Norrie (General officer commanding [GOC] XXX Corps) organised the position and started to construct three defended "boxes". The first and strongest, at El Alamein on the coast, had been partly wired and mined by 1st South African Division. The Bab el Qattara box—some from the coast and south-west of the Ruweisat Ridge—had been dug but had not been wired or mined, while at the Naq Abu Dweis box (on the edge of the Qattara Depression), from the coast, very little work had been done.
The British position in Egypt was desperate, the rout from Mersa Matruh had created a panic in the British headquarters at Cairo, something later called "the Flap". On what came to be referred to as "Ash Wednesday", at British headquarters, rear echelon units and the British Embassy, papers were hurriedly burned in anticipation of the fall of the city. Auchinleck—although believing he could stop Rommel at Alamein—felt he could not ignore the possibility that he might once more be outmanoeuvred or outfought. To maintain his army, plans must be made for the possibility of a further retreat whilst maintaining morale and retaining the support and co-operation of the Egyptians. Defensive positions were constructed west of Alexandria and on the approaches to Cairo while considerable areas in the Nile delta were flooded. The Axis, too, believed that the capture of Egypt was imminent; Italian leader Benito Mussolini—sensing a historic moment—flew to Libya to prepare for his triumphal entry into Cairo.
The scattering of X Corps at Mersa Matruh disrupted Auchinleck's plan for occupying the Alamein defences. On 29 June, he ordered XXX Corps—the 1st South African, 5th and 10th Indian divisions—to take the coastal sector on the right of the front and XIII Corps—the 2nd New Zealand Division and 4th Indian divisions—to be on the left. The remains of the 1st Armoured Division and the 7th Armoured Division were to be held as a mobile army reserve. His intention was for the fixed defensive positions to channel and disorganise the enemy's advance while mobile units would attack their flanks and rear.
On 30 June, Rommel's "Panzerarmee Afrika" approached the Alamein position. The Axis forces were exhausted and understrength. Rommel had driven them forward ruthlessly, being confident that, provided he struck quickly before Eighth Army had time to settle, his momentum would take him through the Alamein position and he could then advance to the Nile with little further opposition. Supplies remained a problem because the Axis staff had originally expected a pause of six weeks after the capture of Tobruk. German air units were also exhausted and providing little help against the RAF's all-out attack on the Axis supply lines which, with the arrival of United States Army Air Forces (USAAF) heavy bombers, could reach as far as Benghazi. Although captured supplies proved useful, water and ammunition were constantly in short supply, while a shortage of transport impeded the distribution of the supplies that the Axis forces did have.
Rommel's plan was for the 90th Light Division and the 15th and 21st "Panzer" divisions of the "Afrika Korps" to penetrate the Eighth Army lines between the Alamein box and Deir el Abyad (which he believed was defended). The 90th Light Division was then to veer north to cut the coastal road and trap the defenders of the Alamein box (which Rommel thought was occupied by the remains of the 50th Infantry Division) and the "Afrika Korps" would veer right to attack the rear of XIII Corps.
An Italian division was to attack the Alamein box from the west and another was to follow the 90th Light Division. The Italian XX Corps was to follow the "Afrika Korps" and deal with the Qattara box while the 133rd Armoured Division "Littorio" and German reconnaissance units would protect the right flank. Rommel had planned to attack on 30 June but supply and transport difficulties had resulted in a day's delay, vital to the defending forces reorganising on the Alamein line. On 30 June, the 90th Light Division was still short of its start line, 21st "Panzer" Division was immobilised through lack of fuel and the promised air support had yet to move into its advanced airfields.
At 03:00 on 1 July, 90th Light Infantry Division advanced east but strayed too far north and ran into the 1st South African Division's defences and became pinned down. The 15th and 21st "Panzer" Divisions of the "Afrika Korps" were delayed by a sandstorm and then a heavy air attack. It was broad daylight by the time they circled round the back of Deir el Abyad where they found the feature to the east of it occupied by 18th Indian Infantry Brigade which, after a hasty journey from Iraq, had occupied the exposed position just west of Ruweisat Ridge and east of Deir el Abyad at Deir el Shein late on 28 June to create one of Norrie's additional defensive boxes.
At about 10:00 on 1 July, 21st "Panzer" Division attacked Deir el Shein. 18th Indian Infantry Brigade—supported by 23 25-pounder gun-howitzers, 16 of the new 6-pounder anti-tank guns and nine Matilda tanks—held out the whole day in desperate fighting but by evening the Germans succeeded in over-running them. The time they bought allowed Auchinleck to organise the defence of the western end of Ruweisat Ridge. The 1st Armoured Division had been sent to intervene at Deir el Shein. They ran into 15th "Panzer" Division just south of Deir el Shein and drove it west. By the end of the day's fighting, the "Afrika Korps" had 37 tanks left out of its initial complement of 55.
During the early afternoon, 90th Light had extricated itself from the El Alamein box defences and resumed its move eastward. It came under artillery fire from the three South African brigade groups and was forced to dig in.
On 2 July, Rommel ordered the resumption of the offensive. Once again, 90th Light failed to make progress so Rommel called the "Afrika Korps" to abandon its planned sweep southward and instead join the effort to break through to the coast road by attacking east toward Ruweisat Ridge. The British defence of Ruweisat Ridge relied on an improvised formation called "Robcol", comprising a regiment each of field artillery and light anti-aircraft artillery and a company of infantry. Robcol—in line with normal British Army practice for "ad hoc" formations—was named after its commander, Brigadier Robert Waller, the Commander Royal Artillery of the 10th Indian Infantry Division. Robcol was able to buy time, and by late afternoon the two British armoured brigades joined the battle with 4th Armoured Brigade engaging 15th "Panzer" and 22nd Armoured Brigade 21st "Panzer" respectively. They drove back repeated attacks by the Axis armour, who then withdrew before dusk. The British reinforced Ruweisat on the night of 2 July. The now enlarged Robcol became "Walgroup". Meanwhile, the Royal Air Force (RAF) made heavy air attacks on the Axis units.
The next day, 3 July, Rommel ordered the "Afrika Korps" to resume its attack on the Ruweisat ridge with the Italian XX Motorised Corps on its southern flank. Italian X Corps, meanwhile were to hold El Mreir. By this stage the "Afrika Korps" had only 26 operational tanks. There was a sharp armoured exchange south of Ruweisat ridge during the morning and the main Axis advance was held. On 3 July, the RAF flew 780 sorties.
To relieve the pressure on the right and centre of the Eighth Army line, XIII Corps on the left advanced from the Qattara box (known to the New Zealanders as the Kaponga box). The plan was that the New Zealand 2nd Division—with the remains of Indian 5th Division and 7th Motor Brigade under its command—would swing north to threaten the Axis flank and rear. This force encountered the "Ariete" Armoured Division's artillery, which was driving on the southern flank of the division as it attacked Ruweisat. The Italian commander ordered his battalions to fight their way out independently but the "Ariete" lost 531 men (about 350 were prisoners), 36 pieces of artillery, six (or eight?) tanks, and 55 trucks. By the end of the day, the "Ariete" Division had only five tanks. The day ended once again with the "Afrika Korps" and "Ariete" coming off second best to the superior numbers of the British 22nd Armoured and 4th Armoured Brigades, frustrating Rommel's attempts to resume his advance. The RAF once again played its part, flying 900 sorties during the day.
To the south, on 5 July the New Zealand group resumed its advance northwards towards El Mreir intending to cut the rear of the "Ariete" Division. Heavy fire from the Italian "Brescia" Motorised Division at El Mreir, however, north of the Qattara box, checked their progress and led XIII Corps to call off its attack.
At this point, Rommel decided his exhausted forces could make no further headway without resting and regrouping. He reported to the German High Command that his three German divisions numbered just 1,200–1,500 men each and resupply was proving highly problematic because of enemy interference from the air. He expected to have to remain on the defensive for at least two weeks.
Rommel was by this time suffering from the extended length of his supply lines. The Allied Desert Air Force (DAF) was concentrating fiercely on his fragile and elongated supply routes while British mobile columns moving west and striking from the south were causing havoc in the Axis rear echelons. Rommel could afford these losses even less since shipments from Italy had been substantially reduced (in June, he received of supplies compared with in May and 400 vehicles (compared with 2,000 in May). Meanwhile, the Eighth Army was reorganising and rebuilding, benefiting from its short lines of communication. By 4 July, the Australian 9th Division had entered the line in the north, and on 9 July the Indian 5th Infantry Brigade also returned, taking over the Ruweisat position. At the same time, the fresh Indian 161st Infantry Brigade reinforced the depleted Indian 5th Infantry Division.
On 8 July, Auchinleck ordered the new XXX Corps commander—Lieutenant-General William Ramsden—to capture the low ridges at Tel el Eisa and Tel el Makh Khad and then to push mobile battle groups south toward Deir el Shein and raiding parties west toward the airfields at El Daba. Meanwhile, XIII Corps would prevent the Axis from moving troops north to reinforce the coastal sector. Ramsden tasked the Australian 9th Division with 44th Royal Tank Regiment under command with the Tel el Eisa objective and the South African 1st Division with eight supporting tanks, Tel el Makh Khad. The raiding parties were to be provided by 1st Armoured Division.
Following a bombardment which started at 03:30 on 10 July, the Australian 26th Brigade launched an attack against the ridge north of Tel el Eisa station along the coast (Trig 33). The bombardment was the heaviest barrage yet experienced in North Africa, which created panic in the inexperienced soldiers of the Italian 60th Infantry Division "Sabratha" who had only just occupied sketchy defences in the sector. The Australian attack took more than 1,500 prisoners, routed an Italian Division and overran the German Signals Intercept Company 621. Meanwhile, the South Africans had by late morning taken Tel el Makh Khad and were in covering positions.
Elements of the German 164th Light Division and Italian 101st Motorised Division "Trieste" arrived to plug the gap torn in the Axis defences. That afternoon and evening, tanks from the German 15th "Panzer" and Italian "Trieste" Divisions launched counter-attacks against the Australian positions, the counter-attacks failing in the face of overwhelming Allied artillery and the Australian anti-tank guns.
At first light on 11 July, the Australian 2/24th Battalion supported by tanks from 44th Royal Tank Regiment attacked the western end of Tel el Eisa hill (Point 24). By early afternoon, the feature was captured and was then held against a series of Axis counter-attacks throughout the day. A small column of armour, motorised infantry, and guns then set off to raid Deir el Abyad and caused a battalion of Italian infantry to surrender. Its progress was checked at the Miteirya ridge and it was forced to withdraw that evening to the El Alamein box. During the day, more than 1,000 Italian prisoners were taken.
On 12 July, the 21st "Panzer" Division launched a counter-attack against Trig 33 and Point 24, which was beaten off after a 2½-hour fight, with more than 600 German dead and wounded left strewn in front of the Australian positions. The next day, 21. "Panzerdivision" launched an attack against Point 33 and South African positions in the El Alamein box. The attack was halted by intense artillery fire from the defenders. Rommel was still determined to drive the British forces from the northern salient. Although the Australian defenders had been forced back from Point 24, heavy casualties had been inflicted on 21st "Panzer" Division. Another attack was mounted on 15 July but made no ground against tenacious resistance. On 16 July, the Australians—supported by British tanks—launched an attack to try to take Point 24 but were forced back by German counter-attacks, suffering nearly fifty percent casualties.
After seven days of fierce fighting, the battle in the north for Tel el Eisa salient petered out. Australian 9th Division estimated at least 2,000 Axis troops had been killed and more than 3,700 prisoners of war taken in the battle. Possibly the most important feature of the battle, however, was that the Australians had captured Signals Intercept Company 621. This unit had provided Rommel with priceless intelligence, gleaned from intercepting British radio communications. That source of intelligence was now lost to Rommel.
As the Axis forces dug in, Auchinleck—having drawn a number of German units to the coastal sector during the Tel el Eisa fighting—developed a plan—codenamed Operation Bacon—to attack the Italian "Pavia" and "Brescia" Divisions in the centre of the front at the Ruweisat ridge. Signals intelligence was giving Auchinleck clear details of the Axis order of battle and force dispositions. His policy was to "...hit the Italians wherever possible in view of their low morale and because the Germans cannot hold extended fronts without them."
The intention was for the 4th New Zealand Brigade and 5th New Zealand Brigade (on 4th Brigade's right) to attack north-west to seize the western part of the ridge and on their right the Indian 5th Infantry Brigade to capture the eastern part of the ridge in a night attack. Then 2nd Armoured Brigade would pass through the centre of the infantry objectives to exploit toward Deir el Shein and the Miteirya Ridge. On the left, the 22nd Armoured Brigade would be ready to move forward to protect the infantry as they consolidated on the ridge.
The attack commenced at 23:00 on 14 July. The two New Zealand brigades shortly before dawn on 15 July took their objectives, but minefields and pockets of resistance created disarray among the attackers. A number of pockets of resistance were left behind the forward troops' advance which impeded the move forward of reserves, artillery, and support arms. As a result, the New Zealand brigades occupied exposed positions on the ridge without support weapons except for a few anti-tank guns. More significantly, communications with the two British armoured brigades failed, and the British armour did not move forwards to protect the infantry. At first light, a detachment from 15th "Panzer" division's 8th "Panzer" Regiment launched a counter-attack against New Zealand 4th Brigade's 22nd Battalion. A sharp exchange knocked out their anti-tank guns and the infantry found themselves exposed in the open with no alternative but to surrender. About 350 New Zealanders were taken prisoner.
While the 2nd New Zealand Division attacked the western slopes of Ruweisat Ridge, the Indian 5th Brigade made small gains on Ruweisat ridge to the east. By 07:00, word was finally got to 2nd Armoured Brigade which started to move north west. Two regiments became embroiled in a minefield but the third was able to join Indian 5th Infantry 5th Brigade as it renewed its attack. With the help of the armour and artillery, the Indians were able to take their objectives by early afternoon. Meanwhile, the 22nd Armoured Brigade had been engaged at Alam Nayil by 90th Light Division and the "Ariete" Armoured Division, advancing from the south. While—with help from mobile infantry and artillery columns from 7th Armoured Division—they pushed back the Axis probe with ease, they were prevented from advancing north to protect the New Zealand flank.
Seeing the "Brescia" and "Pavia" under pressure, Rommel rushed German troops to Ruweisat. By 15:00, the 3rd Reconnaissance Regiment and part of 21st "Panzer" Division from the north and 33rd Reconnaissance Regiment and the Baade Group comprising elements from 15th "Panzer" Division from the south were in place under Lieutenant-General ("General der Panzertruppe") Walther Nehring. At 17:00, Nehring launched his counter-attack. 4th New Zealand Brigade were still short of support weapons and also, by this time, ammunition. Once again, the anti-tank defences were overwhelmed and about 380 New Zealanders were taken prisoner including Captain Charles Upham who gained a second Victoria Cross for his actions including destroying a German tank and several guns and vehicles with grenades despite being shot through the elbow by a machine gun bullet and having his arm broken. At about 18:00, the brigade HQ was overrun. At about 18:15, 2nd Armoured Brigade engaged the German armour and halted the Axis eastward advance. At dusk, Nehring broke off the action.
Early on 16 July, Nehring renewed his attack. The 5th Indian Infantry Brigade pushed them back but it was clear from intercepted radio traffic that a further attempt would be made. Strenuous preparations to dig in anti-tank guns were made, artillery fire plans organised and a regiment from the 22nd Armoured Brigade was sent to reinforce the 2nd Armoured Brigade. When the attack resumed late in the afternoon, it was repulsed. After the battle, the Indians counted 24 knocked out tanks, as well as armoured cars and numerous anti-tank guns left on the battlefield.
In three days' fighting, the Allies took more than 2,000 Axis prisoners, mostly from the Italian "Brescia" and "Pavia" Divisions; the New Zealand division suffered 1,405 casualties. The fighting at Tel el Eisa and Ruweisat had caused the destruction of three Italian divisions, forced Rommel to redeploy his armour from the south, made it necessary to lay minefields in front of the remaining Italian divisions and stiffen them with detachments of German troops.
To relieve pressure on Ruweisat ridge, Auchinleck ordered the Australian 9th Division to make another attack from the north. In the early hours of 17 July, the Australian 24th Brigade—supported by 44th Royal Tank Regiment (RTR) and strong fighter cover from the air—assaulted Miteirya ridge (known as "Ruin ridge" to the Australians). The initial night attack went well, with 736 prisoners taken, mostly from the Italian "Trento" and "Trieste" motorised divisions. Once again, however, a critical situation for the Axis forces was retrieved by vigorous counter-attacks from hastily assembled German and Italian forces, which forced the Australians to withdraw back to their start line with 300 casualties. Although the Australian Official History of the 24th Brigade's 2/32nd Battalion describes the counter-attack force as "German", the Australian historian Mark Johnston reports that German records indicate that it was the "Trento" Division that overran the Australian battalion.
The Eighth Army now enjoyed a massive superiority in material over the Axis forces: 1st Armoured Division had 173 tanks and more in reserve or in transit, including 61 Grants while Rommel possessed only 38 German tanks and 51 Italian tanks although his armoured units had some 100 tanks awaiting repair.
Auchinleck's plan was for Indian Infantry 161st Brigade to attack along Ruweisat ridge to take Deir el Shein, while the New Zealand 6th Brigade attacked from south of the ridge to the El Mreir depression. At daylight, two British armoured brigades—2nd Armoured Brigade and the fresh 23rd Armoured Brigade—would sweep through the gap created by the infantry. The plan was complicated and ambitious.
The infantry night attack began at 16:30 on 21 July. The New Zealand attack took their objectives in the El Mreir depression but, once again, many vehicles failed to arrive and they were short of support arms in an exposed position. At daybreak on 22 July, the British armoured brigades again failed to advance. At daybreak on 22 July, Nehring's 5th and 8th "Panzer" Regiments responded with a rapid counter-attack which quickly overran the New Zealand infantry in the open, inflicting more than 900 casualties on the New Zealanders. 2nd Armoured Brigade sent forward two regiments to help but they were halted by mines and anti-tank fire.
The attack by Indian 161st Brigade had mixed fortunes. On the left, the initial attempt to clear the western end of Ruweisat failed but at 08:00 a renewed attack by the reserve battalion succeeded. On the right, the attacking battalion broke into the Deir el Shein position but was driven back in hand-to-hand fighting.
Compounding the disaster at El Mreir, at 08:00 the commander of 23rd Armoured Brigade ordered his brigade forward, intent on following his orders to the letter. Major-General Gatehouse—commanding 1st Armoured Division—had been unconvinced that a path had been adequately cleared in the minefields and had suggested the advance be cancelled. However, XIII Corps commander—Lieutenant-General William Gott—rejected this and ordered the attack but on a centre line south of the original plan which he incorrectly believed was mine-free. These orders failed to get through and the attack went ahead as originally planned. The brigade found itself mired in mine fields and under heavy fire. They were then counter-attacked by 21st Panzer at 11:00 and forced to withdraw. The 23rd Armoured Brigade was destroyed, with the loss of 40 tanks destroyed and 47 badly damaged.
At 17:00, Gott ordered 5th Indian Infantry Division to execute a night attack to capture the western half of Ruweisat ridge and Deir el Shein. 3/14th Punjab Regiment from 9th Indian Infantry Brigade attacked at 02:00 on 23 July but failed as they lost their direction. A further attempt in daylight succeeded in breaking into the position but intense fire from three sides resulted in control being lost as the commanding officer was killed, and four of his senior officers were wounded or went missing.
To the north, Australian 9th Division continued its attacks. At 06:00 on 22 July, Australian 26th Brigade attacked Tel el Eisa and Australian 24th Brigade attacked Tel el Makh Khad toward Miteirya (Ruin Ridge). It was during this fighting that Arthur Stanley Gurney performed the actions for which he was posthumously awarded the Victoria Cross. The fighting for Tel el Eisa was costly, but by the afternoon the Australians controlled the feature. That evening, Australian 24th Brigade attacked Tel el Makh Khad with the tanks of 50th RTR in support. The tank unit had not been trained in close infantry support and failed to co-ordinate with the Australian infantry. The result was that the infantry and armour advanced independently and having reached the objective 50th RTR lost 23 tanks because they lacked infantry support.
Once more, the Eighth Army had failed to destroy Rommel's forces, despite its overwhelming superiority in men and equipment. On the other hand, for Rommel the situation continued to be grave as, despite successful defensive operations, his infantry had suffered heavy losses and he reported that "the situation is critical in the extreme".
On 26/27 July, Auchinleck launched Operation Manhood in the northern sector in a final attempt to break the Axis forces. XXX Corps was reinforced with 1st Armoured Division (less 22nd Armoured Brigade), 4th Light Armoured Brigade, and 69th Infantry Brigade. The plan was to break the enemy line south of Miteirya ridge and exploit north-west. The South Africans were to make and mark a gap in the minefields to the south-east of Miteirya by midnight of 26/27 July. By 01:00 on 27 July, 24th Australian Infantry Brigade was to have captured the eastern end of the Miteirya ridge and would exploit toward the north-west. The 69th Infantry Brigade would pass through the minefield gap created by the South Africans to Deir el Dhib and clear and mark gaps in further minefields. The 2nd Armoured Brigade would then pass through to El Wishka and would be followed by 4th Light Armoured Brigade which would attack the Axis lines of communication.
This was the third attempt to break through in the northern sector, and the Axis defenders were expecting the attack. Like the previous attacks, it was hurriedly and therefore poorly planned. The Australian 24th Brigade managed to take their objectives on Miteirya Ridge by 02:00 of 27 July. To the south, the British 69th Brigade set off at 01:30 and managed to take their objectives by about 08:00. However, the supporting anti-tank units became lost in the darkness or delayed by minefields, leaving the attackers isolated and exposed when daylight came. There followed a period during which reports from the battlefront regarding the minefield gaps were confused and conflicting. As a consequence, the advance of 2nd Armoured Brigade was delayed. Rommel launched an immediate counter-attack and the German armoured battlegroups overran the two forward battalions of 69th Brigade. Meanwhile, 50th RTR supporting the Australians was having difficulty locating the minefield gaps made by Australian 2/24th Battalion. They failed to find a route through and in the process were caught by heavy fire and lost 13 tanks. The unsupported 2/28th Australian battalion on the ridge was overrun. The 69th Brigade suffered 600 casualties and the Australians 400 for no gain.
The Eighth Army was exhausted, and on 31 July Auchinleck ordered an end to offensive operations and the strengthening of the defences to meet a major counter-offensive.
Rommel was later to blame the failure to break through to the Nile on how the sources of supply to his army had dried up and how:
Rommel complained bitterly about the failure of important Italian convoys to get through to him desperately needed tanks and supplies, always blaming the Italian Supreme Command, never suspecting British code breaking.
According to Dr James Sadkovich and others, Rommel often displayed a distinct tendency to blame and scapegoat his Italian allies to cover up his own mistakes and deficiencies as a commander in the field. For example, while Rommel was a very good tactical commander, the Italian and German High Commands were concerned that he lacked operational awareness and a sense of strategic objectives. Dr Sadkovich points out that he would often out-run his logistics and squander valuable (mostly Italian) military hardware and resources in battle after battle without clear strategic goals and an appreciation of the limited logistics his Italian allies were desperately trying to provide him.
The battle was a stalemate, but it had halted the Axis advance on Alexandria (and then Cairo and ultimately the Suez Canal). The Eighth Army had suffered over 13,000 casualties in July, including 4,000 in the 2nd New Zealand Division, 3,000 in the 5th Indian Infantry Division and 2,552 battle casualties in the 9th Australian Division but had taken 7,000 prisoners and inflicted heavy damage on Axis men and machines. In his appreciation of 27 July, Auchinleck wrote that the Eighth Army would not be ready to attack again until mid-September at the earliest. He believed that because Rommel understood that with the passage of time the Allied situation would only improve, he was compelled to attack as soon as possible and before the end of August when he would have superiority in armour. Auchinleck therefore made plans for a defensive battle.
In early August, Winston Churchill and General Sir Alan Brooke—the Chief of the Imperial General Staff (CIGS)—visited Cairo on their way to meet Joseph Stalin in Moscow. They decided to replace Auchinleck, appointing the XIII Corps commander, William Gott, to the Eighth Army command and General Sir Harold Alexander as C-in-C Middle East Command. Persia and Iraq were to be split from Middle East Command as a separate Persia and Iraq Command and Auchinleck was offered the post of C-in-C (which he refused). Gott was killed on the way to take up his command when his aircraft was shot down. Lieutenant-General Bernard Montgomery was appointed in his place and took command on 13 August.

</doc>
<doc id="11776" url="https://en.wikipedia.org/wiki?curid=11776" title="First Italo-Ethiopian War">
First Italo-Ethiopian War

The First Italo-Ethiopian War was fought between Italy and Ethiopia from 1895 to 1896. It originated from the disputed Treaty of Wuchale, which the Italians claimed turned Ethiopia into an Italian protectorate. Full-scale war broke out in 1895, with Italian troops from Italian Eritrea having initial success until Ethiopian troops counterattacked Italian positions and besieged the Italian fort of Mekele, forcing its surrender. 
Italian defeat came about after the Battle of Adwa, where the Ethiopian army dealt the heavily outnumbered Italian soldiers and Eritrean askaris a decisive blow and forced their retreat back into Eritrea. Some Eritreans, regarded as traitors by the Ethiopians, were also captured and mutilated. The war concluded with the Treaty of Addis Ababa. Because this was one of the first decisive victories by African forces over a European colonial power, this war became a preeminent symbol of pan-Africanism and secured Ethiopia's sovereignty until 1936.
The Khedive of Egypt Isma'il Pasha, better known as "Isma'il the Magnificent" had conquered Eritrea as part of his efforts to give Egypt an African empire. Isma'il had tried to follow up that conquest with Ethiopia, but the Egyptian attempts to conquer that realm ended in humiliating defeat. After Egypt's bankruptcy in 1876 followed by the "Ansar" revolt under the leadership of the Mahdi in 1881, the Egyptian position in Eritrea was hopeless with the Egyptian forces cut off and unpaid for years. By 1884 the Egyptians began to pull out of both Sudan and Eritrea.
Egypt had been very much in the French sphere of influence until 1882 when Britain occupied Egypt. A major goal of French foreign policy until 1904 was to diminish British power in Egypt and restore it to its place in the French sphere of influence, and in 1883 the French created the colony of French Somaliland which allowed for the establishment of a French naval base at Djibouti on the Red Sea. The opening of the Suez Canal in 1869 had turned the Horn of Africa into a very strategic region as a navy based in the Horn could interdict any shipping going up and down the Red Sea. By building naval bases on the Red Sea that could intercept British shipping in the Red Sea, the French hoped to reduce the value of the Suez Canal for the British, and thus lever them out of Egypt. A French historian in 1900 wrote: "The importance of Djibouti lies almost solely in the uniqueness of its geographic position, which makes it a port of transit and natural entrepôt for areas more infinitely more populated than its own territory...the rich provinces of central Ethiopia." The British historian Harold Marcus noted that for the French: "Ethiopia represented the entrance to the Nile valley; if she could obtain hegemony over Ethiopia, her dream of a west to east French African empire would be closer to reality". In response, Britain consistently supported Italian ambitions in the Horn of Africa as the best way of keeping the French out.
On 3 June 1884, the Hewett Treaty was signed between Britain, Egypt and Ethiopia that allowed the Ethiopians to occupy parts of Eritrea and allowed the Ethiopian goods to pass in and out of Massawa duty-free. From the viewpoint of Britain, it was highly undesirable that the French replace the Egyptians in Eritrea as that would allow the French to have more naval bases on the Red Sea that could interfere with British shipping using the Suez Canal, and as the British did not want the financial burden of ruling Eritrea, they looked for another power to replace the Egyptians. The Hewett treaty seemed to suggest that Eritrea would fall into the Ethiopian sphere of influence as the Egyptians pulled out. After initially encouraging the Emperor Yohannes IV to move into Eritrea to replace the Egyptians, London decided to have the Italians move into Eritrea. In his history of Ethiopia, Augustus Wylde wrote: "England made use of King John [Emperor Yohannes] as long as he was of any service and then threw him over to the tender mercies of Italy...It is one of our worst bits of business out of the many we have been guilty of in Africa...one of the vilest bites of treachery". After the French had unexpectedly made Tunis into their protectorate in 1881, outraging opinion in Italy over the so-called "Schiaffo di Tunisi" (the "slap of Tunis"), Italian foreign policy had been extremely anti-French, and from the British viewpoint the best way of ensuring the Eritrean ports on the Red Sea stayed out of French hands was by having the staunchly anti-French Italians move in. In 1882, Italy had joined the Triple Alliance, allying herself with Austria and Germany against France.
On 5 February 1885 Italian troops landed at Massawa to replace the Egyptians. The Italian government for its part was more than happy to embark upon an imperialist policy to distract its people from the failings in post "Risorgimento" Italy. In 1861, the unification of Italy was supposed to mark the beginning of a glorious new era in Italian life, and many Italians were gravely disappointed to find that not much had changed in the new Kingdom of Italy with the vast majority of Italians still living in abject poverty. To compensate, a chauvinist mood was rampant among the upper classes in Italy with the newspaper "Il Diritto" writing in an editorial: "Italy must be ready. The year 1885 will decide her fate as a great power. It is necessary to feel the responsibility of the new era; to become again strong men afraid of nothing, with the sacred love of the fatherland, of all Italy, in our hearts". On the Ethiopian side, the wars that Emperor Yohannes had waged first against the invading Egyptians in the 1870s and then more so against the Sudanese "Mahdiyya" state in the 1880s had been presented by him to his subjects as holy wars in defense of Orthodox Christianity against Islam, reinforcing the Ethiopian belief that their country was an especially virtuous and holy land. The struggle against the "Ansar" from Sudan complicated Yohannes's relations with the Italians, whom he sometimes asked to provide him with guns to fight the "Ansar" and other times he resisted the Italians and proposed a truce with the "Ansar".
On 18 January 1887, at a village named Saati, an advancing Italian Army detachment defeated the Ethiopians in a skirmish, but it ended with the numerically superior Ethiopians surrounding the Italians in Saati after they retreated in face of the enemy's numbers. Some 500 Italian soldiers under Colonel de Christoforis together with 50 Eritrean auxiliaries were sent to support the besieged garrison at Saati. At Dogali on his way to Saati, de Christoforis was ambushed by an Ethiopian force under "Ras" Alula, whose men armed with spears skillfully encircled the Italians who retreated to one hill and then to another higher hill. After the Italians ran out of ammunition, "Ras" Alula ordered his men to charge and the Ethiopians swiftly overwhelmed the Italians in an action that featured bayonets against spears. The Battle of Dogali ended with the Italians losing 23 officers and 407 other ranks killed. As a result of the defeat at Dogali, the Italians abandoned Saati and retreated back to the Red Sea coast. Italians newspapers called the battle a "massacre" and excoriated the "Regio Esercito " for not assigning de Chistoforis enough ammunition. Having, at first, encouraged Emperor Yohannes to move into Eritrea, and then having encouraged the Italians to also do so, London realised a war was brewing and decided to try to mediate, largely out of the fear that the Italians might actually lose.
The British consul in Zanzibar, Gerald Portal, was sent in 1887 to mediate between the Ethiopians and Italians before war broke out. Portal set sail on an Egyptian ship, the "Narghileh", which he called a "small, dirty, greasy steamer bound for Jeddah, Suakin and Massawa, in which we very soon discovered that our traveling companions consisted of cockroaches and other smaller animals innumerable, a flock of sheep, a few cows, many cocks, hens, turkeys and geese, and a dozen of the evil-looking Greek adventurers who always appear like vultures around a dead carcass whenever there is a possibility of a campaign in North Africa." Portal upon meeting the Emperor Yohannes on 4 December 1887 presented him with gifts and a letter from Queen Victoria urging him to settle with the Italians. Portal reported: "What might have been possible in August or September was impossible in December, when the whole of the immense available forces in the country were already under arms; and that there now remains no hope of a satisfactory adjustment of the difficulties between Italy and Abyssinia [Ethiopia] until the question of the relative supremacy of these two nations has been decided by an appeal to the fortunes of war... No one who has once seen the nature of the gorges, ravines and mountain passes near the Abyssinian frontier can doubt for a moment that any advance by a civilised army in the face of the hostile Abyssinian hordes would be accomplished at the price of a fearful loss of life on both sides. ... The Abyssinians are savage and untrustworthy, but they are also redeemed by the possession of an unbounded courage, by a disregard of death, and by a national pride which leads them to look down on every human being who has not had the good fortune to be born an Abyssinian". Portal ended by writing that the Italians were making a mistake in preparing to go war against Ethiopia: "It is the old, old story, contempt of a gallant enemy because his skin happens to be chocolate or brown or black, and because his men have not gone through orthodox courses of field-firing, battalion drill, or 'autumn maneuvers'".
The defeat at Dogali made the Italians cautious for a moment, but on 10 March 1889, Emperor Yohannes died after being wounded in battle against the "Ansar" and on his deathbed admitted that "Ras" Mengesha, the supposed son of his brother, was actually his own son and asked that he succeed him. The revelation that the emperor had slept with his brother's wife scandalised intensely Orthodox Ethiopia, and instead the "Negus" Menelik was proclaimed emperor on 26 March 1889. "Ras" Mengesha, one of the most powerful Ethiopian noblemen, was unhappy about being by-passed in the succession and for a time allied himself with the Italians against the Emperor Menelik. Under the feudal Ethiopian system, there was no standing army, and instead, the nobility raised up armies on behalf of the Emperor. In December 1889, the Italians advanced inland again and took the cities of Asmara and Keren and in January 1890 took Adowa.
On March 25, 1889, the Shewa ruler Menelik II, having conquered Tigray and Amhara, declared himself Emperor of Ethiopia (or "Abyssinia", as it was commonly called in Europe at the time). Barely a month later, on May 2, he signed the Treaty of Wuchale with the Italians, which apparently gave them control over Eritrea, the Red Sea coast to the northeast of Ethiopia, in return for recognition of Menelik's rule. Menelik II continued the policy of Tewodros II of integrating Ethiopia.
However, the bilingual treaty did not say the same thing in Italian and Amharic; the Italian version did not give the Ethiopians the "significant autonomy" written into the Amharic translation. The former text established an Italian protectorate over Ethiopia, but the Amharic version merely stated that Menelik could contact foreign powers and conduct foreign affairs through Italy if he so chose. Italian diplomats, however, claimed that the original Amharic text included the clause and Menelik knowingly signed a modified copy of the Treaty. In October 1889, the Italians informed all of the other European governments because of the Treaty of Wuchale that Ethiopia was now an Italian protectorate and therefore the other European nations could not conduct diplomatic relations with Ethiopia. With the exceptions of the Ottoman Empire, which still maintained its claim to Eritrea, and Russia, which disliked the idea of an Orthodox nation being subjugated to a Roman Catholic nation, all of the European powers accepted the Italian claim to a protectorate.
The Italian claim that Menelik was aware of Article XVII turning his nation into an Italian protectorate seems unlikely given that the Emperor Menelik sent letters to Queen Victoria and Emperor Wilhelm II in late 1889 and was informed in the replies in early 1890 that neither Britain nor Germany could have diplomatic relations with Ethiopia on the account of Article XVII of the Treaty of Wuchale, a revelation that came as a great shock to the Emperor. Victoria's letter was polite whereas Wilhelm's letter was somewhat more rude, saying that King Umberto I was a great friend of Germany and Menelik's violation of the supposed Italian protectorate was a grave insult to Umberto, adding that he never wanted to hear from Menelik again. Moreover, Menelik did not know Italian and only signed the Amharic text of the treaty, being assured that there were no differences between the Italian and Amharic texts before he signed. The differences between the Italian and Amharic texts were due to the Italian minister in Addis Ababa, Count Pietro Antonelli, who had been instructed by his government to gain as much territory as possible in negotiating with the Emperor Menelik. However, knowing Menelik was now enthroned as the King of Kings and had a strong position, Antonelli was in the unenviable situation of negotiating a treaty that his own government might disallow. Therefore, he inserted the statement making Ethiopia give up its right to conduct its foreign affairs to Italy as a way of pleasing his superiors who might otherwise have fired him for only making small territorial gains. Antonelli was fluent in Amharic and given that Menelik only signed the Amharic text he could not have been unaware that the Amharic version of Article XVII only stated that the King of Italy places the services of his diplomats at the disposal of the Emperor of Ethiopia to represent him abroad if he so wished. When his subterfuge was exposed in 1890 with Menelik indignantly saying he would never sign away his country's independence to anybody, Antonelli who left Addis Ababa in mid 1890 resorted to racism, telling his superiors in Rome that as Menelik was a black man, he was thus intrinsically dishonest and it was only natural the Emperor would lie about the protectorate he supposedly willingly turned his nation into.
Francesco Crispi, the Italian Prime Minister was an ultra-imperialist who believed the newly unified Italian state required "the grandeur of a second Roman empire". Crispi believed that the Horn of Africa was the best place for the Italians to start building the new Roman empire. The American journalist James Perry wrote that "Crispi was a fool, a bigot and a very dangerous man". Because of the Ethiopian refusal to abide by the Italian version of the treaty and despite economic handicaps at home, the Italian government decided on a military solution to force Ethiopia to abide by the Italian version of the treaty. In doing so, they believed that they could exploit divisions within Ethiopia and rely on tactical and technological superiority to offset any inferiority in numbers. The efforts of Emperor Menelik, viewed as pro-French in London, to unify Ethiopia and thus bring control source of the Blue Nile under his rule was perceived in Whitehall as a threat to keeping Egypt in the British sphere of influence. As Menelik became increasingly successful in unifying Ethiopia, London brought more pressure to bear on Rome for the Italians to move inland and conquer Ethiopia once and for all.
There was a broader, European background as well: the Triple Alliance of Germany, Austria–Hungary, and Italy was under some stress, with Italy being courted by England. Two secret Anglo-Italian protocols in 1891, left most of Ethiopia in Italy's sphere of influence. France, one of the members of the opposing Franco-Russian Alliance, had its own claims on Eritrea and was bargaining with Italy over giving up those claims in exchange for a more secure position in Tunisia. Meanwhile, Russia was supplying weapons and other aid to Ethiopia. It had been trying to gain a foothold in Ethiopia, and in 1894, after denouncing the Treaty of Wuchale in July, it received an Ethiopian mission in St. Petersburg and sent arms and ammunition to Ethiopia. This support continued after the war ended. The Russian travel writer Alexander Bulatovich who went to Ethiopia to serve as a Red Cross volunteer with the Emperor Menelik made a point of emphasizing in his books that the Ethiopians converted to Christianity before any of the Europeans ever did, described the Ethiopians as a deeply religious people like the Russians, and argued the Ethiopians did not have the "low cultural level" of the other African peoples, making them equal to the Europeans. Germany and Austria supported their ally in the Triple Alliance Italy while France and Russia supported Ethiopia.
In 1893, judging that his power over Ethiopia was secure, Menelik repudiated the treaty; in response the Italians ramped up the pressure on his domain in a variety of ways, including the annexation of small territories bordering their original claim under the Treaty of Wuchale, and finally culminating with a military campaign and across the Mareb River into Tigray (on the border with Eritrea) in December 1894. The Italians expected disaffected potentates like Negus Tekle Haymanot of Gojjam, Ras Mengesha Yohannes, and the Sultan of Aussa to join them; instead, all of the ethnic Tigrayan or Amharic peoples flocked to the Emperor Menelik's side in a display of both nationalism and anti-Italian feeling, while other peoples of dubious loyalty (e.g. the Sultan of Aussa) were watched by Imperial garrisons. In June 1894, "Ras" Mengesha and his generals had appeared in Addis Ababa carrying large stones which they dropped before the Emperor Menelik (a gesture that is a symbol of submission in Ethiopian culture). In Ethiopia, the popular saying at the time was: "Of a black snake's bite, you may be cured, but from the bite of a white snake, you will never recover." There was an overwhelming national unity in Ethiopia as various feuding noblemen rallied behind the emperor who insisted that Ethiopia, unlike the other African nations, would retain its freedom and not be subjected to Italy. The ethnic rivalries between the Tigrians and the Amhara that the Italians were counting upon did not prove to be a factor as Menelik pointed out that the Italians held all Ethnic Africans, regardless of their individual ethnic backgrounds, in contempt, noting the segregation policies in Eritrea applied to all Ethnic Africans. Further, Menelik had spent much of the previous four years building up a supply of modern weapons and ammunition, acquired from the French, British, and the Italians themselves, as the European colonial powers sought to keep each other's North African aspirations in check. They also used the Ethiopians as a proxy army against the Sudanese Mahdists.
In December 1894, Bahta Hagos led a rebellion against the Italians in Akkele Guzay, claiming support of Mengesha. Units of General Oreste Baratieri's army under Major Pietro Toselli crushed the rebellion and killed Bahta at the Battle of Halai. The Italian army then occupied the Tigrian capital, Adwa. Baratieri suspected that Mengesha would invade Eritrea, and met him at the Battle of Coatit in January 1895. The victorious Italians chased the retreating Mengesha, capturing weapons and important documents proving his complicity with Menelik. The victory in this campaign, along with previous victories against the Sudanese Mahdists, led the Italians to underestimate the difficulties to overcome in a campaign against Menelik. At this point, Emperor Menelik turned to France, offering a treaty of alliance; the French response was to abandon the Emperor in order to secure Italian approval of the Treaty of Bardo which would secure French control of Tunisia. Virtually alone, on 17 September 1895, Emperor Menelik issued a proclamation calling up the men of Shewa to join his army at Were Ilu.
As the Italians were poised to enter Ethiopian territory, the Ethiopians mobilised en masse all over the country. Helping it was the newly updated imperial fiscal and taxation system. As a result, a hastily mobilised army of 196,000 men gathered from all parts of Abyssinia, more than half of whom were armed with modern rifles, rallied at Addis Ababa in support of the Emperor and defence of their country.
The only European ally of Ethiopia was Russia. The Ethiopian emperor sent his first diplomatic mission to St. Petersburg in 1895. In June 1895, the newspapers in St. Petersburg wrote, "Along with the expedition, Menelik II sent his diplomatic mission to Russia, including his princes and his bishop". Many citizens of the capital came to meet the train that brought Prince Damto, General Genemier, Prince Belyakio, Bishop of Harer Gabraux Xavier and other members of the delegation to St. Petersburg. On the eve of war, an agreement providing military help for Ethiopia was concluded.
The next clash came at Amba Alagi on 7 December 1895, when Ethiopian soldiers overran the Italian positions dug in on the natural fortress, and forced the Italians to retreat back to Eritrea. The remaining Italian troops under General Giuseppe Arimondi reached the unfinished Italian fort at Mekele. Arimondi left there a small garrison of approximately 1,150 Askaris and 200 Italians, commanded by Major Giuseppe Galliano, and took the bulk of his troops to Adigrat, where Oreste Baratieri, the Italian Commander, was concentrating the Italian Army.
The first Ethiopian troops reached Mekele in the following days. Ras Makonnen surrounded the fort at Mekele on 18 December, but the Italian Commander adroitly used promises of a negotiated surrender to prevent the Ras from attacking the fort. By the first days of January, Emperor Menelik, accompanied by his Queen Taytu Betul, had led large forces into Tigray, and besieged the Italians for sixteen days (6–21 January 1896), making several unsuccessful attempts to carry the fort by storm, until the Italians surrendered with permission from the Italian Headquarters. Menelik allowed them to leave Mekele with their weapons, and even provided the defeated Italians mules and pack animals to rejoin Baratieri. While some historians read this generous act as a sign that Emperor Menelik still hoped for a peaceful resolution to the war, Harold Marcus points out that this escort allowed him a tactical advantage: "Menelik craftily managed to establish himself in Hawzien, at Gendepata, near Adwa, where the mountain passes were not guarded by Italian fortifications."
Heavily outnumbered, Baratieri refused to engage, knowing that due to their lack of infrastructure the Ethiopians could not keep large numbers of troops in the field much longer. However, Baratieri also never knew about the true numerical strength of the Ethiopian army that was to face his army, so he rather further fortified his positions in the Tigray. But the Italian government of Francesco Crispi was unable to accept being stymied by non-Europeans. The prime minister specifically ordered Baratieri to advance deep into enemy territory and bring about a battle.
The decisive battle of the war was the Battle of Adwa on March 1, 1896, which took place in the mountainous country north of the actual town of Adwa (or Adowa). The Italian army comprised four brigades totaling approximately 17,700 men, with fifty-six artillery pieces; the Ethiopian army comprised several brigades numbering between 73,000 and 120,000 men (80–100,000 with firearms: according to Richard Pankhurst, the Ethiopians were armed with approximately 100,000 rifles of which about half were quick-firing), with almost fifty artillery pieces.
General Baratieri planned to surprise the larger Ethiopian force with an early morning attack, expecting his enemy to be asleep. However, the Ethiopians had risen early for Church services and, upon learning of the Italian advance, promptly attacked. The Italian forces were hit by wave after wave of attacks, until Menelik released his reserve of 25,000 men, destroying an Italian brigade. Another brigade was cut off, and destroyed by a cavalry charge. The last two brigades were destroyed piecemeal. By noon, the Italian survivors were in full retreat.
While Menelik's victory was in a large part due to the sheer force of numbers, his troops were well-armed because of his careful preparations. The Ethiopian army only had a feudal system of organisation but proved capable of properly executing the strategic plan drawn up in Menelik's headquarters. However, the Ethiopian army also had its problems. The first was the quality of its arms, as the Italian and British colonial authorities could sabotage the transportation of 30,000–60,000 modern Mosin–Nagant rifles and Berdan rifles from Russia into landlocked Ethiopia. The rest of the Ethiopian army was equipped with swords and spears. Secondly, the Ethiopian army's feudal organisation meant that nearly the entire force was composed of peasant militia. Russian military experts advising Menelik II suggested a full-contact battle with Italians, to neutralise the Italian fire superiority, instead of engaging in a campaign of harassment designed to nullify problems with arms, training, and organisation.
Some Russian councillors of Menelik II and a team of fifty Russian volunteers participated in the battle, among them Nikolay Leontiev, an officer of the Kuban Cossack army. Russian support for Ethiopia also led to a Russian Red Cross mission, which arrived in Addis Ababa some three months after Menelik's Adwa victory.
The Italians suffered about 7,000 killed and 1,500 wounded in the battle and subsequent retreat back into Eritrea, with 3,000 taken prisoner; Ethiopian losses have been estimated around 4,000 killed and 8,000 wounded. In addition, 2,000 Eritrean Askaris were killed or captured. Italian prisoners were treated as well as possible under difficult circumstances, but 800 captured Askaris, regarded as traitors by the Ethiopians, had their right hands and left feet amputated. Menelik, knowing that the war was very unpopular in Italy with the Italian Socialists in particular condemning the policy of the Crispi government, chose to be a magnanimous victor, making it clear that he saw a difference between the Italian people and Crispi.
Menelik was a well respected ruler whose lineage was allegedly traced back to King Solomon and the Queen of Sheba. He used that status and its power to peacefully create alliances and to conquer those who opposed him. He was such a skillful negotiator that he was able to unify almost all of the Northern, Western, and Central territories peacefully. He made Ras Mengesha Yohannes the prince of Tigray, and along with the threat of the Italians, convinced him to join him. Menelik not only conquered large groups of people like the Oromo, Guarage, and Wolayta, he also managed to incorporate leaders from those groups into his own government, and war council. Whether conquered peacefully or militarily, almost all groups had a voice under Menelik.
From 1888 to 1892, one third of the Ethiopian population died from what would become known as The Great Famine. On the heels of this disaster, Menelik used his relationship with the Europeans to help modernise Ethiopia. The Europeans soon flooded the Ethiopian economy looking for business opportunities. Meanwhile, Menelik established the first national bank, a national currency, a postal system, railroads, modern roads, and electricity. The bank and currency unified the people economically and helped establish economic stability. The railways, roads, and postal system connected the people and tribes as a nation as well as physically. Possibly his greatest achievement in creating a national identity was through the creation of Addis Ababa. This was an important psychological component in the establishment of a nation. It provided a metaphorical ‘head’ for the nation. It became permanent location for the entire country to look upon for support and for guidance.
Menelik retired in good order to his capital, Addis Ababa, and waited for the fallout of the victory to hit Italy. Riots broke out in several Italian cities, and within two weeks, the Crispi government collapsed amidst Italian disenchantment with "foreign adventures".
Menelik secured the Treaty of Addis Ababa in October, which delineated the borders of Eritrea and forced Italy to recognise the independence of Ethiopia. Delegations from the United Kingdom and France—whose colonial possessions lay next to Ethiopia—soon arrived in the Ethiopian capital to negotiate their own treaties with this newly proven power. Owing to Russia's diplomatic support of her fellow Orthodox nation, Russia's prestige greatly increased in Ethiopia. The adventuresome Seljan brothers, Mirko and Stjepan, who were actually Catholic Croats, were warmly welcomed when they arrived in Ethiopia in 1899 when they misinformed their hosts by saying they were Russians. As France supported Ethiopia with weapons, French influence increased markedly. Prince Henri of Orléans, the French traveller, wrote: "France gave rifles to this country and taking the hand of its Emperor like an elder sister has explained to him the old motto which has guided her across the centuries of greatness and glory: Honor and Country!". In December 1896, a French diplomatic mission in Addis Ababa arrived and on 20 March 1897 signed a treaty that was described as ""véritable traité d'alliance". In turn, the increase in French influence in Ethiopia led to fears in London that the French would gain control of the Blue Nile and would be able to "lever" the British out of Egypt. To keep control of the Nile in Egypt, the British decided in March 1896 to advance down the Nile from Egypt into the Sudan to liquidate the "Mahdiyya" state. On 12 March 1896, upon hearing of the Italian defeat at the Battle of Adwa, the Prime Minister Lord Salisbury, gave instructions for the British forces in Egypt to occupy the Sudan before the French could liquidate the "Mahdiyya" state, stating that no hostile power would be allowed to control the Nile.
In 1935, Italy launched a second invasion, which resulted in an Italian victory and the annexation of Ethiopia to Italian East Africa until the Italians were defeated in the Second World War and expelled by the British, with some assistance from Ethiopian Arbegnochs. The Italians successively started a guerrilla war until 1943 in some areas of northern Ethiopia, supporting the rebellion of the Galla in 1942.

</doc>
<doc id="11778" url="https://en.wikipedia.org/wiki?curid=11778" title="Frederick Soddy">
Frederick Soddy

Frederick Soddy FRS (2 September 1877 – 22 September 1956) was an English radiochemist who explained, with Ernest Rutherford, that radioactivity is due to the transmutation of elements, now known to involve nuclear reactions. He also proved the existence of isotopes of certain radioactive elements.
Soddy was born at 5 Bolton Road, Eastbourne, England, the son of Benjamin Soddy, corn merchant, and his wife Hannah Green. He went to school at Eastbourne College, before going on to study at University College of Wales at Aberystwyth and at Merton College, Oxford, where he graduated in 1898 with first class honours in chemistry. He was a researcher at Oxford from 1898 to 1900.
In 1900 he became a demonstrator in chemistry at McGill University in Montreal, Quebec, where he worked with Ernest Rutherford on radioactivity.
He and Rutherford realized that the anomalous behaviour of radioactive elements was because they decayed into other elements.
This decay also produced alpha, beta, and gamma radiation. When radioactivity was first discovered, no one was sure what the cause was. It needed careful work by Soddy and Rutherford to prove that atomic transmutation was in fact occurring.
In 1903, with Sir William Ramsay at University College London, Soddy showed that the decay of radium produced helium gas. In the experiment a sample of radium was enclosed in a thin-walled glass envelope sited within an evacuated glass bulb. After leaving the experiment running for a long period of time, a spectral analysis of the contents of the former evacuated space revealed the presence of helium. Later in 1907, Rutherford and Thomas Royds showed that the helium was first formed as positively charged nuclei of helium (He) which were identical to alpha particles, which could pass through the thin glass wall but were contained within the surrounding glass envelope.
From 1904 to 1914, Soddy was a lecturer at the University of Glasgow. Ruth Pirret worked as his research assistant during this time. 
In May 1910 Soddy was elected a Fellow of the Royal Society. In 1914 he was appointed to a chair at the University of Aberdeen, where he worked on research related to World War I.
The work that Soddy and his research assistant Ada Hitchins did at Glasgow and Aberdeen showed that uranium decays to radium. It also showed that a radioactive element may have more than one atomic mass though the chemical properties are identical. Soddy named this concept isotope meaning "same place". The word was initially suggested to him by Margaret Todd. Later, J. J. Thomson showed that non-radioactive elements can also have multiple isotopes.
In 1913, Soddy also showed that an atom moves lower in atomic number by two places on alpha emission, higher by one place on beta emission. This was discovered at about the same time by Kazimierz Fajans, and is known as the radioactive displacement law of Fajans and Soddy, a fundamental step toward understanding the relationships among families of radioactive elements. Soddy published "The Interpretation of Radium" (1909) and "Atomic Transmutation" (1953).
In 1918 he announced discovery of a stable isotope of Protactinium, working with John Arnold Cranston. This slightly post-dated its discovery by German counterparts; however, it is said their discovery was actually made in 1915 but its announcement was delayed due to Cranston's notes being locked away whilst on active service in the First World War.
In 1919 he moved to the University of Oxford as Dr Lee's Professor of Chemistry, where, in the period up till 1936, he reorganized the laboratories and the syllabus in chemistry. He received the 1921 Nobel Prize in chemistry for his research in radioactive decay and particularly for his formulation of the theory of isotopes.
His work and essays popularising the new understanding of radioactivity was the main inspiration for H. G. Wells's "The World Set Free" (1914), which features atomic bombs dropped from biplanes in a war set many years in the future. Wells's novel is also known as "The Last War" and imagines a peaceful world emerging from the chaos. In "Wealth, Virtual Wealth and Debt" Soddy praises Wells’s "The World Set Free". He also says that radioactive processes probably power the stars.
In four books written from 1921 to 1934, Soddy carried on a "campaign for a radical restructuring of global monetary relationships", offering a perspective on economics rooted in physics – the laws of thermodynamics, in particular – and was "roundly dismissed as a crank". While most of his proposals – "to abandon the gold standard, let international exchange rates float, use federal surpluses and deficits as macroeconomic policy tools that could counter cyclical trends, and establish bureaus of economic statistics (including a consumer price index) in order to facilitate this effort" – are now conventional practice, his critique of fractional-reserve banking still "remains outside the bounds of conventional wisdom" although a recent paper by the IMF reinvigorated his proposals. Soddy wrote that financial debts grew exponentially at compound interest but the real economy was based on exhaustible stocks of fossil fuels. Energy obtained from the fossil fuels could not be used again. This criticism of economic growth is echoed by his intellectual heirs in the now emergent field of ecological economics.
In "Wealth, Virtual Wealth and Debt" Soddy cited the (fraudulent) Protocols of the Learned Elders of Zion as evidence for the belief, which was relatively widespread at the time, of a "financial conspiracy to enslave the world". He used the imagery of a Jewish conspiracy to buttress his claim that "A corrupt monetary system strikes at the very life of the nation." In the same document, he made reference to "the semi-Oriental" who is "supreme" in "high finance" and to an "iridescent bubble of beliefs blown around the world by the Hebraic hierarchy". Later in life he published a pamphlet "Abolish Private Money, or Drown in Debt" (1939) with a noted publisher of anti-Semitic texts. The influence of his writing can be gauged, for example, in this quote from Ezra Pound:
"Professor Frederick Soddy states that the Gold Standard monetary system has wrecked a scientific age! ... The world's bankers ... have not been content to take their share of modern wealth production – great as it has been – but they have refused to allow the masses of mankind to receive theirs."
He rediscovered the Descartes' theorem in 1936 and published it as a poem, "The Kiss Precise", quoted at Problem of Apollonius. The kissing circles in this problem are sometimes known as Soddy circles.
He received the Nobel Prize in Chemistry in 1921 and the same year he was elected member of the International Atomic Weights Committee. A small crater on the far side of the Moon as well as the radioactive uranium mineral soddyite are named after him.
In 1908, Soddy married Winifred Moller Beilby (1885-1936), the daughter of industrial chemist Sir George Beilby and Lady Emma Bielby, a philanthropist to women's causes. The couple worked together and co-published a paper in 1910 on the absorption of gamma rays from radium. He died in Brighton, England in 1956, twenty days after his 79th birthday.

</doc>
<doc id="11780" url="https://en.wikipedia.org/wiki?curid=11780" title="Fur seal">
Fur seal

Fur seals are any of nine species of pinnipeds belonging to the subfamily Arctocephalinae in the family Otariidae. They are much more closely related to sea lions than true seals, and share with them external ears (pinnae), relatively long and muscular foreflippers, and the ability to walk on all fours. They are marked by their dense underfur, which made them a long-time object of commercial hunting. Eight species belong to the genus "Arctocephalus" and are found primarily in the Southern Hemisphere, while a ninth species also sometimes called fur seal, the northern fur seal ("Callorhinus ursinus"), belongs to a different genus and inhabits the North Pacific.
Fur seals and sea lions make up the family Otariidae. Along with the Phocidae and Odobodenidae, ottariids are pinnipeds descending from a common ancestor most closely related to modern bears (as hinted by the subfamily Arctocephalinae, meaning "bear-headed"). The name pinniped refers to mammals with front and rear flippers. Otariids arose about 15-17 million years ago in the Miocene, and were originally land mammals that rapidly diversified and adapted to a marine environment, giving rise to the semiaquatic marine mammals that thrive today. Fur seals and sea lions are closely related and commonly known together as the "eared seals". 
Until recently, fur seals were all grouped under a single subfamily of Pinnipedia, called the Arctocephalinae, to contrast them with Otariinae – the sea lions – based on the most prominent common feature, namely the coat of dense underfur intermixed with guard hairs. Recent genetic evidence, however, suggests "Callorhinus" is more closely related to some sea lion species, and the fur seal/sea lion subfamily distinction has been eliminated from many taxonomies. Nonetheless, all fur seals have certain features in common: the fur, generally smaller sizes, farther and longer foraging trips, smaller and more abundant prey items, and greater sexual dimorphism. For these reasons, the distinction remains useful. Fur seals comprise two genera: "Callorhinus", and "Arctocephalus". "Callorhinus" is represented by just one species in the Northern Hemisphere, the northern fur seal ("Callorhinus ursinus"), and "Arctocephalus" is represented by eight species in the Southern Hemisphere. The southern fur seals comprising the genus "Arctocephalus" include Antarctic fur seals, Galapagos fur seals, Juan Fernandez fur seals, New Zealand fur seals, brown fur seals, South American fur seals, and subantarctic fur seals.
Along with the previously mentioned thick underfur, fur seals are distinguished from sea lions by their smaller body structure, greater sexual dimorphism, smaller prey, and longer foraging trips during the feeding cycle. The physical appearance of fur seals varies with individual species, but the main characteristics remain constant. 
Fur seals are characterized by their external pinnae, dense underfur, vibrissae, and long, muscular limbs. They share with other otariids the ability to rotate their rear limbs forward, supporting their bodies and allowing them to ambulate on land. In water, their front limbs, typically measuring about a fourth of their body length, act as oars and can propel them forward for optimal mobility. The surfaces of these long, paddle-like fore limbs are leathery with small claws. Otariids have a dog-like head, sharp, well-developed canines, sharp eyesight, and keen hearing. 
They are extremely sexually dimorphic mammals, with the males often two to five times the size of the females, with proportionally larger heads, necks, and chests. Size ranges from about 1.5 m, 64 kg in the male Galapagos fur seal (also the smallest pinniped) to 2.5 m, 180 kg in the adult male New Zealand fur seal. Most fur seal pups are born with a black-brown coat that molts at 2–3 months, revealing a brown coat that typically gets darker with age. Some males and females within the same species have significant differences in appearance, further contributing to the sexual dimorphism. Females and juveniles often have a lighter colored coat overall or only on the chest, as seen in South American fur seals. In a northern fur seal population, the females are typically silvery-gray on the dorsal side and reddish-brown on their ventral side with a light gray patch on their chest. This makes them easily distinguished from the males with their brownish-gray to reddish-brown or black coats.
Of the fur seal family, eight species are considered southern fur seals, and only one is found in the Northern Hemisphere. The southern group includes Antarctic, Galapagos, Guadalupe, Juan Fernandez, New Zealand, brown, South American, and subantarctic fur seals. They typically spend about 70% of their lives in subpolar, temperate, and equatorial waters. Colonies of fur seals can be seen throughout the Pacific and Southern Oceans from south Australia, Africa, and New Zealand, to the coast of Peru and north to California. They are typically nonmigrating mammals, with the exception of the northern fur seal, which has been known to travel distances up to 10,000 km. Fur seals are often found near isolated islands or peninsulas, and can be seen hauling out onto the mainland during winter. Although they are not migratory, they have been observed wandering hundreds of miles from their breeding grounds in times of scarce resources. For example, the subantarctic fur seal typically resides near temperate islands in the South Atlantic and Indian Oceans north of the Antarctic Polar Front, but juvenile males have been seen wandering as far north as Brazil and South Africa.
Typically, fur seals gather during the summer in large rookeries at specific beaches or rocky outcrops to give birth and breed. All species are polygynous, meaning dominant males reproduce with more than one female. For most species, total gestation lasts about 11.5 months, including a several-month period of delayed implantation of the embryo. Northern fur seal males aggressively select and defend the specific females in their harems. Females typically reach sexual maturity around 3–4 years. The males reach sexual maturity around the same time, but do not become territorial or mate until 6–10 years. 
The breeding season typically begins in November and lasts 2–3 months. The northern fur seals begin their breeding season as early as June due to their region, climate, and resources. In all cases, the males arrive a few weeks early to fight for their territory and groups of females with which to mate. They congregate at rocky, isolated breeding grounds and defend their territory through fighting and vocalization. Males typically do not leave their territory for the entirety of the breeding season, fasting and competing until all energy sources are depleted. 
The Juan Fernandez fur seals deviate from this typical behavior, using aquatic breeding territories not seen in other fur seals. They use rocky sites for breeding, but males fight for territory on land and on the shoreline and in the water. Upon arriving to the breeding grounds, females give birth to their pups from the previous season. About a week later, the females mate again and shortly after begin their feeding cycle, which typically consists of foraging and feeding at sea for about 5 days, then returning to the breeding grounds to nurse the pups for about 2 days. Mothers and pups locate each other using call recognition during nursing period. The Juan Fernandez fur seal has a particularly long feeding cycle, with about 12 days of foraging and feeding and 5 days of nursing. Most fur seals continue this cycle for about 9 months until they wean their pup. The exception to this is the Antarctic fur seal, which has a feeding cycle that lasts only 4 months. During foraging trips, most female fur seals travel around 200 km from the breeding site, and can dive around 200 m depending on food availability.
The remainder of the year, fur seals lead a largely pelagic existence in the open sea, pursuing their prey wherever it is abundant. They feed on moderately sized fish, squid, and krill. Several species of the southern fur seal also have sea birds, especially penguins, as part of their diets. Fur seals, in turn, are preyed upon by sharks, killer whales, and occasionally by larger sea lions. These opportunistic mammals tend to feed and dive in shallow waters at night, when their prey are swimming near the surface. South American fur seals exhibit a different diet; adults feed almost exclusively on anchovies, while juveniles feed on demersal fish, most likely due to availability.
When fur seals were hunted in the late 18th and early 19th centuries, they hauled out on remote islands where no predators were present. The hunters reported being able to club the unwary animals to death one after another, making the hunt profitable, though the price per seal skin was low.
The average lifespan of fur seals varies with different species from 13 to 25 years, with females typically living longer. Most populations continue to expand as they recover from previous commercial hunting and environmental threats. 
Many species were heavily exploited by commercial sealers, especially during the 19th century, when their fur was highly valued. Beginning in the 1790s, the ports of Stonington and New Haven, Connecticut, were leaders of the American fur seal trade, which primarily entailed clubbing fur seals to death on uninhabited South Pacific islands, skinning them, and selling the hides in China. Many populations, notably the Guadalupe fur seal, northern fur seal, and Cape fur seal, suffered dramatic declines and are still recovering. Currently, most species are protected, and hunting is mostly limited to subsistence harvest. Globally, most populations can be considered healthy, mostly because they often prefer remote habitats that are relatively inaccessible to humans. Nonetheless, environmental degradation, competition with fisheries, and climate change potentially pose threats to some populations.

</doc>
<doc id="11781" url="https://en.wikipedia.org/wiki?curid=11781" title="Frisian">
Frisian

Frisian usually refers to:
Frisian or Friesian may also refer to:

</doc>
<doc id="11784" url="https://en.wikipedia.org/wiki?curid=11784" title="Fauna (disambiguation)">
Fauna (disambiguation)

Fauna is a collective term for animal life.
Fauna may also refer to:

</doc>
<doc id="11786" url="https://en.wikipedia.org/wiki?curid=11786" title="Federico Fellini">
Federico Fellini

Federico Fellini, (; 20 January 1920 – 31 October 1993) was an Italian film director and screenwriter known for his distinctive style, which blends fantasy and baroque images with earthiness. He is recognized as one of the greatest and most influential filmmakers of all time. His films have ranked highly in critical polls such as that of "Cahiers du cinéma" and "Sight & Sound", which lists his 1963 film "" as the 10th-greatest film.
Fellini won the Palme d'Or for "La Dolce Vita", was nominated for twelve Academy Awards, and won four in the category of Best Foreign Language Film, the most for any director in the history of the Academy. He received an honorary award for Lifetime Achievement at the 65th Academy Awards in Los Angeles. His other well-known films include "La Strada" (1954), "Nights of Cabiria" (1957), "Juliet of the Spirits" (1967), "Satyricon" (1969), "Roma" (1972), "Amarcord" (1973), and "Fellini's Casanova" (1976).
Fellini was born on 20 January 1920, to middle-class parents in Rimini, then a small town on the Adriatic Sea. On 25 January, at the San Nicolò church he was baptized Federico Domenico Marcello Fellini. His father, Urbano Fellini (1894–1956), born to a family of Romagnol peasants and small landholders from Gambettola, moved to Rome in 1915 as a baker apprenticed to the Pantanella pasta factory. His mother, Ida Barbiani (1896–1984), came from a bourgeois Catholic family of Roman merchants. Despite her family's vehement disapproval, she had eloped with Urbano in 1917 to live at his parents' home in Gambettola. A civil marriage followed in 1918 with the religious ceremony held at Santa Maria Maggiore in Rome a year later.
The couple settled in Rimini where Urbano became a traveling salesman and wholesale vendor. Fellini had two siblings: Riccardo (1921–1991), a documentary director for RAI Television, and Maria Maddalena (m. Fabbri; 1929–2002).
In 1924, Fellini started primary school in an institute run by the nuns of San Vincenzo in Rimini, attending the Carlo Tonni public school two years later. An attentive student, he spent his leisure time drawing, staging puppet shows and reading "Il corriere dei piccoli", the popular children's magazine that reproduced traditional American cartoons by Winsor McCay, George McManus and Frederick Burr Opper. (Opper's "Happy Hooligan" would provide the visual inspiration for Gelsomina in Fellini's 1954 film "La Strada"; McCay's "Little Nemo" would directly influence his 1980 film "City of Women".) In 1926, he discovered the world of Grand Guignol, the circus with Pierino the Clown and the movies. Guido Brignone’s "Maciste all’Inferno" (1926), the first film he saw, would mark him in ways linked to Dante and the cinema throughout his entire career.
Enrolled at the Ginnasio Giulio Cesare in 1929, he made friends with Luigi "Titta" Benzi, later a prominent Rimini lawyer (and the model for young Titta in "Amarcord" (1973)). In Mussolini’s Italy, Fellini and Riccardo became members of the "Avanguardista", the compulsory Fascist youth group for males. He visited Rome with his parents for the first time in 1933, the year of the maiden voyage of the transatlantic ocean liner "SS Rex" (which is shown in "Amarcord"). The sea creature found on the beach at the end of "La Dolce Vita" (1960) has its basis in a giant fish marooned on a Rimini beach during a storm in 1934.
Although Fellini adapted key events from his childhood and adolescence in films such as "I Vitelloni" (1953), "" (1963), and "Amarcord" (1973), he insisted that such autobiographical memories were inventions: 
In 1937, Fellini opened Febo, a portrait shop in Rimini, with the painter Demos Bonini. His first humorous article appeared in the "Postcards to Our Readers" section of Milan's "Domenica del Corriere". Deciding on a career as a caricaturist and gag writer, Fellini travelled to Florence in 1938, where he published his first cartoon in the weekly "420". According to a biographer, Fellini found school "exasperating" and, in one year, had 67 absences. Failing his military culture exam, he graduated from high school in July 1938 after doubling the exam.
In September 1939, he enrolled in law school at the University of Rome to please his parents. Biographer Hollis Alpert reports that "there is no record of his ever having attended a class". Installed in a family "pensione", he met another lifelong friend, the painter Rinaldo Geleng. Desperately poor, they unsuccessfully joined forces to draw sketches of restaurant and café patrons. Fellini eventually found work as a cub reporter on the dailies "Il Piccolo" and "Il Popolo di Roma", but quit after a short stint, bored by the local court news assignments.
Four months after publishing his first article in "Marc’Aurelio", the highly influential biweekly humour magazine, he joined the editorial board, achieving success with a regular column titled "But Are You Listening?" Described as “the determining moment in Fellini’s life”, the magazine gave him steady employment between 1939 and 1942, when he interacted with writers, gagmen, and scriptwriters. These encounters eventually led to opportunities in show business and cinema. Among his collaborators on the magazine's editorial board were the future director Ettore Scola, Marxist theorist and scriptwriter Cesare Zavattini, and Bernardino Zapponi, a future Fellini screenwriter. Conducting interviews for "CineMagazzino" also proved congenial: when asked to interview Aldo Fabrizi, Italy's most popular variety performer, he established such immediate personal rapport with the man that they collaborated professionally. Specializing in humorous monologues, Fabrizi commissioned material from his young protégé.
Retained on business in Rimini, Urbano sent wife and family to Rome in 1940 to share an apartment with his son. Fellini and Ruggero Maccari, also on the staff of "Marc’Aurelio", began writing radio sketches and gags for films.
Not yet twenty and with Fabrizi's help, Fellini obtained his first screen credit as a comedy writer on Mario Mattoli’s "Il pirata sono io" ("The Pirate's Dream"). Progressing rapidly to numerous collaborations on films at Cinecittà, his circle of professional acquaintances widened to include novelist Vitaliano Brancati and scriptwriter Piero Tellini. In the wake of Mussolini’s declaration of war against France and Britain on 10 June 1940, Fellini discovered Kafka’s "The Metamorphosis", Gogol, John Steinbeck and William Faulkner along with French films by Marcel Carné, René Clair, and Julien Duvivier. In 1941 he published "Il mio amico Pasqualino", a 74-page booklet in ten chapters describing the absurd adventures of Pasqualino, an alter ego.
Writing for radio while attempting to avoid the draft, Fellini met his future wife Giulietta Masina in a studio office at the Italian public radio broadcaster EIAR in the autumn of 1942. Well-paid as the voice of Pallina in Fellini's radio serial, "Cico and Pallina", Masina was also well known for her musical-comedy broadcasts which cheered an audience depressed by the war. In November 1942, Fellini was sent to Libya, occupied by Fascist Italy, to work on the screenplay of "I cavalieri del deserto" ("Knights of the Desert", 1942), directed by Osvaldo Valenti and Gino Talamo. Fellini welcomed the assignment as it allowed him "to secure another extension on his draft order". Responsible for emergency re-writing, he also directed the film's first scenes. When Tripoli fell under siege by British forces, he and his colleagues made a narrow escape by boarding a German military plane flying to Sicily. His African adventure, later published in "Marc’Aurelio" as "The First Flight", marked “the emergence of a new Fellini, no longer just a screenwriter, working and sketching at his desk, but a filmmaker out in the field”.
The apolitical Fellini was finally freed of the draft when an Allied air raid over Bologna destroyed his medical records. Fellini and Giulietta hid in her aunt's apartment until Mussolini's fall on 25 July 1943. After dating for nine months, the couple were married on 30 October 1943. Several months later, Masina fell down the stairs and suffered a miscarriage. She gave birth to a son, Pierfederico, on 22 March 1945, but the child died of encephalitis a month later on 24 April 1945. The tragedy had enduring emotional and artistic repercussions.
After the Allied liberation of Rome on 4 June 1944, Fellini and Enrico De Seta opened the Funny Face Shop where they survived the postwar recession drawing caricatures of American soldiers. He became involved with Italian Neorealism when Roberto Rossellini, at work on "Stories of Yesteryear" (later "Rome, Open City"), met Fellini in his shop, and proposed he contribute gags and dialogue for the script. Aware of Fellini's reputation as Aldo Fabrizi's “creative muse”, Rossellini also requested that he try to convince the actor to play the role of Father Giuseppe Morosini, the parish priest executed by the SS on 4 April 1944.
In 1947, Fellini and Sergio Amidei received an Oscar nomination for the screenplay of "Rome, Open City".
Working as both screenwriter and assistant director on Rossellini's "Paisà" ("Paisan") in 1946, Fellini was entrusted to film the Sicilian scenes in Maiori. In February 1948, he was introduced to Marcello Mastroianni, then a young theatre actor appearing in a play with Giulietta Masina. Establishing a close working relationship with Alberto Lattuada, Fellini co-wrote the director's "Senza pietà" ("Without Pity") and "Il mulino del Po" ("The Mill on the Po"). Fellini also worked with Rossellini on the anthology film "L'Amore" (1948), co-writing the screenplay and in one segment titled, "The Miracle", acting opposite Anna Magnani. To play the role of a vagabond rogue mistaken by Magnani for a saint, Fellini had to bleach his black hair blond.
In 1950 Fellini co-produced and co-directed with Alberto Lattuada "Variety Lights" ("Luci del varietà"), his first feature film. A backstage comedy set among the world of small-time travelling performers, it featured Giulietta Masina and Lattuada's wife, Carla Del Poggio. Its release to poor reviews and limited distribution proved disastrous for all concerned. The production company went bankrupt, leaving both Fellini and Lattuada with debts to pay for over a decade. In February 1950, "Paisà" received an Oscar nomination for the screenplay by Rossellini, Sergio Amidei, and Fellini.
After travelling to Paris for a script conference with Rossellini on "Europa '51", Fellini began production on "The White Sheik" in September 1951, his first solo-directed feature. Starring Alberto Sordi in the title role, the film is a revised version of a treatment first written by Michelangelo Antonioni in 1949 and based on the "fotoromanzi", the photographed cartoon strip romances popular in Italy at the time. Producer Carlo Ponti commissioned Fellini and Tullio Pinelli to write the script but Antonioni rejected the story they developed. With Ennio Flaiano, they re-worked the material into a light-hearted satire about newlywed couple Ivan and Wanda Cavalli (Leopoldo Trieste, Brunella Bovo) in Rome to visit the Pope. Ivan's prissy mask of respectability is soon demolished by his wife's obsession with the White Sheik. Highlighting the music of Nino Rota, the film was selected at Cannes (among the films in competition was Orson Welles’s "Othello") and then retracted. Screened at the 13th Venice International Film Festival, it was razzed by critics in "the atmosphere of a soccer match”. One reviewer declared that Fellini had “not the slightest aptitude for cinema direction".
In 1953, "I Vitelloni" found favour with the critics and public. Winning the Silver Lion Award in Venice, it secured Fellini his first international distributor.
Fellini directed "La Strada" based on a script completed in 1952 with Pinelli and Flaiano. During the last three weeks of shooting, Fellini experienced the first signs of severe clinical depression. Aided by his wife, he undertook a brief period of therapy with Freudian psychoanalyst Emilio Servadio.
Fellini cast American actor Broderick Crawford to interpret the role of an aging swindler in "Il Bidone". Based partly on stories told to him by a petty thief during production of "La Strada", Fellini developed the script into a con man's slow descent towards a solitary death. To incarnate the role's "intense, tragic face", Fellini's first choice had been Humphrey Bogart, but after learning of the actor's lung cancer, chose Crawford after seeing his face on the theatrical poster of "All the King’s Men" (1949). The film shoot was wrought with difficulties stemming from Crawford's alcoholism. Savaged by critics at the 16th Venice International Film Festival, the film did miserably at the box office and did not receive international distribution until 1964.
During the autumn, Fellini researched and developed a treatment based on a film adaptation of Mario Tobino’s novel, "The Free Women of Magliano". Set in a mental institution for women, the project was abandoned when financial backers considered the subject had no potential.
While preparing "Nights of Cabiria" in spring 1956, Fellini learned of his father’s death by cardiac arrest at the age of sixty-two. Produced by Dino De Laurentiis and starring Giulietta Masina, the film took its inspiration from news reports of a woman’s severed head retrieved in a lake and stories by Wanda, a shantytown prostitute Fellini met on the set of "Il Bidone". Pier Paolo Pasolini was hired to translate Flaiano and Pinelli’s dialogue into Roman dialect and to supervise researches in the vice-afflicted suburbs of Rome. The movie won the Academy Award for Best Foreign Language Film at the 30th Academy Awards and brought Masina the Best Actress Award at Cannes for her performance.
With Pinelli, he developed "Journey with Anita" for Sophia Loren and Gregory Peck. An "invention born out of intimate truth", the script was based on Fellini's return to Rimini with a mistress to attend his father's funeral. Due to Loren's unavailability, the project was shelved and resurrected twenty-five years later as "Lovers and Liars" (1981), a comedy directed by Mario Monicelli with Goldie Hawn and Giancarlo Giannini. For Eduardo De Filippo, he co-wrote the script of "Fortunella", tailoring the lead role to accommodate Masina's particular sensibility.
The Hollywood on the Tiber phenomenon of 1958 in which American studios profited from the cheap studio labour available in Rome provided the backdrop for photojournalists to steal shots of celebrities on the via Veneto. The scandal provoked by Turkish dancer Haish Nana's improvised striptease at a nightclub captured Fellini's imagination: he decided to end his latest script-in-progress, "Moraldo in the City", with an all-night "orgy" at a seaside villa. Pierluigi Praturlon’s photos of Anita Ekberg wading fully dressed in the Trevi Fountain provided further inspiration for Fellini and his scriptwriters. 
Changing the title of the screenplay to "La Dolce Vita", Fellini soon clashed with his producer on casting: the director insisted on the relatively unknown Mastroianni while De Laurentiis wanted Paul Newman as a hedge on his investment. Reaching an impasse, De Laurentiis sold the rights to publishing mogul Angelo Rizzoli. Shooting began on 16 March 1959 with Anita Ekberg climbing the stairs to the cupola of Saint Peter’s in a mammoth décor constructed at Cinecittà. The statue of Christ flown by helicopter over Rome to Saint Peter's Square was inspired by an actual media event on 1 May 1956, which Fellini had witnessed. The film wrapped August 15 on a deserted beach at Passo Oscuro with a bloated mutant fish designed by Piero Gherardi.
"La Dolce Vita" broke all box office records. Despite scalpers selling tickets at 1000 lire, crowds queued in line for hours to see an “immoral movie” before the censors banned it. At an exclusive Milan screening on 5 February 1960, one outraged patron spat on Fellini while others hurled insults. Denounced in parliament by right-wing conservatives, undersecretary Domenico Magrì of the Christian Democrats demanded tolerance for the film's controversial themes. The Vatican's official press organ, "l'Osservatore Romano", lobbied for censorship while the Board of Roman Parish Priests and the Genealogical Board of Italian Nobility attacked the film. In one documented instance involving favourable reviews written by the Jesuits of San Fedele, defending "La Dolce Vita" had severe consequences. In competition at Cannes alongside Antonioni's "L’Avventura", the film won the Palme d'Or awarded by presiding juror Georges Simenon. The Belgian writer was promptly “hissed at” by the disapproving festival crowd.
A major discovery for Fellini after his Italian neorealism period (1950–1959) was the work of Carl Jung. After meeting Jungian psychoanalyst Dr. Ernst Bernhard in early 1960, he read Jung's autobiography, "Memories, Dreams, Reflections" (1963) and experimented with LSD. Bernhard also recommended that Fellini consult the "I Ching" and keep a record of his dreams. What Fellini formerly accepted as "his extrasensory perceptions" were now interpreted as psychic manifestations of the unconscious. Bernhard's focus on Jungian depth psychology proved to be the single greatest influence on Fellini's mature style and marked the turning point in his work from neorealism to filmmaking that was "primarily oneiric". As a consequence, Jung's seminal ideas on the "anima" and the "animus", the role of archetypes and the collective unconscious directly influenced such films as "" (1963), "Juliet of the Spirits" (1965), "Fellini Satyricon" (1969), "Casanova" (1976), and "City of Women" (1980). Other key influences on his work include Luis Buñuel. Charlie Chaplin, Sergei Eisenstein, Buster Keaton, Laurel and Hardy, the Marx Brothers, and Roberto Rossellini.
Exploiting "La Dolce Vita"’s success, financier Angelo Rizzoli set up Federiz in 1960, an independent film company, for Fellini and production manager Clemente Fracassi to discover and produce new talent. Despite the best intentions, their overcautious editorial and business skills forced the company to close down soon after cancelling Pasolini’s project, "Accattone" (1961).
Condemned as a "public sinner", for "La Dolce Vita", Fellini responded with "The Temptations of Doctor Antonio", a segment in the omnibus "Boccaccio '70". His second colour film, it was the sole project green-lighted at Federiz. Infused with the surrealistic satire that characterized the young Fellini's work at "Marc’Aurelio", the film ridiculed a crusader against vice, interpreted by Peppino De Filippo, who goes insane trying to censor a billboard of Anita Ekberg espousing the virtues of milk.
In an October 1960 letter to his colleague Brunello Rondi, Fellini first outlined his film ideas about a man suffering creative block: "Well then - a guy (a writer? any kind of professional man? a theatrical producer?) has to interrupt the usual rhythm of his life for two weeks because of a not-too-serious disease. It’s a warning bell: something is blocking up his system." Unclear about the script, its title, and his protagonist's profession, he scouted locations throughout Italy “looking for the film”, in the hope of resolving his confusion. Flaiano suggested "La bella confusione" (literally "The Beautiful Confusion") as the movie's title. Under pressure from his producers, Fellini finally settled on "", a self-referential title referring principally (but not exclusively) to the number of films he had directed up to that time.
Giving the order to start production in spring 1962, Fellini signed deals with his producer Rizzoli, fixed dates, had sets constructed, cast Mastroianni, Anouk Aimée, and Sandra Milo in lead roles, and did screen tests at the Scalera Studios in Rome. He hired cinematographer Gianni Di Venanzo, among key personnel. But apart from naming his hero Guido Anselmi, he still couldn't decide what his character did for a living. The crisis came to a head in April when, sitting in his Cinecittà office, he began a letter to Rizzoli confessing he had "lost his film" and had to abandon the project. Interrupted by the chief machinist requesting he celebrate the launch of "", Fellini put aside the letter and went on the set. Raising a toast to the crew, he "felt overwhelmed by shame… I was in a no exit situation. I was a director who wanted to make a film he no longer remembers. And lo and behold, at that very moment everything fell into place. I got straight to the heart of the film. I would narrate everything that had been happening to me. I would make a film telling the story of a director who no longer knows what film he wanted to make". The self-mirroring structure makes that the entire film is inseparable from its reflecting construction.
Shooting began on 9 May 1962. Perplexed by the seemingly chaotic, incessant improvisation on the set, Deena Boyer, the director's American press officer at the time, asked for a rationale. Fellini told her that he hoped to convey the three levels "on which our minds live: the past, the present, and the conditional - the realm of fantasy". After shooting wrapped on 14 October, Nino Rota composed various circus marches and fanfares that would later become signature tunes of the maestro's cinema. Nominated for four Oscars, "" won awards for best foreign language film and best costume design in black-and-white. In California for the ceremony, Fellini toured Disneyland with Walt Disney the day after.
Increasingly attracted to parapsychology, Fellini met the Turin magician Gustavo Rol in 1963. Rol, a former banker, introduced him to the world of Spiritism and séances. In 1964, Fellini took LSD under the supervision of Emilio Servadio, his psychoanalyst during the 1954 production of "La Strada". For years reserved about what actually occurred that Sunday afternoon, he admitted in 1992 that
objects and their functions no longer had any significance. All I perceived was perception itself, the hell of forms and figures devoid of human emotion and detached from the reality of my unreal environment. I was an instrument in a virtual world that constantly renewed its own meaningless image in a living world that was itself perceived outside of nature. And since the appearance of things was no longer definitive but limitless, this paradisiacal awareness freed me from the reality external to my self. The fire and the rose, as it were, became one.
Fellini's hallucinatory insights were given full flower in his first colour feature "Juliet of the Spirits" (1965), depicting Giulietta Masina as Juliet, a housewife who rightly suspects her husband's infidelity and succumbs to the voices of spirits summoned during a séance at her home. Her sexually voracious next door neighbor Suzy (Sandra Milo) introduces Juliet to a world of uninhibited sensuality but Juliet is haunted by childhood memories of her Catholic guilt and a teenaged friend who committed suicide. Complex and filled with psychological symbolism, the film is set to a jaunty score by Nino Rota.
To help promote "Satyricon" in the United States, Fellini flew to Los Angeles in January 1970 for interviews with Dick Cavett and David Frost. He also met with film director Paul Mazursky who wanted to star him alongside Donald Sutherland in his new film, "Alex in Wonderland". In February, Fellini scouted locations in Paris for "The Clowns", a docufiction both for cinema and television, based on his childhood memories of the circus and a "coherent theory of clowning." As he saw it, the clown "was always the caricature of a well-established, ordered, peaceful society. But today all is temporary, disordered, grotesque. Who can still laugh at clowns?... All the world plays a clown now."
In March 1971, Fellini began production on "Roma", a seemingly random collection of episodes informed by the director's memories and impressions of Rome. The "diverse sequences," writes Fellini scholar Peter Bondanella, "are held together only by the fact that they all ultimately originate from the director’s fertile imagination." The film's opening scene anticipates "Amarcord" while its most surreal sequence involves an ecclesiastical fashion show in which nuns and priests roller skate past shipwrecks of cobwebbed skeletons.
Over a period of six months between January and June 1973, Fellini shot the Oscar-winning "Amarcord". Loosely based on the director's 1968 autobiographical essay "My Rimini", the film depicts the adolescent Titta and his friends working out their sexual frustrations against the religious and Fascist backdrop of a provincial town in Italy during the 1930s. Produced by Franco Cristaldi, the seriocomic movie became Fellini's second biggest commercial success after "La Dolce Vita". Circular in form, "Amarcord" avoids plot and linear narrative in a way similar to "The Clowns" and "Roma". The director's overriding concern with developing a poetic form of cinema was first outlined in a 1965 interview he gave to "The New Yorker" journalist Lillian Ross: "I am trying to free my work from certain constrictions – a story with a beginning, a development, an ending. It should be more like a poem with metre and cadence."
Organized by his publisher Diogenes Verlag in 1982, the first major exhibition of 63 drawings by Fellini was held in Paris, Brussels, and the Pierre Matisse Gallery in New York. A gifted caricaturist, much of the inspiration for his sketches was derived from his own dreams while the films-in-progress both originated from and stimulated drawings for characters, decor, costumes and set designs. Under the title, "I disegni di Fellini" (Fellini's Designs), he published 350 drawings executed in pencil, watercolours, and felt pens.
On 6 September 1985 Fellini was awarded the Golden Lion for lifetime achievement at the 42nd Venice Film Festival. That same year, he became the first non-American to receive the Film Society of Lincoln Center’s annual award for cinematic achievement.
Long fascinated by Carlos Castaneda’s "", Fellini accompanied the Peruvian author on a journey to the Yucatán to assess the feasibility of a film. After first meeting Castaneda in Rome in October 1984, Fellini drafted a treatment with Pinelli titled "Viaggio a Tulun". Producer Alberto Grimaldi, prepared to buy film rights to all of Castaneda's work, then paid for pre-production research taking Fellini and his entourage from Rome to Los Angeles and the jungles of Mexico in October 1985. When Castaneda inexplicably disappeared and the project fell through, Fellini's mystico-shamanic adventures were scripted with Pinelli and serialized in "Corriere della Sera" in May 1986. A barely veiled satirical interpretation of Castaneda's work, "Viaggio a Tulun" was published in 1989 as a graphic novel with artwork by Milo Manara and as "Trip to Tulum" in America in 1990.
For "Intervista", produced by Ibrahim Moussa and RAI Television, Fellini intercut memories of the first time he visited Cinecittà in 1939 with present-day footage of himself at work on a screen adaptation of Franz Kafka’s "Amerika". A meditation on the nature of memory and film production, it won the special 40th Anniversary Prize at Cannes and the 15th Moscow International Film Festival Golden Prize. In Brussels later that year, a panel of thirty professionals from eighteen European countries named Fellini the world’s best director and "" the best European film of all time.
In early 1989 Fellini began production on "The Voice of the Moon", based on Ermanno Cavazzoni’s novel, "Il poema dei lunatici" ("The Lunatics' Poem"). A small town was built at Empire Studios on the via Pontina outside Rome. Starring Roberto Benigni as Ivo Salvini, a madcap poetic figure newly released from a mental institution, the character is a combination of "La Strada"'s Gelsomina, Pinocchio, and Italian poet Giacomo Leopardi. Fellini improvised as he filmed, using as a guide a rough treatment written with Pinelli. Despite its modest critical and commercial success in Italy, and its warm reception by French critics, it failed to interest North American distributors.
Fellini won the "Praemium Imperiale", the equivalent of the Nobel Prize in the visual arts, awarded by the Japan Art Association in 1990.
In July 1991 and April 1992, Fellini worked in close collaboration with Canadian filmmaker Damian Pettigrew to establish "the longest and most detailed conversations ever recorded on film". Described as the "Maestro's spiritual testament” by his biographer Tullio Kezich, excerpts culled from the conversations later served as the basis of their feature documentary, ' (2002) and the book, '. Finding it increasingly difficult to secure financing for feature films, Fellini developed a suite of television projects whose titles reflect their subjects: "Attore", "Napoli", "L’Inferno", "L'opera lirica", and "L’America".
In April 1993 Fellini received his fifth Oscar, for lifetime achievement, "in recognition of his cinematic accomplishments that have thrilled and entertained audiences worldwide". On 16 June, he entered the Cantonal Hospital in Zürich for an angioplasty on his femoral artery but suffered a stroke at the Grand Hotel in Rimini two months later. Partially paralyzed, he was first transferred to Ferrara for rehabilitation and then to the Policlinico Umberto I in Rome to be near his wife, also hospitalized. He suffered a second stroke and fell into an irreversible coma.
Fellini died in Rome on 31 October 1993 at the age of 73 after a heart attack he suffered a few weeks earlier, a day after his 50th wedding anniversary. The memorial service, in Studio 5 at Cinecittà, was attended by an estimated 70,000 people. At Giulietta Masina's request, trumpeter Mauro Maur played Nino Rota's "Improvviso dell'Angelo" during the ceremony.
Five months later, on 23 March 1994, Masina died of lung cancer. Fellini, Masina and their son, Pierfederico, are buried in a bronze sepulchre sculpted by Arnaldo Pomodoro. Designed as a ship's prow, the tomb is at the main entrance to the Cemetery of Rimini. The Federico Fellini Airport in Rimini is named in his honour.
Fellini was raised in a Roman Catholic family and considered himself a Catholic, but avoided formal activity in the Catholic Church. Fellini's films include Catholic themes; some celebrate Catholic teachings, while others criticize or ridicule church dogma.
While Fellini was for the most part indifferent to politics, he had a general dislike of authoritarian institutions, and is interpreted by Bondanella as believing in "the dignity and even the nobility of the individual human being". In a 1966 interview, he said, "I make it a point to see if certain ideologies or political attitudes threaten the private freedom of the individual. But for the rest, I am not prepared nor do I plan to become interested in politics."
Despite various famous Italian actors favouring the Communists, Fellini was not left-wing. It is rumored that he supported Christian Democracy (DC). Bondanella writes that DC "was far too aligned with an extremely conservative and even reactionary pre-Vatican II church to suit Fellini's tastes", but Fellini opposed the '68 Movement and befriended Giulio Andreotti.
Apart from satirizing Silvio Berlusconi and mainstream television in "Ginger and Fred", Fellini rarely expressed political views in public and never directed an overtly political film. He directed two electoral television spots during the 1990s: one for DC and another for the Italian Republican Party (PRI). His slogan "Non si interrompe un'emozione" ("Don't interrupt an emotion") was directed against the excessive use of TV advertisements. The Democratic Party of the Left also used the slogan in the referendums of 1995.
Personal and highly idiosyncratic visions of society, Fellini's films are a unique combination of memory, dreams, fantasy and desire. The adjectives "Fellinian" and "Felliniesque" are "synonymous with any kind of extravagant, fanciful, even baroque image in the cinema and in art in general". "La Dolce Vita" contributed the term "paparazzi" to the English language, derived from Paparazzo, the photographer friend of journalist Marcello Rubini (Marcello Mastroianni).
Contemporary filmmakers such as Tim Burton, Terry Gilliam, Emir Kusturica, and David Lynch have cited Fellini's influence on their work.
Polish director Wojciech Has, whose two best-received films, "The Saragossa Manuscript" (1965) and "The Hour-Glass Sanatorium" (1973), are examples of modernist fantasies, has been compared to Fellini for the sheer "luxuriance of his images".
"I Vitelloni" inspired European directors Juan Antonio Bardem, Marco Ferreri, and Lina Wertmüller and influenced Martin Scorsese's "Mean Streets" (1973), George Lucas's "American Graffiti" (1974), Joel Schumacher's "St. Elmo's Fire" (1985), and Barry Levinson's "Diner" (1987), among many others. When the American magazine "Cinema" asked Stanley Kubrick in 1963 to name his ten favorite films, he ranked "I Vitelloni" number one.
"Nights of Cabiria" was adapted as the Broadway musical "Sweet Charity" and the movie "Sweet Charity" (1969) by Bob Fosse starring Shirley MacLaine. "City of Women" was adapted for the Berlin stage by Frank Castorf in 1992.
' inspired, among others, "Mickey One" (Arthur Penn, 1965), "Alex in Wonderland" (Paul Mazursky, 1970), "Beware of a Holy Whore" (Rainer Werner Fassbinder, 1971), "Day for Night" (François Truffaut, 1973), "All That Jazz" (Bob Fosse, 1979), "Stardust Memories" (Woody Allen, 1980), "Sogni d'oro" (Nanni Moretti, 1981), "Parad Planet" (Vadim Abdrashitov, 1984), "La Pelicula del rey" (Carlos Sorin, 1986), "Living in Oblivion" (Tom DiCillo, 1995), "Women" (Peter Greenaway, 1999), "Falling Down" (Joel Schumacher, 1993), and the Broadway musical "Nine" (Maury Yeston and Arthur Kopit, 1982). "Yo-Yo Boing!" (1998), a Spanish novel by Puerto Rican writer Giannina Braschi, features a dream sequence with Fellini inspired by '.
Fellini's work is referenced on the albums "Fellini Days" (2001) by Fish, "Another Side of Bob Dylan" (1964) by "Bob Dylan" with "Motorpsycho Nitemare", "Funplex" (2008) by the B-52's with the song "Juliet of the Spirits", and in the opening traffic jam of the music video "Everybody Hurts" by R.E.M. American singer Lana Del Rey has cited Fellini as an influence. It influenced the American TV shows "Northern Exposure" and "Third Rock from the Sun". Wes Anderson's short film "Castello Cavalcanti" (2013) is in many places a direct homage to Fellini.
Various film-related material and personal papers of Fellini are in the Wesleyan University Cinema Archives, to which scholars and media experts have full access. In October 2009, the Jeu de Paume in Paris opened an exhibit devoted to Fellini that included ephemera, television interviews, behind-the-scenes photographs, "Book of Dreams" (based on 30 years of the director's illustrated dreams and notes), along with excerpts from "La dolce vita" and "".
In 2014, the Blue Devils Drum and Bugle Corps of Concord, California, performed "Felliniesque", a show themed around Fellini's work, with which they won a record 16th Drum Corps International World Class championship with a record score of 99.650. That same year, the weekly entertainment-trade magazine "Variety" announced that French director Sylvain Chomet was moving forward with "The Thousand Miles", a project based on various Fellini works, including his unpublished drawings and writings.
Television commercials

</doc>
<doc id="11787" url="https://en.wikipedia.org/wiki?curid=11787" title="Fleetwood Mac">
Fleetwood Mac

Fleetwood Mac are a British-American rock band, formed in London in 1967. Fleetwood Mac were founded by guitarist Peter Green, drummer Mick Fleetwood and guitarist Jeremy Spencer, before bassist John McVie joined the lineup for their self-titled debut album. Danny Kirwan joined as a third guitarist in 1968. Keyboardist Christine Perfect, who contributed as a session musician from the second album, married McVie and joined in 1970. 
Primarily a British blues band, Fleetwood Mac scored a UK number one with "Albatross", and had other hits such as the singles "Oh Well" and "Man of the World". All three guitarists left in succession during the early 1970s, to be replaced by guitarists Bob Welch and Bob Weston and vocalist Dave Walker. By 1974, all three had either departed or been dismissed, leaving the band without a male lead vocalist or guitarist. In late 1974, while Fleetwood was scouting studios in Los Angeles, he was introduced to American folk-rock duo Lindsey Buckingham and Stevie Nicks. Fleetwood Mac soon asked Buckingham to be their new lead guitarist, and Buckingham agreed on condition that Nicks would also join the band. 
The addition of Buckingham and Nicks gave the band a more pop rock sound, and their 1975 self-titled album, "Fleetwood Mac", reached No. 1 in the United States. "Rumours" (1977), Fleetwood Mac's second album after the arrival of Buckingham and Nicks, produced four U.S. Top 10 singles and remained at number one on the American albums chart for 31 weeks. It also reached the top spot in various countries around the world and won a Grammy Award for Album of the Year in 1978. "Rumours" has sold over 40 million copies worldwide, making it the one of the best-selling albums in history. The band went through personal turmoil while recording the album, as both the romantic partnerships in the band (one being John and Christine McVie, and the other being Buckingham and Nicks) separated while continuing to make music together.
The band's personnel remained stable through three more studio albums, but by the late 1980s began to disintegrate. After Buckingham and Nicks each left the band, they were replaced by a number of other guitarists and vocalists. A 1993 one-off performance for the first inauguration of Bill Clinton featured the lineup of Fleetwood, John McVie, Christine McVie, Nicks, and Buckingham back together for the first time in six years. A full reunion occurred four years later, and the group released their fourth U.S. No. 1 album, "The Dance" (1997), a live compilation of their hits, also marking the 20th anniversary of "Rumours". Christine McVie left the band in 1998, but continued to work with the band in a session capacity. Meanwhile, the group remained together as a four-piece, releasing their most recent studio album, "Say You Will", in 2003. Christine McVie rejoined the band full-time in 2014. In 2018, Buckingham was fired from the band and was replaced by Mike Campbell, formerly of Tom Petty and the Heartbreakers, and Neil Finn of Split Enz and Crowded House.
Fleetwood Mac have sold more than 120 million records worldwide, making them one of the world's best-selling bands. In 1979, the group were honoured with a star on the Hollywood Walk of Fame. In 1998 the band was inducted into the Rock and Roll Hall of Fame and received the Brit Award for Outstanding Contribution to Music.
Fleetwood Mac were formed in July 1967 in London, England, when Peter Green left the British blues band John Mayall & the Bluesbreakers. Green had previously replaced guitarist Eric Clapton in the Bluesbreakers and had received critical acclaim for his work on their album "A Hard Road". Green had been in two bands with Mick Fleetwood, Peter B's Looners and the subsequent Shotgun Express (which featured a young Rod Stewart as vocalist), and suggested Fleetwood as a replacement for drummer Aynsley Dunbar when Dunbar left the Bluesbreakers to join the new Jeff Beck/Rod Stewart band. John Mayall agreed and Fleetwood joined the Bluesbreakers.
The Bluesbreakers then consisted of Green, Fleetwood, John McVie and Mayall. Mayall gave Green free recording time as a gift, which Fleetwood, McVie and Green used to record five songs. The fifth song was an instrumental that Green named after the rhythm section, "Fleetwood Mac" ("Mac" being short for McVie).
Soon after this, Green suggested to Fleetwood that they form a new band. The pair wanted McVie on bass guitar and named the band 'Fleetwood Mac' to entice him, but McVie opted to keep his steady income with Mayall rather than take a risk with a new band. In the meantime Peter Green and Mick Fleetwood had teamed up with slide guitarist Jeremy Spencer and bassist Bob Brunning. Brunning was in the band on the understanding that he would leave if McVie agreed to join. The Green, Fleetwood, Spencer, Brunning version of the band made its debut on 13 August 1967 at the Windsor Jazz and Blues Festival as 'Peter Green's Fleetwood Mac, also featuring Jeremy Spencer'. Brunning played only a few gigs with Fleetwood Mac. Within weeks of this show, John McVie agreed to join the band as permanent bassist.
Fleetwood Mac's self-titled debut album was a no-frills blues album and was released by the Blue Horizon label in February 1968. There were no other players on the album (except on the song "Long Grey Mare", which was recorded with Brunning on bass). The album was successful in the UK and reached no. 4, although no tracks were released as singles. Later in the year the singles "Black Magic Woman" (later a big hit for Santana) and "Need Your Love So Bad" were released.
The band's second studio album, "Mr. Wonderful", was released in August 1968. Like their first album, it was all blues. The album was recorded live in the studio with miked amplifiers and a PA system, rather than being plugged into the board. They also added horns and featured a friend of the band on keyboards, Christine Perfect of Chicken Shack.
Shortly after the release of "Mr. Wonderful", Fleetwood Mac recruited 18-year-old guitarist Danny Kirwan. He was in the South London blues trio Boilerhouse, consisting of Kirwan (guitar), Trevor Stevens (bass) and Dave Terrey (drums). Green and Fleetwood had watched Boilerhouse rehearse in a basement boiler-room, and Green had been so impressed that he invited the band to play support slots for Fleetwood Mac. Green wanted Boilerhouse to become a professional band but Stevens and Terrey were not prepared to turn professional, so Green tried to find another rhythm section for Kirwan by placing an ad in Melody Maker. There were over 300 applicants, but when Green and Fleetwood ran auditions at the Nag's Head in Battersea (home of the Mike Vernon Blue Horizon Club) the hard-to-please Green could not find anyone good enough. Fleetwood invited Kirwan to join Fleetwood Mac as a third guitarist.
Green was frustrated that Jeremy Spencer did not contribute to his songs. Kirwan, a talented self-taught guitarist, had a signature vibrato and a unique style that added a new dimension to the band's sound. In November 1968, with Kirwan in the band, they released their first number one single in Europe, "Albatross", on which Kirwan duetted with Green. Green said later that the success of 'Albatross' was thanks to Kirwan. "If it wasn't for Danny, I would never had had a number one hit record." In January 1969 they released their first compilation album "English Rose", which contained half of "Mr Wonderful" plus new songs from Kirwan. Their next and more successful compilation album,"The Pious Bird of Good Omen" was released in August and contained various singles, B-sides and tracks the band had done with Eddie Boyd.
On tour in the US in January 1969, the band recorded "Fleetwood Mac in Chicago" (released in December as a double album) at the soon-to-close Chess Records Studio with some of the blues legends of Chicago, including Willie Dixon, Buddy Guy and Otis Spann. These were Fleetwood Mac's last all-blues recordings. Along with the change of style the band was also going through label changes. Up until that point they had been on the Blue Horizon label, but with Kirwan in the band the musical possibilities had become too diverse for a blues-only label. The band signed with Immediate Records and released the single "Man of the World", which became another British and European hit. For the B-side Spencer fronted Fleetwood Mac as "Earl Vince and the Valiants" and recorded "Somebody's Gonna Get Their Head Kicked In Tonite", typifying the more raucous rock 'n' roll side of the band. Immediate Records was in bad shape, however, and the band shopped around for a new deal. The Beatles wanted the band on Apple Records (Mick Fleetwood and George Harrison were brothers-in-law), but the band's manager Clifford Davis decided to go with Warner Bros. Records (through Reprise Records, a Frank Sinatra-founded label), the label they have stayed with ever since.
Under the wing of Reprise, Fleetwood Mac released their third studio album, "Then Play On", in September 1969. Although the initial pressing of the American release of this album was the same as the British version, it was altered to contain the song "Oh Well", which featured consistently in live performances from the time of its release through 1997 and again starting in 2009. "Then Play On", the band's first rock album, was written by Kirwan and Green, plus a track each by Fleetwood and McVie. Jeremy Spencer, meanwhile, had recorded a solo album of 1950s-style rock and roll songs, backed by the rest of the band except Green.
By 1970, Green, the frontman of the band, had become a user of LSD. During the band's European tour, he experienced a bad acid trip at a hippie commune in Munich. Clifford Davis, the band's manager, singled out this incident as the crucial point in Green's mental decline. He said: "The truth about Peter Green and how he ended up how he did is very simple. We were touring Europe in late 1969. When we were in Germany, Peter told me he had been invited to a party. I knew there were going to be a lot of drugs around and I suggested that he didn't go. But he went anyway and I understand from him that he took what turned out to be very bad, impure LSD. He was never the same again." German author and filmmaker Rainer Langhans stated in his autobiography that he and Uschi Obermaier met Green in Munich and invited him to their Highfisch-Kommune, where the drinks were spiked with acid. Langhans and Obermaier were planning to organise an open-air "Bavarian Woodstock", for which they wanted Jimi Hendrix and The Rolling Stones to be the main acts, and they hoped Green would help them to get in contact with The Rolling Stones.
Green's last hit with Fleetwood Mac was "The Green Manalishi (With the Two-Prong Crown)". The track was recorded at Warner-Reprise's studios in Hollywood on the band's third US tour in April 1970, a few weeks before Green left the band. A live performance was recorded at the Boston Tea Party in February 1970, and the song was later recorded by Judas Priest. "Green Manalishi" was released as Green's mental stability deteriorated. He wanted the band to give all their money to charity, but the other members of the band disagreed.
In April, Green decided to quit the band after the completion of their European tour. His last show with Fleetwood Mac was on 20 May 1970. During that show the band went past their allotted time and the power was shut off, although Mick Fleetwood kept drumming. Some of the Boston Tea Party recordings (5/6/7 February 1970) were eventually released in the 1980s as the "Live in Boston" album. A more complete remastered three-volume compilation was released by Snapper Music in the late 1990s.
Kirwan and Spencer were left with the task of replacing Green in their live shows and on their recordings. In September 1970 Fleetwood Mac released their fourth studio album, "Kiln House." Kirwan's songs on the album moved the band in the direction of rock, while Spencer's contributions focused on re-creating the country-tinged "Sun Sound" of the late 1950s. Christine Perfect, who had retired from the music business after one unsuccessful solo album, contributed (uncredited) to "Kiln House", singing backup vocals and playing keyboards. She also drew the album cover. After "Kiln House", Fleetwood Mac were progressing and developing a new sound, and she was invited to join the band to help fill out the rhythm section. They released a single, Danny Kirwan's "Dragonfly" b/w "The Purple Dancer" in the UK and certain European countries, but despite good notices in the press it was not a success. The B-side has been reissued only once, on a Reprise German and Dutch-only "Best of" album. The single was re-issued on April 19th, 2014 for Record Store Day (RSD) 2014 in Europe on Blue Vinyl and in the U.S. on translucent purple vinyl. 
Christine Perfect, who by this point had married bassist John McVie, made her first appearance with the band as Christine McVie at Bristol University, England, in May 1969, just as she was leaving Chicken Shack. She had had success with the Etta James classic "I'd Rather Go Blind" and was twice voted female artist of the year in England. Christine McVie played her first gig as an official member of Fleetwood Mac on 1 August 1970 in New Orleans, Louisiana. CBS Records, which now owned Blue Horizon (except in the US and Canada), released the band's fourth compilation album, "The Original Fleetwood Mac", containing previously unreleased material. The album was relatively successful, and the band continued to gain popularity.
While on tour in February 1971, Jeremy Spencer said he was going out to "get a magazine" but never returned. After several days of frantic searching the band discovered that Spencer had joined a religious group, the Children of God. The band were liable for the remaining shows on the tour and asked Peter Green to step in as a replacement. Green brought along his friend Nigel Watson, who played the congas. (Twenty-five years later Green and Watson collaborated again to form the Peter Green Splinter Group.) Green was only back with Fleetwood Mac temporarily and the band began a search for a new guitarist. Green insisted on playing only new material and none he had written. He and Watson played only the last week of shows. The San Bernardino show on 20 February was taped.
In the summer of 1971 the band held auditions for a replacement guitarist at their large country home, "Benifold", which they had jointly bought with their manager Davis for £23,000 () prior to the "Kiln House" tour. A friend of the band, Judy Wong, recommended her high school friend Bob Welch, who was living in Paris, France, at the time. The band held a few meetings with Welch and decided to hire him, without actually playing with him, after they heard a tape of his songs.
In September 1971 the band released their fifth studio album, "Future Games". As a result of Welch's arrival and Spencer's departure, the album was different from anything they had done previously. While it became the band's first studio album to miss the charts in the UK, it helped to expand the band's appeal in the United States. In Europe CBS released Fleetwood Mac's first Greatest Hits album, which mostly consisted of songs by Peter Green, with one song by Spencer and one by Kirwan.
In 1972, six months after the release of "Future Games", the band released their sixth studio album, "Bare Trees". Mostly composed by Kirwan, "Bare Trees" featured the Welch-penned single "Sentimental Lady", which would be a much bigger hit for Welch five years later when he re-recorded it for his solo album "French Kiss", backed by Mick Fleetwood and Christine McVie. "Bare Trees" also featured "Spare Me a Little of Your Love", a bright Christine McVie song that became a staple of the band's live act throughout the early to mid-1970s.
While the band was doing well in the studio, their tours started to be problematic. By 1972 Danny Kirwan had developed an alcohol dependency and was becoming alienated from Welch and the McVies. When Kirwan smashed his Gibson Les Paul Custom guitar before a concert on a US tour in August 1972, refused to go on stage and criticised the band afterwards, Fleetwood fired him. Fleetwood said later that the pressure had become too much for Kirwan, and he had suffered a breakdown.
In the three albums they released in this period they constantly changed line-ups. In September 1972 the band added guitarist Bob Weston and vocalist Dave Walker, formerly of Savoy Brown and Idle Race. Bob Weston was well known as a slide guitarist and had known the band from his touring period with Long John Baldry. Fleetwood Mac also hired Savoy Brown's road manager, John Courage. Fleetwood, The McVies, Welch, Weston and Walker recorded the band's seventh studio album, "Penguin", which was released in January 1973. After the tour the band fired Walker because they felt his vocal style and attitude did not fit well with the rest of the band.
The remaining five members carried on and recorded the band's eighth studio album, "Mystery to Me", six months later. This album contained Welch's song "Hypnotized", which received a great amount of airplay on the radio and became one of the band's most successful songs to date in the US. The band was proud of the new album and anticipated that it would be a smash hit. While it did eventually go Gold, personal problems within the band emerged. The McVies' marriage was under a lot of stress, which was aggravated by their constant working with each other and by John McVie's considerable alcohol abuse. Subsequent lack of touring meant that the album was unable to chart as high as the previous one.
During the 1973 US tour to promote "Mystery to Me", Weston had an affair with Fleetwood's wife Jenny Boyd Fleetwood, sister of Pattie Boyd Harrison. Fleetwood was said to have been emotionally devastated by this, and could not continue with the tour. Courage fired Weston and two weeks in, with another twenty-six concerts scheduled, the tour was cancelled. The last date played was Lincoln, Nebraska, on 20 October 1973. In a late-night meeting after that show, the band told their sound engineer that the tour was over and Fleetwood Mac was splitting up.
In late 1973, after the collapse of the US tour, the band's manager, Clifford Davis, was left with major touring commitments to fulfil and no band. Fleetwood Mac had "temporarily disbanded" in Nebraska and its members had gone their separate ways. Davis was concerned that failing to complete the tour would destroy his reputation with bookers and promoters. He sent the band a letter in which he said he "hadn't slaved for years to be brought down by the whims of irresponsible musicians". Davis claimed that he owned the name 'Fleetwood Mac' and the right to choose the band members, and he recruited members of the band Legs, which had recently issued one single under Davis's management, to tour the US in early 1974 under the name 'The New Fleetwood Mac' and perform the rescheduled dates. This band - who former guitarist Dave Walker said were "very good" - consisted of Elmer Gantry (Dave Todd, formerly of Velvet Opera: vocals, guitar), Kirby Gregory (formerly of Curved Air: guitar), Paul Martinez (formerly of the Downliners Sect: bass), John Wilkinson (also known as Dave Wilkinson: 
keyboards) and Australian drummer Craig Collinge (formerly of Manfred Mann Ch III, the Librettos, Procession and Third World War).
The members of this group were told that Fleetwood would join them after the tour had started, to validate the use of the name, and claimed that he had been involved in planning it. Davis and others stated that Fleetwood had committed himself to the project and had given instructions to hire musicians and rehearse the band. Davis said Collinge had been hired only as a temporary stand-in drummer for rehearsals and the first two gigs, and that Fleetwood had agreed to appear on the rest of the tour, but then had backed out after the tour started. Fleetwood said later that he had not promised to appear on the tour.
The 'New Fleetwood Mac' tour began on 16 January 1974 at the Syria Mosque in Pittsburgh, Pennsylvania, and according to one of the band members, the first concert "went down a storm". The promoter was dubious at first, but said later that the crowd had loved the band and they were "actually really good." More successful gigs followed, but then word got around that this was not the real Fleetwood Mac and audiences became hostile. The band was turned away from several gigs and the next half-dozen were pulled by promoters. The band struggled on and played further dates in the face of increasing hostility and heckling, more dates were pulled, the keyboard player quit, and after a concert in Edmonton where bottles were thrown at the stage, the tour collapsed. The band dissolved and the remainder of the tour was cancelled.
The lawsuit that followed regarding who owned the rights to the name 'Fleetwood Mac' put the original Fleetwood Mac on hiatus for almost a year. Although the band was named after Mick Fleetwood and John McVie, they had apparently signed contracts in which they had forfeited the rights to the name. Their record company, Warner Bros. Records, when appealed to, said they didn't know who owned it. The dispute was eventually settled amicably out of court, four years later, in what was described as "a reasonable settlement not unfair to either party." In later years Fleetwood said that, in the end, he was grateful to Davis because the lawsuit was the reason the band moved to California.
Nobody from the alternative lineup was ever made a part of the real Fleetwood Mac, although some of them later played in Danny Kirwan's studio band. Gantry and Gregory went on to become members of Stretch, whose 1975 UK hit single "Why Did You Do It" was written about the touring debacle. Gantry later collaborated with the Alan Parsons Project. Martinez went on to play with the Deep Purple offshoot Paice Ashton Lord, as well as Robert Plant's backing band.
While the other band had been on tour, Welch stayed in Los Angeles and connected with entertainment attorneys. He realised that the original Fleetwood Mac was being neglected by Warner Bros and that they would need to change their base of operation from England to America, to which the rest of the band agreed. Rock promoter Bill Graham wrote a letter to Warner Bros to convince them that the real Fleetwood Mac was, in fact, Fleetwood, Welch, and the McVies. This did not end the legal battle but the band was able to record as Fleetwood Mac again. Instead of hiring another manager, Fleetwood Mac, having re-formed, became the only major rock band managed by the artists themselves.
In September 1974, Fleetwood Mac signed a new recording contract with Warner Bros, but remained on the Reprise label. In the same month the band released their ninth studio album, "Heroes Are Hard to Find". This was the first time Fleetwood Mac had only one guitarist. While on tour they added a second keyboardist, Doug Graves, who had been an engineer on "Heroes Are Hard to Find". In late 1974 Graves was preparing to become a permanent member of the band by the end of their US tour. He said:
However, Graves did not ultimately join full-time. In 1980, Christine McVie explained the decision:
Robert ("Bobby") Hunt, who had been in the band Head West with Bob Welch back in 1970, replaced Graves. Neither musician proved to be a long-term addition to the line-up. Welch left soon after the tour ended (on 5 December 1974 at Cal State University), having grown tired of touring and legal struggles. Nevertheless, the tour had enabled the "Heroes" album to reach a higher position on the American charts than any of the band's previous records.
After Welch decided to leave the band, Fleetwood began searching for a replacement. Whilst he was checking out Sound City Studios in Los Angeles, the house engineer, Keith Olsen, played him a track he had recorded, "Frozen Love", from the album "Buckingham Nicks" (1973). Fleetwood liked it and was introduced to the guitarist from the band, Lindsey Buckingham, who was at Sound City that day recording demos. Fleetwood asked him to join Fleetwood Mac and Buckingham agreed, on the condition that his music partner and girlfriend, Stevie Nicks, be included. Buckingham and Nicks joined the band on New Year's Eve 1974, within four weeks of the previous incarnation splitting.
In 1975, the new line-up released another self-titled album, their tenth studio album. The album was a breakthrough for the band and became a huge hit, reaching No.1 in the US and selling over 7 million copies. Among the hit singles from this album were Christine McVie's "Over My Head" and "Say You Love Me" and Stevie Nicks's "Rhiannon", as well as the much-played album track "Landslide", a live rendition of which became a hit twenty years later on "The Dance" album.
In 1976, the band was suffering from severe stress. With success came the end of John and Christine McVie's marriage, as well as Buckingham and Nicks's long-term romantic relationship. Fleetwood, meanwhile, was in the midst of divorce proceedings from his wife, Jenny. The pressure on Fleetwood Mac to release a successful follow-up album, combined with their new-found wealth, led to creative and personal tensions which were allegedly fuelled by high consumption of drugs and alcohol.
The band's eleventh studio album, "Rumours" (the band's first release on the main Warner label after Reprise was retired and all of its acts were reassigned to the parent label), was released in the spring of 1977. In this album, the band members laid bare the emotional turmoil they were experiencing at the time. "Rumours" was critically acclaimed and won the Grammy Award for Album of the Year in 1977. The album generated four Top Ten singles: Buckingham's "Go Your Own Way", Nicks's US No. 1 "Dreams" and Christine McVie's "Don't Stop" and "You Make Loving Fun". Buckingham's "Second Hand News", Nicks's "Gold Dust Woman" and "The Chain" (the only song written by all five band members) also received significant radio airplay. By 2003 "Rumours" had sold over 19 million copies in the US alone (certified as a diamond album by the RIAA) and a total of 40 million copies worldwide, bringing it to eighth on the list of best-selling albums. Fleetwood Mac supported the album with a lucrative tour.
On 10 October 1979, Fleetwood Mac were honoured with a star on the Hollywood Walk of Fame for their contributions to the music industry at 6608 Hollywood Boulevard.
Buckingham convinced Fleetwood to let his work on their next album be more experimental, and to be allowed to work on tracks at home before bringing them to the rest of the band in the studio. The result of this, the band's twelfth studio album "Tusk", was a 20-track double album released in 1979. It produced three hit singles: Buckingham's "Tusk" (US No. 8), which featured the USC Trojan Marching Band, Christine McVie's "Think About Me" (US No. 20), and 
Nicks's six-and-a-half minute opus "Sara" (US No. 7). "Sara" was cut to four-and-a-half minutes for both the hit single and the first CD-release of the album, but the unedited version has since been restored on the 1988 greatest hits compilation, the 2004 reissue of "Tusk" and Fleetwood Mac's 2002 release of "The Very Best of Fleetwood Mac". Original guitarist Peter Green also took part in the sessions of "Tusk" although his playing, on the Christine McVie track "Brown Eyes", is not credited on the album. In an interview in 2019 Fleetwood described "Tusk" as his "personal favourite" and said, “Kudos to Lindsey ... for us not doing a replica of "Rumours"."
"Tusk" sold four million copies worldwide. Fleetwood blamed the album's relative lack of commercial success on the RKO radio chain having played the album in its entirety prior to release, thereby allowing mass home taping.
The band embarked on an 11-month tour to support and promote "Tusk". They travelled across the world, including the US, Australia, New Zealand, Japan, France, Belgium, Germany, the Netherlands, and the United Kingdom. In Germany, they shared the bill with reggae superstar Bob Marley. On this world tour, the band recorded music for their first live album, which was released at the end of 1980.
The band's thirteenth studio album, "Mirage", was released in 1982. Following 1981 solo albums by Nicks ("Bella Donna"), Fleetwood ("The Visitor"), and Buckingham ("Law and Order"), there was a return to a more conventional approach. Buckingham had been chided by critics, fellow band members and music business managers for the lesser commercial success of "Tusk". Recorded at Château d'Hérouville in France and produced by Richard Dashut, "Mirage" was an attempt to recapture the huge success of "Rumours". Its hits included Christine McVie's "Hold Me" and "Love in Store" (co-written by Robbie Patton and Jim Recor, respectively), Nicks's "Gypsy", and Buckingham's "Oh Diane", which made the Top 10 in the UK. A minor hit was also scored by Buckingham's "Eyes Of The World" and "Can't Go Back".
In contrast to the Tusk Tour the band embarked on only a short tour of 18 American cities, the Los Angeles show being recorded and released on video. They also headlined the first US Festival, on 5 September 1982, for which the band was paid $500,000 ($ today). "Mirage" was certified double platinum in the US.
Following "Mirage" the band went on hiatus, which allowed members to pursue solo careers. Nicks released two more solo albums (1983's "The Wild Heart" and 1985's "Rock a Little"). Buckingham issued "Go Insane" in 1984, the same year that Christine McVie made an eponymous album (yielding the Top 10 hit "Got a Hold on Me" and the Top 40 hit "Love Will Show Us How"). All three met with success, Nicks being the most popular. During this period Fleetwood had filed for bankruptcy, Nicks was admitted to the Betty Ford Clinic for addiction problems and John McVie had suffered an addiction-related seizure, all of which were attributed to the lifestyle of excess afforded to them by their worldwide success. It was rumoured that Fleetwood Mac had disbanded, but Buckingham commented that he was unhappy to allow "Mirage" to remain as the band's last effort.
The "Rumours" line-up of Fleetwood Mac recorded one more album, their fourteenth studio album, "Tango in the Night", in 1987. As with various other Fleetwood Mac albums, the material started off as a Buckingham solo album before becoming a group project. The album went on to become their best-selling release since "Rumours", especially in the UK where it hit No. 1 three times in the following year. The album sold three million copies in the US and contained four hits: Christine McVie's "Little Lies" and "Everywhere" ('Little Lies' being co-written with McVie's new husband Eddy Quintela), Sandy Stewart and Nicks's "Seven Wonders", and Buckingham's "Big Love". "Family Man" (Buckingham and Richard Dashut), and "Isn't It Midnight" (Christine McVie), were also released as singles, with less success.
With a ten-week tour scheduled, Buckingham held back at the last minute, saying he felt his creativity was being stifled. A group meeting at Christine McVie's house on 7 August 1987 resulted in turmoil. Tensions were coming to a head. Fleetwood said in his autobiography that there was a physical altercation between Buckingham and Nicks. Buckingham left the band the following day. After Buckingham's departure Fleetwood Mac added two new guitarists to the band, Billy Burnette and Rick Vito, again without auditions.
Burnette was the son of Dorsey Burnette and nephew of Johnny Burnette, both of The Rock and Roll Trio. He had already worked with Fleetwood in Zoo, with Christine McVie as part of her solo band, had done some session work with Nicks, and backed Buckingham on "Saturday Night Live". Fleetwood and Christine McVie had played on his "Try Me" album in 1985. Vito, a Peter Green admirer, had played with many artists from Bonnie Raitt to John Mayall, to Roger McGuinn in Thunderbyrd and worked with John McVie on two Mayall albums.
The 1987–88 "Shake the Cage" tour was the first outing for this line-up. It was successful enough to warrant the release of a concert video, entitled "Tango in the Night", which was filmed at San Francisco's Cow Palace arena in December 1987.
Capitalising on the success of "Tango in the Night", the band released a "Greatest Hits" album in 1988. It featured singles from the 1975–1988 era and included two new compositions, "No Questions Asked" written by Nicks and "As Long as You Follow", written by Christine McVie and Quintela. 'As Long as You Follow' was released as a single in 1988 but only made No. 43 in the US and No.66 in the UK, although it reached No.1 on the US Adult Contemporary charts. The "Greatest Hits" album, which peaked at No. 3 in the UK and No. 14 in the US (though it has since sold over 8 million copies there) was dedicated by the band to Buckingham, with whom they were now reconciled.
In 1990, Fleetwood Mac released their fifteenth studio album, "Behind the Mask". With this album the band veered away from the stylised sound that Buckingham had evolved during his tenure in the band (which was also evident in his solo work) and developed a more adult contemporary style with producer Greg Ladanyi. The album yielded only one Top 40 hit, Christine McVie's "Save Me". "Behind the Mask" only achieved Gold album status in the US, peaking at No. 18 on the "Billboard" album chart, though it entered the UK Albums Chart at No. 1. It received mixed reviews and was seen by some music critics as a low point for the band in the absence of Buckingham (who had actually made a guest appearance playing on the title track). But "Rolling Stone" magazine said that Vito and Burnette were "the best thing to ever happen to Fleetwood Mac". The subsequent "Behind the Mask" tour saw the band play sold-out shows at London's Wembley Stadium. In the final show in Los Angeles, Buckingham joined the band on stage. The two women of the band, McVie and Nicks, had decided that the tour would be their last (McVie's father had died during the tour), although both stated that they would still record with the band. In 1991, however, Nicks and Rick Vito left Fleetwood Mac altogether.
In 1992, Fleetwood arranged a 4-disc box set, spanning highlights from the band's 25-year history, entitled "25 Years – The Chain" (an edited 2-disc set was also available). A notable inclusion in the box set was "Silver Springs", a Nicks composition that was recorded during the "Rumours" sessions but was omitted from the album and used as the B-side of "Go Your Own Way". Nicks had requested use of this track for her 1991 best-of compilation "TimeSpace", but Fleetwood had refused as he had planned to include it in this collection as a rarity. The disagreement between Nicks and Fleetwood garnered press coverage and was believed to have been the main reason for Nicks leaving the band in 1991. The box set also included a new Nicks/Rick Vito composition, "Paper Doll", which was released in the US as a single and produced by Buckingham and Richard Dashut. There were also two new Christine McVie compositions, "Heart of Stone" and "Love Shines". "Love Shines" was released as a single in the UK and elsewhere. Buckingham also contributed a new song, "Make Me a Mask". Fleetwood also released a deluxe hardcover companion book to coincide with the release of the box set, titled "My 25 Years in Fleetwood Mac". The volume featured notes written by Fleetwood detailing the band's 25-year history and many rare photographs.
The Buckingham/Nicks/McVie/McVie/Fleetwood line-up reunited in 1993 at the request of US President Bill Clinton for his first Inaugural Ball. Clinton had made Fleetwood Mac's "Don't Stop" his campaign theme song. His request for it to be performed at the Inauguration Ball was met with enthusiasm by the band, although this line-up had no intention of reuniting again.
Inspired by the new interest in the band, Mick Fleetwood, John McVie, and Christine McVie recorded another album as Fleetwood Mac, with Billy Burnette taking lead guitar duties. Burnette left in March 1993 to record a country album and pursue an acting career and Bekka Bramlett, who had worked a year earlier with Fleetwood's Zoo, was recruited to take his place. Solo singer-songwriter/guitarist and Traffic member Dave Mason, who had worked with Bekka's parents Delaney & Bonnie twenty-five years earlier, was subsequently added. In March 1994 Billy Burnette, a good friend and co-songwriter with Delaney Bramlett, returned to the band with Fleetwood's blessing.
The band, minus Christine McVie, toured in 1994, opening for Crosby, Stills, & Nash and in 1995 as part of a package with REO Speedwagon and Pat Benatar. This tour saw the band perform classic Fleetwood Mac songs from their 1967–1974 era. In 1995, at a concert in Tokyo, the band was greeted by former member Jeremy Spencer, who performed a few songs with them.
On 10 October 1995, Fleetwood Mac released their sixteenth studio album, "Time", which was not a success. Although it hit the UK Top 60 for one week, the album had zero impact in the US. It failed to graze the "Billboard" Top 200 albums chart, a reversal for a band that had been a mainstay on that chart for most of the previous two decades. Shortly after the album's release, Christine McVie informed the band that the album would be her last. Bramlett and Burnette subsequently formed a country music duo, Bekka & Billy.
Just weeks after disbanding Fleetwood Mac, Mick Fleetwood started working with Lindsey Buckingham again. John McVie was added to the sessions, and later Christine McVie. Stevie Nicks also enlisted Buckingham to produce a song for a soundtrack.
In May 1996 Fleetwood, John McVie, Christine McVie, and Nicks performed together at a private party in Louisville, Kentucky, prior to the Kentucky Derby, with Steve Winwood filling in for Buckingham. A week later the "Twister" film soundtrack was released, which featured the Nicks-Buckingham duet "Twisted", with Fleetwood on drums. This eventually led to a full reunion of the "Rumours" line-up, which officially reformed in March 1997.
The regrouped Fleetwood Mac performed a live concert on a soundstage at Warner Bros. Burbank, California, on 22 May 1997. The concert was recorded, and from this performance came the 1997 live album "The Dance", which brought the band back to the top of the US album charts for the first time in 10 years. "The Dance" returned Fleetwood Mac to a superstar status they had not enjoyed since "Tango in the Night". The album was certified 5 million units by the RIAA. An arena tour followed the MTV premiere of "The Dance" and kept the reunited Fleetwood Mac on the road throughout much of 1997, the 20th anniversary of "Rumours". With additional musicians Neale Heywood on guitar, Brett Tuggle on keyboards, Lenny Castro on percussion and Sharon Celani (who had toured with the band in the late 1980s) and Mindy Stein on backing vocals, this would be the final appearance of the classic line-up including Christine McVie for 16 years. Neale Heywood and Sharon Celani remain touring members to this day.
In 1998 Fleetwood Mac were inducted into the Rock and Roll Hall of Fame. Members inducted included the original band, Mick Fleetwood, John McVie, Peter Green, Jeremy Spencer and Danny Kirwan, and "Rumours"-era members Christine McVie, Stevie Nicks and Lindsey Buckingham. Bob Welch was not included, despite his key role in keeping the band alive during the early 1970s. The "Rumours"-era version of the band performed both at the induction ceremony and at the Grammy Awards programme that year. Peter Green attended the induction ceremony but did not perform with his former bandmates, opting instead to perform his composition "Black Magic Woman" with Santana, who were inducted the same night. Neither Jeremy Spencer nor Danny Kirwan attended. Fleetwood Mac also received the "Outstanding Contribution to Music" award at the Brit Awards (British Phonographic Industry Awards) the same year.
In 1998 Christine McVie left the band. Her departure left Buckingham and Nicks to sing all the lead vocals for the band's seventeenth album, "Say You Will", released in 2003, although Christine contributed some backing vocals and keyboards. The album debuted at No.3 on the "Billboard" 200 chart (No. 6 in the UK) and yielded chart hits with "Peacekeeper" and the title track, and a successful world arena tour which lasted through 2004. The tour grossed $27,711,129 and was ranked No. 21 in the top 25 grossing tours of 2004.
Around 2004–05 there were rumours of a reunion of the early line-up of Fleetwood Mac involving Peter Green and Jeremy Spencer. While these two apparently remained unconvinced, in April 2006 bassist John McVie, during a question-and-answer session on the "Penguin" Fleetwood Mac fan website, said of the reunion idea:
In interviews given in November 2006 to support his solo album "Under the Skin", Buckingham stated that plans for the band to reunite once more for a 2008 tour were still on the cards. Recording plans had been put on hold for the foreseeable future. In an interview Nicks gave to the UK newspaper "The Daily Telegraph" "i" in September 2007, she stated that she was unwilling to carry on with the band unless Christine McVie returned.
In March 2008, it was mooted that Sheryl Crow might work with Fleetwood Mac in 2009. Crow and Stevie Nicks had collaborated in the past and Crow had stated that Nicks had been a great teacher and inspiration to her. Later, Buckingham said that the potential collaboration with Crow had "lost its momentum". and the idea was abandoned.
In March 2009, Fleetwood Mac started their "Unleashed" tour, again without Christine McVie. It was a greatest hits show, although album tracks such as "Storms" and "I Know I'm Not Wrong" were also played. 
During their show on 20 June 2009 in New Orleans, Louisiana, Stevie Nicks premiered part of a new song that she had written about Hurricane Katrina. The song was later released as "New Orleans" on Nicks's 2011 album "In Your Dreams" with Mick Fleetwood on drums. In October 2009 and November the band toured Europe, followed by Australia and New Zealand in December. In October, The Very Best of Fleetwood Mac was re-released in an extended two-disc format (this format having been released in the US in 2002), entering at number six on the UK Albums Chart. On 1 November 2009 a one-hour documentary, "Fleetwood Mac: Don't Stop", was broadcast in the UK on BBC One, featuring recent interviews with all four current band members. During the documentary Nicks gave a candid summary of the current state of her relationship with Buckingham, saying "Maybe when we're 75 and Fleetwood Mac is a distant memory, we might be friends."
On 6 November 2009, Fleetwood Mac played the last show of the European leg of their "Unleashed" tour at London's Wembley Arena. Christine McVie was present in the audience. Nicks paid tribute to her from the stage to a standing ovation from the audience, saying that she thought about her former bandmate "every day", and dedicated that night's performance of "Landslide" to her. On 19 December 2009 Fleetwood Mac played the second-to-last show of their "Unleashed" tour to a sell-out crowd in New Zealand, at what was originally intended to be a one-off event at the TSB Bowl of Brooklands in New Plymouth. Tickets, after pre-sales, sold out within twelve minutes of public release. Another date, Sunday 20 December, was added and also sold out. The tour grossed $84,900,000 and was ranked No. 13 in the highest grossing worldwide tours of 2009. On 19 October 2010, Fleetwood Mac played a private show at the Phoenician Hotel in Scottsdale, Arizona for TPG (Texas Pacific Group).
On 3 May 2011, the Fox Network broadcast an episode of "Glee" entitled "Rumours" that featured six songs from the band's 1977 album. The show sparked renewed interest in the band and its commercially most successful album, and "Rumours" re-entered the "Billboard" 200 chart at No. 11 in the same week that Nicks's new solo album "In Your Dreams" debuted at No. 6. (She was quoted by "Billboard" saying that her new album was "my own little "Rumours".") The two recordings sold about 30,000 and 52,000 units respectively. Music downloads accounted for 91 percent of the "Rumours" sales. The spike in sales for "Rumours" represented an increase of 1,951%. It was the highest chart entry by a previously issued album since "The Rolling Stones"' reissue of "Exile On Main St." re-entered the chart at No. 2 on 5 June 2010. In an interview in July 2012 Nicks confirmed that the band would reunite for a tour in 2013.
Original Fleetwood Mac bassist Bob Brunning died on 18 October 2011 at the age of 68. Former guitarist and singer Bob Weston was found dead on 3 January 2012 at the age of 64. Former singer and guitarist Bob Welch was found dead from a self-inflicted gunshot wound on 7 June 2012 at the age of 66. Don Aaron, a spokesman at the scene, stated, "He died from an apparent self-inflicted gunshot wound to the chest." A suicide note was found. Welch had been struggling with health issues and was dealing with depression. His wife discovered his body.
The band's 2013 tour, which took place in 34 cities, started on 4 April in Columbus, OH. The band performed two new songs ("Sad Angel" and "Without You"), which Buckingham described as some of the most "Fleetwood Mac-ey" sounding songs since "Mirage". 'Without You' was re-recorded from the Buckingham-Nicks era. The band released their first new studio material in ten years, "Extended Play", on 30 April 2013. The EP debuted and peaked at No. 48 in the US and produced one single, "Sad Angel". On 25 and 27 September 2013, the second and third nights of the band's London O2 shows, Christine McVie joined them on stage for "Don't Stop".
On 27 October 2013, the band cancelled their New Zealand and Australian performances after John McVie had been diagnosed with cancer, so that he could undergo treatment. They said: "We are sorry not to be able to play these Australian and New Zealand dates. We hope our Australian and New Zealand fans as well as Fleetwood Mac fans everywhere will join us in wishing John and his family all the best." In November 2013, Christine McVie expressed interest in a return to Fleetwood Mac, and also affirmed that John McVie's prognosis was "really good".
On 11 January 2014, Mick Fleetwood confirmed that Christine McVie would be rejoining Fleetwood Mac. In October 2013 Stevie Nicks appeared in "" with Fleetwood Mac's song "Seven Wonders" playing in the background.
On with the Show, a 33-city North American tour, opened in Minneapolis, Minnesota, on 30 September 2014. A series of May–June 2015 arena dates in the United Kingdom went on sale on 14 November, selling out in minutes. Due to high demand, additional dates were added to the tour, including an Australian leg.
In January 2015, Buckingham suggested that the new album and tour might be Fleetwood Mac's last, and that the band would cease operations in 2015 or soon afterwards. He concluded: "We're going to continue working on the new album and the solo stuff will take a back seat for a year or two. A beautiful way to wrap up this last act." But Mick Fleetwood stated that the new album might take a few years to complete and that they were waiting for contributions from Nicks, who had been ambivalent about committing to a new record.
In August 2016, Fleetwood revealed that while the band had "a huge amount of recorded music", virtually none of it featured Nicks. Buckingham and Christine McVie, however, had contributed multiple songs to the new project. Fleetwood told "Ultimate Classic Rock": "She [McVie] ... wrote up a storm ... She and Lindsey could probably have a mighty strong duet album if they want. In truth, I hope it will come to more than that. There really are dozens of songs. And they’re really good. So we’ll see."
Nicks explained her reluctance to record another album with Fleetwood Mac. "Is it possible that Fleetwood Mac might do another record? I can never tell you yes or no, because I don't know. I honestly don't know... It's like, do you want to take a chance of going in and setting up in a room for like a year [to record an album] and having a bunch of arguing people? And then not wanting to go on tour because you just spent a year arguing?". She also emphasized that people do not buy as many records as they used to.
On 9 June 2017, Buckingham and Christine McVie released a new album, titled "Lindsey Buckingham/Christine McVie", which included contributions from Mick Fleetwood and John McVie. The album was preceded by the single "In My World". A 38-date tour began on 21 June and concluded 16 November. Fleetwood Mac also planned to embark on another tour in 2018. The band headlined the second night of the Classic West concert (on 16 July 2017 at Dodger Stadium in Los Angeles) and the second night of the Classic East concert (at New York's Citi Field on 30 July 2017).
The band received the MusiCares Person of the Year award in 2018 and reunited to perform several songs at the Grammy-hosted gala honouring them. Artists including Lorde, Harry Styles, Little Big Town and Miley Cyrus also performed.
In April 2018, the song "Dreams" re-entered the Hot Rock Songs chart at No. 16 after a viral meme had featured the song. This chart re-entry came 40 years after the song had topped the Hot 100. The song's streaming totals also translated into 7,000 "equivalent album units", a jump of 12 percent, which helped "Rumours" to go from No. 21 to No. 13 on the Top Rock Albums chart.
That month Buckingham departed from the group a second time, having reportedly been dismissed. The reason was said to have been a disagreement about the nature of the tour, and in particular the question of whether newer or less well-known material would be included, as Buckingham wanted. Mick Fleetwood and the band appeared on "CBS This Morning" on 25 April 2018 and said that Buckingham would not sign off on a tour that the group had been planning for a year and a half and they had reached a "huge impasse" and "hit a brick wall". When asked if Buckingham had been fired, he said, "Well, we don't use that word because I think it's ugly." He also said that "Lindsey has huge amounts of respect and kudos to what he's done within the ranks of Fleetwood Mac and always will." In October 2018, Buckingham filed a lawsuit against Fleetwood Mac for breach of fiduciary duty, breach of oral contract and intentional interference with prospective economic advantage, among other charges. He stated that they eventually came to a settlement, which he would not share the terms of, but claimed he was "happy enough with it". Buckingham also told his version of what had led to his departure from the band. Two days after their performance at the MusiCares event he got a phone call from the band's manager Irving Azoff, who had a list of things that, as Buckingham puts it, “Stevie took issue with” that evening, including the guitarist’s outburst just before the band’s set over the intro music [for their acceptance speech being] the studio recording of Nicks’ “Rhiannon” — and the way he “smirked” during Nicks’ thank-you speech. Buckingham concedes the first point. “It wasn’t about it being ‘Rhiannon,’ ” he says. “It just undermined the impact of our entrance. That’s me being very specific about the right and wrong way to do something.” As for smirking, “The irony is that we have this standing joke that Stevie, when she talks, goes on a long time,” Buckingham says. “I may or may not have smirked. But I look over and Christine and Mick are doing the waltz behind her as a joke.” At the end of that call, Buckingham assumed Nicks was quitting Fleetwood Mac. He wrote an e-mail to Fleetwood assuring the drummer that the group could continue. There was no reply. A couple of days later, Buckingham says, “I called Irving and said, ‘This feels funny. Is Stevie leaving the band, or am I getting kicked out?’ ” Azoff told the guitarist he was “getting ousted” and that Nicks gave the rest of the band “an ultimatum: Either you go or she’s gonna go.” 
Former Tom Petty and the Heartbreakers guitarist Mike Campbell and Neil Finn of Crowded House were named to replace Buckingham. On "CBS This Morning", Fleetwood said that Fleetwood Mac had been reborn and that "This is the new lineup of Fleetwood Mac." Aside from touring, the band plans to record new music with Campbell and Finn in the future. The band's "An Evening with Fleetwood Mac" tour started in October 2018. The band launched the tour at the iHeartRadio Music Festival on 21 September 2018 at the T-Mobile Arena in Las Vegas, NV.
Danny Kirwan, guitarist, songwriter and early member of Fleetwood Mac (1968–1972) died in London, England, on 8 June 2018, aged 68. An obituary in "The New York Times" said he had died in his sleep after contracting pneumonia earlier in the year. The British music magazine "Mojo" quoted Christine McVie as saying: "Danny Kirwan was "the" white English blues guy. Nobody else could play like him. He was a one-off ... Danny and Peter [Green] gelled so well together. Danny had a very precise, piercing vibrato – a unique sound ... He was a perfectionist; a fantastic musician and a fantastic writer." One of Kirwan's songs, "Tell Me All the Things You Do" from the 1970 album "Kiln House", was included in the set of the 2018–19 An Evening with Fleetwood Mac tour.
On 28 May 2020 Neil Finn, featuring Nicks and McVie, with Campbell on guitar, released the song “Find Your Way Back Home” for the Auckland, New Zealand homeless shelter Auckland City Mission. 
Founding member Peter Green died on 25 July 2020 at the age of 73.
The following is a list of awards and nominations received by Fleetwood Mac:

</doc>
<doc id="11788" url="https://en.wikipedia.org/wiki?curid=11788" title="Frederick I, Margrave of Brandenburg-Ansbach">
Frederick I, Margrave of Brandenburg-Ansbach

Frederick I of Ansbach and Bayreuth (also known as Frederick V; or ; 8 May 1460 – 4 April 1536) was born at Ansbach as the eldest son of Albert III, Margrave of Brandenburg by his second wife Anna, daughter of Frederick II, Elector of Saxony. His elder half-brother was the Elector Johann Cicero of Brandenburg. Friedrich succeeded his father as Margrave of Ansbach in 1486 and his younger brother Siegmund as Margrave of Bayreuth in 1495. 
After depleting the finances of the margraviate with his lavish lifestyle, Frederick I was deposed by his two elder sons, Casimir and George, in 1515. He was then locked up at Plassenburg Castle by his eldest son Casimir in a tower room from which he could not escape for 12 years. Thereupon, his son Casimir took up the rule of the Margraviate of Bayreuth (Kulmbach) and his son George took up the rule of the Margraviate of Ansbach. However, the overthrow of Frederick did outrage his other younger sons and led to far-reaching political countermeasures. When Elector Joachim I of Brandenburg visited Kulmbach during his journey to Augsburg, and wanted to plead for Frederick's release, he was nevertheless denied entry to Plassenburg Castle. The dispute was finally cleared when an agreement was reached in 1522, in which the demands of the younger sons of Frederick were met. 
On 14 February 1479, at Frankfurt (Oder), Frederick I was married to Princess Sophia of Poland (6 April 1464 – 5 October 1512), daughter of King Casimir IV of Poland by his wife Elisabeth of Austria, and sister of King Sigismund I of Poland. They had seventeen children:
 

</doc>
<doc id="11790" url="https://en.wikipedia.org/wiki?curid=11790" title="F-Zero: Maximum Velocity">
F-Zero: Maximum Velocity

F-Zero: Maximum Velocity is a futuristic racing video game developed by NDcube and published by Nintendo as a launch title for the Game Boy Advance. The game was released in Japan, North America and Europe in 2001. It is the first "F-Zero" game to be released on a handheld game console.
"Maximum Velocity" takes place twenty-five years after "F-Zero", in another F-Zero Grand Prix. The past generations of F-Zero had "piloted their way to fame", so it is the only "F-Zero" game without Captain Falcon, Samurai Goroh, Pico, or Dr. Stewart. Players control fast hovering crafts and use their speed-boosting abilities to navigate through the courses as quickly as possible.
Every race consists of five laps around a race track. A player will lose the race if his or her machine explodes due to either taking too much damage or landing outside of the track, gets ejected from the race due to falling to 20th place or due to completing a lap with a rank outside of the rank limit of that lap, or he or she decides to give up. In the single player Grand Prix mode, all of these conditions requires the player to use an extra machine if and only if he or she has one or more spare machines to try again.
For each lap completed the player is rewarded with a speed boost, to be used once any time, one of the "SSS" marks will be shaded green to indicate that it can be used. A boost will dramatically increase a player's speed, but will decrease their ability to turn. A boost used before a jump will make the player jump farther, which could allow the player to use a shortcut with the right vehicle. Boost time and speed varies according to the machine, and is usually tuned for proper balance. For example, one machine boasts a boost time of twelve seconds, yet has the slowest boost speed of the entire game. Players can also take advantage of the varying deceleration of each vehicle. Some vehicles, such as the Jet Vermilion, take longer than others to decelerate from top boost speed to normal speed, once the boost has been used up. Players can also take advantage of this effect on boost pads.
The Grand Prix is the main single player component of "Maximum Velocity". It consists of four series named after chess pieces: "Pawn", "Knight", "Bishop" and "Queen". The latter of these can be unlocked by winning the others on "Expert" mode. They have five races in four difficulty settings, "Master" mode is unlocked by winning expert mode in each series, the player unlocks a new machine after completing it. The player needs to be in the top three at the end of the last lap in order to continue to the next race. If the player is unable to continue, the player will lose a machine and can try the race again. If the player runs out of machines, then the game ends, and the player has to start the series from the beginning.
Championship is another single player component. It is basically the same as a "Time Attack" mode, except the player can only race on one, special course: the Synobazz Championship Circuit. This special course is not selectable in any other modes.
"Maximum Velocity" can be played in two multiplayer modes using the Game Boy Advance link cable, with one cartridge, or one cartridge per player. Two to four Players can play in both modes.
In single cart, only one player needs to have a cartridge. The other players will boot off the link cable network from the player with the cart using the GBA's netboot capability. All players drive a generic craft, and the game can only be played on one level, Silence. Silence, along with Fire Field, are the only areas to return from previous games. Aptly, Silence in "Maximum Velocity" has no background music, unlike in most other F-Zero games.
In multi cart, each player needs to have a cartridge to play. This has many advantages over single cart: All players can use any machine in this game that has been unlocked by another player. Players can select any course in this game. After the race is finished, all of the players' ranking data are mixed and shared ("Mixed ranking" stored in each cart).
"F-Zero: Maximum Velocity" is one of the first titles to have been developed by NDcube. Like the original "F-Zero" for SNES, "Maximum Velocity" implements a pseudo-3D visual technique based on the scaling and rotation effects of bitmap graphics. In this game, this technique consists of a double-layer; one of which gives the illusion of depth.
"Maximum Velocity" is one of ten Game Boy Advance games released on December 16, 2011 to Nintendo 3DS Ambassadors, a program to give free downloadable games to early adopters who bought a Nintendo 3DS before its price drop. It was also released on the Wii U Virtual Console on April 3, 2014 in Japan and April 17, 2014 in North America and Europe.
On release, "Famitsu" magazine scored the game a 31 out of 40. "F-Zero: Maximum Velocity" went on to sell 334,145 copies in Japan and 273,229 copies in the U.S. as of 2005. The game has total sales of over 1 million copies worldwide and has an overall score of 86% on Metacritic and 83.37% on Game Rankings.

</doc>
<doc id="11794" url="https://en.wikipedia.org/wiki?curid=11794" title="Frederick William I of Prussia">
Frederick William I of Prussia

Frederick William I (; 14 August 1688 – 31 May 1740), known as the "Soldier King" (), was the King in Prussia and Elector of Brandenburg from 1713 until his death in 1740, as well as Prince of Neuchâtel. He was succeeded by his son, Frederick the Great.
He was born in Berlin to King Frederick I of Prussia and Princess Sophia Charlotte of Hanover. During his first years, he was raised by the Huguenot governess Marthe de Roucoulle.
His father had successfully acquired the title King for the Margraves of Brandenburg. On ascending the throne in 1713 (the year before his maternal grandmother’s death and the ascension of his maternal uncle George I of Great Britain to the British throne) the new King sold most of his father's horses, jewels and furniture; he did not intend to treat the treasury as his personal source of revenue the way Frederick I and many of the other German Princes had. Throughout his reign, Frederick William was characterized by his frugal, austere and militaristic lifestyle, as well as his devout Calvinist faith. He practiced rigid management of the treasury, never started a war, and led a simple and austere lifestyle, in contrast to the lavish court his father had presided over. At his death, Prussia had a sound exchequer and a full treasury, in contrast to the other German states.
Frederick William I did much to improve Prussia economically and militarily. He replaced mandatory military service among the middle class with an annual tax, and he established schools and hospitals. The king encouraged farming, reclaimed marshes, stored grain in good times and sold it in bad times. He dictated the manual of Regulations for State Officials, containing 35 chapters and 297 paragraphs in which every public servant in Prussia could find his duties precisely set out: a minister or councillor failing to attend a committee meeting, for example, would lose six months' pay; if he absented himself a second time, he would be discharged from the royal service. In short, Frederick William I concerned himself with every aspect of his relatively small country, ruling an absolute monarchy with great energy and skill.
In 1732, the king invited the Salzburg Protestants to settle in East Prussia, which had been depopulated by plague in 1709. Under the terms of the Peace of Augsburg, the Prince-Archbishop of Salzburg could require his subjects to practice the Catholic faith, but Protestants had the right to emigrate to a Protestant state. Prussian commissioners accompanied 20,000 Protestants to their new homes on the other side of Germany. Frederick William I personally welcomed the first group of migrants and sang Protestant hymns with them.
Frederick William intervened briefly in the Great Northern War, allied with Peter the Great of Russia, in order to gain a small portion of Swedish Pomerania; this gave Prussia new ports on the Baltic Sea coast. More significantly, aided by his close friend Prince Leopold of Anhalt-Dessau, the "Soldier-King" made considerable reforms to the Prussian army's training, tactics and conscription program—introducing the canton system, and greatly increasing the Prussian infantry's rate of fire through the introduction of the iron ramrod. Frederick William's reforms left his son Frederick with the most formidable army in Europe, which Frederick used to increase Prussia's power. The observation that "the pen is mightier than the sword" has sometimes been attributed to him. ("See as well:" "Prussian virtues".)
Although a highly effective ruler, Frederick William had a perpetually short temper which sometimes drove him to physically attack servants (or even his own children) with a cane at the slightest provocation. His violent, harsh nature was further exacerbated by his inherited porphyritic disease, which gave him gout, obesity and frequent crippling stomach pains. He also had a notable contempt for France, and would sometimes fly into a rage at the mere mention of that country, although this did not stop him from encouraging the immigration of French Huguenot refugees to Prussia.
Frederick William died in 1740 at age 51 and was interred at the Garrison Church in Potsdam. During World War II, in order to protect it from advancing allied forces, Hitler ordered the king's coffin, as well as those of Frederick the Great and Paul von Hindenburg, into hiding, first to Berlin and later to a salt mine outside of Bernterode. The coffins were later discovered by occupying American forces, who re-interred the bodies in St. Elisabeth's Church in Marburg in 1946. In 1953 the coffin was moved to Burg Hohenzollern, where it remained until 1991, when it was finally laid to rest on the steps of the altar in the Kaiser Friedrich Mausoleum in the Church of Peace on the palace grounds of Sanssouci. The original black marble sarcophagus collapsed at Burg Hohenzollern—the current one is a copper copy.
His eldest surviving son was Frederick II (Fritz), born in 1712. Frederick William wanted him to become a fine soldier. As a small child, Fritz was awakened each morning by the firing of a cannon. At the age of 6, he was given his own regiment of children to drill as cadets, and a year later, he was given a miniature arsenal.
The love and affection Frederick William had for his heir initially was soon destroyed due to their increasingly different personalities. Frederick William ordered Fritz to undergo a minimal education, live a simple Protestant lifestyle, and focus on the Army and statesmanship as he had. However, the intellectual Fritz was more interested in music, books and French culture, which were forbidden by his father as decadent and unmanly. As Fritz's defiance for his father's rules increased, Frederick William would frequently beat or humiliate Fritz (he preferred his younger sibling Augustus William). Fritz was beaten for being thrown off a bolting horse and wearing gloves in cold weather. After the prince attempted to flee to England with his tutor, Hans Hermann von Katte, the enraged King had Katte beheaded before the eyes of the prince, who himself was court-martialled. The court declared itself not competent in this case. Whether it was the king's intention to have his son executed as well (as Voltaire claims) is not clear. However, the Holy Roman Emperor Charles VI intervened, claiming that a prince could only be tried by the Imperial Diet of the Holy Roman Empire itself. Frederick was imprisoned in the Fortress of Küstrin from 2 September to 19 November 1731 and exiled from court until February 1732, during which time he was rigorously schooled in matters of state. After achieving a measure of reconciliation, Frederick William had his son married to Princess Elizabeth of Brunswick-Wolfenbüttel, whom Frederick despised, but then grudgingly allowed him to indulge in his musical and literary interests again. He also gifted him a stud farm in East Prussia, and Rheinsberg Palace. By the time of Frederick William's death in 1740, he and Frederick were on at least reasonable terms with each other.
Although the relationship between Frederick William and Frederick was clearly hostile, Frederick himself later wrote that his father "penetrated and understood great objectives, and knew the best interests of his country better than any minister or general."
Frederick William married his first cousin Sophia Dorothea of Hanover, George II's younger sister (daughter of his uncle, King George I of Great Britain and Sophia Dorothea of Celle) on 28 November 1706. Frederick William was faithful and loving to his wife but they did not have a happy relationship: Sophia Dorothea feared his unpredictable temper and resented him, both for allowing her no influence at court and for refusing to marry her children to their English cousins. She also abhorred his cruelty towards their son and heir Frederick (with whom she was close), although rather than trying to mend the relationship between father and son she frequently spurred Frederick on in his defiance. They had fourteen children, including:
He was the godfather of the Prussian envoy Friedrich Wilhelm von Thulemeyer and of his grand-nephew, Prince Edward Augustus of Great Britain.

</doc>
<doc id="11795" url="https://en.wikipedia.org/wiki?curid=11795" title="Felsic">
Felsic

In geology, felsic is an adjective describing igneous rocks that are relatively rich in elements that form feldspar and quartz. It is contrasted with mafic rocks, which are relatively richer in magnesium and iron. Felsic refers to silicate minerals, magma, and rocks which are enriched in the lighter elements such as silicon, oxygen, aluminium, sodium, and potassium. Felsic magma or lava is higher in viscosity than mafic magma/lava.
Felsic rocks are usually light in color and have specific gravities less than 3. The most common felsic rock is granite. Common felsic minerals include quartz, muscovite, orthoclase, and the sodium-rich plagioclase feldspars (albite-rich).
In modern usage, the term "acid rock", although sometimes used as a synonym, normally now refers specifically to a high-silica-content (greater than 63% SiO by weight) volcanic rock, such as rhyolite. Older, broader usage is now considered archaic. That usage, with the contrasting term "basic rock", was based on an incorrect idea, dating from the 19th century, that "silicic acid" was the chief form of silicon occurring in rocks.
The term "felsic" combines the words "feldspar" and "silica". The similarity of the resulting term "felsic" to the German "felsig", "rocky" (from "Fels", "rock"), is purely accidental. "Feldspar" is linked to German. It is a borrowing of "Feldspat". The link is therefore to German "Feld", meaning "field".
In order for a rock to be classified as felsic, it generally needs to contain more than 75% felsic minerals; namely quartz, orthoclase and plagioclase. Rocks with greater than 90% felsic minerals can also be called leucocratic, from the Greek words for white and dominance.
Felsite is a petrologic field term used to refer to very fine-grained or aphanitic, light-colored volcanic rocks which might be later reclassified after a more detailed microscopic or chemical analysis.
In some cases, felsic volcanic rocks may contain phenocrysts of mafic minerals, usually hornblende, pyroxene or a feldspar mineral, and may need to be named after their phenocryst mineral, such as 'hornblende-bearing felsite'.
The chemical name of a felsic rock is given according to the TAS classification of Le Maitre (1975). However, this only applies to volcanic rocks. If the rock is analyzed and found to be felsic but is metamorphic and has no definite volcanic protolith, it may be sufficient to simply call it a 'felsic schist'. There are examples known of highly sheared granites which can be mistaken for rhyolites.
For phaneritic felsic rocks, the QAPF diagram should be used, and a name given according to the granite nomenclature. Often the species of mafic minerals is included in the name, for instance, hornblende-bearing granite, pyroxene tonalite or augite megacrystic monzonite, because the term "granite" already assumes content with feldspar and quartz.
The rock texture thus determines the basic name of a felsic rock.

</doc>
<doc id="11797" url="https://en.wikipedia.org/wiki?curid=11797" title="Frisians">
Frisians

The Frisians are a Germanic ethnic group indigenous to the coastal parts of the Netherlands and northwestern Germany. They inhabit an area known as Frisia and are concentrated in the Dutch provinces of Friesland and Groningen and, in Germany, East Frisia and North Frisia (which was a part of Denmark until 1864). The Frisian languages are still spoken by more than 500,000 people; West Frisian is officially recognised in the Netherlands (in Friesland), and North Frisian and Saterland Frisian are recognised as regional languages in Germany.
The ancient Frisii enter recorded history in the Roman account of Drusus's 12 BC war against the Rhine Germans and the Chauci. They occasionally appear in the accounts of Roman wars against the Germanic tribes of the region, up to and including the Revolt of the Batavi around 70 AD. Frisian mercenaries were hired to assist the Roman invasion of Britain in the capacity of cavalry. They are not mentioned again until 296, when they were deported into Roman territory as "laeti" (i.e., Roman-era serfs; see Binchester Roman Fort and Cuneus Frisionum). The discovery of a type of earthenware unique to 4th century Frisia, called "terp Tritzum", shows that an unknown number of them were resettled in Flanders and Kent, probably as "laeti" under Roman coercion.
From the 3rd through the 5th centuries Frisia suffered marine transgressions that made most of the land uninhabitable, aggravated by a change to a cooler and wetter climate. Whatever population may have remained dropped dramatically, and the coastal lands remained largely unpopulated for the next two centuries. When conditions improved, Frisia received an influx of new settlers, mostly Angles and Saxons. These people would eventually be referred to as 'Frisians', though they were not necessarily descended from the ancient Frisii. It is these 'new Frisians' who are largely the ancestors of the medieval and modern Frisians.
By the end of the 6th century, Frisian territory had expanded westward to the North Sea coast and, in the 7th century, southward down to Dorestad. This farthest extent of Frisian territory is sometimes referred to as "Frisia Magna". Early Frisia was ruled by a High King, with the earliest reference to a 'Frisian King' being dated 678.
In the early 8th century the Frisian nobles came into increasing conflict with the Franks to their south, resulting in a series of wars in which the Frankish Empire eventually subjugated Frisia in 734. These wars benefited attempts by Anglo-Irish missionaries (which had begun with Saint Boniface) to convert the Frisian populace to Christianity, in which Saint Willibrord largely succeeded.
Some time after the death of Charlemagne, the Frisian territories were in theory under the control of the Count of Holland, but in practice the Hollandic counts, starting with Count Arnulf in 993, were unable to assert themselves as the sovereign lords of Frisia. The resulting stalemate resulted in a period of time called the 'Frisian freedom', a period in which feudalism and serfdom (as well as central or judicial administration) did not exist, and in which the Frisian lands only owed their allegiance to the Holy Roman Emperor.
During the 13th century, however, the counts of Holland became increasingly powerful and, starting in 1272, sought to reassert themselves as rightful lords of the Frisian lands in a series of wars, which (with a series of lengthy interruptions) ended in 1422 with the Hollandic conquest of Western Frisia and with the establishment of a more powerful noble class in Central and Eastern Frisia.
In 1524, Frisia became part of the Seventeen Provinces and in 1568 joined the Dutch revolt against Philip II, king of Spain, heir of the Burgundian territories; Central Frisia has remained a part of the Netherlands ever since. The eastern periphery of Frisia would become part of various German states (later Germany) and Denmark. An old tradition existed in the region of exploitation of peatlands.
Though impossible to know exact numbers and migration patterns, research has indicated that many Frisians were part of the wave of ethnic groups to colonise areas of present day England alongside the Angles, Saxons and Jutes, starting from around the fifth century when Frisians arrived along the coastline of Kent. Studies have found the DNA of people tested in Central England to be "indistinguishable" from that of Frisians.
Frisians principally settled in modern-day Kent, East Anglia, the East Midlands, North East England, and Yorkshire. Across these areas, evidence of their settlement includes place names of Frisian origin, such as Frizinghall in Bradford and Frieston in Lincolnshire.
Similarities in dialect between Great Yarmouth and Friesland have been noted, originating from trade between these areas during the Middle Ages. Frisians are also known to have founded the Freston area of Ipswich.
In Scotland, historians have noted that colonies of Angles and Frisians settled as far north as the River Forth. This corresponds to those areas of Scotland which historically constituted part of Northumbria.
As both the Anglo-Saxons of England and the early Frisians were formed from similar tribal confederacies, their respective languages were very similar, together forming the Anglo-Frisian family. Old Frisian is the most closely attested language to Old English and the modern Frisian dialects are in turn the closest related languages to contemporary English that do not themselves derive from Old English (although the modern Frisian and English are not mutually intelligible).
The Frisian language group is divided into three mutually unintelligible languages:
Of these three languages both Saterland Frisian (2,000 speakers) and North Frisian (10,000 speakers) are endangered. West Frisian is spoken by around 350,000 native speakers in Friesland, and as many as 470,000 when including speakers in neighbouring Groningen province. West Frisian is not listed as threatened, although research published by Radboud University in 2016 has challenged that assumption.
Today there exists a tripartite division of the Frisians, into North Frisians, East Frisians and West Frisians, caused by Frisia's constant loss of territory in the Middle Ages. The West Frisians, in general, do not see themselves as part of a larger group of Frisians, and, according to a 1970 poll, identify themselves more with the Dutch than with the East or North Frisians. Therefore, the term 'Frisian', when applied to the speakers of all three Frisian languages, is a linguistic, ethnic and/or cultural concept, not a political one.

</doc>
<doc id="11800" url="https://en.wikipedia.org/wiki?curid=11800" title="Futurism (disambiguation)">
Futurism (disambiguation)

Futurism is an artistic and social movement that originated in Italy in the early 20th century.
Futures studies, also known as futurology, is the study of possible futures.
Futurism may also refer to:

</doc>
<doc id="11801" url="https://en.wikipedia.org/wiki?curid=11801" title="Filippo Tommaso Marinetti">
Filippo Tommaso Marinetti

Filippo Tommaso Emilio Marinetti (; 22 December 1876 – 2 December 1944) was an Italian poet, editor, art theorist, and founder of the Futurist movement. He was associated with the utopian and Symbolist artistic and literary community Abbaye de Créteil between 1907 and 1908. Marinetti is best known as the author of the first "Futurist Manifesto", which was written and published in 1909.
Emilio Angelo Carlo Marinetti (some documents give his name as "Filippo Achille Emilio Marinetti") spent the first years of his life in Alexandria, Egypt, where his father (Enrico Marinetti) and his mother (Amalia Grolli) lived together "more uxorio" (as if married). Enrico was a lawyer from Piedmont, and his mother was the daughter of a literary professor from Milan. They had come to Egypt in 1865, at the invitation of Khedive Isma'il Pasha, to act as legal advisers for foreign companies that were taking part in his modernization program.
His love for literature developed during the school years. His mother was an avid reader of poetry, and introduced the young Marinetti to the Italian and European classics. At age seventeen he started his first school magazine, "Papyrus"; the Jesuits threatened to expel him for publicizing Émile Zola's scandalous novels in the school.
He first studied in Egypt then in Paris, obtaining a "baccalauréat" degree in 1894 at the Sorbonne, and in Italy, graduating in law at the University of Pavia in 1899.
He decided not to be a lawyer but to develop a literary career. He experimented with every type of literature (poetry, narrative, theatre, "words in liberty"), signing everything "Filippo Tommaso Marinetti".
Marinetti and Constantin Brâncuși were visitors of the Abbaye de Créteil c. 1908 along with young writers like Roger Allard (one of the first to defend Cubism), Pierre Jean Jouve, and Paul Castiaux, who wanted to publish their works through the Abbaye. The Abbaye de Créteil was a "phalanstère" community founded in the autumn of 1906 by the painter Albert Gleizes, and the poets , , Alexandre Mercereau and Charles Vildrac. The movement drew its inspiration from the "Abbaye de Thélème," a fictional creation by Rabelais in his novel "Gargantua". It was closed down by its members early in 1908.
Marinetti is known best as the author of the "Futurist Manifesto", which he wrote in 1909. It was published in French on the front page of the most prestigious French daily newspaper, "Le Figaro", on 20 February 1909. In "The Founding and Manifesto of Futurism", Marinetti declared that "Art, in fact, can be nothing but violence, cruelty, and injustice." Georges Sorel, who influenced the entire political spectrum from anarchism to Fascism, also argued for the importance of violence. Futurism had both anarchist and Fascist elements; Marinetti later became an active supporter of Benito Mussolini.
Marinetti, who admired speed, had a minor car accident outside Milan in 1908 when he veered into a ditch to avoid two cyclists. He referred to the accident in the Futurist Manifesto: the Marinetti who was helped out of the ditch was a new man, determined to end the pretense and decadence of the prevailing Liberty style. He discussed a new and strongly revolutionary programme with his friends, in which they should end every artistic relationship with the past, "destroy the museums, the libraries, every type of academy". Together, he wrote, "We will glorify war—the world's only hygiene—militarism, patriotism, the destructive gesture of freedom-bringers, beautiful ideas worth dying for, and scorn for woman".
The Futurist Manifesto was read and debated all across Europe, but Marinetti's first 'Futurist' works were not as successful. In April, the opening night of his drama "Le Roi bombance" (The Feasting King), written in 1905, was interrupted by loud, derisive whistling by the audience... and by Marinetti himself, who thus introduced another element of Futurism, "the desire to be heckled." Marinetti did, however, fight a duel with a critic he considered too harsh.
His drama "La donna è mobile" (Poupées électriques), first presented in Turin, was not successful either. Nowadays, the play is remembered through a later version, named "Elettricità sessuale" (Sexual Electricity), and mainly for the appearance onstage of humanoid automatons, ten years before the Czech writer Karel Čapek would invent the term "robot".
In 1910 his first novel, "Mafarka il futurista", was cleared of all charges by an obscenity trial. That year, Marinetti discovered some allies in three young painters (Umberto Boccioni, Carlo Carrà, Luigi Russolo), who adopted the Futurist philosophy. Together with them (and with poets such as Aldo Palazzeschi), Marinetti began a series of Futurist Evenings, theatrical spectacles in which Futurists declaimed their manifestos in front of a crowd that in part attended the performances to throw vegetables at them.
The most successful "happening" of that period was the publicization of the "Manifesto Against Past-Loving Venice" in Venice. In the flier, Marinetti demands "fill(ing) the small, stinking canals with the rubble from the old, collapsing and leprous palaces" to "prepare for the birth of an industrial and militarized Venice, capable of dominating the great Adriatic, a great Italian lake."
In 1911, the Italo-Turkish War began and Marinetti departed for Libya as war correspondent for a French newspaper. His articles were eventually collected and published in "The Battle of Tripoli". He then covered the First Balkan War of 1912–13, witnessing the surprise success of Bulgarian troops against the Ottoman Empire in the Siege of Adrianople. In this period he also made a number of visits to London, which he considered 'the Futurist city par excellence', and where a number of exhibitions, lectures and demonstrations of Futurist music were staged. However, although a number of artists, including Wyndham Lewis, were interested in the new movement, only one British convert was made, the young artist C.R.W. Nevinson. Nevertheless, Futurism was an important influence upon Lewis's Vorticist philosophy.
About the same time Marinetti worked on a very anti-Roman Catholic and anti-Austrian verse-novel, "Le monoplan du Pape" ("The Pope's Aeroplane", 1912) and edited an anthology of futurist poets. But his attempts to renew the style of poetry did not satisfy him. So much so that, in his foreword to the anthology, he declared a new revolution: it was time to be done with traditional syntax and to use "words in freedom" ("parole in libertà"). His sound-poem "Zang Tumb Tumb", an account of the Battle of Adrianople, exemplifies words in freedom. Recordings can be heard of Marinetti reading some of his sound poems: "Battaglia, Peso + Odore" (1912); "Dune, parole in libertà" (1914); "La Battaglia di Adrianopoli" (1926) (recorded 1935).
Marinetti agitated for Italian involvement in World War I, and once Italy was engaged, promptly volunteered for service. In the fall of 1915 he and several other Futurists who were members of the Lombard Volunteer Cyclists were stationed at Lake Garda, in Trentino province, high in the mountains along the Italo-Austrian border. They endured several weeks of fighting in harsh conditions before the cyclists units, deemed inappropriate for mountain warfare, were disbanded.
Marinetti spent most of 1916 supporting Italy's war effort with speeches, journalism, and theatrical work, then returned to military service as a regular army officer in 1917. In May of that year he was seriously wounded while serving with an artillery battalion on the Isonzo front; he returned to service after a long recovery, and participated in the decisive Italian victory at Vittorio Veneto in October 1918.
After an extended courtship, in 1923 Marinetti married Benedetta Cappa (1897–1977), a writer and painter and a pupil of Giacomo Balla. Born in Rome, she had joined the Futurists in 1917. They'd met in 1918, moved in together in Rome, and chose to marry only to avoid legal complications on a lecture tour of Brazil. They would have three daughters: Vittoria, Ala, and Luce.
Cappa and Marinetti collaborated on a genre of mixed-media assemblages in the mid-1920s they called "tattilismo" ("Tactilism"), and she was a strong proponent and practitioner of the aeropittura movement after its inception in 1929. She also produced three experimental novels. Cappa's major public work is likely a series of five murals at the Palermo Post Office (1926–1935) for the Fascist public-works architect Angiolo Mazzoni.
In early 1918 he founded the "Partito Politico Futurista" or Futurist Political Party, which only a year later merged with Benito Mussolini's "Fasci Italiani di Combattimento". Marinetti was one of the first affiliates of the Italian Fascist Party. In 1919 he co-wrote with Alceste De Ambris the Fascist Manifesto, the original manifesto of Italian Fascism. He opposed Fascism's later exaltation of existing institutions, terming them "reactionary," and, after walking out of the 1920 Fascist party congress in disgust, withdrew from politics for three years. However, he remained a notable force in developing the party philosophy throughout the regime's existence. For example, at the end of the "Congress of Fascist Culture" that was held in Bologna on 30 March 1925, Giovanni Gentile addressed Sergio Panunzio on the need to define Fascism more purposefully by way of Marinetti's opinion, stating, "Great spiritual movements make recourse to precision when their primitive inspirations—what F. T. Marinetti identified this morning as artistic, that is to say, the creative and truly innovative ideas, from which the movement derived its first and most potent impulse—have lost their force. We today find ourselves at the very beginning of a new life and we experience with joy this obscure need that fills our hearts—this need that is our inspiration, the genius that governs us and carries us with it."
As part of his campaign to overturn tradition, Marinetti also attacked traditional Italian food. His "Manifesto of Futurist Cooking" was published in the Turin "Gazzetta del Popolo" on 28 December 1930. Arguing that "People think, dress and act in accordance with what they drink and eat", Marinetti proposed wide-ranging changes to diet. He condemned pasta, blaming it for lassitude, pessimism and lack of virility, and promoted the eating of Italian-grown rice. In this, as in other ways, his proposed Futurist cooking was nationalistic, rejecting foreign foods and food names. It was also militaristic, seeking to stimulate men to be fighters.
Marinetti also sought to increase creativity. His attraction to whatever was new made scientific discoveries appealing to him, but his views on diet were not scientifically based. He was fascinated with the idea of processed food, predicting that someday pills would replace food as a source of energy, and calling for the creation of "plastic complexes" to replace natural foods. Food, in turn, would become a matter of artistic expression. Many of the meals Marinetti described and ate resemble performance art, such as the "Tactile Dinner", recreated in 2014 for an exhibit at the Guggenheim Museum. Participants wore pajamas decorated with sponge, sandpaper, and aluminum, and ate salads without using cutlery.
During the Fascist regime Marinetti sought to make Futurism the official state art of Italy but failed to do so. Mussolini was personally uninterested in art and chose to give patronage to numerous styles to keep artists loyal to the regime. Opening the exhibition of art by the Novecento Italiano group in 1923, he said: "I declare that it is far from my idea to encourage anything like a state art. Art belongs to the domain of the individual. The state has only one duty: not to undermine art, to provide humane conditions for artists, to encourage them from the artistic and national point of view." Mussolini's mistress, Margherita Sarfatti, successfully promoted the rival Novecento Group, and even persuaded Marinetti to be part of its board.
In Fascist Italy, modern art was tolerated and even approved by the Fascist hierarchy. Towards the end of the 1930s, some Fascist ideologues (for example, the ex-Futurist Ardengo Soffici) wished to import the concept of "degenerate art" from Germany to Italy and condemned modernism, although their demands were ignored by the regime. In 1938, hearing that Adolf Hitler wanted to include Futurism in a traveling exhibition of degenerate art, Marinetti persuaded Mussolini to refuse to let it enter Italy.
On 17 November 1938, Italy passed The Racial Laws, discriminating against Italian Jews, much as the discrimination pronounced in the Nuremberg Laws. The anti-Semitic trend in Italy resulted in attacks against modern art, judged too foreign, too radical and anti-nationalist. In 11 January 1939 issue of the Futurist journal "Artecrazia" Marinetti expressed his condemnation of such attacks on modern art, noting Futurism is both Italian and nationalist, not foreign, and that there are no Jews in Futurism. Furthermore, he claimed Jews were not active in the development of modern art. Regardless, the Italian state shut down "Artecrazia".
Marinetti made numerous attempts to ingratiate himself with the regime, becoming less radical and avant garde with each attempt. He relocated from Milan to Rome. He became an academician despite his condemnation of academies, saying, "It is important that Futurism be represented in the Academy."
He was an atheist, but by the mid 1930s he had come to accept the influence of the Catholic Church on Italian society. In "Gazzetta del Popolo", 21 June 1931, Marinetti proclaimed that "Only Futurist artists...are able to express clearly...the simultaneous dogmas of the Catholic faith, such as the Holy Trinity, the Immaculate Conception and Christ's Calvary." In his last works, written just before his death in 1944 "L'aeropoema di Gesù" ("The Aeropoem of Jesus") and "Quarto d'ora di poesia per the X Mas" ("A Fifteen Minutes' Poem of the X Mas"), Marinetti sought to reconcile his newfound love for God and his passion for the action that accompanied him throughout his life.
There were other contradictions in his character: despite his nationalism, he was international, educated in Egypt and France, writing his first poems in French, publishing the Futurist Manifesto in a French newspaper and traveling to promote his ideas.
Marinetti volunteered for active service in the Second Italo-Abyssinian War and the Second World War, serving on the Eastern Front for a few weeks in the Summer and Autumn of 1942 at the age of 65.
He died of cardiac arrest in Bellagio on 2 December 1944 while working on a collection of poems praising the wartime achievements of the Decima Flottiglia MAS.

</doc>
<doc id="11803" url="https://en.wikipedia.org/wiki?curid=11803" title="Franz Mesmer">
Franz Mesmer

Franz Anton Mesmer (; ; 23 May 1734 – 5 March 1815) was a doctor with an interest in astronomy. He theorised the existence of a natural energy transference occurring between all animated and inanimate objects; this he called "animal magnetism", sometimes later referred to as "mesmerism". (In modern times New Age spiritualists have revived a similar idea.) Mesmer's theory attracted a wide following between about 1780 and 1850, and continued to have some influence until the end of the 19th century. In 1843 the Scottish doctor James Braid proposed the term "hypnosis" for a technique derived from animal magnetism; today the word "mesmerism" generally functions as a synonym of "hypnosis". He also supported the arts, specifically music; he was on friendly terms with Haydn and Mozart.
Mesmer was born in the village of Iznang, on the shore of Lake Constance in Swabia, a son of master forester Anton Mesmer (1701—after 1747) and his wife, Maria/Ursula (née Michel; 1701—1770). After studying at the Jesuit universities of Dillingen and Ingolstadt, he took up the study of medicine at the University of Vienna in 1759. In 1766 he published a doctoral dissertation with the Latin title "De planetarum influxu in corpus humanum" ("On the Influence of the Planets on the Human Body"), which discussed the influence of the moon and the planets on the human body and on disease. This was not medical astrology. Building largely on Isaac Newton's theory of the tides, Mesmer expounded on certain tides in the human body that might be accounted for by the movements of the sun and moon. Evidence assembled by Frank A. Pattie suggests that Mesmer plagiarized a part of his dissertation from a work by Richard Mead, an eminent English physician and Newton's friend. However, in Mesmer's day doctoral theses were not expected to be original.
In January 1768, Mesmer married Anna Maria von Posch, a wealthy widow, and established himself as a doctor in Vienna. In the summers he lived on a splendid estate and became a patron of the arts. In 1768, when court intrigue prevented the performance of "La finta semplice" (K. 51), for which the twelve-year-old Wolfgang Amadeus Mozart had composed 500 pages of music, Mesmer is said to have arranged a performance in his garden of Mozart's "Bastien und Bastienne" (K. 50), a one-act opera, though Mozart's biographer Nissen found no proof that this performance actually took place. Mozart later immortalized his former patron by including a comedic reference to Mesmer in his opera "Così fan tutte".
In 1774, Mesmer produced an "artificial tide" in a patient, Francisca Österlin, who suffered from hysteria, by having her swallow a preparation containing iron and then attaching magnets to various parts of her body. She reported feeling streams of a mysterious fluid running through her body and was relieved of her symptoms for several hours. Mesmer did not believe that the magnets had achieved the cure on their own. He felt that he had contributed animal magnetism, which had accumulated in his work, to her. He soon stopped using magnets as a part of his treatment.
In the same year Mesmer collaborated with Maximilian Hell.
In 1775, Mesmer was invited to give his opinion before the Munich Academy of Sciences on the exorcisms carried out by Johann Joseph Gassner (Gaßner), a priest and healer who grew up in Vorarlberg, Austria. Mesmer said that while Gassner was sincere in his beliefs, his cures resulted because he possessed a high degree of animal magnetism. This confrontation between Mesmer's secular ideas and Gassner's religious beliefs marked the end of Gassner's career as well as, according to Henri Ellenberger, the emergence of dynamic psychiatry.
The scandal that followed Mesmer's only partial success in curing the blindness of an 18-year-old musician, Maria Theresia Paradis, led him to leave Vienna in 1777. In February 1778 Mesmer moved to Paris, rented an apartment in a part of the city preferred by the wealthy and powerful, and established a medical practice. There he would reunite with Mozart who often visited him. Paris soon divided into those who thought he was a charlatan who had been forced to flee from Vienna and those who thought he had made a great discovery.
In his first years in Paris, Mesmer tried and failed to get either the Royal Academy of Sciences or the Royal Society of Medicine to provide official approval for his doctrines. He found only one physician of high professional and social standing, Charles d'Eslon, to become a disciple. In 1779, with d'Eslon's encouragement, Mesmer wrote an 88-page book, "Mémoire sur la découverte du magnétisme animal", to which he appended his famous 27 Propositions. These propositions outlined his theory at that time. Some contemporary scholars equate Mesmer's animal magnetism with the Qi (chi) of Traditional Chinese Medicine and mesmerism with medical Qigong practices.
According to d'Eslon, Mesmer understood health as the free flow of the process of life through thousands of channels in our bodies. Illness was caused by obstacles to this flow. Overcoming these obstacles and restoring flow produced crises, which restored health. When Nature failed to do this spontaneously, contact with a conductor of animal magnetism was a necessary and sufficient remedy. Mesmer aimed to aid or provoke the efforts of Nature. To cure an insane person, for example, involved causing a fit of madness. The advantage of magnetism involved accelerating such crises without danger.
Mesmer treated patients both individually and in groups. With individuals he would sit in front of his patient with his knees touching the patient's knees, pressing the patient's thumbs in his hands, looking fixedly into the patient's eyes. Mesmer made "passes", moving his hands from patients' shoulders down along their arms. He then pressed his fingers on the patient's hypochondrium region (the area below the diaphragm), sometimes holding his hands there for hours. Many patients felt peculiar sensations or had convulsions that were regarded as crises and supposed to bring about the cure. Mesmer would often conclude his treatments by playing some music on a glass armonica.
By 1780 Mesmer had more patients than he could treat individually and he established a collective treatment known as the "baquet." An English doctor who observed Mesmer described the treatment as follows:In the middle of the room is placed a vessel of about a foot and a half high which is called here a "baquet". It is so large that twenty people can easily sit round it; near the edge of the lid which covers it, there are holes pierced corresponding to the number of persons who are to surround it; into these holes are introduced iron rods, bent at right angles outwards, and of different heights, so as to answer to the part of the body to which they are to be applied. Besides these rods, there is a rope which communicates between the baquet and one of the patients, and from him is carried to another, and so on the whole round. The most sensible effects are produced on the approach of Mesmer, who is said to convey the fluid by certain motions of his hands or eyes, without touching the person. I have talked with several who have witnessed these effects, who have convulsions occasioned and removed by a movement of the hand...
In 1784, without Mesmer requesting it, King Louis XVI appointed four members of the Faculty of Medicine as commissioners to investigate animal magnetism as practiced by d'Eslon. At the request of these commissioners the King appointed five additional commissioners from the Royal Academy of Sciences. These included the chemist Antoine Lavoisier, the doctor Joseph-Ignace Guillotin, the astronomer Jean Sylvain Bailly, and the American ambassador Benjamin Franklin.
The commission conducted a series of experiments aimed not at determining whether Mesmer's treatment worked, but whether he had discovered a new physical fluid. The commission concluded that there was no evidence for such a fluid. Whatever benefit the treatment produced was attributed to "imagination." But one of the commissioners, the botanist Antoine Laurent de Jussieu took exception to the official reports. He wrote a dissenting opinion that declared Mesmer's theory credible and worthy of further investigation.
The commission did not examine Mesmer, but investigated the practice of d'Eslon.
In August 1784 Mesmer visited a Mesmeric society in Lyon. In 1785 Mesmer left Paris. In 1790 he was in Vienna again to settle the estate of his deceased wife Maria Anna. When he sold his house in Vienna in 1801 he was in Paris.
Mesmer was driven into exile soon after the investigations on animal magnetism although his influential student, Armand-Marie-Jacques de Chastenet, Marquis de Puségur (1751-1825), continued to have many followers until his death. Mesmer continued to practice in Frauenfeld, Switzerland, for a number of years and died in 1815 in Meersburg.
Abbé Faria, an Indo-Portuguese monk in Paris and a contemporary of Mesmer, claimed that "nothing comes from the magnetizer; everything comes from the subject and takes place in his imagination, i.e. autosuggestion generated from within the mind."

</doc>
<doc id="11806" url="https://en.wikipedia.org/wiki?curid=11806" title="Foix–Alajouanine syndrome">
Foix–Alajouanine syndrome

Foix–Alajouanine syndrome, also called subacute ascending necrotizing myelitis, is a disease caused by an arteriovenous malformation of the spinal cord. The patients present with symptoms indicating spinal cord involvement (paralysis of arms and legs, numbness and loss of sensation and sphincter dysfunction), and pathological examination reveals disseminated nerve cell death in the spinal cord and abnormally dilated and tortuous vessels situated on the surface of the spinal cord. Surgical treatment can be tried in some cases. If surgical intervention is contraindicated, corticosteroids may be used.
The condition is named after Charles Foix and Théophile Alajouanine.

</doc>
<doc id="11807" url="https://en.wikipedia.org/wiki?curid=11807" title="Ferromagnetism">
Ferromagnetism

Ferromagnetism is the basic mechanism by which certain materials (such as iron) form permanent magnets, or are attracted to magnets. In physics, several different types of magnetism are distinguished. Ferromagnetism (along with the similar effect ferrimagnetism) is the strongest type and is responsible for the common phenomenon of magnetism in magnets encountered in everyday life. Substances respond weakly to magnetic fields with three other types of magnetism—paramagnetism, diamagnetism, and antiferromagnetism—but the forces are usually so weak that they can be detected only by sensitive instruments in a laboratory. An everyday example of ferromagnetism is a refrigerator magnet used to hold notes on a refrigerator door. The attraction between a magnet and ferromagnetic material is "the quality of magnetism first apparent to the ancient world, and to us today".
Permanent magnets (materials that can be magnetized by an external magnetic field and remain magnetized after the external field is removed) are either ferromagnetic or ferrimagnetic, as are the materials that are noticeably attracted to them. Only a few substances are ferromagnetic. The common ones are iron, cobalt, nickel and most of their alloys, and some compounds of rare earth metals.
Ferromagnetism is very important in industry and modern technology, and is the basis for many electrical and electromechanical devices such as electromagnets, electric motors, generators, transformers, and magnetic storage such as tape recorders, and hard disks, and nondestructive testing of ferrous materials.
Ferromagnetic materials can be divided into magnetically "soft" materials like annealed iron, which can be magnetized but do not tend to stay magnetized, and magnetically "hard" materials, which do. Permanent magnets are made from "hard" ferromagnetic materials such as alnico and ferrite that are subjected to special processing in a strong magnetic field during manufacture to align their internal microcrystalline structure, making them very hard to demagnetize. To demagnetize a saturated magnet, a certain magnetic field must be applied, and this threshold depends on coercivity of the respective material. "Hard" materials have high coercivity, whereas "soft" materials have low coercivity. The overall strength of a magnet is measured by its magnetic moment or, alternatively, the total magnetic flux it produces. The local strength of magnetism in a material is measured by its magnetization.
Historically, the term "ferromagnetism" was used for any material that could exhibit spontaneous magnetization: a net magnetic moment in the absence of an external magnetic field; that is any material that could become a magnet. This general definition is still in common use.
However, in a landmark paper in 1948, Louis Néel showed there are two levels of magnetic alignment that result in this behavior. One is ferromagnetism in the strict sense, where all the magnetic moments are aligned. The other is "ferrimagnetism", where some magnetic moments point in the opposite direction but have a smaller contribution, so there is still a spontaneous magnetization. 
In the special case where the opposing moments balance completely, the alignment is known as "antiferromagnetism". Therefore antiferromagnets do not have a spontaneous magnetization.
Ferromagnetism is an unusual property that occurs in only a few substances. The common ones are the transition metals iron, nickel, cobalt and their alloys, and alloys of rare earth metals. It is a property not just of the chemical make-up of a material, but of its crystalline structure and microstructure. There are ferromagnetic metal alloys whose constituents are not themselves ferromagnetic, called Heusler alloys, named after Fritz Heusler. Conversely there are non-magnetic alloys, such as types of stainless steel, composed almost exclusively of ferromagnetic metals.
Amorphous (non-crystalline) ferromagnetic metallic alloys can be made by very rapid quenching (cooling) of a liquid alloy. These have the advantage that their properties are nearly isotropic (not aligned along a crystal axis); this results in low coercivity, low hysteresis loss, high permeability, and high electrical resistivity. One such typical material is a transition metal-metalloid alloy, made from about 80% transition metal (usually Fe, Co, or Ni) and a metalloid component (B, C, Si, P, or Al) that lowers the melting point.
A relatively new class of exceptionally strong ferromagnetic materials are the rare-earth magnets. They contain lanthanide elements that are known for their ability to carry large magnetic moments in well-localized f-orbitals.
The table lists a selection of ferromagnetic and ferrimagnetic compounds, along with the temperature above which they cease to exhibit spontaneous magnetization (see Curie temperature).
Most ferromagnetic materials are metals, since the conducting electrons are often responsible for mediating the ferromagnetic interactions. It is therefore a challenge to develop ferromagnetic insulators, especially multiferroic materials, which are both ferromagnetic and ferroelectric.
A number of actinide compounds are ferromagnets at room temperature or exhibit ferromagnetism upon cooling. PuP is a paramagnet with cubic symmetry at room temperature, but which undergoes a structural transition into a tetragonal state with ferromagnetic order when cooled below its T = 125 K. In its ferromagnetic state, PuP's easy axis is in the <100> direction.
In NpFe the easy axis is <111>. Above NpFe is also paramagnetic and cubic. Cooling below the Curie temperature produces a rhombohedral distortion wherein the rhombohedral angle changes from 60° (cubic phase) to 60.53°. An alternate description of this distortion is to consider the length "c" along the unique trigonal axis (after the distortion has begun) and "a" as the distance in the plane perpendicular to "c". In the cubic phase this reduces to . Below the Curie temperature
which is the largest strain in any actinide compound. NpNi undergoes a similar lattice distortion below , with a strain of (43 ± 5) × 10. NpCo is a ferrimagnet below 15 K.
In 2009, a team of MIT physicists demonstrated that a lithium gas cooled to less than one kelvin can exhibit ferromagnetism. The team cooled fermionic lithium-6 to less than (150 billionths of one kelvin) using infrared laser cooling. This demonstration is the first time that ferromagnetism has been demonstrated in a gas.
In 2018, a team of University of Minnesota physicists demonstrated that body-centered tetragonal ruthenium exhibits ferromagnetism at room temperature.
Recent research has shown evidence that ferromagnetism can be induced in some materials by an electric current or voltage. Antiferromagnetic LaMnO3 and SrCoO has been switched to ferromagnetic by a current. In July 2020 scientists reported inducing ferromagnetism in the abundant diamagnetic material iron pyrite ("fool's gold") by an applied voltage. In these experiments the ferromagnetism was limited to a thin surface layer.
The Bohr–van Leeuwen theorem, discovered in the 1910s, showed that classical physics theories are unable to account for any form of magnetism, including ferromagnetism. Magnetism is now regarded as a purely quantum mechanical effect. Ferromagnetism arises due to two effects from quantum mechanics: spin and the Pauli exclusion principle.
One of the fundamental properties of an electron (besides that it carries charge) is that it has a magnetic dipole moment, i.e., it behaves like a tiny magnet, producing a magnetic field. This dipole moment comes from the more fundamental property of the electron that it has quantum mechanical spin. Due to its quantum nature, the spin of the electron can be in one of only two states; with the magnetic field either pointing "up" or "down" (for any choice of up and down). The spin of the electrons in atoms is the main source of ferromagnetism, although there is also a contribution from the orbital angular momentum of the electron about the nucleus. When these magnetic dipoles in a piece of matter are aligned, (point in the same direction) their individually tiny magnetic fields add together to create a much larger macroscopic field.
However, materials made of atoms with filled electron shells have a total dipole moment of zero: because the electrons all exist in pairs with opposite spin, every electron's magnetic moment is cancelled by the opposite moment of the second electron in the pair. Only atoms with partially filled shells (i.e., unpaired spins) can have a net magnetic moment, so ferromagnetism occurs only in materials with partially filled shells. Because of Hund's rules, the first few electrons in a shell tend to have the same spin, thereby increasing the total dipole moment.
These unpaired dipoles (often called simply "spins" even though they also generally include orbital angular momentum) tend to align in parallel to an external magnetic field, an effect called paramagnetism. Ferromagnetism involves an additional phenomenon, however: in a few substances the dipoles tend to align spontaneously, giving rise to a spontaneous magnetization, even when there is no applied field.
When two nearby atoms have unpaired electrons, whether the electron spins are parallel or antiparallel affects whether the electrons can share the same orbit as a result of the quantum mechanical effect called the exchange interaction. This in turn affects the electron location and the Coulomb (electrostatic) interaction and thus the energy difference between these states.
The exchange interaction is related to the Pauli exclusion principle, which says that two electrons with the same spin cannot also be in the same spatial state (orbital). This is a consequence of the spin-statistics theorem and that electrons are fermions. Therefore, under certain conditions, when the orbitals of the unpaired outer valence electrons from adjacent atoms overlap, the distributions of their electric charge in space are farther apart when the electrons have parallel spins than when they have opposite spins. This reduces the electrostatic energy of the electrons when their spins are parallel compared to their energy when the spins are anti-parallel, so the parallel-spin state is more stable. In simple terms, the electrons, which are attracted to the nuclei, can change their spatial state so that they both are closer to both nuclei by aligning their spins in opposite directions, so the spins of these electrons tend to be antiparallel. This difference in energy is called the exchange energy.
This energy difference can be orders of magnitude larger than the energy differences associated with the magnetic dipole-dipole interaction due to dipole orientation, which tends to align the dipoles antiparallel. In certain doped semiconductor oxides RKKY interactions have been shown to bring about periodic longer-range magnetic interactions, a phenomenon of significance in the study of spintronic materials.
The materials in which the exchange interaction is much stronger than the competing dipole-dipole interaction are frequently called "magnetic materials". For instance, in iron (Fe) the exchange force is about 1000 times stronger than the dipole interaction. Therefore, below the Curie temperature virtually all of the dipoles in a ferromagnetic material will be aligned. In addition to ferromagnetism, the exchange interaction is also responsible for the other types of spontaneous ordering of atomic magnetic moments occurring in magnetic solids, antiferromagnetism and ferrimagnetism.
There are different exchange interaction mechanisms which create the magnetism in different ferromagnetic, ferrimagnetic, and antiferromagnetic substances. These mechanisms include direct exchange, RKKY exchange, double exchange, and superexchange.
Although the exchange interaction keeps spins aligned, it does not align them in a particular direction. Without magnetic anisotropy, the spins in a magnet randomly change direction in response to thermal fluctuations and the magnet is superparamagnetic. There are several kinds of magnetic anisotropy, the most common of which is magnetocrystalline anisotropy. This is a dependence of the energy on the direction of magnetization relative to the crystallographic lattice. Another common source of anisotropy, inverse magnetostriction, is induced by internal strains. Single-domain magnets also can have a "shape anisotropy" due to the magnetostatic effects of the particle shape. As the temperature of a magnet increases, the anisotropy tends to decrease, and there is often a blocking temperature at which a transition to superparamagnetism occurs.
The above would seem to suggest that every piece of ferromagnetic material should have a strong magnetic field, since all the spins are aligned, yet iron and other ferromagnets are often found in an "unmagnetized" state. The reason for this is that a bulk piece of ferromagnetic material is divided into tiny regions called "magnetic domains" (also known as "Weiss domains"). Within each domain, the spins are aligned, but (if the bulk material is in its lowest energy configuration; i.e. "unmagnetized"), the spins of separate domains point in different directions and their magnetic fields cancel out, so the object has no net large scale magnetic field.
Ferromagnetic materials spontaneously divide into magnetic domains because the "exchange interaction" is a short-range force, so over long distances of many atoms the tendency of the magnetic dipoles to reduce their energy by orienting in opposite directions wins out. If all the dipoles in a piece of ferromagnetic material are aligned parallel, it creates a large magnetic field extending into the space around it. This contains a lot of magnetostatic energy. The material can reduce this energy by splitting into many domains pointing in different directions, so the magnetic field is confined to small local fields in the material, reducing the volume of the field. The domains are separated by thin domain walls a number of molecules thick, in which the direction of magnetization of the dipoles rotates smoothly from one domain's direction to the other.
Thus, a piece of iron in its lowest energy state ("unmagnetized") generally has little or no net magnetic field. However, the magnetic domains in a material are not fixed in place; they are simply regions where the spins of the electrons have aligned spontaneously due to their magnetic fields, and thus can be altered by an external magnetic field. If a strong enough external magnetic field is applied to the material, the domain walls will move by the process of the spins of the electrons in atoms near the wall in one domain turning under the influence of the external field to face in the same direction as the electrons in the other domain, thus reorienting the domains so more of the dipoles are aligned with the external field. The domains will remain aligned when the external field is removed, creating a magnetic field of their own extending into the space around the material, thus creating a "permanent" magnet. The domains do not go back to their original minimum energy configuration when the field is removed because the domain walls tend to become 'pinned' or 'snagged' on defects in the crystal lattice, preserving their parallel orientation. This is shown by the Barkhausen effect: as the magnetizing field is changed, the magnetization changes in thousands of tiny discontinuous jumps as the domain walls suddenly "snap" past defects.
This magnetization as a function of the external field is described by a hysteresis curve. Although this state of aligned domains found in a piece of magnetized ferromagnetic material is not a minimal-energy configuration, it is metastable, and can persist for long periods, as shown by samples of magnetite from the sea floor which have maintained their magnetization for millions of years.
Heating and then cooling (annealing) a magnetized material, subjecting it to vibration by hammering it, or applying a rapidly oscillating magnetic field from a degaussing coil tends to release the domain walls from their pinned state, and the domain boundaries tend to move back to a lower energy configuration with less external magnetic field, thus "demagnetizing" the material.
Commercial magnets are made of "hard" ferromagnetic or ferrimagnetic materials with very large magnetic anisotropy such as alnico and ferrites, which have a very strong tendency for the magnetization to be pointed along one axis of the crystal, the "easy axis". During manufacture the materials are subjected to various metallurgical processes in a powerful magnetic field, which aligns the crystal grains so their "easy" axes of magnetization all point in the same direction. Thus the magnetization, and the resulting magnetic field, is "built in" to the crystal structure of the material, making it very difficult to demagnetize.
As the temperature increases, thermal motion, or entropy, competes with the ferromagnetic tendency for dipoles to align. When the temperature rises beyond a certain point, called the Curie temperature, there is a second-order phase transition and the system can no longer maintain a spontaneous magnetization, so its ability to be magnetized or attracted to a magnet disappears, although it still responds paramagnetically to an external field. Below that temperature, there is a spontaneous symmetry breaking and magnetic moments become aligned with their neighbors. The Curie temperature itself is a critical point, where the magnetic susceptibility is theoretically infinite and, although there is no net magnetization, domain-like spin correlations fluctuate at all length scales.
The study of ferromagnetic phase transitions, especially via the simplified Ising spin model, had an important impact on the development of statistical physics. There, it was first clearly shown that mean field theory approaches failed to predict the correct behavior at the critical point (which was found to fall under a "universality class" that includes many other systems, such as liquid-gas transitions), and had to be replaced by renormalization group theory.

</doc>
<doc id="11809" url="https://en.wikipedia.org/wiki?curid=11809" title="Francesco Cossiga">
Francesco Cossiga

Francesco Cossiga (, ; 1928 – 2010) was an Italian politician. A member of the Christian Democratic Party of Italy, he was Prime Minister of Italy from 1979 to 1980 and the eighth President of Italy from 1985 to 1992. Cossiga is widely considered one of the most prominent and influential politicians of the First Republic.
Cossiga also served as minister on several occasions, most notably as Italian Minister of the Interior. In that position he re-structured the Italian police, civil protection and secret services. Due to his repressive approach to public protests, he has been described as a strongman and labeled "iron minister". He was in office at the time of the kidnapping and murder of Aldo Moro by Red Brigades, and resigned as Minister of the Interior when Moro was found dead in 1978. Cossiga was Prime Minister during the Bologna station bombing in 1980.
Before his political career, Cossiga was a professor of constitutional law at the University of Sassari.
Francesco Cossiga was born in Sassari on 26 July 1928, into a republican and anti-fascist middle-bourgeois family. He was the second-degree cousin of Enrico and Giovanni Berlinguer. Although he was commonly called "Cossìga" , the original pronunciation of the surname is "Còssiga" . His surname in Sardinian and Sassarese means "Corsica", likely pointing to the family's origin.
At the age of sixteen, he graduated, three years in advance, at the classical lyceum Domenico Alberto Azuni. The following year he joined in the Christian Democracy, and three years later, at only 19 years old, he graduated in law and started a university career as professor of constitutional law at the faculty of jurisprudence of the University of Sassari.
During his period at the university he became a member of the Catholic Federation of University Students (FUCI), becoming the association's leader for Sassari.
After the 1958 general election Cossiga was elected in the Chamber of Deputies for the first time, representing the constituency of Cagliari–Sassari.
In February 1966 he became the youngest Undersecretary of the Ministry of Defence, in the government of Aldo Moro. In this role he had to face the aftermath of Piano Solo, an envisaged plot for an Italian "coup d'état" requested by then President Antonio Segni, two years before.
From November 1974 to February 1976 Cossiga was Minister of Public Administration in Moro's fourth government.
On 12 February 1976, Cossiga was appointed Minister of the Interior, by Prime Minister Moro. During his term he re-structured the Italian police, civil protection and secret services. Cossiga has been often described as a strongman and labeled "iron minister", for repressing public protests. Moreover, during his tenure his surname was often stylized as "Koiga", using the "SS" symbol.
In 1977 the city of Bologna was the scene of violent street clashes. In particular, on 11 March a militant of the far-left organization "Lotta Continua", Francesco Lorusso, was killed by a gunshot to the back (probably fired by a policeman), when police dispersed protesters against a mass meeting of Communion and Liberation, which was being held that morning at the University. This event served as a detonator for a long series of clashes with security forces for two days, which affected the entire city of Bologna.
Cossiga sent armored vehicles into the university area and other hot spots of the city to quell what he perceived as guerrilla warfare. Clashes with the police caused numerous casualties among people who got caught up in the riots, including uninvolved locals. No old leftist party, except the Youth Socialist Federation, led by local secretary Emilio Lonardo, participated at the funeral of the student Lorusso, showing the dramatic split between the movement and the historical left parties.
Turin was also the scene of bloody clashes and attacks. On 1 October 1977, after a procession had started with an attack on the headquarters of the Italian Social Movement (MSI), a group of militants of "Lotta Continua" reached a downtown bar, "L'angelo azzurro" (The Blue Angel), frequented by young right-wing activists. They threw two Molotov cocktails, and Roberto Crescenzio, a totally apolitical student, died of burns. The perpetrators of the murder were never identified. "Lotta Continua" leader Silvio Viale called it a "tragic accident".
Another innocent victim of the riots of that year was Giorgiana Masi, who was killed in Rome by a gunshot during an event organized by the Radical Party to celebrate the third anniversary of the victory in the referendum on divorce. As the perpetrators of the murder remained unknown, the movement attributed the responsibility of the crime to police officers in plain clothes, which were immortalized at that time dressed in clothing of the style of young people of the movement.
Cossiga was in office at the time of the kidnapping and murder of the Christian Democratic leader Aldo Moro by the Marxist-Leninist extreme-left terrorist group Red Brigades. On the morning of 16 March 1978, the day on which the new cabinet led by Giulio Andreotti was supposed to have undergone a confidence vote in the Italian Parliament, the car of Moro, former prime minister and then president of DC, was assaulted by a group of Red Brigades terrorists in Via Fani in Rome. Firing automatic weapons, the terrorists killed Moro's bodyguards, (two Carabinieri in Moro's car and three policemen in the following car) and kidnapped him.
Cossiga formed immediately two "crisis committees". The first one was a technical-operational-political committee, chaired by Cossiga himself and, in his absence, by undersecretary Nicola Lettieri. Other members included the supreme commanders of the Italian Police Forces, of the Carabinieri, the Guardia di Finanza, the recently named directors of SISMI and SISDE (respectively, Italy's military and civil intelligence services), the national secretary of CESIS (a secret information agency), the director of UCIGOS and the police prefect of Rome. The second one was an information committee, including members of CESIS, SISDE, SISMI and SIOS, another military intelligence office.
A third unofficial committee was created which never met officially, called the "comitato di esperti" ("committee of experts"). Its existence was not disclosed until 1981, by Cossiga himself, in his interrogation by the Italian Parliament's Commission about the Moro affair. He omitted to reveal the decisions and the activities of the committee however. This committee included: Steve Pieczenik, a psychologist of the anti-terrorism section of the US State Department, and notable Italian criminologists. Pieczenik later declared that there were numerous leaks about the discussions made at the committee, and accused Cossiga.
However, on 9 May 1978 Moro's body was found in the trunk of a Renault 4 in Via Caetani after 55 days of imprisonment, during which Moro was submitted to a political trial by the so-called "people's court" set up by the Brigate Rosse and the Italian government was asked for an exchange of prisoners. Despite the common interpretation, the car location in Via Caetani was not halfway between the locations of the national offices of DC and of the Italian Communist Party (PCI) in Rome. After two days, Cossiga resigned as Minister of the Interior. According to Italian journalist Enrico Deaglio, Cossiga, to justify his lack of action, "accused the leaders of CGIL and of the Communist Party of knowing where Moro was detained". Cossiga was also accused by Moro himself, in his letters who wrote during his detention, saying that "his blood will fall over him".
One year after Moro's death and the subsequent Cossiga's resignation as Interior Minister, he was appointed Prime Minister of Italy. He led a government's coalition composed by Christian Democrats, Socialists, Democratic Socialists, Republicans and Liberals.
Cossiga was head of the government during the Bologna massacre, a terrorist bombing of the Bologna Central Station on the morning of 2 August 1980, which killed 85 people and wounded more than 200. The attack was attributed to the neo-fascist terrorist organization "Nuclei Armati Rivoluzionari" (Armed Revolutionary Nucleus), which always denied any involvement; other theories have been proposed, especially in correlation with the strategy of tension.
Francesco Cossiga first assumed the explosion to have been caused by an accident (the explosion of an old boiler located in the basement of the station). Nevertheless, soon the evidence gathered on site of the explosion made it clear that the attack constituted an act of terrorism. "L'Unità", the newspaper of the Communist Party on 3 August already attributed responsibility for the attack to neo-fascists. Later, in a special session to the Senate, Cossiga supported the theory that neofascists were behind the attack, "unlike leftist terrorism, which strikes at the heart of the state through its representatives, black terrorism prefers the massacre because it promotes panic and impulsive reactions."
Later, according to media reports in 2004, taken up again in 2007, Cossiga, in a letter addressed to Enzo Fragala, leader of the National Alliance section in the Mitrokhin Committee, suggested Palestinian involvement of George Habash's Popular Front for the Liberation of Palestine and the Separat group of Ilich Ramirez Sanchez, known as "Carlos the Jackal". In addition, in 2008 Cossiga gave an interview to "BBC" in which it reaffirmed his belief that the massacre would not be attributable to black terrorism, but to an "incident" of Palestinian resistance groups operating in Italy. He declared also being convinced of the innocence of Francesca Mambro and Giuseppe Valerio Fioravanti, the two neo-fascist terrorists accused of the massacre. The PFLP has always denied responsibility.
In October 1980, Cossiga resigned as Prime Minister after the rejection of the annual budget bill by the Italian Parliament.
Following the 1983 general election, Cossiga became a member of the Italian Senate; on 12 July, he was elected President of the Senate.
In the 1985 presidential election, Cossiga was elected as President of Italy with 752 votes out of 977. His candidacy was endorsed by the Christian Democracy, but supported also by communists, socialists, social democrats, liberals and republicans. This was the first time an Italian presidential candidate had won the election on the first ballot, where a two-thirds majority is necessary.
The Cossiga presidency was essentially divided into two phases related to the attitudes of the head of state. In the first five years, Cossiga played its role in a traditional way, caring for the role of the republican institutions under the Constitution, which makes the President of the Republic a kind of arbitrator in relations between the powers of the state.
It was in his last two years as president that Cossiga began to express some unusual opinions regarding the Italian political system. He opined that the Italian parties, especially the Christian Democrats and the Communists had to take into account the deep changes brought about by the fall of the Berlin Wall and the end of the Cold War. According to him, DC and PCI would therefore have been seriously affected by this change, but Cossiga believed that political parties and the same institutions refused to recognize it.
Thus, a period of conflict and political controversy began, often provocative and deliberately excessive, and with very strong media exposure. These statements, soon dubbed "esternazioni", or "mattock blows" ("picconate"), were considered by many to be inappropriate for a President, and often beyond his constitutional powers; also, his mental health was doubted and Cossiga had to declare "I am the fake madman who speaks the truth." Cossiga suffered from bipolar disorder and depression in the last years of his life.
Among the statements of the President there were also allegations of excessive politicization of the judiciary system, and the stigmatization of the fact that young magistrates, who just came into service, were immediately destined for the Sicilian prosecutor to carry out mafia proceedings.
For his changed attitude, Cossiga received various criticisms by almost every party, with the exception of the Italian Social Movement, which stood beside him in defense of the "picconate". He will, amongst other things, be considered one of the first "cleansers" of MSI, who recognized it as a constitutional and democratic force.
Tension developed between Cossiga and Prime Minister Giulio Andreotti. This tension emerged when Andreotti revealed the existence of Gladio, a stay-behind organization with the official aim of countering a possible Soviet invasion through sabotage and guerrilla warfare behind enemy lines. Cossiga acknowledged his involvement in the establishment of the organization. The Democratic Party of the Left (successor to the Communist Party) started the procedure of impeachment (Presidents of Italy can be impeached only for high treason against the State or for an attempt to overthrow the Constitution). Although he threatened to prevent the impeachment procedure by dissolving Parliament, the impeachment request was ultimately dismissed.
Cossiga resigned two months before the end of his term, on 25 April 1992. In his last speech as president he stated "To young people I want to say to love the fatherland, to honor the nation, to serve the Republic, to believe in freedom and to believe in our country".
According to the Italian Constitution, after his resignation from the office of President, Cossiga became senator for life, joining his predecessors in the upper house of Parliament, with whom he also shared the title of President Emeritus of the Italian Republic.
On 12 February 1997, Cossiga survived unscathed a railway accident (), while traveling on a high-speed train from Milan to Rome that derailed near Piacenza.
In February 1998, Cossiga created the Democratic Union for the Republic (UDR), a Christian democratic political party, declaring it to be politically central. The UDR was a crucial component of the majority that supported the Massimo D'Alema government in October 1998, after the fall of the Romano Prodi's government which lost a vote of confidence. Cossiga declared that his support for D'Alema was intended to end the conventional exclusion of the former communist leaders from the premiership in Italy.
In 1999 UDR was dissolved and Cossiga returned to his activities as a senator, with competences in the Military Affairs' Commission.
In May 2006, Cossiga gave his support to the formation of Prodi's second government. In the same month, he brought in a bill that would allow the region of South Tyrol to hold a referendum, where the local electorate could decide whether to remain within the Republic of Italy, take independence, or become part of Austria again.
On 27 November 2006, he resigned from his position as a lifetime senator. His resignation was, however, rejected on 31 January 2007 by a vote of the Senate.
In May 2008, Cossiga voted in favor of the government of Silvio Berlusconi.
Cossiga died on 17 August 2010 from respiratory problems at the Agostino Gemelli Polyclinic. After his death, four letters written by Cossiga were sent to the four highest authorities of the state in office at the time of his death, President of the Republic Giorgio Napolitano, President of the Senate Renato Schifani, President of the Chamber of Deputies Gianfranco Fini and Prime Minister Silvio Berlusconi.
The funeral took place in his hometown, Sassari, at the Church of San Giuseppe. Cossiga is buried in the public cemetery of Sassari, in the family tomb, not far from one of his predecessors as President of Italy, Antonio Segni.
In 2007, Cossiga wrote (referring to the 2001 September 11 attacks): "all democratic circles in America and of Europe, especially those of the Italian centre-left, now know that the disastrous attack was planned and realized by the American CIA and Mossad with the help of the Zionist world, to place the blame on Arab countries and to persuade the Western powers to intervene in Iraq and Afghanistan". However, the previous year Cossiga had stated that he rejects theoretical conspiracies and that it "seems unlikely that September 11 was the result of an American plot."
In the same statement, Cossiga claimed that a video tape circulated by Osama bin Laden's al Qaeda and containing threats against Silvio Berlusconi was "produced in the studios of Mediaset in Milan" and forwarded to the "Islamist Al-Jazeera television network." The purpose of that video tape (which was actually an audio tape) was to raise "a wave of solidarity to Berlusconi" who was, at the time, facing political difficulties.
In 2008, Francesco Cossiga said that Mario Draghi was "a craven moneyman".
Cossiga blamed the loss of Itavia Flight 870, a passenger jet that crashed in 1980 with the loss of all 81 people on board, on a missile fired from a French Navy aircraft. On 23 January 2013 Italy's top criminal court ruled that there was "abundantly" clear evidence that the flight was brought down by a missile.
As President of the Republic, Cossiga was Head (and also Knight Grand Cross with Grand Cordon) of the Order of Merit of the Italian Republic (from 3 July 1985 to 28 April 1992), Military Order of Italy, Order of the Star of Italian Solidarity, Order of Merit for Labour and Order of Vittorio Veneto and Grand Cross of Merit of the Italian Red Cross. He has also been given honours and awards by other countries.
 

</doc>
<doc id="11812" url="https://en.wikipedia.org/wiki?curid=11812" title="Lockheed Martin F-35 Lightning II">
Lockheed Martin F-35 Lightning II

The Lockheed Martin F-35 Lightning II is an American family of single-seat, single-engine, all-weather stealth multirole combat aircraft that is intended to perform both air superiority and strike missions. It is also able to provide electronic warfare and intelligence, surveillance, and reconnaissance capabilities. Lockheed Martin is the prime F-35 contractor, with principal partners Northrop Grumman and BAE Systems. The aircraft has three main variants: the conventional takeoff and landing F-35A (CTOL), the short take-off and vertical-landing F-35B (STOVL), and the carrier-based F-35C (CV/CATOBAR).
The aircraft descends from the Lockheed Martin X-35, which in 2001 beat the Boeing X-32 to win the Joint Strike Fighter (JSF) program. Its development is principally funded by the United States, with additional funding from program partner countries from NATO and close U.S. allies, including the United Kingdom, Italy, Australia, Canada, Norway, Denmark, the Netherlands, and formerly Turkey. Several other countries have ordered, or are considering ordering, the aircraft. The program has drawn much scrutiny and criticism for its unprecedented size, complexity, ballooning costs, and much-delayed deliveries. The acquisition strategy of concurrent production of the aircraft while it was still in development and testing led to expensive design changes and retrofits.
The F-35B entered service with the U.S. Marine Corps in July 2015, followed by the U.S. Air Force F-35A in August 2016 and the U.S. Navy F-35C in February 2019. The F-35 was first used in combat in 2018 by the Israeli Air Force. The U.S. plans to buy 2,456 F-35s through 2044, which will represent the bulk of the crewed tactical airpower of the U.S. Air Force, Navy, and Marine Corps for several decades. The aircraft is projected to operate until 2070.
The F-35 was the product of the Joint Strike Fighter (JSF) program, which was the merger of various combat aircraft programs from the 1980s and 1990s. One progenitor program was the Defense Advanced Research Projects Agency (DARPA) Advanced Short Take-Off/Vertical Landing (ASTOVL) which ran from 1983 to 1994; ASTOVL aimed to develop a Harrier Jump Jet replacement for the U.K. Royal Navy and the U.S. Marine Corps (USMC). Under one of ASTOVL's classified programs, the Supersonic STOVL Fighter (SSF), Lockheed Skunk Works conducted research for a stealthy supersonic STOVL fighter intended for both U.S. Air Force (USAF) and USMC; a key technology explored was the shaft-driven lift fan (SDLF) system. Lockheed's concept was a single-engine canard delta aircraft weighing about empty. ASTOVL was rechristened as the Common Affordable Lightweight Fighter (CALF) in 1993 and involved Lockheed, McDonnell Douglas, and Boeing.
In 1993, the Joint Advanced Strike Technology (JAST) program emerged following the USAF's Multi-Role Fighter (MRF) and U.S. Navy's (USN) Advanced Fighter-Attack (A/F-X) programs cancellations. MRF, a program for a relatively affordable F-16 replacement, was scaled back and delayed due to post-Cold War defense cuts easing F-16 fleet usage and thus extending its service life as well as increasing budget pressure from the F-22 program. The A/F-X, initially known as the Advanced-Attack (A-X), began in 1991 as the USN's follow-on to the Advanced Tactical Aircraft (ATA) program for an A-6 replacement; the resulting A-12 Avenger II was cancelled due to problems and cost overruns in 1991. In the same year, the termination of the Naval Advanced Tactical Fighter (NATF), an offshoot of USAF's Advanced Tactical Fighter (ATF) program, to replace the F-14 resulted in additional fighter capability being added to A-X, which was then renamed A/F-X. Amid increased budget pressure, the Department of Defenses (DoD) Bottom-Up Review (BUR) in September 1993 announced MRF's and A/F-X's cancellations, with applicable experience brought to the emerging JAST program. JAST was not meant to develop a new aircraft, instead developing requirements, maturing technologies, and demonstrating concepts for advanced strike warfare.
As JAST progressed, the need for concept demonstrator aircraft by 1996 emerged, which would coincide with the full-scale flight demonstrator phase of ASTOVL/CALF. Because the ASTOVL/CALF concept appeared to align with the JAST charter, the two programs were eventually merged in 1994 under the JAST name, with the program now serving the USAF, USMC, and USN. JAST was subsequently renamed the Joint Strike Fighter (JSF) in 1995, with STOVL submissions by McDonnell Douglas, Northrop Grumman, Lockheed Martin, and Boeing. The JSF was expected to eventually replace large numbers of multi-role and strike fighters in the inventories of the US and its allies, including the Harrier, F-16, F/A-18, A-10, and F-117.
International participation is a key aspect of the JSF program, starting with United Kingdom participation in the ASTOVL program. Many international partners requiring modernization of their air forces that deployed the F-16 and F/A-18 were interested in the JSF. The United Kingdom joined JAST/JSF as a founding member in 1995 and thus became the only Tier 1 partner of the JSF program; Italy, the Netherlands, Denmark, Norway, Canada, Australia, and Turkey joined the program during the Concept Demonstration Phase (CDP), with Italy and the Netherlands being Tier 2 partners and the rest Tier 3. Consequently, the aircraft was developed in cooperation with international partners and available for export.
Boeing and Lockheed Martin were selected in early 1997 for CDP, with their concept demonstrator aircraft designated X-32 and X-35 respectively; the McDonnell Douglas team was eliminated and Northrop Grumman and British Aerospace joined the Lockheed Martin team. Each firm would produce two prototype air vehicles to demonstrate conventional takeoff and landing (CTOL), carrier takeoff and landing (CV), and STOVL. Lockheed Martin's design would leverage the work on the SDLF system conducted under the ASTOVL/CALF program. The key aspect of the X-35 that enabled STOVL operation, the SDLF system consists of the lift fan in the forward center fuselage that could be activated by engaging a clutch that connects the drive shaft to the turbines and thus augmenting the thrust from the engine's swivel nozzle. Research from prior aircraft incorporating similar systems, such as the Convair Model 200, Rockwell XFV-12, and Yakovlev Yak-141, were also taken into consideration. By contrast, Boeing's X-32 employed direct lift system that the augmented turbofan would be reconfigured to when engaging in STOVL operation.
Lockheed Martin's commonality strategy was to replace the STOVL variant's SDLF with a fuel tank and the aft swivel nozzle with a two-dimensional thrust vectoring nozzle for the CTOL variant. This would enable identical aerodynamic configuration for the STOVL and CTOL variants, while the CV variant would have an enlarged wing in order to reduce landing speed for carrier recovery. Due to aerodynamic characteristics and carrier recovery requirements from the JAST merger, the design configuration would settle on a conventional tail compared to the canard delta design from the ASTOVL/CALF; notably, the conventional tail configuration offers much lower risk for carrier recovery compared to the ASTOVL/CALF canard configuration, which was designed without carrier compatibility in mind. This enabled greater commonality between all three variants, as commonality goal was still very high at this stage of the design. Lockheed Martin's prototypes would consist of the X-35A for demonstrating CTOL before converting it to the X-35B for STOVL demonstration and the larger-winged X-35C for CV compatibility demonstration.
The X-35A first flew on 24 October 2000 and conducted flight tests for subsonic and supersonic flying qualities, handling, range, and maneuver performance. After 28 flights, the aircraft was then converted into the X-35B for STOVL testing, with key changes including the addition of the SDLF, the three-bearing swivel module (3BSM), and roll-control ducts. The X-35B would successfully demonstrate the SDLF system by performing stable hover, vertical landing, and short takeoff in less than . The X-35C first flew on 16 December 2000 and conducted field landing carrier practice tests.
On 26 October 2001, Lockheed Martin was declared the winner and was awarded the System Development and Demonstration (SDD) contract; Pratt & Whitney was separately awarded to develop the F135 engine for the JSF. The F-35 designation, which was out of sequence with standard DoD numbering, was allegedly determined on the spot by program manager Major General Mike Hough; this came as a surprise even to Lockheed Martin, which had expected the "F-24" designation for the JSF.
As the JSF program moved into the SDD phase, the X-35 demonstrator design was modified to create the F-35 combat aircraft. The forward fuselage was lengthened by to make room for mission avionics, while the horizontal stabilizers were moved aft to retain balance and control. The diverterless supersonic inlet changed from a four-sided to a three-sided cowl shape and was moved aft. The fuselage section was fuller, the top surface raised by along the centerline to accommodate weapons bays. Following the designation of the X-35 prototypes, the three variants were designated F-35A (CTOL), F-35B (STOVL), and F-35C (CV). Prime contractor Lockheed Martin performs overall systems integration and final assembly and checkout (FACO), while Northrop Grumman and BAE Systems supply components for mission systems and airframe.
Adding the systems of a fighter aircraft added weight. The F-35B gained the most, largely due to a 2003 decision to enlarge the weapons bays for commonality between variants; the total weight growth was reportedly up to , over 8%, causing all STOVL key performance parameter (KPP) thresholds to be missed. In December 2003, the STOVL Weight Attack Team (SWAT) was formed to reduce the weight increase; changes included more engine thrust, thinned airframe members, smaller weapons bays and vertical stabilizers, less thrust fed to the roll-post outlets, and redesigning the wing-mate joint, electrical elements, and the airframe immediately aft of the cockpit. Many changes from the SWAT effort were applied to all three variants for commonality. By September 2004, these efforts had reduced the F-35B's weight by over , while the F-35A and F-35C were reduced in weight by and respectively. The weight reduction work cost $6.2 billion and caused an 18-month delay.
The first F-35A, designated AA-1, was rolled out in Fort Worth, Texas, on 19 February 2006 and first flew on 15 December 2006. The aircraft was given the name "Lightning II" in 2006.
The software was developed as six releases, or Blocks, for SDD. The first two Blocks, 1A and 1B, readied the F-35 for initial pilot training and multi-level security. Block 2A improved the training capabilities, while 2B was the first combat-ready release planned for the USMC's Initial Operating Capability (IOC). Block 3i retains the capabilities of 2B while having new hardware and was planned for the USAF's IOC. The final release for SDD, Block 3F, would have full flight envelope and all baseline combat capabilities. Alongside software releases, each block also incorporates avionics hardware updates and air vehicle improvements from flight and structural testing. In what is known as "concurrency", some low rate initial production (LRIP) aircraft lots would be delivered in early Block configurations and eventually upgraded to Block 3F once development is complete. After 17,000 flight test hours, the final flight for the SDD phase was completed in April 2018. Like the F-22, the F-35 has been targeted by cyberattacks and technology theft efforts, as well as potential vulnerabilities in the integrity of the supply chain.
Testing found several major problems: early F-35B airframes had premature cracking, the F-35C arrestor hook design was unreliable, fuel tanks were too vulnerable to lightning strikes, the helmet display had problems, and more. Software was repeatedly delayed due to its unprecedented scope and complexity. In 2009, the DoD Joint Estimate Team (JET) estimated that the program was 30 months behind the public schedule. In 2011, the program was "re-baselined"; that is, its cost and schedule goals were changed, pushing the IOC from the planned 2010 to July 2015. The decision to simultaneously test, fix defects, and begin production was criticized as inefficient; in 2014, Under Secretary of Defense for Acquisition Frank Kendall called it "acquisition malpractice". The three variants shared just 25% of their parts, far below the anticipated commonality of 70%. The program received considerable criticism for cost overruns and for the total projected lifetime cost, as well as quality management shortcomings by contractors.
The JSF program was expected to cost about $200 billion in acquisition in base-year 2002 dollars when SDD was awarded in 2001. As early as 2005, the Government Accountability Office (GAO) had identified major program risks in cost and schedule. The costly delays strained the relationship between the Pentagon and contractors; Program Executive Officer Lt. General Christopher Bogdan highlighted the frayed relationship in 2012. By 2017, delays and cost overruns had pushed the F-35 program's expected lifetime (i.e., to 2070) cost to $1.5 trillion in then-year dollars: $406.5 billion for acquisition plus $1.1 trillion for operations and maintenance. The unit cost of LRIP lot 13 F-35A was $79.2 million. Delays in development and operational test & evaluation has pushed full-rate production to 2021.
The first combat-capable Block 2B configuration, which had basic air-to-air and strike capabilities, was declared ready by the USMC in July 2015. The Block 3F configuration began operational test and evaluation (OT&E) in December 2018, the completion of which will conclude SDD. The F-35 program is also conducting sustainment and upgrade development, with early LRIP aircraft gradually upgraded to the baseline Block 3F standard by 2021.
The F-35 is expected to be continually upgraded over its lifetime. The first upgrade program, called Continuous Capability Development and Delivery (C2D2) began in 2019 and is currently planned to run to 2024. The near-term development priority of C2D2 is Block 4, which would integrate additional weapons, including those unique to international customers, refresh the avionics, improve ESM capabilities, and add Remotely Operated Video Enhanced Receiver (ROVER) support. C2D2 also places greater emphasis on agile software development to enable quicker releases. In 2018, the Air Force Life Cycle Management Center (AFLCMC) awarded contracts to General Electric and Pratt & Whitney to develop more powerful and efficient adaptive cycle engines for potential application in the F-35, leveraging the research done under the Adaptive Engine Transition Program (AETP).
Defense contractors have offered upgrades to the F-35 outside of official program contracts. In 2013, Northrop Grumman disclosed its development of a directional infrared countermeasures (DIRCM) suite, named Threat Nullification Defensive Resource (ThNDR). The countermeasure system would share the same space as the Distributed Aperture System (DAS) sensors and acts as a laser missile jammer to protect against infrared-homing missiles.
The United States is the primary customer and financial backer, with planned procurement of 1,763 F-35As for the USAF, 353 F-35Bs and 67 F-35Cs for the USMC, and 273 F-35Cs for the USN. Additionally, the United Kingdom, Italy, the Netherlands, Canada, Turkey, Australia, Norway, and Denmark have agreed to contribute US$4.375 billion towards development costs, with the United Kingdom contributing about 10% of the planned development costs as the sole Tier 1 partner. The initial plan was that the U.S. and eight major partner nations would acquire over 3,100 F-35s through 2035. The three tiers of international participation generally reflect financial stake in the program, the amount of technology transfer and subcontracts open for bid by national companies, and the order in which countries can obtain production aircraft. Alongside program partner countries, Israel and Singapore have joined as Security Cooperative Participants (SCP). Sales to SCP and non-partner nations are made through the Pentagon's Foreign Military Sales program. Turkey was removed from the F-35 program in July 2019 over security concerns.
Japan announced on 20 December 2011 its intent to purchase 42 F-35s to replace the F-4 Phantom II, with 38 to be assembled domestically and deliveries beginning in 2016. Due to delays in development and testing, many initial orders have been postponed. Italy reduced its order from 131 to 90 F-35s in 2012. Australia decided to buy the F/A-18F Super Hornet in 2006 and the EA-18G Growler in 2013 as interim measures.
On 3 April 2012, the Auditor General of Canada Michael Ferguson published a report outlining problems with Canada's procurement of the jet; the report states that the government knowingly understated the final cost of 65 F-35s by $10 billion. Following the 2015 Federal Election, the Canadian government under the Liberal Party decided not to proceed with a sole-sourced purchase and launched a competition to choose an aircraft.
In January 2019, Singapore officially announced its plan to buy a small number of F-35s for an evaluation of capabilities and suitability before deciding on more to replace its F-16 fleet. In May 2019, Poland announced plans to buy 32 F-35As to replace its Soviet-era jets; the contract was signed on 31 January 2020.
The F-35 is a family of single-engine, supersonic, stealth multirole fighters. The second fifth generation fighter to enter US service and the first operational supersonic STOVL stealth fighter, the F-35 emphasizes low observables, advanced avionics and sensor fusion that enable a high level of situational awareness and long range lethality; the USAF considers the aircraft its primary strike fighter for conducting suppression of enemy air defense (SEAD) missions, owing to the advanced sensors and mission systems.
The F-35 has a wing-tail configuration with two vertical stabilizers canted for stealth. Flight control surfaces include leading-edge flaps, flaperons, rudders, and all-moving horizontal tails (stabilators); leading edge root extensions also run forwards to the inlets. The relatively short 35-foot wingspan of the F-35A and F-35B is set by the requirement to fit inside USN amphibious assault ship parking areas and elevators; the F-35C's larger wing is more fuel efficient. The fixed diverterless supersonic inlets (DSI) use a bumped compression surface and forward-swept cowl to shed the boundary layer of the forebody away from the inlets, which form a Y-duct for the engine. Structurally, the F-35 drew upon lessons from the F-22; composites comprise 35% of airframe weight, with the majority being bismaleimide and composite epoxy materials as well as some carbon nanotube-reinforced epoxy in newer production lots. The F-35 is considerably heavier than the lightweight fighters it replaces, with the lightest variant having an empty weight of ; much of the weight can be attributed to the internal weapons bays and the extensive avionics carried.
While lacking the raw performance of the larger twin-engine F-22, the F-35 has kinematics competitive with fourth generation fighters such as the F-16 and F/A-18, especially with ordnance mounted because the F-35's internal weapons carriage eliminates parasitic drag from external stores. All variants have a top speed of Mach 1.6, attainable with full internal payload. The powerful F135 engine gives good subsonic acceleration and energy, with supersonic dash in afterburner. The large stabilitors, leading edge extensions and flaps, and canted rudders provide excellent high alpha (angle-of-attack) characteristics, with a trimmed alpha of 50°. Relaxed stability and fly-by-wire controls provide excellent handling qualities and departure resistance. Having over double the F-16's internal fuel, the F-35 has considerably greater combat radius, while stealth also enables a more efficient mission flight profile.
The F-35's mission systems are among the most complex aspects of the aircraft. The avionics and sensor fusion are designed to enhance the pilot's situational awareness and command and control capabilities and facilitate network-centric warfare. Key sensors include the Northrop Grumman AN/APG-81 active electronically scanned array (AESA) radar, BAE Systems AN/ASQ-239 Barracuda electronic warfare system, Northrop Grumman/Raytheon AN/AAQ-37 Distributed Aperture System (DAS), Lockheed Martin AN/AAQ-40 Electro-Optical Targeting System (EOTS) and Northrop Grumman AN/ASQ-242 Communications, Navigation, and Identification (CNI) suite. The F-35 was designed with sensor intercommunication to provide a cohesive image of the local battlespace and availability for any possible use and combination with one another; for example, the APG-81 radar also acts as a part of the electronic warfare system.
Much of the F-35's software was developed in C and C++ programming languages, while Ada83 code from the F-22 was also used; the Block 3F software has 8.6 million lines of code. The Green Hills Software Integrity DO-178B real-time operating system (RTOS) runs on integrated core processors (ICPs); data networking includes the IEEE 1394b and Fibre Channel buses. To enable fleet software upgrades for the software-defined radio systems and greater upgrade flexibility and affordability, the avionics leverage commercial off-the-shelf (COTS) components when practical. The mission systems software, particularly for sensor fusion, was one of the program's most difficult parts and responsible for substantial program delays.
The APG-81 radar uses electronic scanning for rapid beam agility and incorporates passive and active air-to-air modes, strike modes, and synthetic aperture radar (SAR) capability, with multiple target tracking at ranges in excess of . The antenna is tilted backwards for stealth. Complementing the radar is the AAQ-37 DAS, which consists of six infrared sensors that provide all-aspect missile launch warning and target tracking; the DAS acts as a situational awareness infrared search-and-track (SAIRST) and gives the pilot spherical infrared and night-vision imagery on the helmet visor. The ASQ-239 Barracuda electronic warfare system has ten radio frequency antennas embedded into the edges of the wing and tail for all-aspect radar warning receiver (RWR). It also provides sensor fusion of radio frequency and infrared tracking functions, geolocation threat targeting, and multispectral image countermeasures for self-defense against missiles. The electronic warfare system is capable of detecting and jamming hostile radars. The AAQ-40 EOTS is mounted internally behind a faceted low-observable window under the nose and performs laser targeting, forward-looking infrared (FLIR), and long range IRST functions. The ASQ-242 CNI suite uses a half dozen different physical links, including the Multifunction Advanced Data Link (MADL), for covert CNI functions. Through sensor fusion, information from radio frequency receivers and infrared sensors are combined to form a single tactical picture for the pilot. The all-aspect target direction and identification can be shared via MADL to other platforms without compromising low observability, while Link 16 is present for communication with legacy systems.
The F-35 was designed from the outset to incorporate improved processors, sensors, and software enhancements over its lifespan. Technology Refresh 3, which includes a new core processor and a new cockpit display, is planned for Lot 15 aircraft. Lockheed Martin has offered the Advanced EOTS for the Block 4 configuration; the improved sensor fits into the same area as the baseline EOTS with minimal changes. In June 2018, Lockheed Martin picked Raytheon for improved DAS. The USAF has studied the potential for the F-35 to orchestrate attacks by unmanned combat aerial vehicles (UCAVs) via its sensors and communications equipment.
Stealth is a key aspect of the F-35s design, and radar cross-section (RCS) is minimized through careful shaping of the airframe and the use of radar-absorbent materials (RAM); visible measures to reduce RCS include alignment of edges, serration of skin panels, and the masking of the engine face and turbine. Additionally, the F-35's diverterless supersonic inlet (DSI) uses a compression bump and forward-swept cowl rather than a splitter gap or bleed system to divert the boundary layer away from the inlet duct, eliminating the diverter cavity and further reducing radar signature. The RCS of the F-35 has been characterized as lower than a metal golf ball at certain frequencies and angles; in some conditions, the F-35 compares favorably to the F-22 in stealth. For maintainability, the F-35's stealth design took lessons learned from prior stealth aircraft such as the F-22; the F-35's radar-absorbent fibermat skin is more durable and requires less maintenance than older topcoats. The aircraft also has reduced infrared and visual signatures as well as strict controls of radio frequency emitters to prevent their detection. The F-35's stealth design is primarily focused on high-frequency X-band wavelengths; low-frequency radars can spot stealthy aircraft due to Rayleigh scattering, but such radars are also conspicuous, susceptible to clutter, and lack precision. To disguise its RCS, the aircraft can mount four Luneburg lens reflectors.
Noise from the F-35 caused concerns in residential areas near potential bases for the aircraft, and residents near two such bases—Luke Air Force Base, Arizona, and Eglin Air Force Base, Florida—requested environmental impact studies in 2008 and 2009 respectively. Although the noise level in decibels were comparable to those of prior fighters such as the F-16, the sound power of the F-35 is stronger particularly at lower frequencies. Subsequent surveys and studies have indicated that the noise of the F-35 was not perceptibly different from the F-16 and F/A-18E/F, though the greater low-frequency noise was noticeable for some observers.
The glass cockpit was designed to give the pilot good situational awareness. The main display is a 20- by 8-inch (50 by 20 cm) panoramic touchscreen, which shows flight instruments, stores management, CNI information, and integrated caution and warnings; the pilot can customize the arrangement of the information. Below the main display is a smaller stand-by display. The cockpit has a speech-recognition system developed by Adacel. The F-35 does not have a head-up display; instead, flight and combat information is displayed on the visor of the pilot's helmet in a helmet-mounted display system (HMDS). The one-piece tinted canopy is hinged at the front and has an internal frame for structural strength. The Martin-Baker US16E ejection seat is launched by a twin-catapult system housed on side rails. There is a right-hand side stick and throttle hands-on throttle-and-stick system. For life support, an onboard oxygen-generation system (OBOGS) is fitted and powered by the Integrated Power Package (IPP), with an auxiliary oxygen bottle and backup oxygen system for emergencies.
The Vision Systems International helmet display is a key piece of the F-35's human-machine interface. Instead of the head-up display mounted atop the dashboard of earlier fighters, the HMDS puts flight and combat information on the helmet visor, allowing the pilot to see it no matter which way he or she is facing. Infrared and night vision imagery from the Distributed Aperture System can be displayed directly on the HMDS and enables the pilot to "see through" the aircraft. The HDMS allows an F-35 pilot to fire missiles at targets even when the nose of the aircraft is pointing elsewhere by cuing missile seekers at high angles off-boresight. Each helmet costs $400,000. The HMDS weighs more than traditional helmets, and there is concern that it can endanger lightweight pilots during ejection.
Due to the HMDS's vibration, jitter, night-vision and sensor display problems during development, Lockheed Martin and Elbit issued a draft specification in 2011 for an alternative HMDS based on the AN/AVS-9 night vision goggles as backup, with BAE Systems chosen later that year. A cockpit redesign would be needed to adopt an alternative HMDS. Following progress on the baseline helmet, development on the alternative HMDS was halted in October 2013. In 2016, the Gen 3 helmet with improved night vision camera, new liquid crystal displays, automated alignment and software enhancements was introduced with LRIP lot 7.
To preserve its stealth shaping, the F-35 has two internal weapons bays with four weapons stations. The two outboard weapon stations each can carry ordnance up to , or for F-35B, while the two inboard stations carry air-to-air missiles. Air-to-surface weapons for the outboard station include the Joint Direct Attack Munition (JDAM), Paveway series of bombs, Joint Standoff Weapon (JSOW), and cluster munitions (Wind Corrected Munitions Dispenser). The station can also carry multiple smaller munitions such as the GBU-39 Small Diameter Bombs (SDB), GBU-53/B SDB II, and the SPEAR 3 anti-tank missiles; up to four SDBs can be carried per station for the F-35A and F-35C, and three for F-35B. The inboard station can carry the AIM-120 AMRAAM. Two compartments behind the weapons bays contain flares, chaff, and towed decoys.
The aircraft can use six external weapons stations for missions that do not require stealth. The wingtip pylons each can carry an AIM-9X or AIM-132 ASRAAM and are canted outwards to reduce their radar cross-section. Additionally, each wing has a inboard station and a middle station, or for F-35B. The external wing stations can carry large air-to-surface weapons that would not fit inside the weapons bays such as the AGM-158 Joint Air to Surface Stand-off Missile (JASSM) cruise missile. An air-to-air missile load of eight AIM-120s and two AIM-9s is possible using internal and external weapons stations; a configuration of six bombs, two AIM-120s and two AIM-9s can also be arranged. The F-35A is armed with a 25 mm GAU-22/A rotary cannon mounted internally near the left wing root with 182 rounds carried; the gun is more effective against ground targets than the 20 mm cannon carried by other USAF fighters. The F-35B and F-35C have no internal gun and instead can use a Terma A/S multi-mission pod (MMP) carrying the GAU-22/A and 220 rounds; the pod is mounted on the centerline of the aircraft and shaped to reduce its radar cross-section. In lieu of the gun, the pod can also be used for different equipment and purposes, such as electronic warfare, aerial reconnaissance, or rear-facing tactical radar.
Lockheed Martin is developing a weapon rack called Sidekick that would enable the internal outboard station to carry two AIM-120s, thus increasing the internal air-to-air payload to six missiles, currently offered for Block 4. Block 4 will also have a rearranged hydraulic line and bracket to allow the F-35B to carry four SDBs per internal outboard station; integration of the MBDA Meteor is also planned. The USAF and USN are planning to integrate the AGM-88G AARGM-ER internally in the F-35A and F-35C. Norway and Australia are funding an adaptation of the Naval Strike Missile (NSM) for the F-35; designated Joint Strike Missile (JSM), two missiles can be carried internally with an additional four externally. Nuclear weapons delivery via internal carriage of the B61 nuclear bomb is planned for Block 4B in 2024. Both hypersonic missiles and direct energy weapons such as solid-state laser are currently being considered as future upgrades. Lockheed Martin is studying integrating a fiber laser that uses spectral beam combining multiple individual laser modules into a single high-power beam, which can be scaled to various levels.
The USAF plans for the F-35A to take up the close air support (CAS) mission in contested environments; amid criticism that it is not as well suited as a dedicated attack platform, USAF chief of staff Mark Welsh placed a focus on weapons for CAS sorties, including guided rockets, fragmentation rockets that shatter into individual projectiles before impact, and more compact ammunition for higher capacity gun pods. Fragmentary rocket warheads create greater effects than cannon shells as each rocket creates a "thousand-round burst", delivering more projectiles than a strafing run.
The single-engine aircraft is powered by the Pratt & Whitney F135 low-bypass augmented turbofan with rated thrust of . Derived from the Pratt & Whitney F119 used by the F-22, the F135 has a larger fan and higher bypass ratio to increase subsonic fuel efficiency, and unlike the F119, is not optimized for supercruise. The engine contributes to the F-35's stealth by having a low-observable augmenter, or afterburner, that incorporates fuel injectors into thick curved vanes; these vanes are covered by ceramic radar-absorbent materials and mask the turbine. The stealthy augmenter had problems with pressure pulsations, or "screech", at low altitude and high speed early in its development. The low-observable axisymmetric nozzle consists of 15 partially overlapping flaps that create a sawtooth pattern at the trailing edge, which reduces radar signature and creates shed vortices that reduce the infrared signature of the exhaust plume. Due to the engines large dimensions, the USN had to modify its underway replenishment system to facilitate at-sea logistics support.
The F135-PW-600 variant for the F-35B incorporates the SDLF to allow STOVL operations. Designed by Lockheed Martin and developed by Rolls-Royce, the SDLF, also known as the Rolls-Royce LiftSystem, consists of the lift fan, drive shaft, two roll posts, and a "three-bearing swivel module" (3BSM). The thrust vectoring 3BSM nozzle allows the main engine exhaust to be deflected downward at the tail of the aircraft and is moved by a "fueldraulic" actuator that uses pressurized fuel as the working fluid. Unlike the Harriers Rolls-Royce Pegasus engine that entirely uses direct engine thrust for lift, the F-35B's system augments the swivel nozzle's thrust with the lift fan; the fan is powered by the low-pressure turbine through a drive shaft when engaged with a clutch and placed near the front of the aircraft to provide a counterbalancing thrust. Roll control during slow flight is achieved by diverting unheated engine bypass air through wing-mounted thrust nozzles called roll posts.
An alternative engine, the General Electric/Rolls-Royce F136, was being developed in the 2000s; originally, F-35 engines from Lot 6 onward were competitively tendered. Using technology from the General Electric YF120, The F136 was claimed to have a greater temperature margin than the F135. The F136 was canceled in December 2011 due to lack of funding.
In 2016, the Adaptive Engine Transition Program (AETP) was launched to develop and test adaptive cycle engines, with one major potential application being the re-engining of the F-35. Both GE and P&W were awarded contracts to develop class demonstrators, with the designations XA100 and XA101 respectively. In 2017, P&W announced the F135 Growth Option 1.0 and 2.0; Growth Option 1.0, which had finished testing and was production ready in May 2017, was a power module upgrade that offered 6–10% thrust improvement and 5–6% fuel burn reduction. The power module could be retrofitted onto older engines and seamlessly added to future engines at low cost rise and no impact on delivery. Growth Option 2.0 would be the adaptive cycle XA101. In June 2018, Pratt & Whitney changed its development plan for the F135, and instead offered an adaptive three-stream fan as Growth Option 2.0 that's separate from the XA101, which would instead have a new engine core.
The F-35 is designed to require less maintenance than earlier stealth aircraft. Some 95% of all field-replaceable parts are "one deep"—that is, nothing else need be removed to reach the desired part; for instance, the ejection seat can be replaced without removing the canopy. The F-35 has a fibermat radar-absorbent material (RAM) baked into the skin, which is more durable, easier to work with, and faster to cure than older RAM coatings; similar coatings are currently being considered for application on older stealth aircraft such as the F-22. Skin corrosion on the F-22 led the F-35's designers to use a less galvanic corrosion-inducing skin gap filler and to use fewer gaps in the airframe skin needing filler and better drainage. The flight control system uses electro-hydrostatic actuators rather than traditional hydraulic systems; these controls can be powered by lithium-ion batteries in case of emergency. Commonality between the different variants allowed the USMC to create their first aircraft maintenance Field Training Detachment to apply the USAF's lessons to their F-35 operations.
The F-35 was intended to be supported by a computerized maintenance management system named Autonomic Logistics Information System (ALIS). In concept, any aircraft can be serviced at any F-35 maintenance facility and for all parts to be globally tracked and shared as needed. Due to numerous problems, such as unreliable diagnoses, excessive connectivity requirements, and security vulnerabilities, program officials plan to replace ALIS with the cloud-based Operational Data Integrated Network (ODIN) by 2022.
The first F-35A, AA-1, conducted its engine run in September 2006 and first flew on 15 December 2006. Unlike all subsequent aircraft, AA-1 did not have the weight optimization from SWAT; consequently, it mainly tested subsystems common to subsequent aircraft, such as the propulsion, electrical system, and cockpit displays. This aircraft was retired from flight testing in December 2009 and was used for live-fire testing at NAS China Lake.
The first F-35B, BF-1, flew on 11 June 2008, while the first weight-optimized F-35A and F-35C, AF-1 and CF-1, flew on 14 November 2009 and 6 June 2010 respectively. The F-35B's first hover was on 17 March 2010, followed by its first vertical landing the next day. The F-35 Integrated Test Force (ITF) consisted of 18 aircraft at Edwards Air Force Base and Naval Air Station Patuxent River. Nine aircraft at Edwards, five F-35As, three F-35Bs, and one F-35C, performed flight sciences testing such as F-35A envelope expansion, flight loads, stores separation, as well as mission systems testing. The other nine aircraft at Patuxent River, five F-35Bs and four F-35Cs, were responsible for F-35B and C envelope expansion and STOVL and CV suitability testing. Additional carrier suitability testing was conducted at Naval Air Warfare Center Aircraft Division at Lakehurst, New Jersey. Two non-flying aircraft of each variant were used to test static loads and fatigue. For testing avionics and mission systems, a modified Boeing 737-300 with a duplication of the cockpit, the Lockheed Martin CATBird has been used. Field testing of the F-35's sensors were conducted during Exercise Northern Edge 2009 and 2011, serving as significant risk-reduction steps.
Flight tests revealed several serious deficiencies that required costly redesigns, caused delays, and resulted in several fleet-wide groundings. In 2011, the F-35C failed to catch the arresting wire in all eight landing tests; a redesigned tail hook was delivered two years later. By June 2009, many of the initial flight test targets had been accomplished but the program was behind schedule. Software and mission systems were among the biggest sources of delays for the program, with sensor fusion proving especially challenging. In fatigue testing, the F-35B suffered several premature cracks, requiring a redesign of the structure. A third non-flying F-35B is currently planned to test the redesigned structure. The F-35B and C also had problems with the horizontal tails suffering heat damage from prolonged afterburner use. Early flight control laws had problems with "wing drop" and also made the airplane sluggish, with high angles-of-attack tests in 2015 against an F-16 showing a lack of energy.
At-sea testing of the F-35B was first conducted aboard . In October 2011, two F-35Bs conducted three weeks of initial sea trials, called Development Test I. The second F-35B sea trials, Development Test II, began in August 2013, with tests including nighttime operations; two aircraft completed 19 nighttime vertical landings using DAS imagery. The first operational testing involving six F-35Bs was done on the "Wasp" in May 2015. The final Development Test III on involving operations in high sea states was completed in late 2016. A Royal Navy F-35 conducted the first "rolling" landing on board in October 2018.
After the redesigned tail hook arrived, the F-35C's carrier-based Development Test I began in November 2014 aboard and focused on basic day carrier operations and establishing launch and recovery handling procedures. Development Test II, which focused on night operations, weapons loading, and full power launches, took place in October 2015. The final Development Test III was completed in August 2016, and included tests of asymmetric loads and certifying systems for landing qualifications and interoperability. Operational test of the F-35C began in 2018.
The F-35's reliability and availability have fallen short of requirements, especially during early years of testing. The ALIS maintenance and logistics system was plagued by excessive connectivity requirements and faulty diagnoses. In late 2017, the GAO reported the time needed to repair an F-35 part averaged 172 days, which was "twice the program's objective," and that shortage of spare parts was degrading readiness. In 2019, while individual F-35 units have achieved mission capable rates of over the target of 80% for short periods during deployed operations, fleet-wide rates remained below target. The fleet availability goal of 65% was also not met, although the trend shows improvement. Gun accuracy of the F-35A remains unacceptable.
Operational test and evaluation (OT&E) with Block 3F, the final configuration for SDD, began in December 2018.
The F-35A and F-35B were cleared for basic flight training in early 2012. However, lack of system maturity at the time led to concerns over safety as well as concerns by the Director of Operational Test & Evaluation (DOT&E) over electronic warfare testing, budget, and concurrency for the Operational Test and Evaluation master plan. On 10 September 2012, despite problems remaining in the operational testing plan, the USAF began an operational utility evaluation (OUE) of the F-35A, including logistical support, maintenance, personnel training, and pilot execution. OUE flights began on 26 October and were completed on 14 November after 24 flights, each pilot having completed six flights. On 16 November 2012, the USMC received the first F-35B at MCAS Yuma, although Marine pilots had several flight restrictions. During the Low Rate Initial Production (LRIP) phase, the three U.S. military services jointly developed tactics and procedures using flight simulators, testing effectiveness, discovering problems and refining design. In January 2013, training began at Eglin Air Force Base with capacity for 100 pilots and 2,100 maintainers at once. On 8 January 2015, RAF Lakenheath in the UK was chosen as the first base in Europe to station two USAF F-35 squadrons, with 48 aircraft adding to the 48th Fighter Wing's existing F-15C and F-15E squadrons.
The USMC declared Initial Operational Capability (IOC) for the F-35B in the Block 2B configuration on 31 July 2015 after operational trials. However, limitations remained in night operations, communications, software and weapons carriage capabilities. USMC F-35Bs participated in their first Red Flag exercise in July 2016 with 67 sorties conducted. USAF F-35A in the Block 3i configuration achieved IOC with the USAF on 2 August 2016, and the F-35C in Block 3F with the USN on 28 February 2019. USAF F-35As conducted their first Red Flag exercise in 2017; system maturity had improved and the aircraft scored a kill ratio of 15:1 against an F-16 aggressor squadron in a high-threat environment.
The F-35's operating cost is higher than those of some older fighters. In fiscal year 2018, the F-35A's cost per flight hour (CPFH) was $44,000, a number that was reduced to $35,000 in 2019. For comparison, in 2015 the CPFH of the A-10 was $17,716; the F-15C, $41,921; and the F-16C, $22,514. Lockheed Martin hopes to reduce it to $25,000 by 2025 through performance-based logistics and other measures.
The USMC plans to disperse its F-35Bs among forward deployed bases to enhance survivability while remaining close to a battlespace, similar to RAF Harrier deployment in the Cold War, which relied on the use of off-base locations that offered short runways, shelter, and concealment. Known as distributed STOVL operations (DSO), F-35Bs would operate from temporary bases in allied territory within the range of hostile ballistic and cruise missiles and be moved between temporary locations inside the enemy's 24- to 48-hour targeting cycle; this strategy accounts for the F-35B's short range, the shortest of the three variants, with mobile forward arming and refueling points (M-Farps) accommodating KC-130 and MV-22 Osprey aircraft to rearm and refuel the jets, as well as littoral areas for sea links of mobile distribution sites. M-Farps can be based on small airfields, multi-lane roads, or damaged main bases, while F-35Bs return to rear-area USAF bases or friendly ships for scheduled maintenance. Helicopter-portable metal planking is needed to protect unprepared roads from the F-35B's engine exhaust; the USMC are studying lighter heat-resistant alternatives.
The first U.S. combat employment began in July 2018 with USMC F-35Bs from the amphibious assault ship , with the first combat strike on 27 September 2018 against a Taliban target in Afghanistan. This was followed by a USAF deployment to Al Dhafra Air Base, UAE on 15 April 2019. On 27 April 2019, USAF F-35As were first used in combat in an airstrike on an Islamic State tunnel network in northern Iraq.
In service, some USAF pilots have nicknamed the aircraft "Panther" in lieu of the official "Lightning II".
The United Kingdom's Royal Air Force and Royal Navy both operate the F-35B, known simply as the Lightning in British service; it has replaced the Harrier GR9, which was retired in 2010, and Tornado GR4, which was retired in 2019. The F-35 is to be Britain's primary strike aircraft for the next three decades. One of the Royal Navy's requirements for the F-35B was a Shipborne Rolling and Vertical Landing (SRVL) mode to increase maximum landing weight by using wing lift during landing. In July 2013, Chief of the Air Staff, Air Chief Marshal Sir Stephen Dalton announced that No. 617 (The Dambusters) Squadron would be the RAF's first operational F-35 squadron. The second operational squadron will be the Fleet Air Arm's 809 Naval Air Squadron in April 2023.
No. 17 (Reserve) Test and Evaluation Squadron (TES) stood-up on 12 April 2013 as the Operational Evaluation Unit for the Lightning, becoming the first British squadron to operate the type. By June 2013, the RAF had received three F-35s of the 48 on order, all initially based at Eglin Air Force Base. In June 2015, the F-35B undertook its first launches from a ski-jump at NAS Patuxent River. When operated at sea, British F-35B shall use ships fitted with ski-jumps, as will the Italian Navy. British F-35Bs are not intended to receive the Brimstone 2 missile. On 5 July 2017, it was announced the second UK-based RAF squadron would be No. 207 Squadron, which reformed on 1 August 2019 as the Lightning Operational Conversion Unit. No. 617 Squadron reformed on 18 April 2018 during a ceremony in Washington, D.C., US, becoming the first RAF front-line squadron to operate the type; receiving its first four F-35Bs on 6 June, flying from MCAS Beaufort to RAF Marham. Both No. 617 Squadron and its F-35s were declared combat ready on 10 January 2019.
In April 2019, No. 617 Squadron deployed to RAF Akrotiri, Cyprus, the type's first overseas deployment. On 25 June 2019, the first combat use of an RAF F-35B was reportedly undertaken as armed reconnaissance flights searching for Islamic State targets in Iraq and Syria. In October 2019, "the Dambusters" and No. 17 TES F-35s were embarked on for the first time. No. 617 Squadron departed RAF Marham on 22 January 2020 for their first Exercise Red Flag with the Lightning.
The Israeli Air Force declared the F-35 operationally capable on 6 December 2017. According to Kuwaiti newspaper "Al Jarida", in July 2018, a test mission of at least three IAF F-35s flew to Iran's capital Tehran and back from Tel Aviv. While publicly unconfirmed, regional leaders acted on the report; Iran's supreme leader Ali Khamenei reportedly fired the air force chief and commander of Iran's Revolutionary Guard Corps over the mission.
On 22 May 2018, Israeli Air Force chief Amikam Norkin said that the service had employed their F-35Is in two attacks on two battle fronts, marking the first combat operation of an F-35 by any country. Norkin said it had been flown "all over the Middle East", and showed photos of an F-35I flying over Beirut in daylight. In July 2019, Israel reportedly expanded its strikes against Iranian missile shipments; IAF F-35Is allegedly struck Iranian targets in Iraq twice.
The F-35A is the conventional takeoff and landing (CTOL) variant intended for the USAF and other air forces. It is the smallest, lightest version and capable of 9 g, the highest of all variants.
Although the F-35A currently conducts aerial refueling via boom and receptacle method, the aircraft can be modified for probe-and-drogue refueling if needed by the customer. A drag chute pod can be installed on the F-35A, with the Royal Norwegian Air Force being the first operator to adopt it.
The F-35B is the short takeoff and vertical landing (STOVL) variant of the aircraft. Similar in size to the A variant, the B sacrifices about a third of the A variant's fuel volume to accommodate the SDLF. This variant is limited to 7 g. Unlike other variants, the F-35B has no landing hook. The "STOVL/HOOK" control instead engages conversion between normal and vertical flight.
The F-35C variant is designed for catapult-assisted take-off but arrested recovery operations from aircraft carriers. Compared to the F-35A, the F-35C features larger wings with foldable wingtip sections, larger wing and tail control surfaces for improved low-speed control, stronger landing gear for the stresses of carrier arrested landings, a twin-wheel nose gear, and a stronger tailhook for use with carrier arrestor cables. The larger wing area allows for decreased landing speed while increasing both range and payload. The F-35C is limited to 7.5 g. 
A study for a possible upgrade of the F-35A to be fielded by the 2035 target date of the USAF's Future Operating Concept.
The F-35I "Adir" (, meaning "Awesome", or "Mighty One") is an F-35A with unique Israeli modifications. The US initially refused to allow such changes before permitting Israel to integrate its own electronic warfare systems, including sensors and countermeasures. The main computer has a plug-and-play function for add-on systems; proposals include an external jamming pod, and new Israeli air-to-air missiles and guided bombs in the internal weapon bays. A senior IAF official said that the F-35's stealth may be partly overcome within 10 years despite a 30 to 40 year service life, thus Israel's insistence on using their own electronic warfare systems. Israel Aerospace Industries (IAI) has considered a two-seat F-35 concept; an IAI executive noted: "There is a known demand for two seats not only from Israel but from other air forces". IAI plans to produce conformal fuel tanks.
The Canadian CF-35 is a proposed variant that would differ from the F-35A through the addition of a drogue parachute and may include an F-35B/C-style refueling probe. In 2012, it was revealed that the CF-35 would employ the same boom refueling system as the F-35A. One alternative proposal would have been the adoption of the F-35C for its probe refueling and lower landing speed; however, the Parliamentary Budget Officer's report cited the F-35C's limited performance and payload as being too high a price to pay. Following the 2015 Federal Election the Liberal Party, whose campaign had included a pledge to cancel the F-35 procurement, formed a new government and commenced an open competition to replace the existing CF-18 Hornet.
On 23 June 2014, an F-35A's engine caught fire at Eglin Air Force Base. The pilot escaped unharmed, while the aircraft sustained an estimated US$50 million of damages. The accident caused all flights to be halted on 3 July. The fleet returned to flight on 15 July with flight envelope restrictions. In June 2015, the USAF Air Education and Training Command (AETC) issued its official report, which blamed the failure on the third stage rotor of the engine's fan module, pieces of which cut through the fan case and upper fuselage. Pratt & Whitney applied an extended "rub-in" to increase the gap between the second stator and the third rotor integral arm seal, as well as design alterations to pre-trench the stator by early 2016.
The first crash occurred on 28 September 2018 involving a USMC F-35B near Marine Corps Air Station Beaufort, South Carolina; the pilot ejected safely. The cause of the crash was attributed to a faulty fuel tube; all F-35s were grounded on 11 October pending a fleet-wide inspection of the tubes. The next day, most USAF and USN F-35s returned to flight status following the inspection.
On 9 April 2019, a Japan Air Self-Defense Force F-35A attached to Misawa Air Base disappeared from radar about 84 miles (135 km) east of the Aomori Prefecture during a training mission over the Pacific Ocean. The pilot, Major Akinori Hosomi, had radioed his intention to abort the drill before disappearing. Both US and Japanese Navy assets searched for the missing aircraft and pilot, finding debris on the water that confirmed its crash; Hosomi's remains were recovered in June. In response, Japan grounded its 12 F-35As. There was speculation that China or Russia might attempt to salvage it; the Japanese Defense Ministry announced there had been no "reported activities" from either country. The F-35 reportedly did not send a distress signal nor did the pilot attempt any recovery maneuvers as the aircraft descended at a rapid rate. The accident report attributed the cause to the pilot's spatial disorientation.
On 19 May 2020, a USAF F-35A from the 58th Fighter Squadron crashed while landing at Eglin Air Force Base, Florida. The pilot ejected and was in stable condition.
On 29 September 2020, a USMC F-35B fighter jet crashed in Imperial County, California, after colliding with a Marine Corps KC-130 during air-to-air refuelling. The F-35B pilot was injured in the ejection, but the KC-130 crash-landed gear up in a field.

</doc>
<doc id="11815" url="https://en.wikipedia.org/wiki?curid=11815" title="Food additive">
Food additive

Food additives are substances added to food to preserve flavor or enhance its taste, appearance, or other qualities. Some additives have been used for centuries; for example, preserving food by pickling (with vinegar), salting, as with bacon, preserving sweets or using sulfur dioxide as with wines. With the advent of processed foods in the second half of the twentieth century, many more additives have been introduced, of both natural and artificial origin. Food additives also include substances that may be introduced to food indirectly (called "indirect additives") in the manufacturing process, through packaging, or during storage or transport.
To regulate these additives and inform consumers, each additive is assigned a unique number called an "E number", which is used in Europe for all approved additives. This numbering scheme has now been adopted and extended by the "Codex Alimentarius" Commission to internationally identify all additives, regardless of whether they are approved for use.
E numbers are all prefixed by "E", but countries outside Europe use only the number, whether the additive is approved in Europe or not.
For example, acetic acid is written as E260 on products sold in Europe, but is simply known as additive 260 in some countries. Additive 103, alkannin, is not approved for use in Europe so does not have an E number, although it is approved for use in Australia and New Zealand. Since 1987, Australia has had an approved system of labelling for additives in packaged foods. Each food additive has to be named or numbered. The numbers are the same as in Europe, but without the prefix "E".
The United States Food and Drug Administration (FDA) lists these items as "generally recognized as safe" (GRAS); they are listed under both their Chemical Abstracts Service number and FDA regulation under the United States Code of Federal Regulations.
Food additives can be divided into several groups, although there is some overlap because some additives exert more than one effect. For example, salt is both a preservative as well as a flavor.
With the increasing use of processed foods since the 19th century, food additives are more widely used. Many countries regulate their use. For example, boric acid was widely used as a food preservative from the 1870s to the 1920s, but was banned after World War I due to its toxicity, as demonstrated in animal and human studies. During World War II, the urgent need for cheap, available food preservatives led to it being used again, but it was finally banned in the 1950s. Such cases led to a general mistrust of food additives, and an application of the precautionary principle led to the conclusion that only additives that are known to be safe should be used in foods. In the United States, this led to the adoption of the Delaney clause, an amendment to the Federal Food, Drug, and Cosmetic Act of 1938, stating that no carcinogenic substances may be used as food additives. However, after the banning of cyclamates in the United States and Britain in 1969, saccharin, the only remaining legal artificial sweetener at the time, was found to cause cancer in rats. Widespread public outcry in the United States, partly communicated to Congress by postage-paid postcards supplied in the packaging of sweetened soft drinks, led to the retention of saccharin, despite its violation of the Delaney clause. However, in 2000, saccharin was found to be carcinogenic in rats due only to their unique urine chemistry.
Periodically, concerns have been expressed about a linkage between additives and hyperactivity, however "no clear evidence of ADHD was
provided".
In 2007, Food Standards Australia New Zealand published an official shoppers' guidance with which the concerns of food additives and their labeling are mediated. In the EU it can take 10 years or more to obtain approval for a new food additive. This includes five years of safety testing, followed by two years for evaluation by the European Food Safety Authority and another three years before the additive receives an EU-wide approval for use in every country in the European Union. Apart from testing and analyzing food products during the whole production process to ensure safety and compliance with regulatory standards, Trading Standards officers (in the UK) protect the public from any illegal use or potentially dangerous mis-use of food additives by performing random testing of food products.
There has been significant controversy associated with the risks and benefits of food additives. Natural additives may be similarly harmful or be the cause of allergic reactions in certain individuals. For example, safrole was used to flavor root beer until it was shown to be carcinogenic. Due to the application of the Delaney clause, it may not be added to foods, even though it occurs naturally in sassafras and sweet basil.
A subset of food additives, micronutrients added in food fortification processes preserve nutrient value by providing vitamins and minerals to foods such as flour, cereal, margarine and milk which normally would not retain such high levels. Added ingredients, such as air, bacteria, fungi, and yeast, also contribute manufacturing and flavor qualities, and reduce spoilage.
ISO has published a series of standards regarding the topic and these standards are covered by ICS 67.220.

</doc>
<doc id="11820" url="https://en.wikipedia.org/wiki?curid=11820" title="Fridtjof Nansen">
Fridtjof Nansen

Fridtjof Wedel-Jarlsberg Nansen (; 10 October 1861 – 13 May 1930) was a Norwegian explorer, scientist, diplomat, humanitarian and Nobel Peace Prize laureate. He led the team that made the first crossing of the Greenland interior in 1888, traversing the island on cross-country skis. He won international fame after reaching a record northern latitude of 86°14′ during his "Fram" expedition of 1893–1896. Although he retired from exploration after his return to Norway, his techniques of polar travel and his innovations in equipment and clothing influenced a generation of subsequent Arctic and Antarctic expeditions.
Nansen studied zoology at the Royal Frederick University in Christiania and later worked as a curator at the University Museum of Bergen where his research on the central nervous system of lower marine creatures earned him a doctorate and helped establish neuron doctrine. Later, neuroscientist Santiago Ramón y Cajal won the 1906 Nobel Prize in Medicine for his research on the same subject. After 1896 his main scientific interest switched to oceanography; in the course of his research he made many scientific cruises, mainly in the North Atlantic, and contributed to the development of modern oceanographic equipment. 
As one of his country's leading citizens, in 1905 Nansen spoke out for the ending of Norway's union with Sweden, and was instrumental in persuading Prince Carl of Denmark to accept the throne of the newly independent Norway. Between 1906 and 1908 he served as the Norwegian representative in London, where he helped negotiate the Integrity Treaty that guaranteed Norway's independent status.
In the final decade of his life, Nansen devoted himself primarily to the League of Nations, following his appointment in 1921 as the League's High Commissioner for Refugees. In 1922 he was awarded the Nobel Peace Prize for his work on behalf of the displaced victims of the First World War and related conflicts. Among the initiatives he introduced was the "Nansen passport" for stateless persons, a certificate that used to be recognised by more than 50 countries. He worked on behalf of refugees until his sudden death in 1930, after which the League established the Nansen International Office for Refugees to ensure that his work continued. This office received the Nobel Peace Prize in 1938. His name is commemorated in numerous geographical features, particularly in the polar regions.
The Nansen family originated in Denmark. Hans Nansen (1598–1667), a trader, was an early explorer of the White Sea region of the Arctic Ocean. In later life he settled in Copenhagen, becoming the city's "borgmester" in 1654. Later generations of the family lived in Copenhagen until the mid-18th century, when Ancher Antoni Nansen moved to Norway (then in a union with Denmark). His son, Hans Leierdahl Nansen (1764–1821), was a magistrate first in the Trondheim district, later in Jæren. After Norway's separation from Denmark in 1814, he entered national political life as the representative for Stavanger in the first Storting, and became a strong advocate of union with Sweden. After suffering a paralytic stroke in 1821 Hans Leierdahl Nansen died, leaving a four-year-old son, Baldur Fridtjof Nansen, the explorer's father.
Baldur was a lawyer without ambitions for public life, who became Reporter to the Supreme Court of Norway. He married twice, the second time to Adelaide Johanne Thekla Isidore Bølling Wedel-Jarlsberg from Bærum, a niece of Herman Wedel-Jarlsberg who had helped frame the Norwegian constitution of 1814 and was later the Swedish king's Norwegian Viceroy. Baldur and Adelaide settled at Store Frøen, an estate at Aker, a few kilometres north of Norway's capital city, Christiania (since renamed Oslo). The couple had three children; the first died in infancy, the second, born 10 October 1861, was Fridtjof Wedel-Jarlsberg Nansen.
Store Frøen's rural surroundings shaped the nature of Nansen's childhood. In the short summers the main activities were swimming and fishing, while in the autumn the chief pastime was hunting for game in the forests. The long winter months were devoted mainly to skiing, which Nansen began to practice at the age of two, on improvised skis. At the age of 10 he defied his parents and attempted the ski jump at the nearby Huseby installation. This exploit had near-disastrous consequences, as on landing the skis dug deep into the snow, pitching the boy forward: "I, head first, described a fine arc in the air ... [W]hen I came down again I bored into the snow up to my waist. The boys thought I had broken my neck, but as soon as they saw there was life in me ... a shout of mocking laughter went up." Nansen's enthusiasm for skiing was undiminished, though as he records, his efforts were overshadowed by those of the skiers from the mountainous region of Telemark, where a new style of skiing was being developed. "I saw this was the only way", wrote Nansen later.
At school, Nansen worked adequately without showing any particular aptitude. Studies took second place to sports, or to expeditions into the forests where he would live "like Robinson Crusoe" for weeks at a time. Through such experiences Nansen developed a marked degree of self-reliance. He became an accomplished skier and a highly proficient skater. Life was disrupted when, in the summer of 1877, Adelaide Nansen died suddenly. Distressed, Baldur Nansen sold the Store Frøen property and moved with his two sons to Christiania. Nansen's sporting prowess continued to develop; at 18 he broke the world one-mile (1.6 km) skating record, and in the following year won the national cross-country skiing championship, a feat he would repeat on 11 subsequent occasions.
In 1880 Nansen passed his university entrance examination, the "examen artium". He decided to study zoology, claiming later that he chose the subject because he thought it offered the chance of a life in the open air. He began his studies at the Royal Frederick University in Christiania early in 1881.
Early in 1882 Nansen took "...the first fatal step that led me astray from the quiet life of science." Professor Robert Collett of the university's zoology department proposed that Nansen take a sea voyage, to study Arctic zoology at first hand. Nansen was enthusiastic, and made arrangements through a recent acquaintance, Captain Axel Krefting, commander of the sealer "Viking". The voyage began on 11 March 1882 and extended over the following five months. In the weeks before sealing started, Nansen was able to concentrate on scientific studies. From water samples he showed that, contrary to previous assumption, sea ice forms on the surface of the water rather than below. His readings also demonstrated that the Gulf Stream flows beneath a cold layer of surface water. Through the spring and early summer "Viking" roamed between Greenland and Spitsbergen in search of seal herds. Nansen became an expert marksman, and on one day proudly recorded that his team had shot 200 seal. In July, "Viking" became trapped in the ice close to an unexplored section of the Greenland coast; Nansen longed to go ashore, but this was impossible. However, he began to develop the idea that the Greenland icecap might be explored, or even crossed. On 17 July the ship broke free from the ice, and early in August was back in Norwegian waters.
Nansen did not resume formal studies at the university. Instead, on Collett's recommendation, he accepted a post as curator in the zoological department of the Bergen Museum. He was to spend the next six years of his life there—apart from a six-month sabbatical tour of Europe—working and studying with leading figures such as Gerhard Armauer Hansen, the discoverer of the leprosy bacillus, and Daniel Cornelius Danielssen, the museum's director who had turned it from a backwater collection into a centre of scientific research and education. Nansen's chosen area of study was the then relatively unexplored field of neuroanatomy, specifically the central nervous system of lower marine creatures. Before leaving for his sabbatical in February 1886 he published a paper summarising his research to date, in which he stated that "anastomoses or unions between the different ganglion cells" could not be demonstrated with certainty. This unorthodox view was confirmed by the simultaneous researches of the embryologist Wilhelm His and the psychiatrist August Forel. Nansen is considered the first Norwegian defender of the neuron theory, originally proposed by Santiago Ramón y Cajal. His subsequent paper, "The Structure and Combination of Histological Elements of the Central Nervous System", published in 1887, became his doctoral thesis.
The idea of an expedition across the Greenland icecap grew in Nansen's mind throughout his Bergen years. In 1887, after the submission of his doctoral thesis, he finally began organising this project. Before then, the two most significant penetrations of the Greenland interior had been those of Adolf Erik Nordenskiöld in 1883, and Robert Peary in 1886. Both had set out from Disko Bay on the western coast, and had travelled about eastward before turning back. By contrast, Nansen proposed to travel from east to west, ending rather than beginning his trek at Disko Bay. A party setting out from the inhabited west coast would, he reasoned, have to make a return trip, as no ship could be certain of reaching the dangerous east coast and picking them up. By starting from the east—assuming that a landing could be made there—Nansen's would be a one-way journey towards a populated area. The party would have no line of retreat to a safe base; the only way to go would be forward, a situation that fitted Nansen's philosophy completely.
Nansen rejected the complex organisation and heavy manpower of other Arctic ventures, and instead planned his expedition for a small party of six. Supplies would be manhauled on specially designed lightweight sledges. Much of the equipment, including sleeping bags, clothing and cooking stoves, also needed to be designed from scratch. These plans received a generally poor reception in the press; one critic had no doubt that "if [the] scheme be attempted in its present form ... the chances are ten to one that he will ... uselessly throw his own and perhaps others' lives away". The Norwegian parliament refused to provide financial support, believing that such a potentially risky undertaking should not be encouraged. The project was eventually launched with a donation from a Danish businessman, Augustin Gamél; the rest came mainly from small contributions from Nansen's countrymen, through a fundraising effort organised by students at the university.
Despite the adverse publicity, Nansen received numerous applications from would-be adventurers. He wanted expert skiers, and attempted to recruit from the skiers of Telemark, but his approaches were rebuffed. Nordenskiöld had advised Nansen that Sami people, from Finland in the far north of Norway, were expert snow travellers, so Nansen recruited a pair, Samuel Balto and Ole Nielsen Ravna. The remaining places went to Otto Sverdrup, a former sea-captain who had more recently worked as a forester; Oluf Christian Dietrichson, an army officer, and Kristian Kristiansen, an acquaintance of Sverdrup's. All had experience of outdoor life in extreme conditions, and were experienced skiers. Just before the party's departure, Nansen attended a formal examination at the university, which had agreed to receive his doctoral thesis. In accordance with custom he was required to defend his work before appointed examiners acting as "devil's advocates". He left before knowing the outcome of this process.
The sealer "Jason" picked up Nansen's party on 3 June 1888 from the Icelandic port of Ísafjörður. They sighted the Greenland coast a week later, but thick pack ice hindered progress. With the coast still away, Nansen decided to launch the small boats. They were within sight of Sermilik Fjord on 17 July; Nansen believed it would offer a route up the icecap.
The expedition left "Jason" "in good spirits and with the highest hopes of a fortunate result." Days of extreme frustration followed as they drifted south. Weather and sea conditions prevented them from reaching the shore. They spent most time camping on the ice itself—it was too dangerous to launch the boats.
By 29 July, they found themselves south of the point where they left the ship. That day they finally reached land but were too far south to begin the crossing. Nansen ordered the team back into the boats after a brief rest and to begin rowing north. The party battled northward along the coast through the ice floes for the next 12 days. They encountered a large Eskimo encampment on the first day, near Cape Steen Bille. Occasional contacts with the nomadic native population continued as the journey progressed.
The party reached Umivik Bay on 11 August, after covering . Nansen decided they needed to begin the crossing. Although they were still far south of his intended starting place; the season was becoming too advanced. After they landed at Umivik, they spent the next four days preparing for their journey. They set out on the evening of 15 August, heading north-west towards Christianhaab on the western shore of Disko Bay— away.
Over the next few days, the party struggled to ascend. The inland ice had a treacherous surface with many hidden crevasses and the weather was bad. Progress stopped for three days because of violent storms and continuous rain one time. The last ship was due to leave Christianhaab by mid-September. They would not be able to reach it in time, Nansen concluded on 26 August. He ordered a change of course due west, towards Godthaab; a shorter journey by at least . The rest of the party, according to Nansen, "hailed the change of plan with acclamation." 
They continued climbing until 11 September and reached a height of above sea level. Temperatures on the icecap summit of the icecap dropped to at night. From then on the downward slope made travelling easier. Yet, the terrain was rugged and the weather remained hostile. Progress was slow: fresh snowfalls made dragging the sledges like pulling them through sand.
On 26 September, they battled their way down the edge of a fjord westward towards Godthaab. Sverdrup constructed a makeshift boat out of parts of the sledges, willows, and their tent. Three days later, Nansen and Sverdrup began the last stage of the journey; rowing down the fjord.
On 3 October, they reached Godthaab, where the Danish town representative greeted them. He first informed Nansen that he secured his doctorate, a matter that "could not have been more remote from [Nansen's] thoughts at that moment." The team accomplished their crossing in 49 days. Throughout the journey, they maintained meteorological and geographical and other records relating to the previously unexplored interior.
The rest of the team arrived in Godthaab on 12 October. Nansen soon learned no ship was likely to call at Godthaab until the following spring. Still, they were able to send letters back to Norway via a boat leaving Ivigtut at the end of October. He and his party spent the next seven months in Greenland. On 15 April 1889, the Danish ship "Hvidbjørnen" finally entered the harbour. Nansen recorded: "It was not without sorrow that we left this place and these people, among whom we had enjoyed ourselves so well."
"Hvidbjørnen" reached Copenhagen on 21 May 1889. News of the crossing had preceded its arrival, and Nansen and his companions were feted as heroes. This welcome, however, was dwarfed by the reception in Christiania a week later, when crowds of between thirty and forty thousand—a third of the city's population—thronged the streets as the party made its way to the first of a series of receptions. The interest and enthusiasm generated by the expedition's achievement led directly to the formation that year of the Norwegian Geographical Society.
Nansen accepted the position of curator of the Royal Frederick University's zoology collection, a post which carried a salary but involved no duties; the university was satisfied by the association with the explorer's name. Nansen's main task in the following weeks was writing his account of the expedition, but he found time late in June to visit London, where he met the Prince of Wales (the future Edward VII), and addressed a meeting of the Royal Geographical Society (RGS).
The RGS president, Sir Mountstuart Elphinstone Grant Duff, said that Nansen has claimed "the foremost place amongst northern travellers", and later awarded him the Society's prestigious Founder's Medal. This was one of many honours Nansen received from institutions all over Europe. He was invited by a group of Australians to lead an expedition to Antarctica, but declined, believing that Norway's interests would be better served by a North Pole conquest.
On 11 August 1889 Nansen announced his engagement to Eva Sars, the daughter of Michael Sars, a zoology professor who had died when Eva was 11 years old. The couple had met some years previously, at the skiing resort of Frognerseteren, where Nansen recalled seeing "two feet sticking out of the snow". Eva was three years older than Nansen, and despite the evidence of this first meeting, was an accomplished skier. She was also a celebrated classical singer who had been coached in Berlin by Désirée Artôt, one-time paramour of Tchaikovsky. The engagement surprised many; since Nansen had previously expressed himself forcefully against the institution of marriage, Otto Sverdrup assumed he had read the message wrongly. The wedding took place on 6 September 1889, less than a month after the engagement.
Nansen first began to consider the possibility of reaching the North Pole after reading meteorologist Henrik Mohn's theory on polar drift in 1884. Artefacts found on the coast of Greenland were identified to have come from the "Jeannette" expedition. In June 1881, was crushed and sunk off the Siberian coast—the opposite side of the Arctic Ocean. Mohn surmised the location of the artefacts indicated the existence of an ocean current from east to west, all the way across the polar sea and possibly over the pole itself.
The idea remained fixated in Nansen's mind for the next couple of years. He developed a detailed plan for a polar venture after his triumphant return from Greenland. He made his idea public in February 1890, at a meeting of the newly-formed Norwegian Geographical Society. Previous expeditions, he argued, approached the North Pole from the west and failed because they were working against the prevailing east-west current; the secret was to work with the current. 
A workable plan would require a sturdy and manoeuvrable small ship, capable of carrying fuel and provisions for twelve men for five years. This ship would enter the ice pack close to the approximate location of "Jeannette's" sinking, drifting west with the current towards the pole and beyond it—eventually reaching the sea between Greenland and Spitsbergen.
Experienced polar explorers were dismissive: Adolphus Greely called the idea "an illogical scheme of self-destruction". Equally dismissive were Sir Allen Young, a veteran of the searches for Franklin's lost expedition, and Sir Joseph Dalton Hooker, who had sailed to the Antarctic on the Ross expedition. Nansen still managed to secure a grant from the Norwegian parliament after an impassioned speech. Additional funding was secured through a national appeal for private donations.
Nansen chose naval engineer Colin Archer to design and build a ship. Archer designed an extraordinarily sturdy vessel with an intricate system of crossbeams and braces of the toughest oak timbers. Its rounded hull was designed to push the ship upwards when beset by pack ice. Speed and manoeuvrability were to be secondary to its ability as a safe and warm shelter during their predicted confinement.
The length-to-beam ratio— and —gave it a stubby appearance, justified by Archer: "A ship that is built with exclusive regard to its suitability for [Nansen's] object must differ essentially from any known vessel." It was christened "Fram" and launched on 6 October 1892.
Nansen selected a party of twelve from thousands of applicants. Otto Sverdrup, who took part in Nansen's earlier Greenland expedition was appointed as the expedition's second-in-command. Competition was so fierce that army lieutenant and dog-driving expert Hjalmar Johansen signed on as ship's stoker, the only position still available.
"Fram" left Christiania on 24 June 1893, cheered on by thousands of well-wishers. After a slow journey around the coast, the final port of call was Vardø, in the far north-east of Norway. "Fram" left Vardø on 21 July, following the North-East Passage route pioneered by Nordenskiöld in 1878–1879, along the northern coast of Siberia. Progress was impeded by fog and ice conditions in the mainly uncharted seas.
The crew also experienced the dead water phenomenon, where a ship's forward progress is impeded by friction caused by a layer of fresh water lying on top of heavier salt water. Nevertheless, Cape Chelyuskin, the most northerly point of the Eurasian continental mass, was passed on 10 September.
Heavy pack ice was sighted ten days later at around latitude 78°N, as "Fram" approached the area in which was crushed. Nansen followed the line of the pack northwards to a position recorded as , before ordering engines stopped and the rudder raised. From this point "Fram's" drift began. The first weeks in the ice were frustrating, as the drift moved unpredictably; sometimes north, sometimes south.
By 19 November, "Fram's" latitude was south of that at which she had entered the ice. Only after the turn of the year, in January 1894, did the northerly direction become generally settled; the 80°N mark was finally passed on 22 March. Nansen calculated that, at this rate, it might take the ship five years to reach the pole. As the ship's northerly progress continued at a rate rarely above a kilometre and a half per day, Nansen began privately to consider a new plan—a dog sledge journey towards the pole. With this in mind, he began to practice dog-driving, making many experimental journeys over the ice. 
In November, Nansen announced his plan: when the ship passed latitude 83°N, he and Hjalmar Johansen would leave the ship with the dogs and make for the pole while "Fram", under Sverdrup, continued its drift until it emerged from the ice in the North Atlantic. After reaching the pole, Nansen and Johansen would make for the nearest known land, the recently discovered and sketchily mapped Franz Josef Land. They would then cross to Spitzbergen where they would find a ship to take them home.
The crew spent the rest of the winter of 1894 preparing clothing and equipment for the forthcoming sledge journey. Kayaks were built, to be carried on the sledges until needed for the crossing of open water. Preparations were interrupted early in January when violent tremors shook the ship. The crew disembarked, fearing the vessel would be crushed, but "Fram" proved herself equal to the danger. On 8 January 1895, the ship's position was 83°34′N, above Greely's previous record of 83°24′N.
With the ship's latitude at 84°4′N and after two false starts, Nansen and Johansen began their journey on 14 March 1895. Nansen allowed 50 days to cover the to the pole, an average daily journey of . After a week of travel, a sextant observation indicated they averaged per day, which put them ahead of schedule. However, uneven surfaces made skiing more difficult, and their speeds slowed. They also realised they were marching against a southerly drift, and that distances travelled did not necessarily equate to distance progressed. 
On 3 April, Nansen began to doubt whether the pole was attainable. Unless their speed improved, their food would not last them to the pole and back to Franz Josef Land. He confided in his diary: "I have become more and more convinced we ought to turn before time." Four days later, after making camp, he observed the way ahead was "... a veritable chaos of iceblocks stretching as far as the horizon." Nansen recorded their latitude as 86°13′6″N—almost three degrees beyond the previous record—and decided to turn around and head back south.
At first Nansen and Johansen made good progress south, but suffered a serious setback on 13 April, when in his eagerness to break camp, they had forgotten to wind their chronometers, which made it impossible to calculate their longitude and accurately navigate to Franz Josef Land. They restarted the watches based on Nansen's guess they were at 86°E. From then on were uncertain of their true position. The tracks of an Arctic fox were observed towards the end of April. It was the first trace of a living creature other than their dogs since they left "Fram". They soon saw bear tracks and by the end of May saw evidence of nearby seals, gulls and whales.
On 31 May, Nansen calculated they were only from Cape Fligely, Franz Josef Land's northernmost point. Travel conditions worsened as increasingly warmer weather caused the ice to break up. On 22 June, the pair decided to rest on a stable ice floe while they repaired their equipment and gathered strength for the next stage of their journey. They remained on the floe for a month.
The day after leaving this camp, Nansen recorded: "At last the marvel has come to pass—land, land, and after we had almost given up our belief in it!" Whether this still-distant land was Franz Josef Land or a new discovery they did not know—they had only a rough sketch map to guide them. The edge of the pack ice was reached on 6 August and they shot the last of their dogs—the weakest of which they killed regularly to feed the others since 24 April. The two kayaks were lashed together, a sail was raised, and they made for the land.
It soon became clear this land was part of an archipelago. As they moved southwards, Nansen tentatively identified a headland as Cape Felder on the western edge of Franz Josef Land. Towards the end of August, as the weather grew colder and travel became increasingly difficult, Nansen decided to camp for the winter. In a sheltered cove, with stones and moss for building materials, the pair erected a hut which was to be their home for the next eight months. With ready supplies of bear, walrus and seal to keep their larder stocked, their principal enemy was not hunger but inactivity. After muted Christmas and New Year celebrations, in slowly improving weather, they began to prepare to leave their refuge, but it was 19 May 1896 before they were able to resume their journey.
On 17 June, during a stop for repairs after the kayaks had been attacked by a walrus, Nansen thought he heard a dog barking as well as human voices. He went to investigate, and a few minutes later saw the figure of a man approaching. It was the British explorer Frederick Jackson, who was leading an expedition to Franz Josef Land and was camped at Cape Flora on nearby Northbrook Island. The two were equally astonished by their encounter; after some awkward hesitation Jackson asked: "You are Nansen, aren't you?", and received the reply "Yes, I am Nansen."
Johansen was picked up and the pair were taken to Cape Flora where, during the following weeks, they recuperated from their ordeal. Nansen later wrote that he could "still scarcely grasp" their sudden change of fortune; had it not been for the walrus attack that caused the delay, the two parties might have been unaware of each other's existence.
On 7 August, Nansen and Johansen boarded Jackson's supply ship "Windward", and sailed for Vardø where they arrived on the 13th. They were greeted by Hans Mohn, the originator of the polar drift theory, who was in the town by chance. The world was quickly informed by telegram of Nansen's safe return, but as yet there was no news of "Fram".
Taking the weekly mail steamer south, Nansen and Johansen reached Hammerfest on 18 August, where they learned that "Fram" had been sighted. She had emerged from the ice north and west of Spitsbergen, as Nansen had predicted, and was now on her way to Tromsø. She had not passed over the pole, nor exceeded Nansen's northern mark. Without delay Nansen and Johansen sailed for Tromsø, where they were reunited with their comrades.
The homeward voyage to Christiania was a series of triumphant receptions at every port. On 9 September, "Fram" was escorted into Christiania's harbour and welcomed by the largest crowds the city had ever seen. The crew were received by King Oscar, and Nansen, reunited with family, remained at the palace for several days as special guests. Tributes arrived from all over the world; typical was that from the British mountaineer Edward Whymper, who wrote that Nansen had made "almost as great an advance as has been accomplished by all other voyages in the nineteenth century put together".
Nansen's first task on his return was to write his account of the voyage. This he did remarkably quickly, producing 300,000 words of Norwegian text by November 1896; the English translation, titled "Farthest North", was ready in January 1897. The book was an instant success, and secured Nansen's long-term financial future. Nansen included without comment the one significant adverse criticism of his conduct, that of Greely, who had written in "Harper's Weekly" on Nansen's decision to leave "Fram" and strike for the pole: "It passes comprehension how Nansen could have thus deviated from the most sacred duty devolving on the commander of a naval expedition."
During the 20 years following his return from the Arctic, Nansen devoted most of his energies to scientific work. In 1897 he accepted a professorship in zoology at the Royal Frederick University, which gave him a base from which he could tackle the major task of editing the reports of the scientific results of the "Fram" expedition. This was a much more arduous task than writing the expedition narrative. The results were eventually published in six volumes, and according to a later polar scientist, Robert Rudmose-Brown, "were to Arctic oceanography what the "Challenger" expedition results had been to the oceanography of other oceans."
In 1900, Nansen became director of the Christiania-based International Laboratory for North Sea Research, and helped found the International Council for the Exploration of the Sea. Through his connection with the latter body, in the summer of 1900 Nansen embarked on his first visit to Arctic waters since the "Fram" expedition, a cruise to Iceland and Jan Mayen Land on the oceanographic research vessel "Michael Sars", named after Eva's father. Shortly after his return he learned that his Farthest North record had been passed, by members of the Duke of the Abruzzi's Italian expedition. They had reached 86°34′N on 24 April 1900, in an attempt to reach the North Pole from Franz Josef Land. Nansen received the news philosophically: "What is the value of having goals for their own sake? They all vanish ... it is merely a question of time."
Nansen was now considered an oracle by all would-be explorers of the north and south polar regions. Abruzzi had consulted him, as had the Belgian Adrien de Gerlache, each of whom took expeditions to the Antarctic. Although Nansen refused to meet his own countryman and fellow-explorer Carsten Borchgrevink (whom he considered a fraud), he gave advice to Robert Falcon Scott on polar equipment and transport, prior to the 1901–04 "Discovery" expedition. At one point Nansen seriously considered leading a South Pole expedition himself, and asked Colin Archer to design two ships. However, these plans remained on the drawing board.
By 1901 Nansen's family had expanded considerably. A daughter, Liv, had been born just before "Fram" set out; a son, Kåre was born in 1897 followed by a daughter, Irmelin, in 1900 and a second son Odd in 1901. The family home, which Nansen had built in 1891 from the profits of his Greenland expedition book, was now too small. Nansen acquired a plot of land in the Lysaker district and built, substantially to his own design, a large and imposing house which combined some of the characteristics of an English manor house with features from the Italian renaissance.
The house was ready for occupation by April 1902; Nansen called it "Polhøgda" (in English "polar heights"), and it remained his home for the rest of his life. A fifth and final child, son Asmund, was born at Polhøgda in 1903.
The union between Norway and Sweden, imposed by the Great Powers in 1814, had been under considerable strain through the 1890s, the chief issue in question being Norway's rights to its own consular service. Nansen, although not by inclination a politician, had spoken out on the issue on several occasions in defence of Norway's interests. It seemed, early in the 20th century that agreement between the two countries might be possible, but hopes were dashed when negotiations broke down in February 1905. The Norwegian government fell, and was replaced by one led by Christian Michelsen, whose programme was one of separation from Sweden.
In February and March Nansen published a series of newspaper articles which placed him firmly in the separatist camp. The new prime minister wanted Nansen in the cabinet, but Nansen had no political ambitions. However, at Michelsen's request he went to Berlin and then to London where, in a letter to "The Times", he presented Norway's legal case for a separate consular service to the English-speaking world. On 17 May 1905, Norway's Constitution Day, Nansen addressed a large crowd in Christiania, saying: "Now have all ways of retreat been closed. Now remains only one path, the way forward, perhaps through difficulties and hardships, but forward for our country, to a free Norway". He also wrote a book, "Norway and the Union with Sweden", to promote Norway's case abroad.
On 23 May the Storting passed the Consulate Act establishing a separate consular service. King Oscar refused his assent; on 27 May the Norwegian cabinet resigned, but the king would not recognise this step. On 7 June the Storting unilaterally announced that the union with Sweden was dissolved. In a tense situation the Swedish government agreed to Norway's request that the dissolution should be put to a referendum of the Norwegian people. This was held on 13 August 1905 and resulted in an overwhelming vote for independence, at which point King Oscar relinquished the crown of Norway while retaining the Swedish throne. A second referendum, held in November, determined that the new independent state should be a monarchy rather than a republic. In anticipation of this, Michelsen's government had been considering the suitability of various princes as candidates for the Norwegian throne. Faced with King Oscar's refusal to allow anyone from his own House of Bernadotte to accept the crown, the favoured choice was Prince Charles of Denmark. In July 1905 Michelsen sent Nansen to Copenhagen on a secret mission to persuade Charles to accept the Norwegian throne. Nansen was successful; shortly after the second referendum Charles was proclaimed king, taking the name Haakon VII. He and his wife, the British princess Maud, were crowned in the Nidaros Cathedral in Trondheim on 22 June 1906.
In April 1906 Nansen was appointed Norway's first Minister in London. His main task was to work with representatives of the major European powers on an Integrity Treaty which would guarantee Norway's position. Nansen was popular in England, and got on well with King Edward, though he found court functions and diplomatic duties disagreeable; "frivolous and boring" was his description. However, he was able to pursue his geographical and scientific interests through contacts with the Royal Geographical Society and other learned bodies. The Treaty was signed on 2 November 1907, and Nansen considered his task complete. Resisting the pleas of, among others, King Edward that he should remain in London, on 15 November Nansen resigned his post. A few weeks later, still in England as the king's guest at Sandringham, Nansen received word that Eva was seriously ill with pneumonia. On 8 December he set out for home, but before he reached Polhøgda he learned, from a telegram, that Eva had died.
After a period of mourning, Nansen returned to London. He had been persuaded by his government to rescind his resignation until after King Edward's state visit to Norway in April 1908. His formal retirement from the diplomatic service was dated 1 May 1908, the same day on which his university professorship was changed from zoology to oceanography. This new designation reflected the general character of Nansen's more recent scientific interests.
In 1905, he had supplied the Swedish physicist Walfrid Ekman with the data which established the principle in oceanography known as the Ekman spiral. Based on Nansen's observations of ocean currents recorded during the "Fram" expedition, Ekman concluded that the effect of wind on the sea's surface produced currents which "formed something like a spiral staircase, down towards the depths".
In 1909 Nansen combined with Bjørn Helland-Hansen to publish an academic paper, "The Norwegian Sea: its Physical Oceanography", based on the "Michael Sars" voyage of 1900. Nansen had by now retired from polar exploration, the decisive step being his release of "Fram" to fellow Norwegian Roald Amundsen, who was planning a North Pole expedition. When Amundsen made his controversial change of plan and set out for the South Pole, Nansen stood by him.
Between 1910 and 1914, Nansen participated in several oceanographic voyages. In 1910, aboard the Norwegian naval vessel "Fridtjof", he carried out researches in the northern Atlantic, and in 1912 he took his own yacht, "Veslemøy", to Bear Island and Spitsbergen. The main objective of the "Veslemøy" cruise was the investigation of salinity in the North Polar Basin. One of Nansen's lasting contributions to oceanography was his work designing instruments and equipment; the "Nansen bottle" for taking deep water samples remained in use into the 21st century, in a version updated by Shale Niskin.
At the request of the Royal Geographical Society, Nansen began work on a study of Arctic discoveries, which developed into a two-volume history of the exploration of the northern regions up to the beginning of the 16th century. This was published in 1911 as "Nord i Tåkeheimen" ("In Northern Mists"). That year he renewed an acquaintance with Kathleen Scott, wife of Robert Falcon Scott whose Terra Nova Expedition had sailed for Antarctica in 1910.
Biographer Roland Huntford has asserted, without any compelling evidence, that Nansen and Kathleen Scott had a brief love affair. Louisa Young, in her biography of Lady Scott, refutes the claim. Many women were attracted to Nansen, and he had a reputation as a womaniser. His personal life was troubled around this time; in January 1913 he received news of the suicide of Hjalmar Johansen, who had returned in disgrace from Amundsen's successful South Pole expedition. In March 1913, Nansen's youngest son Asmund died after a long illness.
In the summer of 1913, Nansen travelled to the Kara Sea, by the invitation of Jonas Lied, as part of a delegation investigating a possible trade route between Western Europe and the Siberian interior. The party then took a steamer up the Yenisei River to Krasnoyarsk, and travelled on the Trans-Siberian Railway to Vladivostok before turning for home. Nansen published a report from the trip in "Through Siberia". The life and culture of the Russian peoples aroused in Nansen an interest and sympathy he would carry through to his later life. Immediately before the First World War, Nansen joined Helland-Hansen in an oceanographical cruise in eastern Atlantic waters.
On the outbreak of war in 1914, Norway declared its neutrality, alongside Sweden and Denmark. Nansen was appointed as the president of the Norwegian Union of Defence, but had few official duties, and continued with his professional work as far as circumstances permitted. As the war progressed, the loss of Norway's overseas trade led to acute shortages of food in the country, which became critical in April 1917, when the United States entered the war and placed extra restrictions on international trade. Nansen was dispatched to Washington by the Norwegian government; after months of discussion, he secured food and other supplies in return for the introduction of a rationing system. When his government hesitated over the deal, he signed the agreement on his own initiative.
Within a few months of the war's end in November 1918, a draft agreement had been accepted by the Paris Peace Conference to create a League of Nations, as a means of resolving disputes between nations by peaceful means. The foundation of the League at this time was providential as far as Nansen was concerned, giving him a new outlet for his restless energy. He became president of the Norwegian League of Nations Society, and although the Scandinavian nations with their traditions of neutrality initially held themselves aloof, his advocacy helped to ensure that Norway became a full member of the League in 1920, and he became one of its three delegates to the League's General Assembly.
In April 1920, at the League's request, Nansen began organising the repatriation of around half a million prisoners of war, stranded in various parts of the world. Of these, 300,000 were in Russia which, gripped by revolution and civil war, had little interest in their fate. Nansen was able to report to the Assembly in November 1920 that around 200,000 men had been returned to their homes. "Never in my life", he said, "have I been brought into touch with so formidable an amount of suffering."
Nansen continued this work for a further two years until, in his final report to the Assembly in 1922, he was able to state that 427,886 prisoners had been repatriated to around 30 different countries. In paying tribute to his work, the responsible committee recorded that the story of his efforts "would contain tales of heroic endeavour worthy of those in the accounts of the crossing of Greenland and the great Arctic voyage."
Even before this work was complete, Nansen was involved in a further humanitarian effort. On 1 September 1921, prompted by the British delegate Philip Noel-Baker, he accepted the post of the League's High Commissioner for Refugees. His main brief was the resettlement of around two million Russian refugees displaced by the upheavals of the Russian Revolution.
At the same time he tried to tackle the urgent problem of famine in Russia; following a widespread failure of crops around 30 million people were threatened with starvation and death. Despite Nansen's pleas on behalf of the starving, Russia's revolutionary government was feared and distrusted internationally, and the League was reluctant to come to its peoples' aid. Nansen had to rely largely on fundraising from private organisations, and his efforts met with limited success. Later he was to express himself bitterly on the matter:
A major problem impeding Nansen's work on behalf of refugees was that most of them lacked documentary proof of identity or nationality. Without legal status in their country of refuge, their lack of papers meant they were unable to go anywhere else. To overcome this, Nansen devised a document that became known as the "Nansen passport", a form of identity for stateless persons that was in time recognised by more than 50 governments, and which allowed refugees to cross borders legally. Although the passport was created initially for refugees from Russia, it was extended to cover other groups.
While attending the Conference of Lausanne in November 1922, Nansen learned that he had been awarded the Nobel Peace Prize for 1922. The citation referred to "his work for the repatriation of the prisoners of war, his work for the Russian refugees, his work to bring succour to the millions of Russians afflicted by famine, and finally his present work for the refugees in Asia Minor and Thrace". Nansen donated the prize money to international relief efforts.
After the Greco-Turkish War of 1919–1922, Nansen travelled to Constantinople to negotiate the resettlement of hundreds of thousands of refugees, mainly ethnic Greeks who had fled from Turkey after the defeat of the Greek Army. The impoverished Greek state was unable to take them in, and so Nansen devised a scheme for a population exchange whereby half a million Turks in Greece were returned to Turkey, with full financial compensation, while further loans facilitated the absorption of the refugee Greeks into their homeland. Despite some controversy over the principle of a population exchange, the plan was implemented successfully over a period of several years.
From 1925 onwards, Nansen devoted much time trying to help Armenian refugees, victims of Armenian genocide at the hands of the Ottoman Empire during the First World War and further ill-treatment thereafter. His goal was the establishment of a national home for these refugees, within the borders of Soviet Armenia. His main assistant in this endeavour was Vidkun Quisling, the future Nazi collaborator and head of a Norwegian puppet government during the Second World War.
After visiting the region, Nansen presented the Assembly with a modest plan for the irrigation of on which 15,000 refugees could be settled. The plan ultimately failed, because even with Nansen's unremitting advocacy the money to finance the scheme was not forthcoming. Despite this failure, his reputation among the Armenian people remains high.
Nansen wrote "Armenia and the Near East" (1923) wherein he describes the plight of the Armenians in the wake of losing its independence to the Soviet Union. The book was translated into many languages. After his visit to Armenia, Nansen wrote two additional books: "Across Armenia" (1927) and "Through the Caucasus to the Volga" (1930).
Within the League's Assembly, Nansen spoke out on many issues besides those related to refugees. He believed that the Assembly gave the smaller countries such as Norway a "unique opportunity for speaking in the councils of the world." He believed that the extent of the League's success in reducing armaments would be the greatest test of its credibility. He was a signatory to the Slavery Convention of 25 September 1926, which sought to outlaw the use of forced labour. He supported a settlement of the post-war reparations issue and championed Germany's membership of the League, which was granted in September 1926 after intensive preparatory work by Nansen.
On 17 January 1919 Nansen married Sigrun Munthe, a long-time friend with whom he had had a love affair in 1905, while Eva was still alive. The marriage was resented by the Nansen children, and proved unhappy; an acquaintance writing of them in the 1920s said Nansen appeared unbearably miserable and Sigrun steeped in hate.
Nansen's League of Nations commitments through the 1920s meant that he was mostly absent from Norway, and was able to devote little time to scientific work. Nevertheless, he continued to publish occasional papers. He entertained the hope that he might travel to the North Pole by airship, but could not raise sufficient funding. In any event he was forestalled in this ambition by Amundsen, who flew over the pole in Umberto Nobile's airship "Norge" in May 1926. Two years later Nansen broadcast a memorial oration to Amundsen, who had disappeared in the Arctic while organising a rescue party for Nobile whose airship had crashed during a second polar voyage. Nansen said of Amundsen: "He found an unknown grave under the clear sky of the icy world, with the whirring of the wings of eternity through space."
In 1926 Nansen was elected Rector of the University of St Andrews in Scotland, the first foreigner to hold this largely honorary position. He used the occasion of his inaugural address to review his life and philosophy, and to deliver a call to the youth of the next generation. He ended:
We all have a Land of Beyond to seek in our life—what more can we ask? Our part is to find the trail that leads to it. A long trail, a hard trail, maybe; but the call comes to us, and we have to go. Rooted deep in the nature of every one of us is the spirit of adventure, the call of the wild—vibrating under all our actions, making life deeper and higher and nobler.
Nansen largely avoided involvement in domestic Norwegian politics, but in 1924 he was persuaded by the long-retired former Prime Minister Christian Michelsen to take part in a new anti-communist political grouping, the Fatherland League. There were fears in Norway that should the Marxist-oriented Labour Party gain power it would introduce a revolutionary programme. At the inaugural rally of the League in Oslo (as Christiania had now been renamed), Nansen declared: "To talk of the right of revolution in a society with full civil liberty, universal suffrage, equal treatment for everyone ... [is] idiotic nonsense." 
Following continued turmoil between the centre-right parties, there was even an independent petition in 1926 gaining some momentum that proposed for Nansen to head a centre-right national unity government on a balanced budget program, an idea he did not reject. He was the headline speaker at the single largest Fatherland League rally with 15,000 attendees in Tønsberg in 1928. In 1929 he went on his final tour for the League on the ship "Stella Polaris", holding speeches from Bergen to Hammerfest.
In between his various duties and responsibilities, Nansen had continued to take skiing holidays when he could. In February 1930, aged 68, he took a short break in the mountains with two old friends, who noted that Nansen was slower than usual and appeared to tire easily. On his return to Oslo he was laid up for several months, with influenza and later phlebitis, and was visited on his sickbed by King Haakon VII.
Nansen was a close friend of a clergyman named Wilhelm. Nansen himself was an atheist.
Nansen died of a heart attack on 13 May 1930. He was given a non-religious state funeral before cremation, after which his ashes were laid under a tree at Polhøgda. Nansen's daughter Liv recorded that there were no speeches, just music: Schubert's "Death and the Maiden", which Eva used to sing. 
In his lifetime and thereafter, Nansen received honours and recognition from many countries. Among the many tributes paid to him subsequently was that of Lord Robert Cecil, a fellow League of Nations delegate, who spoke of the range of Nansen's work, done with no regard for his own interests or health: "Every good cause had his support. He was a fearless peacemaker, a friend of justice, an advocate always for the weak and suffering."
Nansen was a pioneer and innovator in many fields. As a young man he embraced the revolution in skiing methods that transformed it from a means of winter travel to a universal sport, and quickly became one of Norway's leading skiers. He was later able to apply this expertise to the problems of polar travel, in both his Greenland and his "Fram" expeditions.
He invented the "Nansen sledge" with broad, ski-like runners, the "Nansen cooker" to improve the heat efficiency of the standard spirit stoves then in use, and the layer principle in polar clothing, whereby the traditionally heavy, awkward garments were replaced by layers of lightweight material. In science, Nansen is recognised both as one of the founders of modern neurology, and as a significant contributor to early oceanographical science, in particular for his work in establishing the Central Oceanographic Laboratory in Christiania.
Through his work on behalf of the League of Nations, Nansen helped to establish the principle of international responsibility for refugees. Immediately after his death the League set up the Nansen International Office for Refugees, a semi-autonomous body under the League's authority, to continue his work. The Nansen Office faced great difficulties, in part arising from the large numbers of refugees from the European dictatorships during the 1930s. Nevertheless, it secured the agreement of 14 countries (including a reluctant Great Britain) to the Refugee Convention of 1933.
It also helped to repatriate 10,000 Armenians to Yerevan in Soviet Armenia, and to find homes for a further 40,000 in Syria and Lebanon. In 1938, the year in which it was superseded by a wider-ranging body, the Nansen Office was awarded the Nobel Peace Prize. In 1954, the League's successor body, the United Nations, established the Nansen Medal, later named the Nansen Refugee Award, given annually by the United Nations High Commissioner for Refugees to an individual, group or organisation "for outstanding work on behalf of the forcibly displaced".
Numerous geographical features bear his name: the Nansen Basin and the Nansen-Gakkel Ridge in the Arctic Ocean; Mount Nansen in the Yukon region of Canada; Mount Nansen, Mount Fridtjof Nansen and Nansen Island, all in Antarctica; as well as Nansen Island in the Kara Sea, Nansen Land in Greenland and Nansen Island in Franz Josef Land; 853 Nansenia, an asteroid; Nansen crater at the Moon's north pole and Nansen crater on Mars. His Polhøgda mansion is now home to the Fridtjof Nansen Institute, an independent foundation which engages in research on environmental, energy and resource management politics.
"Just a life – the story of Fridtjof Nansen" was released, a 1968 Norwegian/Soviet biographical film with Knut Wigert as Nansen.
The Royal Norwegian Navy launched the first of a series of five s in 2004, with as its lead ship. Cruise ship was launched in 2020.

</doc>
<doc id="11824" url="https://en.wikipedia.org/wiki?curid=11824" title="Frederick Augustus II of Saxony">
Frederick Augustus II of Saxony

Frederick Augustus II (; 18 May 1797 in Dresden – 9 August 1854 in Brennbüchel, Karrösten, Tyrol) was King of Saxony and a member of the House of Wettin.
He was the eldest son of Maximilian, Prince of Saxony – younger son of the Elector Frederick Christian of Saxony – by his first wife, Caroline of Bourbon, Princess of Parma.
From his birth, it was clear that one day Frederick Augustus would become the ruler of Saxony. His father was the only son of the Elector Frederick Christian of Saxony who left surviving male issue. When the King Frederick Augustus I died (1827) and Anton succeeded him as King, Frederick Augustus became second in line to the throne, preceded only by his father Maximilian.
He was an officer in the War of the Sixth Coalition. However, he had little interest in military affairs.
The July Revolution of 1830 in France marked the beginning of disturbances in Saxony that autumn. The people claimed a change in the constitution and demanded a young regent of the kingdom to share the government with the King Anton. On 1 September the Prince Maximilian renounced his rights of succession in favor of his son Frederick Augustus, who was proclaimed Prince Co-Regent (de: "Prinz-Mitregenten") of Saxony. On 2 February 1832 Frederick Augustus brought Free Autonomy to the cities. Also, by an edict of 17 March of that year, the farmers were freed from the corvée and hereditary submission.
On 6 June 1836, King Anton died and Frederick Augustus succeeded him. As an intelligent man, he was quickly popular with the people as he had been since the time of his regency. The new king solved political questions only from a pure sense of duty. Mostly he preferred to leave these things on the hands of his ministers.
A standardized jurisdiction for Saxony created the Criminal Code of 1836. During the Revolutionary disturbances of 1848 (March Revolution), he appointed liberal ministers in the government, lifted censorship, and remitted a liberal electoral law. Later his attitude changed. On 28 April Frederick August II dissolved the Parliament. In 1849, Frederick Augustus was forced to flee to the Königstein Fortress. The May Uprising was crushed by Saxon and Prussian troops and Frederick was able to return after only a few days.
In 1844 Frederick Augustus, accompanied by his personal physician Carl Gustav Carus, made an informal ("incognito") visit to England and Scotland. Among places they visited were Lyme Regis where he purchased from the local fossil collector and dealer, Mary Anning, an ichthyosaur skeleton for his own extensive natural history collection. It was not a state visit, but the King was the guest of Queen Victoria and Prince Albert at Windsor Castle, visited many of the sights in London and in the university cities of Oxford and Cambridge, and toured widely in England, Wales and Scotland.
During a journey in Tyrol, he had an accident in Brennbüchel in which he fell in front of a horse that stepped on his head. On 8 August 1854, he died in the Gasthof Neuner. He was buried on 16 August in the Katholische Hofkirche of Dresden. In his memory, the Dowager Queen Maria arranged to establish the Königskapelle (King's Chapel) at the accident place, which was consecrated one year later, some of the last members of the Saxon royal family, including Maria Emanuel, Margrave of Meissen, are buried beside the chapel
In Vienna on 26 September 1819 (by proxy) and again in Dresden on 7 October 1819 (in person), Frederick Augustus married firstly with the Archduchess Maria Caroline of Austria (Maria Karoline Ferdinande Theresia Josephine Demetria), daughter of Emperor Francis I of Austria. They had no children.
In Dresden on 24 April 1833 Frederick Augustus married secondly with the Princess Maria Anna of Bavaria (Maria Anna Leopoldine Elisabeth Wilhelmine), daughter of the King Maximilian I Joseph of Bavaria. Like his first marriage, this was childless.
The musician Theodor Uhlig (1822–1853) was an illegitimate son of Frederick Augustus.
Without legitimate issue, after his death Frederick Augustus was succeeded by his younger brother, Johann.

</doc>
<doc id="11826" url="https://en.wikipedia.org/wiki?curid=11826" title="Free market">
Free market

In economics, a free market is a system in which the prices for goods and services are self-regulated by the open market and by consumers. In a free market, the laws and forces of supply and demand are free from any intervention by a government or other authority, and from all forms of economic privilege, monopolies and artificial scarcities. Proponents of the concept of free market contrast it with a regulated market in which a government intervenes in supply and demand through various methods such as tariffs used to restrict trade and to protect the local economy. In an idealized free-market economy, prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy.
Scholars contrast the concept of a free market with the concept of a coordinated market in fields of study such as political economy, new institutional economics, economic sociology and political science. All of these fields emphasize the importance in currently existing market systems of rule-making institutions external to the simple forces of supply and demand which create space for those forces to operate to control productive output and distribution. Although free markets are commonly associated with capitalism within a market economy in contemporary usage and popular culture, free markets have also been advocated by anarchists, socialists and some proponents of cooperatives and advocates of profit sharing.
Criticism of the theoretical concept may regard systems with significant market power, inequality of bargaining power, or information asymmetry as less than free, with regulation being necessary to control those imbalances in order to allow markets to function more efficiently as well as produce more desirable social outcomes.
The Heritage Foundation, a conservative think tank based in Washington, D.C. that defines capitalism as the free market which is free from state intervention and government regulation, tried to identify the key factors necessary to measure the degree of freedom of economy of a particular country. In 1986, they introduced the Index of Economic Freedom which is based on some fifty variables. While this and other similar indices do not necessarily define a free market, The Heritage Foundation measures the degree to which a modern economy is free. The variables are divided into the following major groups:
According to The Heritage Foundation, these free market principles are what helped the United States transition to a free-market economy. International free trade improved the country and in order for Americans to prosper from a strong economy they had no choice but to embrace it. Each group is assigned a numerical value between 1 and 5 as the index is the arithmetical mean of the values, rounded to the nearest hundredth. Initially, countries which were traditionally considered capitalistic received high ratings, but the method improved over time. Some economists such as Milton Friedman and other "laissez-faire" economists have argued that there is a direct relationship between economic growth and economic freedom and some studies suggest this is true. Ongoing debates exist among scholars regarding methodological issues in empirical studies of the connection between economic freedom and economic growth. These debates and studies continue to explore just what that relationship entails.
The Free Market Monument Foundation defines the principles of a free market as such:
For classical economists such as Adam Smith, the term free market does not necessarily refer to a market free from government interference, but rather free from all forms of economic privilege, monopolies and artificial scarcities. This implies that economic rents, i.e. profits generated from a lack of perfect competition, must be reduced or eliminated as much as possible through free competition.
Economic theory suggests the returns to land and other natural resources are economic rents that cannot be reduced in such a way because of their perfect inelastic supply. Some economic thinkers emphasize the need to share those rents as an essential requirement for a well functioning market. It is suggested this would both eliminate the need for regular taxes that have a negative effect on trade (see deadweight loss) as well as release land and resources that are speculated upon or monopolised. Two features that improve the competition and free market mechanisms. Winston Churchill supported this view by the following statement: "Land is the mother of all monopoly". The American economist and social philosopher Henry George, the most famous proponent of this thesis, wanted to accomplish this through a high land value tax that replaces all other taxes. Followers of his ideas are often called Georgists or geoists and geolibertarians.
Léon Walras, one of the founders of the neoclassical economics who helped formulate the general equilibrium theory, had a very similar view. He argued that free competition could only be realized under conditions of state ownership of natural resources and land. Additionally, income taxes could be eliminated because the state would receive income to finance public services through owning such resources and enterprises.
The "laissez-faire" principle expresses a preference for an absence of non-market pressures on prices and wages such as those from discriminatory government taxes, subsidies, tariffs, regulations of purely private behavior, or government-granted or coercive monopolies. In "The Pure Theory of Capital", Friedrich Hayek argued that the goal is the preservation of the unique information contained in the price itself.
The definition of free market has been disputed and made complex by collectivist political philosophers and socialist economic ideas. This contention arose from the divergence from classical economists such as Richard Cantillon, Adam Smith, David Ricardo and Thomas Robert Malthus and from the continental economics developed primarily by the Spanish scholastic and French classical economists, including Anne-Robert-Jacques Turgot, Baron de Laune, Jean-Baptiste Say and Frédéric Bastiat. During the marginal revolution, subjective value theory was rediscovered.
Although "laissez-faire" has been commonly associated with capitalism, there is a similar economic theory associated with socialism called left-wing or socialist "laissez-faire", also known as free-market anarchism, free-market anti-capitalism and free-market socialism to distinguish it from "laissez-faire" capitalism. Critics of "laissez-faire" as commonly understood argue that a truly "laissez-faire" system would be anti-capitalist and socialist. American individualist anarchists such as Benjamin Tucker saw themselves as economic free-market socialists and political individualists while arguing that their "anarchistic socialism" or "individual anarchism" was "consistent Manchesterism".
Various forms of socialism based on free markets have existed since the 19th century. Early notable socialist proponents of free markets include Pierre-Joseph Proudhon, Benjamin Tucker and the Ricardian socialists. These economists believed that genuinely free markets and voluntary exchange could not exist within the exploitative conditions of capitalism. These proposals ranged from various forms of worker cooperatives operating in a free-market economy such as the mutualist system proposed by Proudhon, to state-owned enterprises operating in unregulated and open markets. These models of socialism are not to be confused with other forms of market socialism (e.g. the Lange model) where publicly owned enterprises are coordinated by various degrees of economic planning, or where capital good prices are determined through marginal cost pricing.
Advocates of free-market socialism such as Jaroslav Vanek argue that genuinely free markets are not possible under conditions of private ownership of productive property. Instead, he contends that the class differences and inequalities in income and power that result from private ownership enable the interests of the dominant class to skew the market to their favor, either in the form of monopoly and market power, or by utilizing their wealth and resources to legislate government policies that benefit their specific business interests. Additionally, Vanek states that workers in a socialist economy based on cooperative and self-managed enterprises have stronger incentives to maximize productivity because they would receive a share of the profits (based on the overall performance of their enterprise) in addition to receiving their fixed wage or salary. The stronger incentives to maximize productivity that he conceives as possible in a socialist economy based on cooperative and self-managed enterprises might be accomplished in a free-market economy if employee-owned companies were the norm as envisioned by various thinkers including Louis O. Kelso and James S. Albus.
Socialists also assert that free-market capitalism leads to an excessively skewed distributions of income and economic instabilities which in turn leads to social instability. Corrective measures in the form of social welfare, re-distributive taxation and regulatory measures and their associated administrative costs which are required create agency costs for society. These costs would not be required in a self-managed socialist economy.
With varying degrees of mathematical rigor over time, the general equilibrium theory has demonstrated that under certain conditions of competition the law of supply and demand predominates in this ideal free and competitive market, influencing prices toward an equilibrium that balances the demands for the products against the supplies. At these equilibrium prices, the market distributes the products to the purchasers according to each purchaser's preference or utility for each product and within the relative limits of each buyer's purchasing power. This result is described as market efficiency, or more specifically a Pareto optimum.
This equilibrating behavior of free markets requires certain assumptions about their agents—collectively known as perfect competition—which therefore cannot be results of the market that they create. Among these assumptions are several which are impossible to fully achieve in a real market, such as complete information, interchangeable goods and services and lack of market power. The question then is what approximations of these conditions guarantee approximations of market efficiency and which failures in competition generate overall market failures. Several Nobel Prizes in Economics have been awarded for analyses of market failures due to asymmetric information.
A free market does not require the existence of competition, however it does require a framework that allows new market entrants. Hence, in the lack of coercive barriers, for example, paid licensing certification for certain services and businesses, competition between businesses flourishes all through the demands of consumers, or buyers. It often suggests the presence of the profit motive, although neither a profit motive or profit itself are necessary for a free market. All modern free markets are understood to include entrepreneurs, both individuals and businesses. Typically, a modern free-market economy would include other features such as a stock exchange and a financial services sector, but they do not define it.
Conditions that must exist for unregulated markets to behave as free markets are summarized at perfect competition. An absence of any of these perfect competition ideal conditions is a market failure. Most schools of economics allow that regulatory intervention may provide a substitute force to counter a market failure. Under this thinking, this form of market regulation may be better than an unregulated market at providing a free market.
Friedrich Hayek popularized the view that market economies promote spontaneous order which results in a better "allocation of societal resources than any design could achieve". According to this view, market economies are characterized by the formation of complex transactional networks which produce and distribute goods and services throughout the economy. These networks are not designed, but they nevertheless emerge as a result of decentralized individual economic decisions. The idea of spontaneous order is an elaboration on the invisible hand proposed by Adam Smith in "The Wealth of Nations". About the individual, Smith wrote:
By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for society that it was no part of it. By pursuing his own interest, he frequently promotes that of the society more effectually than when he really intends to promote it. I have never known much good done by those who affected to trade for the public good.
Smith pointed out that one does not get one's dinner by appealing to the brother-love of the butcher, the farmer or the baker. Rather, one appeals to their self-interest and pays them for their labor, arguing:
It is not from the benevolence of the butcher, the brewer or the baker, that we expect our dinner, but from their regard to their own self-interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages.
Supporters of this view claim that spontaneous order is superior to any order that does not allow individuals to make their own choices of what to produce, what to buy, what to sell and at what prices due to the number and complexity of the factors involved. They further believe that any attempt to implement central planning will result in more disorder, or a less efficient production and distribution of goods and services.
Critics such as political economist Karl Polanyi question whether a spontaneously ordered market can exist, completely free of distortions of political policy, claiming that even the ostensibly freest markets require a state to exercise coercive power in some areas, namely to enforce contracts, govern the formation of labor unions, spell out the rights and obligations of corporations, shape who has standing to bring legal actions and define what constitutes an unacceptable conflict of interest.
Demand for an item (such as goods or services) refers to the economic market pressure from people trying to buy it. Buyers have a maximum price they are willing to pay and sellers have a minimum price they are willing to offer their product. The point at which the supply and demand curves meet is the equilibrium price of the good and quantity demanded. Sellers willing to offer their goods at a lower price than the equilibrium price receive the difference as producer surplus. Buyers willing to pay for goods at a higher price than the equilibrium price receive the difference as consumer surplus.
The model is commonly applied to wages in the market for labor. The typical roles of supplier and consumer are reversed. The suppliers are individuals, who try to sell (supply) their labor for the highest price. The consumers are businesses, which try to buy (demand) the type of labor they need at the lowest price. As more people offer their labor in that market, the equilibrium wage decreases and the equilibrium level of employment increases as the supply curve shifts to the right. The opposite happens if fewer people offer their wages in the market as the supply curve shifts to the left.
In a free market, individuals and firms taking part in these transactions have the liberty to enter, leave and participate in the market as they so choose. Prices and quantities are allowed to adjust according to economic conditions in order to reach equilibrium and properly allocate resources. However, in many countries around the world governments seek to intervene in the free market in order to achieve certain social or political agendas. Governments may attempt to create social equality or equality of outcome by intervening in the market through actions such as imposing a minimum wage (price floor) or erecting price controls (price ceiling). Other lesser-known goals are also pursued, such as in the United States, where the federal government subsidizes owners of fertile land to not grow crops in order to prevent the supply curve from further shifting to the right and decreasing the equilibrium price. This is done under the justification of maintaining farmers' profits; due to the relative inelasticity of demand for crops, increased supply would lower the price but not significantly increase quantity demanded, thus placing pressure on farmers to exit the market. Those interventions are often done in the name of maintaining basic assumptions of free markets such as the idea that the costs of production must be included in the price of goods. Pollution and depletion costs are sometimes not included in the cost of production (a manufacturer that withdraws water at one location then discharges it polluted downstream, avoiding the cost of treating the water), therefore governments may opt to impose regulations in an attempt to try to internalize all of the cost of production and ultimately include them in the price of the goods.
Advocates of the free market contend that government intervention hampers economic growth by disrupting the natural allocation of resources according to supply and demand while critics of the free market contend that government intervention is sometimes necessary to protect a country's economy from better-developed and more influential economies, while providing the stability necessary for wise long-term investment. Milton Friedman pointed to failures of central planning, price controls and state-owned corporations, particularly in the Soviet Union and China while Ha-Joon Chang cites the examples of post-war Japan and the growth of South Korea's steel industry.
Critics of the free market have argued that in real world situations it has proven to be susceptible to the development of price fixing monopolies. Such reasoning has led to government intervention, e.g. the United States antitrust law.
Two prominent Canadian authors argue that government at times has to intervene to ensure competition in large and important industries. Naomi Klein illustrates this roughly in her work "The Shock Doctrine" and John Ralston Saul more humorously illustrates this through various examples in "The Collapse of Globalism and the Reinvention of the World". While its supporters argue that only a free market can create healthy competition and therefore more business and reasonable prices, opponents say that a free market in its purest form may result in the opposite. According to Klein and Ralston, the merging of companies into giant corporations or the privatization of government-run industry and national assets often result in monopolies or oligopolies requiring government intervention to force competition and reasonable prices. Another form of market failure is speculation, where transactions are made to profit from short term fluctuation, rather from the intrinsic value of the companies or products. This criticism has been challenged by historians such as Lawrence Reed, who argued that monopolies have historically failed to form even in the absence of antitrust law. This is because monopolies are inherently difficult to maintain as a company that tries to maintain its monopoly by buying out new competitors, for instance, is incentivizing newcomers to enter the market in hope of a buy-out.
American philosopher and author Cornel West has derisively termed what he perceives as dogmatic arguments for "laissez-faire" economic policies as free-market fundamentalism. West has contended that such mentality "trivializes the concern for public interest" and "makes money-driven, poll-obsessed elected officials deferential to corporate goals of profit – often at the cost of the common good". American political philosopher Michael J. Sandel contends that in the last thirty years the United States has moved beyond just having a market economy and has become a market society where literally everything is for sale, including aspects of social and civic life such as education, access to justice and political influence. The economic historian Karl Polanyi was highly critical of the idea of the market-based society in his book "The Great Transformation", noting that any attempt at its creation would undermine human society and the common good.
Critics of free market economics range from those who reject markets entirely in favour of a planned economy as advocated by various Marxists to those who wish to see market failures regulated to various degrees or supplemented by government interventions. Keynesians support market roles for government such as using fiscal policy for economic stimulus when actions in the private sector lead to sub-optimal economic outcomes of depressions or recessions. Business cycle is used by Keynesians to explain liquidity traps, by which underconsumption occurs, to argue for government intervention with fiscal policy. David McNally of the University of Houston argues in the Marxist tradition that the logic of the market inherently produces inequitable outcomes and leads to unequal exchanges, arguing that Adam Smith's moral intent and moral philosophy espousing equal exchange was undermined by the practice of the free market he championed. According to McNally, the development of the market economy involved coercion, exploitation and violence that Smith's moral philosophy could not countenance. McNally also criticizes market socialists for believing in the possibility of fair markets based on equal exchanges to be achieved by purging parasitical elements from the market economy such as private ownership of the means of production, arguing that market socialism is an oxymoron when socialism is defined as an end to wage labour.
Some would argue that only one known example of a true free market exists, namely the black market. The black market is under constant threat by the police, but under no circumstances do the police regulate the substances that are being created. The black market produces wholly unregulated goods and are purchased and consumed unregulated. That is to say, anyone can produce anything at any time and anyone can purchase anything available at any time. The alternative view is that the black market is not a free market at all since high prices and natural monopolies are often enforced through murder, theft and destruction. Black markets can only exist peripheral to regulated markets where laws are being regularly enforced.

</doc>
<doc id="11830" url="https://en.wikipedia.org/wiki?curid=11830" title="Ford GT40">
Ford GT40

The Ford GT40 is an American high-performance endurance racing car. The Mk I, Mk II, and Mk III variants were designed and built in England based upon the British Lola Mk6. Only the Mk IV model was designed and built in the United States. The range was powered by a series of American-built Ford V8 engines modified for racing. 
The GT40 effort was launched by Ford Motor Company to win long-distance sports car races against Ferrari, which won every 24 Hours of Le Mans race from 1960 to 1965. The GT40 broke Ferrari's streak in 1966 and went on to win the next three annual races. The Mk II's victory was the first win for an American manufacturer in a major European race since Jimmy Murphy's triumph with Duesenberg at the 1921 French Grand Prix. In 1967, the Mk IV became the only car designed and built entirely in the United States to achieve the overall win at Le Mans. 
The Mk 1, the oldest of the cars, won in 1968 and 1969, the second chassis to win Le Mans more than once. (This Ford/Shelby chassis #P-1075 was believed to have been the first until the Ferrari 275P chassis 0816 was revealed to have won the 1964 race after winning the 1963 race in 250P configuration and with a 0814 chassis plate). Using an American Ford V8 engine, originally of 4.7-liter displacement capacity (289 cubic inches), it was later enlarged to the 4.9-liter engine (302 cubic inches), with custom alloy Gurney–Weslake cylinder heads. 
Early cars were simply named "Ford GT" for Grand Touring), the name of Ford's project to prepare the cars for the international endurance racing circuit. The "40" represented its height of 40 inches (1.02 m), measured at the windshield, the maximum allowed. The first 12 "prototype" vehicles carried serial numbers GT-101 to GT-112. The "production" began and the subsequent cars: the MkI, MkII, MkIII, and MkIV were numbered GT40P/1000 through GT40P/1145, and thus officially "GT40s". The Mk IVs were numbered J1-J12.
The contemporary Ford GT is a modern homage to the GT40.
Henry Ford II had wanted a Ford at Le Mans since the early 1960s. In early 1963, Ford reportedly received word through a European intermediary that Enzo Ferrari was interested in selling to Ford Motor Company. Ford reportedly spent several million dollars in an audit of Ferrari factory assets and in legal negotiations, only to have Ferrari unilaterally cut off talks at a late stage due to disputes about the ability to direct open-wheel racing. Ferrari, who wanted to remain the sole operator of his company's motorsports division, was angered when he was told that he would not be allowed to race at the Indianapolis 500 if the deal went through since Ford fielded Indy cars using its own engine, and didn't want competition from Ferrari. Enzo cut the deal off out of spite and Henry Ford II, enraged, directed his racing division to find a company that could build a Ferrari-beater on the world endurance-racing circuit.
To this end Ford began negotiation with Lotus, Lola, and Cooper. Cooper had no experience in GT or prototype and its performances in Formula One were declining.
The Lola proposal was chosen since Lola had used a Ford V8 engine in its mid-engined Lola Mk6 (also known as Lola GT). It was one of the most advanced racing cars of the time, and made a noted performance in Le Mans 1963, even though the car did not finish, due to low gearing and slow revving out on the Mulsanne Straight. However, Eric Broadley, Lola Cars' owner and chief designer, agreed on a short-term personal contribution to the project without involving Lola Cars.
The agreement with Broadley included a one-year collaboration between Ford and Broadley, and the sale of the two Lola Mk 6 chassis builds to Ford. To form the development team, Ford also hired the ex-Aston Martin team manager John Wyer. Ford Motor Co. engineer Roy Lunn was sent to England; he had designed the mid-engined Mustang I concept car powered by a 1.7-liter V4. Despite the small engine of the Mustang I, Lunn was the only Dearborn engineer to have some experience with a mid-engined car.
Overseen by Harley Copp, the team of Broadley, Lunn, and Wyer began working on the new car at the Lola Factory in Bromley. At the end of 1963, the team moved to Slough, near Heathrow Airport. Ford then established Ford Advanced Vehicles (FAV) Ltd, a new subsidiary under the direction of Wyer, to manage the project.
The first chassis built by Abbey Panels of Coventry was delivered on 16 March 1964, with fiber-glass moldings produced by Fibre Glass Engineering Ltd of Farnham. The first "Ford GT" the GT/101 was unveiled in England on 1 April and soon after exhibited in New York. Purchase price of the completed car for competition use was £5,200.
It was powered by the 4.7 L 289 cu in Fairlane engine with a Colotti transaxle, the same power plant was used by the Lola GT and the single-seater Lotus 29 that came in a highly controversial second at the Indy 500 in 1963. (An aluminum block DOHC version, known as the Ford Indy Engine, was used in later years at Indy. It won in 1965 in the Lotus 38.)
The Ford GT40 was first raced in May 1964 at the Nürburgring "1000 km race" where it retired with suspension failure after holding second place early in the event. Three weeks later at the 24 Hours of Le Mans, all three entries retired although the Ginther/Gregory car led the field from the second lap until its first pitstop. After a season-long series of dismal results under John Wyer in 1964, the program was handed over to Carroll Shelby after the 1964 Nassau race. The cars were sent directly to Shelby, still bearing the dirt and damage from the Nassau race. Carroll Shelby was noted for complaining that the cars were poorly maintained when he received them, but later information revealed the cars were packed up as soon as the race was over, and FAV never had a chance to clean and organize the cars to be transported to Shelby.
Shelby's first victory came on their maiden race with the Ford program, with Ken Miles and Lloyd Ruby taking a Shelby American-entered Ford GT40 to victory in the Daytona 2000 in February 1965. One month later Ken Miles and Bruce McLaren came in second overall (to the winning Chaparral in the sports class) and first in prototype class at the Sebring 12-hour race. The rest of the season, however, was a disappointment.
The experience gained in 1964 and 1965 allowed the 7-liter Mk II to dominate the following year. In February, the GT40 again won at Daytona. This was the first year Daytona was run in the 24 Hour format and Mk II's finished 1st, 2nd, and 3rd. In March, at the 1966 12 Hours of Sebring, GT40s again took all three top finishes with the X-1 Roadster first, a Mk. II taking second, and a Mk. I in third. Then in June at the 24 Hours of Le Mans the GT40 achieved yet another 1–2–3 result.
The Le Mans finish, however, was clouded in controversy: The No1 car of Ken Miles and Denny Hulme held a four lap lead over the No2 car of Bruce McLaren and Chris Amon. This disintegrated when the No1 car was forced to make a pit-stop for replacement brake rotors, following an incorrect set being fitted a lap prior in a scheduled rotor change. It was found to be a result of the correct brake rotors being taken by the No2 crew. This meant, that in the final few hours, the Ford GT40 of New Zealanders Bruce McLaren and Chris Amon closely trailed the leading Ford GT40 driven by Englishman Ken Miles and New Zealander Denny Hulme. With a multimillion-dollar program finally on the very brink of success, Ford team officials faced a difficult choice. They could allow the drivers to settle the outcome by racing each other—and risk one or both cars breaking down or crashing. They could dictate a finishing order to the drivers—guaranteeing that one set of drivers would be extremely unhappy. Or they could arrange a tie, with the McLaren/Amon and Miles/Hulme cars crossing the line side-by-side.
The team chose the last and informed McLaren and Miles of the decision just before the two got in their cars for the final stint. Then, not long before the finish, the Automobile Club de l'Ouest (ACO), organizers of the Le Mans event, informed Ford that the geographical difference in starting positions would be taken into account at a close finish. This meant that the McLaren/Amon vehicle, which had started perhaps behind the Hulme-Miles car, would have covered slightly more ground over the 24 hours and would, therefore, be the winner. Secondly, Ford officials admitted later, the company's contentious relationship with Miles, its top contract driver, placed executives in a difficult position. They could reward an outstanding driver who had been at times extremely difficult to work with, or they could decide in favor of drivers (McLaren/Amon) who had committed less to the Ford program but who had been easier to deal with. Ford stuck with the orchestrated photo finish but Miles, deeply bitter over this decision after his dedication to the program, issued his own protest by suddenly slowing just yards from the finish and letting McLaren across the line first. Miles died in a testing accident in the J-car (later to become the Mk IV) at Riverside (CA) Raceway just two months later.
Miles' death occurred at the wheel of the Ford "J-car", an iteration of the GT40 that included several unique features. These included an aluminum honeycomb chassis construction and a "bread van" body design that experimented with "Kammback" aerodynamic theories. Unfortunately, the fatal Miles accident was attributed at least partly to the unproven aerodynamics of the J-car design, as well as the experimental chassis' strength. The team embarked on a complete redesign of the car, which became known as the Mk IV. The Mk IV, newer design with a Mk II engine but a different chassis and a different body, won the following year at Le Mans (when four Mark IVs, three Mark IIs, and three Mark Is raced). The high speeds achieved in that race caused a rule change, which already came in effect in 1968: the prototypes were limited to the capacity of 3.0 liters, the same as in Formula One. This took out the V12-powered Ferrari 330P as well as the Chaparral and the Mk. IV.
If at least 50 cars had been built, sportscars like the GT40 and the Lola T70 were allowed, with a maximum of 5.0  L. John Wyer's revised 4.7-liter (bored to 4.9 liter, and O-rings cut and installed between the block and head to prevent head gasket failure, a common problem found with the 4.7 engine) Mk I won the 24 hours of Le Mans race in 1968 against the fragile smaller prototypes. This result added to four other round wins for the GT40, gave Ford victory in the 1968 International Championship for Makes. The GT40's intended 3.0  L replacement, the Ford P68, and Mirage cars proved a dismal failure. While facing more experienced prototypes and the new yet still unreliable 4.5  L flat-12-powered Porsche 917s, Wyer's 1969 24 Hours of Le Mans winners Jacky Ickx/Jackie Oliver managed to beat the remaining 3.0-liter Porsche 908 by just a few seconds with the already outdated GT40 Mk I, in the very car that had won in 1968—the legendary GT40P/1075. Apart from brake wear in the Porsche and the decision not to change brake pads so close to the race end, the winning combination was relaxed driving by both GT40 drivers and heroic efforts at the right time by (at that time Le Mans' rookie) Ickx, who won Le Mans five more times in later years.
In addition to four consecutive overall Le Mans victories, Ford also won the following four FIA international titles (at what was then unofficially known as the World Sportscar Championship) with the GT40:
The Mk.I was the original Ford GT40. Early prototypes were powered by alloy V8 engines and production models were powered by engines as used in the Ford Mustang. Five prototype models were built with roadster bodywork, including the Ford X-1. Two lightweight cars (of a planned five) were built by Alan Mann Racing in 1966, with light alloy bodies and other weight-saving modifications.
The Mk.I met with little success in its initial tune for the 1964 and 1965 Le Mans races. The first success came after their demise at the Nassau Speed Weekend Nov 1964 when the racing was handed over to Carrol Shelby. Shelby's team modified the Ford GT40 and the first win at Daytona February 1965 was achieved. Much was later modified and run by John Wyer in 1968 and 1969, winning Le Mans in both those years and Sebring in 1969. The Mk.II and IV were both obsolete after the FIA had changed the rules to ban unlimited capacity engines, ruling out the Ford V8. However, the Mk.I, with its smaller engine, was legally able to race as a homologated sports car because of its production numbers.
In 1968 competition came from the Porsche 908 which was the first prototype built for the 3-liter Group 6. The result of the 1968 was resounding success at the 24 Hours of Le Mans with Pedro Rodríguez and Lucien Bianchi having a clear lead over the Porsches, driving the almighty #9 car with the 'Gulf Oil' colors. The season began slowly for JW, losing at Sebring and Daytona before taking their first win at the BOAC International 500 at Brands Hatch. Later victories included the Grand Prix de Spa, 21st Annual Watkins Glen Sports Car Road Race and the 1000 km di Monza. The engine installed on this car was a naturally aspirated Windsor V8 engine with a compression ratio of 10.6:1 fuel feed by four 2-barrel 48 IDA Weber carburetors, rated at at 6,000 rpm and a maximum torque of at 4,750 rpm.
31 Mk I cars were built at the Slough factory in "road" trim, which differed little from the race versions. Wire wheels, carpet, ruched fabric map pockets in the doors and a cigarette lighter made up most of the changes. Some cars deleted the ventilated seats, and at least one (chassis 1049) was built with the opening, metal-framed, windows from the Mk III.
The X-1 was a roadster built to contest the Fall 1965 North American Pro Series, a forerunner of Can-Am, entered by the Bruce McLaren team and driven by Chris Amon. The car had an aluminum chassis built at Abbey Panels and was originally powered by a 4.7-liter (289 ci) engine. The real purpose of this car was to test several improvements originating from Kar Kraft, Shelby, and McLaren. Several gearboxes were used: a Hewland LG500 and at least one automatic gearbox. It was later upgraded to Mk.II specifications with a 7.0-liter (427 ci) engine and a standard four ratio Kar Kraft (subsidiary of Ford) gearbox, however, the car kept specific features such as its open roof and lightweight aluminum chassis. The car went on to win the 12 Hours of Sebring in 1966. The X-1 was a one-off and having been built in the United Kingdom and being liable for United States tariffs, was later ordered to be destroyed by United States customs officials.
The Mk.II was very similar in appearance to the Mk.I but used the 7.0-liter FE (427 ci) engine from the Ford Galaxie, used in NASCAR at the time and modified for road course use. The car's chassis was similar to the British-built Mk.I chassis, but it and other parts of the car had to be redesigned and modified by Shelby to accommodate the larger and heavier 427 engine. A new Kar Kraft-built four-speed gearbox replaced the ZF five-speed used in the Mk.I. This car is sometimes called the "Ford Mk.II".
In 1966, the three teams racing the Mk.II (Chris Amon and Bruce McLaren, Denny Hulme and Ken Miles, and Dick Hutcherson and Ronnie Bucknum) dominated Le Mans, taking European audiences by surprise and beating Ferrari to finish 1-2-3 in the standings. The Ford GT40 went on to win the race for the next three years.
For 1967, the Mk.IIs were upgraded to "B" spec; they had re-designed bodywork and twin Holley carburetors for an additional . A batch of improperly heat-treated input shafts in the transaxles sidelined virtually every Ford in the race at Daytona, however, and Ferrari won 1-2-3. The Mk.IIBs were also used for Sebring and Le Mans that year and won the Reims 12 Hours in France. For the Daytona 24 Hours, two Mk II models (chassis 1016 and 1047) had their engines re-badged as Mercury engines; Ford seeing a good opportunity to advertise that division of the company. 
The Mk III was a road-car only, of which seven were built. The car had four headlamps, the rear part of the body was expanded to make room for luggage, the 4.7-liter engine was detuned to , the shock absorbers were softened, the shift lever was moved to the center, an ashtray was added, and the car was available with the steering wheel on the left side of the car. As the Mk III looked significantly different from the racing models many customers interested in buying a GT40 for road use chose to buy a Mk I that was available from Wyer Ltd. Of the seven MK III that were produced four were left-hand drive.
In an effort to develop a car with better aerodynamics (potentially resulting in superior control and speed compared to competitors), the decision was made to re-conceptualize, and redesign everything about the vehicle other than its powerful 7-liter Engine. This would end up resulting in the abandonment of the original Mk.I/Mk.II chassis. In order to bring the car into alignment with Ford’s "in house" ideology at the time, more restrictive partnerships were implemented with English firms, which resulted in the sale of Ford Advanced Vehicles, (acquired by John Wyer) ultimately leading to a new vehicle which would be slated for design by Ford's studios and produced by Ford's subsidiary Kar-Kraft under Ed Hull. Furthermore there was also a partnership with the Brunswick Aircraft Corporation for expertise on the novel use of aluminum honeycomb panels bonded together to form a lightweight, rigid "tub". The car was designated as the J-car, as it was constructed to meet the new Appendix J regulations 
which were introduced by the FIA in 1966.
The first J-car was completed in March 1966 and set the fastest time at the Le Mans trials that year. The tub weighed only , and the entire car weighed only , less than the Mk II. It was decided to run the Mk IIs due to their proven reliability, however, and little or no development was done on the J-car for the rest of the season. Following Le Mans, the development program for the J-car was resumed, and a second car was built. During a test session at Riverside International Raceway in August 1966 with Ken Miles driving, the car suddenly went out of control at the end of Riverside's high-speed, 1-mile-long back straight. The aluminum honeycomb chassis did not live up to its design goal, shattering upon impact. The car burst into flames, killing Miles. It was determined that the unique, flat-topped "bread van" aerodynamics of the car, lacking any sort of spoiler, were implicated in generating excess lift. Therefore, a conventional but significantly more aerodynamic body was designed for the subsequent development of the J-car which was officially known as the Mk IV. A total of nine cars were constructed with J-car chassis numbers although six were designated as Mk IVs and one as the G7A.
The Mk IV was built around a reinforced J chassis powered by the same 7.0 L engine as the Mk II. Excluding the engine, gearbox, some suspension parts and the brakes from the Mk.II, the Mk.IV was totally different from other GT40s, using a specific, all-new chassis and bodywork. It was undoubtedly the most radical and American variant of all the GT40's over the years. As a direct result of the Miles accident, the team installed a NASCAR-style steel-tube roll cage in the Mk.IV, which made it much safer, but the roll cage was so heavy that it negated most of the weight saving of the then-highly advanced, radically innovative honeycomb-panel construction. The Mk. IV had a long, streamlined shape, which gave it exceptional top speed, crucial to do well at Le Mans in those days (a circuit made up predominantly of straights)—the race it was ultimately built for. A 2-speed automatic gearbox was tried, but during the extensive testing of the J-car in 1966 and 1967, it was decided that the 4-speed from the Mk.II would be retained. Dan Gurney often complained about the weight of the Mk.IV, since the car was heavier than the Ferrari 330 P4's. During practice at Le Mans in 1967, in an effort to preserve the highly stressed brakes, Gurney developed a strategy (also adopted by co-driver A.J. Foyt) of backing completely off the throttle several hundred yards before the approach to the Mulsanne hairpin and virtually coasting into the braking area. This technique saved the brakes, but the resulting increase in the car's recorded lap times during practice led to speculation within the Ford team that Gurney and Foyt, in an effort to compromise on chassis settings, had hopelessly "dialed out" their car. The car proved to be fastest in a straight line that year, thanks to its streamlined aerodynamics, achieving 212 mph on the 3.6-mile Mulsanne Straight.
The Mk. IV ran in only two races, the 1967 12 Hours of Sebring and the 1967 24 Hours of Le Mans and won both events. Only one Mk.IV was completed for Sebring; the pressure from Ford had been amped up considerably after Ford's humiliation at Daytona two months earlier. Mario Andretti and Bruce McLaren won Sebring, Dan Gurney and A. J. Foyt won Le Mans (Gurney and Foyt's car was the Mk.IV that was apparently least likely to win), where the Ford-representing Shelby-American and Holman & Moody teams showed up to Le Mans with 2 Mk.IV's each. The installation of the roll cage was ultimately credited by many with saving the life of Andretti, who crashed violently at the Esses during the 1967 Le Mans 24 Hours, but escaped with minor injuries.
Unlike the earlier Mk.I - III cars, which were built in England, the Mk.IVs were built in the United States by Kar Kraft. Le Mans 1967 remains the only all-American victory in Le Mans history—American drivers, team, chassis, engine, and tires. A total of six Mk IVs were constructed. One of the Mk IVs was rebuilt to the Ford G7 in 1968, and used in the Can-Am series for 1969 and 1970, but with no success. This car is sometimes called the "Ford Mk.IV".
For years Peter Thorp had searched for a GT40 in good condition. Most of the cars had problems including the dreaded rust issue. His company, Safir Engineering, was building and fielding Formula 3 race cars, in addition, had a Token Formula One car purchased from the Ron Dennis Company, Rondell Racing. Formula One events in which Safir Engineering competed included Brands Hatch and Silverstone. Safir was also redesigning Range Rovers modifying the unit to six-wheel drive and exporting them. Safir technical capabilities were such that they could rebuild GT40s. It was with this in mind that Thorp approached John Willment for his thoughts. It was soon decided that there would be a limited, further run of the significant GT40. JW Engineering would oversee the build, and Safir was to do the work. The continued JW Engineering/Safir Engineering production would utilize sequential serial numbers starting at the last used GT40 serial number and move forward. Maintaining the GT40 Mark nomenclature, this continued production would be named GT40 MkV.
JW Engineering wished to complete the GT40 chassis numbers GT40P-1087, 1088 and 1089. This was supposed to take place prior to the beginning of Safir production, however, the completion of these three chassis’ was very much delayed.
Ford's Len Bailey was hired to inspect the proposed build and engineer any changes he thought prudent to ensure the car was safe, as well as minimize problems experienced in the past. Baily changed the front suspension to Alan Mann specifications, which minimized nose-dive under braking. Zinc coated steel replaced the previous uncoated rust-prone sheet metal. The vulnerable drive donuts were replaced with CV joints and the leak-prone rubber gas tanks were replaced with aluminum tanks. The GT40 chassis was upgraded without making any major changes.
Tennant Panels supplied the roof structure and the balance of the chassis was completed by Safir. Bill Pink, noted for his electrical experience and the wiring installation of previous GT40s, was brought in. Also, Jim Rose was hired for his experience with working at both Alan Mann and Shelby. After the manufacture of chassis 1120, John Etheridge was hired to manage the GT40 build. The chassis was supplied from Adams McCall Engineering and parts supplied from Tennant panels.
For the most part, the MkV resembled very closely the MkI car, although there were a few changes, and, as with the 60s production, very few cars were identical.
The first car, GT40P-1090, had an open-top in place of roofed doors. Most motors were Ford small block, Webers or 4 Barrel Carburetor. Safir produced five Big Block GT40s, serial numbers GT40P-1128 to GT40P-1132. These aluminum big block cars all had easily removable door roof sections. Most GT40s were high-performance street cars however some of the MkV production can be described as full race. Two road cars GT40P-1133 (roadster) and GT40P-1142 (roofed doors) were built as lightweights which included an aluminum honeycomb chassis and carbon fiber bodywork.
Several kit cars and replicas inspired by the Ford GT40 have been built. They are generally intended for assembly in a home workshop or garage. There are two alternatives to the kit car approach, either continuation models (exact and licensed replicas true to the original GT40) or modernizations (replicas with upgraded components, ergonomics & trim for improved usability, drivability, and performance).
At the 1995 North American International Auto Show, the Ford GT90 concept was shown and at the 2002 show, a new GT40 Concept was unveiled by Ford.
While similar in appearance to the original cars, it was bigger, wider, and 3 inches (76 mm) taller than the original 40 inches (1020 mm). Three production prototype cars were shown in 2003 as part of Ford's centenary, and delivery of the production Ford GT began in the fall of 2004. The Ford GT was assembled in the Ford Wixom plant and painted by Saleen, Incorporated at their Saleen Special Vehicles plant in Troy, Michigan.
A British company, Safir Engineering, who continued to produce a limited number of GT40s (the MkV) in the 1980s under an agreement with Walter Hayes of Ford and John Wilmont of J.W. Automotive Engineering, owned the GT40 trademark at that time, and when they completed production, they sold the excess parts, tooling, design, and trademark to a small American company called Safir GT40 Spares, Limited based in Ohio. Safir GT40 Spares licensed the use of the GT40 trademark to Ford for the initial 2002 show car, but when Ford decided to make the production vehicle, negotiations between the two failed, and as a result, the new Ford GT does not wear the badge GT40. Bob Wood, one of three partners who own Safir GT40 Spares, said: "When we talked with Ford, they asked what we wanted. We said that Ford owns Beanstalk in New York, the company that licenses the Blue Oval for Ford on such things as T-shirts. Since Beanstalk gets 7.5 percent of the retail cost of the item for licensing the name, we suggested 7.5 percent on each GT40 sold." In this instance, Ford wished to purchase, not just license the GT40 trademark. At the then-estimated $125,000 per copy, 7.5% of 4,500 vehicles would have totalled approximately $42,187,500. It was widely and erroneously reported following an "Automotive News Weekly" story that Safir "demanded" the $40 million for the sale of the trademark. Discussions between Safir and Ford ensued. However, in fact, the Ford Motor Company never made an offer in writing to purchase the famed GT40 trademark. Later models or prototypes have also been called the Ford GT but have had different numbering on them such as the Ford GT90 or the Ford GT70. The GT40 name and trademark is currently licensed to Superformance in the USA.
A second-generation Ford GT was unveiled at the 2015 North American International Auto Show. It features a 3.5L twin-turbocharged V6 engine, carbon fiber monocoque and body panels, pushrod suspension and active aerodynamics. It entered the 2016 season of the FIA World Endurance Championship and the United SportsCar Championship, and started being sold in a street-legal version at Ford dealerships in 2017.

</doc>
<doc id="11835" url="https://en.wikipedia.org/wiki?curid=11835" title="Glycine">
Glycine

Glycine (symbol Gly or G; ) is an amino acid that has a single hydrogen atom as its side chain. It is the simplest amino acid (since carbamic acid is unstable), with the chemical formula NH‐CH‐COOH. Glycine is one of the proteinogenic amino acids. It is encoded by all the codons starting with GG (GGU, GGC, GGA, GGG). Glycine is integral to the formation of alpha-helices in secondary protein structure due to its compact form. For the same reason, it is the most abundant amino acid in collagen triple-helices. Glycine is also an inhibitory neurotransmitter - interference with its release within the spinal cord (such as during a "Clostridium tetani" infection) can cause spastic paralysis due to uninhibited muscle contraction.
Glycine is a colorless, sweet-tasting crystalline solid. It is the only achiral proteinogenic amino acid. It can fit into hydrophilic or hydrophobic environments, due to its minimal side chain of only one hydrogen atom. The acyl radical is glycyl.
Glycine was discovered in 1820 by the French chemist Henri Braconnot when he hydrolyzed gelatin by boiling it with sulfuric acid. He originally called it "sugar of gelatin", but the French chemist Jean-Baptiste Boussingault showed that it contained nitrogen. The American scientist Eben Norton Horsford, then a student of the German chemist Justus von Liebig, proposed the name "glycocoll"; however, the Swedish chemist Berzelius suggested the simpler name "glycine". The name comes from the Greek word γλυκύς "sweet tasting" (which is also related to the prefixes "glyco-" and "gluco-", as in "glycoprotein" and "glucose"). In 1858, the French chemist Auguste Cahours determined that glycine was an amine of acetic acid.
Although glycine can be isolated from hydrolyzed protein, this is not used for industrial production, as it can be manufactured more conveniently by chemical synthesis. The two main processes are amination of chloroacetic acid with ammonia, giving glycine and ammonium chloride, and the Strecker amino acid synthesis, which is the main synthetic method in the United States and Japan. About 15 thousand tonnes are produced annually in this way.
Glycine is also cogenerated as an impurity in the synthesis of EDTA, arising from reactions of the ammonia coproduct.<ref name="Ullmann/Roger"></ref>
Its acid–base properties are most important. In aqueous solution, glycine itself is amphoteric: at low pH the molecule can be protonated with a p"K" of about 2.4 and at high pH it loses a proton with a p"K" of about 9.6 (precise values of p"K" depend on temperature and ionic strength).
Glycine functions as a bidentate ligand for many metal ions. A typical complex is Cu(glycinate), i.e. Cu(HNCHCO), which exists both in cis and trans isomers.
As a bifunctional molecule, glycine reacts with many reagents. These can be classified into N-centered and carboxylate-center reactions.
The amine undergoes the expected reactions. With acid chlorides, one obtains the amidocarboxylic acid, such as hippuric acid and acetylglycine. With nitrous acid, one obtains glycolic acid (van Slyke determination). With methyl iodide, the amine becomes quaternized to give betaine, a natural product:
Glycine condenses with itself to give peptides, beginning with the formation of glycylglycine:
Pyrolysis of glycine or glycylglycine gives 2,5-diketopiperazine, the cyclic diamide.
Glycine is not essential to the human diet, as it is biosynthesized in the body from the amino acid serine, which is in turn derived from 3-phosphoglycerate, but the metabolic capacity for glycine biosynthesis does not satisfy the need for collagen synthesis. In most organisms, the enzyme serine hydroxymethyltransferase catalyses this transformation via the cofactor pyridoxal phosphate:
In the liver of vertebrates, glycine synthesis is catalyzed by glycine synthase (also called glycine cleavage enzyme). This conversion is readily reversible:
Glycine is degraded via three pathways. The predominant pathway in animals and plants is the reverse of the glycine synthase pathway mentioned above. In this context, the enzyme system involved is usually called the glycine cleavage system:
In the second pathway, glycine is degraded in two steps. The first step is the reverse of glycine biosynthesis from serine with serine hydroxymethyl transferase. Serine is then converted to pyruvate by serine dehydratase.
In the third pathway of its degradation, glycine is converted to glyoxylate by D-amino acid oxidase. Glyoxylate is then oxidized by hepatic lactate dehydrogenase to oxalate in an NAD-dependent reaction.
The half-life of glycine and its elimination from the body varies significantly based on dose. In one study, the half-life varied between 0.5 and 4.0 hours.
Glycine is extremely sensitive to antibiotics which target folate, and blood Glycine levels drop severely within a minute of antibiotic injections. Some antibiotics can deplete more than 90% of Glycine within a few minutes of being administered.
The principal function of glycine is as a precursor to proteins. Most proteins incorporate only small quantities of glycine, a notable exception being collagen, which contains about 35% glycine due to its periodically repeated role in the formation of collagen's helix structure in conjunction with hydroxyproline. In the genetic code, glycine is coded by all codons starting with GG, namely GGU, GGC, GGA and GGG.
In higher eukaryotes, δ-aminolevulinic acid, the key precursor to porphyrins, is biosynthesized from glycine and succinyl-CoA by the enzyme ALA synthase. Glycine provides the central CN subunit of all purines.
Glycine is an inhibitory neurotransmitter in the central nervous system, especially in the spinal cord, brainstem, and retina. When glycine receptors are activated, chloride enters the neuron via ionotropic receptors, causing an inhibitory postsynaptic potential (IPSP). Strychnine is a strong antagonist at ionotropic glycine receptors, whereas bicuculline is a weak one. Glycine is a required co-agonist along with glutamate for NMDA receptors. In contrast to the inhibitory role of glycine in the spinal cord, this behaviour is facilitated at the (NMDA) glutamatergic receptors which are excitatory. The of glycine is 7930 mg/kg in rats (oral), and it usually causes death by hyperexcitability.
In the US, glycine is typically sold in two grades: United States Pharmacopeia (“USP”), and technical grade. USP grade sales account for approximately 80 to 85 percent of the U.S. market for glycine. If purity greater than the USP standard is needed, for example for intravenous injections, a more expensive pharmaceutical grade glycine can be used. Technical grade glycine, which may or may not meet USP grade standards, is sold at a lower price for use in industrial applications, e.g., as an agent in metal complexing and finishing.
Glycine is not widely used in foods for its nutrional value, except in infusions. Instead glycine's role in food chemistry is as a flavorant. It is mildly sweet, and it counters the aftertaste of saccharine. It also has preservative properties, perhaps owing to its complexation to metal ions. Metal glycinate complexes, e.g. copper(II) glycinate are used as supplements for animal feeds.
Glycine is an intermediate in the synthesis of a variety of chemical products. It is used in the manufacture of the herbicides glyphosate, iprodione, glyphosine, imiprothrin, and eglinazine. It is used as an intermediate of the medicine such as thiamphenicol.
Glycine is a significant component of some solutions used in the SDS-PAGE method of protein analysis. It serves as a buffering agent, maintaining pH and preventing sample damage during electrophoresis. Glycine is also used to remove protein-labeling antibodies from Western blot membranes to enable the probing of numerous proteins of interest from SDS-PAGE gel. This allows more data to be drawn from the same specimen, increasing the reliability of the data, reducing the amount of sample processing, and number of samples required. This process is known as stripping.
The presence of glycine outside the earth was confirmed in 2009, based on the analysis of samples that had been taken in 2004 by the NASA spacecraft Stardust from comet Wild 2 and subsequently returned to earth. Glycine had previously been identified in the Murchison meteorite in 1970. The discovery of cometary glycine bolstered the theory of panspermia, which claims that the "building blocks" of life are widespread throughout the Universe. In 2016, detection of glycine within Comet 67P/Churyumov-Gerasimenko by the Rosetta spacecraft was announced.
The detection of glycine outside the solar system in the interstellar medium has been debated. In 2008, the Max Planck Institute for Radio Astronomy discovered the spectral lines of a glycine-like molecule aminoacetonitrile in the Large Molecule Heimat, a giant gas cloud near the galactic center in the constellation Sagittarius.

</doc>
<doc id="11839" url="https://en.wikipedia.org/wiki?curid=11839" title="Wikipedia:GNUStufF">
Wikipedia:GNUStufF


</doc>
<doc id="11844" url="https://en.wikipedia.org/wiki?curid=11844" title="GeekSpeak">
GeekSpeak

GeekSpeak is a podcast with two to four hosts who focus on technology and technology news of the week. Though originally a radio tech call-in program, which first aired in 1998 on KUSP, GeekSpeak has been a weekly podcast since 2004.
The program's slogan is ""Bridging the gap between geeks and the rest of humanity"."
GeekSpeak was created and originally broadcast on KUSP by Chris Neklason of Cruzio, Steve Schaefer of Guenther Computer, and board operator Ray Price from KUSP. Shortly there after Mark Hanford of Cruzio joined the program.
Currently, the host/producer is Lyle Troxell, who took over in September 2000.
In April 2016, citing financial difficulties, KUSP stopped broadcasting GeekSpeak with its final broadcast on May 5, 2016.
GeekSpeak episodes have been distributed as an archive on the internet since 2001. The podcast went live prior to March 5, 2005 with its first episode December 3, 2004.

</doc>
<doc id="11846" url="https://en.wikipedia.org/wiki?curid=11846" title="Guitar">
Guitar

The guitar is a fretted musical instrument that usually has six strings. It is typically played with both hands by strumming or plucking the strings with either a guitar pick or the fingers/fingernails of one hand, while simultaneously fretting (pressing the strings against the frets) with the fingers of the other hand. The sound of the vibrating strings is projected either acoustically, by means of the hollow chamber of the guitar (for an acoustic guitar), or through an electrical amplifier and a speaker.
The guitar is a type of chordophone, traditionally constructed from wood and strung with either gut, nylon or steel strings and distinguished from other chordophones by its construction and tuning. The modern guitar was preceded by the gittern, the vihuela, the four-course Renaissance guitar, and the five-course baroque guitar, all of which contributed to the development of the modern six-string instrument.
There are three main types of modern acoustic guitar: the classical guitar (Spanish guitar/nylon-string guitar), the steel-string acoustic guitar and the archtop guitar, which is sometimes called a "jazz guitar". The tone of an acoustic guitar is produced by the strings' vibration, amplified by the hollow body of the guitar, which acts as a resonating chamber. The classical guitar is often played as a solo instrument using a comprehensive finger-picking technique where each string is plucked individually by the player's fingers, as opposed to being strummed. The term "finger-picking" can also refer to a specific tradition of folk, blues, bluegrass, and country guitar playing in the United States. The acoustic bass guitar is a low-pitched instrument that is one octave below a regular guitar.
Electric guitars, introduced in the 1930s, use an amplifier and a loudspeaker that both makes the sound of the instrument loud enough for the performers and audience to hear, and, given that it produces an electric signal when played, that can electronically manipulate and shape the tone using an equalizer (e.g., bass and treble tone controls) and a huge variety of electronic effects units, the most commonly used ones being distortion (or "overdrive") and reverb. Early amplified guitars employed a hollow body, but solid wood guitars began to dominate during the 1960s and 1970s, as they are less prone to unwanted acoustic feedback "howls". As with acoustic guitars, there are a number of types of electric guitars, including hollowbody guitars, archtop guitars (used in jazz guitar, blues and rockabilly) and solid-body guitars, which are widely used in rock music.
The loud, amplified sound and sonic power of the electric guitar played through a guitar amp has played a key role in the development of blues and rock music, both as an accompaniment instrument (playing riffs and chords) and performing guitar solos, and in many rock subgenres, notably heavy metal music and punk rock. The electric guitar has had a major influence on popular culture. The guitar is used in a wide variety of musical genres worldwide. It is recognized as a primary instrument in genres such as blues, bluegrass, country, flamenco, folk, jazz, jota, mariachi, metal, punk, reggae, rock, soul, and pop.
Before the development of the electric guitar and the use of synthetic materials, a guitar was defined as being an instrument having "a long, fretted neck, flat wooden soundboard, ribs, and a flat back, most often with incurved sides." The term is used to refer to a number of chordophones that were developed and used across Europe, beginning in the 12th century and, later, in the Americas. A 3,300-year-old stone carving of a Hittite bard playing a stringed instrument is the oldest iconographic representation of a chordophone and clay plaques from Babylonia show people playing an instrument that has a strong resemblance to the guitar, indicating a possible Babylonian origin for the guitar.
The modern word "guitar," and its antecedents, has been applied to a wide variety of chordophones since classical times and as such causes confusion. The English word "guitar," the German "," and the French ' were all adopted from the Spanish ', which comes from the Andalusian Arabic (') and the Latin ', which in turn came from the Ancient Greek . The early Greek Kithara had only 4 strings when they were introduced from abroad. The Greeks hellenified the old Persian name for a 4-stringed instrument (""). "Chartar" derives from "char" which means four in Persian and "tar" means string. Kithara appears in the Bible four times (1 Cor. 14:7, Rev. 5:8, 14:2 and 15:2), and is usually translated into English as "harp".
Many influences are cited as antecedents to the modern guitar. Although the development of the earliest "guitars" is lost in the history of medieval Spain, two instruments are commonly cited as their most influential predecessors, the European lute and its cousin, the four-string oud; the latter was brought to Iberia by the Moors in the 8th century.
At least two instruments called "guitars" were in use in Spain by 1200: the ' (Latin guitar) and the so-called ' (Moorish guitar). The guitarra morisca had a rounded back, wide fingerboard, and several sound holes. The guitarra Latina had a single sound hole and a narrower neck. By the 14th century the qualifiers "moresca" or "morisca" and "latina" had been dropped, and these two chordophones were simply referred to as guitars.
The Spanish vihuela, called in Italian the "", a guitar-like instrument of the 15th and 16th centuries, is widely considered to have been the single most important influence in the development of the baroque guitar. It had six courses (usually), lute-like tuning in fourths and a guitar-like body, although early representations reveal an instrument with a sharply cut waist. It was also larger than the contemporary four-course guitars. By the 16th century, the vihuela's construction had more in common with the modern guitar, with its curved one-piece ribs, than with the viols, and more like a larger version of the contemporary four-course guitars. The vihuela enjoyed only a relatively short period of popularity in Spain and Italy during an era dominated elsewhere in Europe by the lute; the last surviving published music for the instrument appeared in 1576.
Meanwhile, the five-course baroque guitar, which was documented in Spain from the middle of the 16th century, enjoyed popularity, especially in Spain, Italy and France from the late 16th century to the mid-18th century. In Portugal, the word "viola" referred to the guitar, as "guitarra" meant the "Portuguese guitar", a variety of cittern.
There were many different plucked instruments that were being invented and used in Europe, during the Middle Ages. By the 16th century, most of the forms of guitar had fallen off, to never be seen again. However, midway through the 16th century, the five-course guitar was established. It was not a straightforward process. There were two types of five-course guitars, they differed in the location of the major third and in the interval pattern. The fifth course can be placed on the instrument, because it was known to play seventeen notes or more. Because the guitar had a fifth string, it was capable of playing that amount of notes. The guitar's strings were tuned in unison, so, in other words, it was tuned by placing a finger on the second fret of the thinnest string and tuning the guitar bottom to top. The strings were a whole octave apart from one another, which is the reason for the different method of tuning. Because it was so different, there was major controversy as to who created the five course guitar. A literary source, Lope de Vega's Dorotea, gives the credit to the poet and musician Vicente Espinel. This claim was also repeated by Nicolas Doizi de Velasco in 1640, however this claim has been refuted by others who state that Espinel's birth year (1550) make it impossible for him to be responsible for the tradition. He believed that the tuning was the reason the instrument became known as the Spanish guitar in Italy. Even later, in the same century, Gaspar Sanz wrote that other nations such as Italy or France added to the Spanish guitar. All of these nations even imitated the five-course guitar by "recreating" their own.
Finally, circa 1850, the form and structure of the modern guitar is followed by different Spanish makers such as Manuel de Soto y Solares and perhaps the most important of all guitar makers Antonio Torres Jurado, who increased the size of the guitar body, altered its proportions, and invented the breakthrough fan-braced pattern. Bracing, which refers to the internal pattern of wood reinforcements used to secure the guitar's top and back and prevent the instrument from collapsing under tension, is an important factor in how the guitar sounds. Torres' design greatly improved the volume, tone, and projection of the instrument, and it has remained essentially unchanged since.
Guitars can be divided into two broad categories, acoustic and electric guitars. Within each of these categories, there are also further sub-categories. For example, an electric guitar can be purchased in a six-string model (the most common model) or in seven- or twelve-string models.
Acoustic guitars form several notable subcategories within the acoustic guitar group: classical and flamenco guitars; steel-string guitars, which include the flat-topped, or "folk", guitar; twelve-string guitars; and the arched-top guitar. The acoustic guitar group also includes unamplified guitars designed to play in different registers, such as the acoustic bass guitar, which has a similar tuning to that of the electric bass guitar.
Renaissance and Baroque guitars are the ancestors of the modern classical and flamenco guitar. They are substantially smaller, more delicate in construction, and generate less volume. The strings are paired in courses as in a modern 12-string guitar, but they only have four or five courses of strings rather than six single strings normally used now. They were more often used as rhythm instruments in ensembles than as solo instruments, and can often be seen in that role in early music performances. (Gaspar Sanz's "Instrucción de Música sobre la Guitarra Española" of 1674 contains his whole output for the solo guitar.) Renaissance and Baroque guitars are easily distinguished, because the Renaissance guitar is very plain and the Baroque guitar is very ornate, with ivory or wood inlays all over the neck and body, and a paper-cutout inverted "wedding cake" inside the hole.
Classical guitars, also known as "Spanish" guitars, are typically strung with nylon strings, plucked with the fingers, played in a seated position and are used to play a diversity of musical styles including classical music. The classical guitar's wide, flat neck allows the musician to play scales, arpeggios, and certain chord forms more easily and with less adjacent string interference than on other styles of guitar. Flamenco guitars are very similar in construction, but they are associated with a more percussive tone. In Portugal, the same instrument is often used with steel strings particularly in its role within fado music. The guitar is called viola, or violão in Brazil, where it is often used with an extra seventh string by choro musicians to provide extra bass support.
In Mexico, the popular mariachi band includes a range of guitars, from the small "requinto" to the "guitarrón," a guitar larger than a cello, which is tuned in the bass register. In Colombia, the traditional quartet includes a range of instruments too, from the small "bandola" (sometimes known as the Deleuze-Guattari, for use when traveling or in confined rooms or spaces), to the slightly larger tiple, to the full-sized classical guitar. The requinto also appears in other Latin-American countries as a complementary member of the guitar family, with its smaller size and scale, permitting more projection for the playing of single-lined melodies. Modern dimensions of the classical instrument were established by the Spaniard Antonio de Torres Jurado (1817–1892).
Flat-top or steel-string guitars are similar to the classical guitar, however, within the varied sizes of the steel-stringed guitar the body size is usually significantly larger than a classical guitar, and has a narrower, reinforced neck and stronger structural design. The robust X-bracing typical of the steel-string was developed in the 1840s by German-American luthiers, of whom Christian Friedrich "C. F." Martin is the best known. Originally used on gut-strung instruments, the strength of the system allowed the guitar to withstand the additional tension of steel strings when this fortunate combination arose in the early 20th century. The steel strings produce a brighter tone, and according to many players, a louder sound. The acoustic guitar is used in many kinds of music including folk, country, bluegrass, pop, jazz, and blues. Many variations are possible from the roughly classical-sized OO and Parlour to the large Dreadnought (the most commonly available type) and Jumbo. Ovation makes a modern variation, with a rounded back/side assembly molded from artificial materials.
Archtop guitars are steel-string instruments in which the top (and often the back) of the instrument are carved, from a solid billet, into a curved, rather than a flat, shape. This violin-like construction is usually credited to the American Orville Gibson. Lloyd Loar of the Gibson Mandolin-Guitar Mfg. Co introduced the violin-inspired "F"-shaped hole design now usually associated with archtop guitars, after designing a style of mandolin of the same type. The typical archtop guitar has a large, deep, hollow body whose form is much like that of a mandolin or a violin-family instrument. Nowadays, most archtops are equipped with magnetic pickups, and they are therefore both acoustic and electric. F-hole archtop guitars were immediately adopted, upon their release, by both jazz and country musicians, and have remained particularly popular in jazz music, usually with flatwound strings.
All three principal types of resonator guitars were invented by the Slovak-American John Dopyera (1893–1988) for the National and Dobro (Dopyera Brothers) companies. Similar to the flat top guitar in appearance, but with a body that may be made of brass, nickel-silver, or steel as well as wood, the sound of the resonator guitar is produced by one or more aluminum resonator cones mounted in the middle of the top. The physical principle of the guitar is therefore similar to the loudspeaker.
The original purpose of the resonator was to produce a very loud sound; this purpose has been largely superseded by electrical amplification, but the resonator guitar is still played because of its distinctive tone. Resonator guitars may have either one or three resonator cones. The method of transmitting sound resonance to the cone is either a "biscuit" bridge, made of a small piece of hardwood at the vertex of the cone (Nationals), or a "spider" bridge, made of metal and mounted around the rim of the (inverted) cone (Dobros). Three-cone resonators always use a specialized metal bridge. The type of resonator guitar with a neck with a square cross-section—called "square neck" or "Hawaiian"—is usually played face up, on the lap of the seated player, and often with a metal or glass slide. The round neck resonator guitars are normally played in the same fashion as other guitars, although slides are also often used, especially in blues.
The twelve-string guitar usually has steel strings, and it is widely used in folk music, blues, and rock and roll. Rather than having only six strings, the 12-string guitar has six courses made up of two strings each, like a mandolin or lute. The highest two courses are tuned in unison, while the others are tuned in octaves. The 12-string guitar is also made in electric forms. The chime-like sound of the 12-string electric guitar was the basis of jangle pop.
The acoustic bass guitar is a bass instrument with a hollow wooden body similar to, though usually somewhat larger than, that of a 6-string acoustic guitar. Like the traditional electric bass guitar and the double bass, the acoustic bass guitar commonly has four strings, which are normally tuned E-A-D-G, an octave below the lowest four strings of the 6-string guitar, which is the same tuning pitch as an electric bass guitar. It can, more rarely, be found with 5 or 6 strings, which provides a wider range of notes to be played with less movement up and down the neck.
Electric guitars can have solid, semi-hollow, or hollow bodies; solid bodies produce little sound without amplification. Electromagnetic pickups, and sometimes piezoelectric pickups, convert the vibration of the steel strings into signals, which are fed to an amplifier through a patch cable or radio transmitter. The sound is frequently modified by other electronic devices (effects units) or the natural distortion of valves (vacuum tubes) or the pre-amp in the amplifier. There are two main types of magnetic pickups, single- and double-coil (or humbucker), each of which can be passive or active. The electric guitar is used extensively in jazz, blues, R & B, and rock and roll. The first successful magnetic pickup for a guitar was invented by George Beauchamp, and incorporated into the 1931 Ro-Pat-In (later Rickenbacker) "Frying Pan" lap steel; other manufacturers, notably Gibson, soon began to install pickups in archtop models. After World War II the completely solid-body electric was popularized by Gibson in collaboration with Les Paul, and independently by Leo Fender of Fender Music. The lower fretboard action (the height of the strings from the fingerboard), lighter (thinner) strings, and its electrical amplification lend the electric guitar to techniques less frequently used on acoustic guitars. These include tapping, extensive use of legato through pull-offs and hammer-ons (also known as slurs), pinch harmonics, volume swells, and use of a tremolo arm or effects pedals.
Some electric guitar models feature piezoelectric pickups, which function as transducers to provide a sound closer to that of an acoustic guitar with the flip of a switch or knob, rather than switching guitars. Those that combine piezoelectric pickups and magnetic pickups are sometimes known as hybrid guitars.
Hybrids of acoustic and electric guitars are also common. There are also more exotic varieties, such as guitars with two, three, or rarely four necks, all manner of alternate string arrangements, fretless fingerboards (used almost exclusively on bass guitars, meant to emulate the sound of a stand-up bass), 5.1 surround guitar, and such.
Solid body seven-string guitars were popularized in the 1980s and 1990s. Other artists go a step further, by using an eight-string guitar with two extra low strings. Although the most common seven-string has a low B string, Roger McGuinn (of The Byrds and Rickenbacker) uses an octave G string paired with the regular G string as on a 12-string guitar, allowing him to incorporate chiming 12-string elements in standard six-string playing. In 1982 Uli Jon Roth developed the "Sky Guitar", with a vastly extended number of frets, which was the first guitar to venture into the upper registers of the violin. Roth's seven-string and "Mighty Wing" guitar features a wider octave range.
The bass guitar (also called an "electric bass", or simply a "bass") is similar in appearance and construction to an electric guitar, but with a longer neck and scale length, and four to six strings. The four-string bass, by far the most common, is usually tuned the same as the double bass, which corresponds to pitches one octave lower than the four lowest pitched strings of a guitar (E, A, D, and G). The bass guitar is a transposing instrument, as it is notated in bass clef an octave higher than it sounds (as is the double bass) to avoid excessive ledger lines being required below the staff. Like the electric guitar, the bass guitar has pickups and it is plugged into an amplifier and speaker for live performances.
Modern guitars can be constructed to suit both left- and right-handed players. Normally, the dominant hand (in most people, the right hand) is used to pluck or strum the strings. This is similar to the convention of the violin family of instruments where the right hand controls the bow.
Left-handed players sometimes choose an opposite-handed (mirror) instrument, although some play in a standard-handed manner, others play a standard-handed guitar reversed, and still others (for example Jimi Hendrix) played a standard-handed guitar strung in reverse. This last configuration differs from a true opposite handed guitar in that the saddle is normally angled in such a way that the bass strings are slightly longer than the treble strings to improve intonation. Reversing the strings, therefore, reverses the relative orientation of the saddle, adversely affecting intonation, although in Hendrix's case, this is believed to have been an important element in his unique sound.
The headstock is located at the end of the guitar neck farthest from the body. It is fitted with machine heads that adjust the tension of the strings, which in turn affects the pitch. The traditional tuner layout is "3+3", in which each side of the headstock has three tuners (such as on Gibson Les Pauls). In this layout, the headstocks are commonly symmetrical. Many guitars feature other layouts, including six-in-line tuners (featured on Fender Stratocasters) or even "4+2" (e.g. Ernie Ball Music Man). Some guitars (such as Steinbergers) do not have headstocks at all, in which case the tuning machines are located elsewhere, either on the body or the bridge.
The nut is a small strip of bone, plastic, brass, corian, graphite, stainless steel, or other medium-hard material, at the joint where the headstock meets the fretboard. Its grooves guide the strings onto the fretboard, giving consistent lateral string placement. It is one of the endpoints of the strings' vibrating length. It must be accurately cut, or it can contribute to tuning problems due to string slippage or string buzz. To reduce string friction in the nut, which can adversely affect tuning stability, some guitarists fit a roller nut. Some instruments use a zero fret just in front of the nut. In this case the nut is used only for lateral alignment of the strings, the string height and length being dictated by the zero fret.
A guitar's frets, fretboard, tuners, headstock, and truss rod, all attached to a long wooden extension, collectively constitute its neck. The wood used to make the fretboard usually differs from the wood in the rest of the neck. The bending stress on the neck is considerable, particularly when heavier gauge strings are used (see Tuning), and the ability of the neck to resist bending (see Truss rod) is important to the guitar's ability to hold a constant pitch during tuning or when strings are fretted. The rigidity of the neck with respect to the body of the guitar is one determinant of a good instrument versus a poor-quality one.
The shape of the neck (from a cross-sectional perspective) can also vary, from a gentle "C" curve to a more pronounced "V" curve. There are many different types of neck profiles available, giving the guitarist many options. Some aspects to consider in a guitar neck may be the overall width of the fretboard, scale (distance between the frets), the neck wood, the type of neck construction (for example, the neck may be glued in or bolted on), and the shape (profile) of the back of the neck. Other types of material used to make guitar necks are graphite (Steinberger guitars), aluminum (Kramer Guitars, Travis Bean and Veleno guitars), or carbon fiber (Modulus Guitars and ThreeGuitars). Double neck electric guitars have two necks, allowing the musician to quickly switch between guitar sounds.
The neck joint or heel is the point at which the neck is either bolted or glued to the body of the guitar. Almost all acoustic steel-string guitars, with the primary exception of Taylors, have glued (otherwise known as set) necks, while electric guitars are constructed using both types. Most classical guitars have a neck and headblock carved from one piece of wood, known as a "Spanish heel." Commonly used set neck joints include mortise and tenon joints (such as those used by C. F. Martin & Co.), dovetail joints (also used by C. F. Martin on the D-28 and similar models) and Spanish heel neck joints, which are named after the shoe they resemble and commonly found in classical guitars. All three types offer stability.
Bolt-on necks, though they are historically associated with cheaper instruments, do offer greater flexibility in the guitar's set-up, and allow easier access for neck joint maintenance and repairs. Another type of neck, only available for solid body electric guitars, is the neck-through-body construction. These are designed so that everything from the machine heads down to the bridge are located on the same piece of wood. The sides (also known as wings) of the guitar are then glued to this central piece. Some luthiers prefer this method of construction as they claim it allows better sustain of each note. Some instruments may not have a neck joint at all, having the neck and sides built as one piece and the body built around it.
The fingerboard, also called the fretboard, is a piece of wood embedded with metal frets that comprises the top of the neck. It is flat on classical guitars and slightly curved crosswise on acoustic and electric guitars. The curvature of the fretboard is measured by the fretboard radius, which is the radius of a hypothetical circle of which the fretboard's surface constitutes a segment. The smaller the fretboard radius, the more noticeably curved the fretboard is. Most modern guitars feature a 12" neck radius, while older guitars from the 1960s and 1970s usually feature a 6-8" neck radius. Pinching a string against a fret on the fretboard effectively shortens the vibrating length of the string, producing a higher pitch.
Fretboards are most commonly made of rosewood, ebony, maple, and sometimes manufactured using composite materials such as HPL or resin. See the section "Neck" below for the importance of the length of the fretboard in connection to other dimensions of the guitar. The fingerboard plays an essential role in the treble tone for acoustic guitars. The quality of vibration of the fingerboard is the principal characteristic for generating the best treble tone. For that reason, ebony wood is better, but because of high use, ebony has become rare and extremely expensive. Most guitar manufacturers have adopted rosewood instead of ebony.
Almost all guitars have frets, which are metal strips (usually nickel alloy or stainless steel) embedded along the fretboard and located at exact points that divide the scale length in accordance with a specific mathematical formula. The exceptions include fretless bass guitars and very rare fretless guitars. Pressing a string against a fret determines the strings' vibrating length and therefore its resultant pitch. The pitch of each consecutive fret is defined at a half-step interval on the chromatic scale. Standard classical guitars have 19 frets and electric guitars between 21 and 24 frets, although guitars have been made with as many as 27 frets. Frets are laid out to accomplish an equal tempered division of the octave. Each set of twelve frets represents an octave. The twelfth fret divides the scale length exactly into two halves, and the 24th fret position divides one of those halves in half again.
The ratio of the spacing of two consecutive frets is formula_1 (twelfth root of two). In practice, luthiers determine fret positions using the constant 17.817—an approximation to 1/(1-1/formula_1). If the nth fret is a distance x from the bridge, then the distance from the (n+1)th fret to the bridge is x-(x/17.817). Frets are available in several different gauges and can be fitted according to player preference. Among these are "jumbo" frets, which have a much thicker gauge, allowing for use of a slight vibrato technique from pushing the string down harder and softer. "Scalloped" fretboards, where the wood of the fretboard itself is "scooped out" between the frets, allow a dramatic vibrato effect. Fine frets, much flatter, allow a very low string-action, but require that other conditions, such as curvature of the neck, be well-maintained to prevent buzz.
The truss rod is a thin, strong metal rod that runs along the inside of the neck. It is used to correct changes to the neck's curvature caused by aging of the neck timbers, changes in humidity, or to compensate for changes in the tension of strings. The tension of the rod and neck assembly is adjusted by a hex nut or an allen-key bolt on the rod, usually located either at the headstock, sometimes under a cover, or just inside the body of the guitar underneath the fretboard and accessible through the sound hole. Some truss rods can only be accessed by removing the neck. The truss rod counteracts the immense amount of tension the strings place on the neck, bringing the neck back to a straighter position. Turning the truss rod clockwise tightens it, counteracting the tension of the strings and straightening the neck or creating a backward bow. Turning the truss rod counter-clockwise loosens it, allowing string tension to act on the neck and creating a forward bow.
Adjusting the truss rod affects the intonation of a guitar as well as the height of the strings from the fingerboard, called the action. Some truss rod systems, called "double action" truss systems, tighten both ways, pushing the neck both forward and backward (standard truss rods can only release to a point beyond which the neck is no longer compressed and pulled backward). The artist and luthier Irving Sloane pointed out, in his book "Steel-String Guitar Construction," that truss rods are intended primarily to remedy concave bowing of the neck, but cannot correct a neck with "back bow" or one that has become twisted. Classical guitars do not require truss rods, as their nylon strings exert a lower tensile force with lesser potential to cause structural problems. However, their necks are often reinforced with a strip of harder wood, such as an ebony strip that runs down the back of a cedar neck. There is no tension adjustment on this form of reinforcement.
Inlays are visual elements set into the exterior surface of a guitar, both for decoration and artistic purposes and, in the case of the markings on the 3rd, 5th, 7th and 12th fret (and in higher octaves), to provide guidance to the performer about the location of frets on the instrument. The typical locations for inlay are on the fretboard, headstock, and on acoustic guitars around the soundhole, known as the rosette. Inlays range from simple plastic dots on the fretboard to intricate works of art covering the entire exterior surface of a guitar (front and back). Some guitar players have used LEDs in the fretboard to produce unique lighting effects onstage. Fretboard inlays are most commonly shaped like dots, diamond shapes, parallelograms, or large blocks in between the frets.
Dots are usually inlaid into the upper edge of the fretboard in the same positions, small enough to be visible only to the player. These usually appear on the odd numbered frets, but also on the 12th fret (the one octave mark) instead of the 11th and 13th frets. Some older or high-end instruments have inlays made of mother of pearl, abalone, ivory, colored wood or other exotic materials and designs. Simpler inlays are often made of plastic or painted. High-end classical guitars seldom have fretboard inlays as a well-trained player is expected to know his or her way around the instrument. In addition to fretboard inlay, the headstock and soundhole surround are also frequently inlaid. The manufacturer's logo or a small design is often inlaid into the headstock. Rosette designs vary from simple concentric circles to delicate fretwork mimicking the historic rosette of lutes. Bindings that edge the finger and sound boards are sometimes inlaid. Some instruments have a filler strip running down the length and behind the neck, used for strength or to fill the cavity through which the truss rod was installed in the neck.
In acoustic guitars, string vibration is transmitted through the bridge and saddle to the body via sound board. The sound board is typically made of tone woods such as spruce or cedar. Timbers for tone woods are chosen for both strength and ability to transfer mechanical energy from the strings to the air within the guitar body. Sound is further shaped by the characteristics of the guitar body's resonant cavity. In expensive instruments, the entire body is made of wood. In inexpensive instruments, the back may be made of plastic.
In an acoustic instrument, the body of the guitar is a major determinant of the overall sound quality. The guitar top, or soundboard, is a finely crafted and engineered element made of tonewoods such as spruce and red cedar. This thin piece of wood, often only 2 or 3 mm thick, is strengthened by differing types of internal bracing. Many luthiers consider the top the dominant factor in determining the sound quality. The majority of the instrument's sound is heard through the vibration of the guitar top as the energy of the vibrating strings is transferred to it. The body of an acoustic guitar has a sound hole through which sound projects. The sound hole is usually a round hole in the top of the guitar under the strings. Air inside the body vibrates as the guitar top and body is vibrated by the strings, and the response of the air cavity at different frequencies is characterized, like the rest of the guitar body, by a number of resonance modes at which it responds more strongly.
The top, back and ribs of an acoustic guitar body are very thin (1–2 mm), so a flexible piece of wood called lining is glued into the corners where the rib meets the top and back. This interior reinforcement provides 5 to 20 mm of solid gluing area for these corner joints. Solid linings are often used in classical guitars, while kerfed lining is most often found in steel string acoustics. Kerfed lining is also called kerfing because it is scored, or "kerfed"(incompletely sawn through), to allow it to bend with the shape of the rib). During final construction, a small section of the outside corners is carved or routed out and filled with binding material on the outside corners and decorative strips of material next to the binding, which are called purfling. This binding serves to seal off the end grain of the top and back. Purfling can also appear on the back of an acoustic guitar, marking the edge joints of the two or three sections of the back. Binding and purfling materials are generally made of either wood or plastic.
Body size, shape and style has changed over time. 19th century guitars, now known as salon guitars, were smaller than modern instruments. Differing patterns of internal bracing have been used over time by luthiers. Torres, Hauser, Ramirez, Fleta, and C. F. Martin were among the most influential designers of their time. Bracing not only strengthens the top against potential collapse due to the stress exerted by the tensioned strings, but also affects the resonance characteristics of the top. The back and sides are made out of a variety of timbers such as mahogany, Indian rosewood and highly regarded Brazilian rosewood ("Dalbergia nigra"). Each one is primarily chosen for their aesthetic effect and can be decorated with inlays and purfling.
Instruments with larger areas for the guitar top were introduced by Martin in an attempt to create greater volume levels. The popularity of the larger "dreadnought" body size amongst acoustic performers is related to the greater sound volume produced.
Most electric guitar bodies are made of wood and include a plastic pick guard. Boards wide enough to use as a solid body are very expensive due to the worldwide depletion of hardwood stock since the 1970s, so the wood is rarely one solid piece. Most bodies are made from two pieces of wood with some of them including a seam running down the center line of the body. The most common woods used for electric guitar body construction include maple, basswood, ash, poplar, alder, and mahogany. Many bodies consist of good-sounding, but inexpensive woods, like ash, with a "top", or thin layer of another, more attractive wood (such as maple with a natural "flame" pattern) glued to the top of the basic wood. Guitars constructed like this are often called "flame tops". The body is usually carved or routed to accept the other elements, such as the bridge, pickup, neck, and other electronic components. Most electrics have a polyurethane or nitrocellulose lacquer finish. Other alternative materials to wood are used in guitar body construction. Some of these include carbon composites, plastic material, such as polycarbonate, and aluminum alloys.
The main purpose of the bridge on an acoustic guitar is to transfer the vibration from the strings to the soundboard, which vibrates the air inside of the guitar, thereby amplifying the sound produced by the strings. On all electric, acoustic and original guitars, the bridge holds the strings in place on the body. There are many varied bridge designs. There may be some mechanism for raising or lowering the bridge saddles to adjust the distance between the strings and the fretboard (action), or fine-tuning the intonation of the instrument. Some are spring-loaded and feature a "whammy bar", a removable arm that lets the player modulate the pitch by changing the tension on the strings. The whammy bar is sometimes also called a "tremolo bar". (The effect of rapidly changing pitch is properly called "vibrato". See Tremolo for further discussion of this term.) Some bridges also allow for alternate tunings at the touch of a button.
On almost all modern electric guitars, the bridge has saddles that are adjustable for each string so that intonation stays correct up and down the neck. If the open string is in tune, but sharp or flat when frets are pressed, the bridge saddle position can be adjusted with a screwdriver or hex key to remedy the problem. In general, flat notes are corrected by moving the saddle forward and sharp notes by moving it backwards. On an instrument correctly adjusted for intonation, the actual length of each string from the nut to the bridge saddle is slightly, but measurably longer than the scale length of the instrument. This additional length is called compensation, which flattens all notes a bit to compensate for the sharping of all fretted notes caused by stretching the string during fretting.
The saddle of a guitar refers to the part of the bridge that physically supports the strings. It may be one piece (typically on acoustic guitars) or separate pieces, one for each string (electric guitars and basses). The saddle's basic purpose is to provide the end point for the string's vibration at the correct location for proper intonation, and on acoustic guitars to transfer the vibrations through the bridge into the top wood of the guitar. Saddles are typically made of plastic or bone for acoustic guitars, though synthetics and some exotic animal tooth variations (e.g. fossilized tooth, ivory, etc. ) have become popular with some players. Electric guitar saddles are typically metal, though some synthetic saddles are available.
The pickguard, also known as the scratchplate, is usually a piece of laminated plastic or other material that protects the finish of the top of the guitar from damage due to the use of a plectrum ("pick") or fingernails. Electric guitars sometimes mount pickups and electronics on the pickguard. It is a common feature on steel-string acoustic guitars. Some performance styles that use the guitar as a percussion instrument (tapping the top or sides between notes, etc.), such as flamenco, require that a scratchplate or pickguard be fitted to nylon-string instruments.
The standard guitar has six strings, but four-, seven-, eight-, nine-, ten-, eleven-, twelve-, thirteen- and eighteen-string guitars are also available. Classical and flamenco guitars historically used gut strings, but these have been superseded by polymer materials, such as nylon and fluorocarbon. Modern guitar strings are constructed from metal, polymers, or animal or plant product materials. Instruments utilizing "steel" strings may have strings made from alloys incorporating steel, nickel or phosphor bronze. Bass strings for both instruments are wound rather than monofilament.
Pickups are transducers attached to a guitar that detect (or "pick up") string vibrations and convert the mechanical energy of the string into electrical energy. The resultant electrical signal can then be electronically amplified. The most common type of pickup is electromagnetic in design. These contain magnets that are within a coil, or coils, of copper wire. Such pickups are usually placed directly underneath the guitar strings. Electromagnetic pickups work on the same principles and in a similar manner to an electric generator. The vibration of the strings creates a small electric current in the coils surrounding the magnets. This signal current is carried to a guitar amplifier that drives a loudspeaker.
Traditional electromagnetic pickups are either single-coil or double-coil. Single-coil pickups are susceptible to noise induced by stray electromagnetic fields, usually mains-frequency (60 or 50 hertz) hum. The introduction of the double-coil humbucker in the mid-1950s solved this problem through the use of two coils, one of which is wired in opposite polarity to cancel or "buck" stray fields.
The types and models of pickups used can greatly affect the tone of the guitar. Typically, humbuckers, which are two magnet-coil assemblies attached to each other, are traditionally associated with a heavier sound. Single-coil pickups, one magnet wrapped in copper wire, are used by guitarists seeking a brighter, twangier sound with greater dynamic range.
Modern pickups are tailored to the sound desired. A commonly applied approximation used in selection of a pickup is that less wire (lower electrical impedance) gives brighter sound, more wire gives a "fat" tone. Other options include specialized switching that produces coil-splitting, in/out of phase and other effects. Guitar circuits are either active, needing a battery to power their circuit, or, as in most cases, equipped with a passive circuit.
Fender Stratocaster-type guitars generally utilize three single-coil pickups, while most Gibson Les Paul types use humbucker pickups.
Piezoelectric, or piezo, pickups represent another class of pickup. These employ piezoelectricity to generate the musical signal and are popular in hybrid electro-acoustic guitars. A crystal is located under each string, usually in the saddle. When the string vibrates, the shape of the crystal is distorted, and the stresses associated with this change produce tiny voltages across the crystal that can be amplified and manipulated. Piezo pickups usually require a powered pre-amplifier to lift their output to match that of electromagnetic pickups. Power is typically delivered by an on-board battery.
Most pickup-equipped guitars feature onboard controls, such as volume or tone, or pickup selection. At their simplest, these consist of passive components, such as potentiometers and capacitors, but may also include specialized integrated circuits or other active components requiring batteries for power, for preamplification and signal processing, or even for electronic tuning. In many cases, the electronics have some sort of shielding to prevent pickup of external interference and noise.
Guitars may be shipped or retrofitted with a hexaphonic pickup, which produces a separate output for each string, usually from a discrete piezoelectric or magnetic pickup. This arrangement lets on-board or external electronics process the strings individually for modeling or Musical Instrument Digital Interface (MIDI) conversion. Roland makes "GK" hexaphonic pickups for guitar and bass, and a line of guitar modeling and synthesis products. Line 6's hexaphonic-equipped Variax guitars use on-board electronics to model the sound after various vintage instruments, and vary pitch on individual strings.
MIDI converters use a hexaphonic guitar signal to determine pitch, duration, attack, and decay characteristics. The MIDI sends the note information to an internal or external sound bank device. The resulting sound closely mimics numerous instruments. The MIDI setup can also let the guitar be used as a game controller (i.e., Rock Band Squier) or as an instructional tool, as with the Fretlight Guitar.
Notationally, the guitar is considered a transposing instrument. Its pitch sounds one octave lower than it is notated on a score.
A variety of tunings may be used. The most common tuning, known as "Standard Tuning", has the strings tuned from a low E, to a high E, traversing a two octave range—EADGBE. When all strings are played open the resulting chord is an Em7/add11.
The pitches are as follows:
The table below shows a pitch's name found over the six strings of a guitar in standard tuning, from the nut (zero), to the twelfth fret.
For four strings, the 5th fret on one string is the same open-note as the next string; for example, a 5th-fret note on the sixth string is the same note as the open fifth string. However, between the second and third strings, an irregularity occurs: The "4th"-fret note on the third string is equivalent to the open second string.
Standard tuning has evolved to provide a good compromise between simple fingering for many chords and the ability to play common scales with reasonable left-hand movement. There are also a variety of commonly used alternative tunings, for example, the classes of "open", "regular", and "dropped" tunings.
"Open tuning" refers to a guitar tuned so that strumming the open strings produces a chord, typically a major chord. The base chord consists of at least 3 notes and may include all the strings or a subset. The tuning is named for the open chord, Open D, open G, and open A are popular tunings. All similar chords in the chromatic scale can then be played by barring a single fret. Open tunings are common in blues music and folk music, and they are used in the playing of slide and bottleneck guitars. Many musicians use open tunings when playing slide guitar.
For the standard tuning, there is exactly one interval of a major third between the second and third strings, and all the other intervals are fourths. The irregularity has a price – chords cannot be shifted around the fretboard in the standard tuning E-A-D-G-B-E, which requires four chord-shapes for the major chords. There are separate chord-forms for chords having their root note on the third, fourth, fifth, and sixth strings.
In contrast, "regular" tunings have equal intervals between the strings, and so they have symmetrical scales all along the fretboard. This makes it simpler to translate chords. For the regular tunings, chords may be moved diagonally around the fretboard. The diagonal movement of chords is especially simple for the regular tunings that are repetitive, in which case chords can be moved vertically: Chords can be moved three strings up (or down) in major-thirds tuning and chords can be moved two strings up (or down) in augmented-fourths tuning. Regular tunings thus appeal to new guitarists and also to jazz-guitarists, whose improvisation is simplified by regular intervals.
On the other hand, some chords are more difficult to play in a regular tuning than in standard tuning. It can be difficult to play conventional chords, especially in augmented-fourths tuning and all-fifths tuning, in which the large spacings require hand stretching. Some chords, which are conventional in folk music, are difficult to play even in all-fourths and major-thirds tunings, which do not require more hand-stretching than standard tuning.
Another class of alternative tunings are called drop tunings, because the tuning "drops down" the lowest string. Dropping down the lowest string a whole tone results in the "drop-D" (or "dropped D") tuning. Its open-string notes DADGBE (from low to high) allow for a deep bass D note, which can be used in keys such as D major, d minor and G major. It simplifies the playing of simple fifths (powerchords). Many contemporary rock bands re-tune all strings down, making, for example, Drop-C or Drop-B tunings.
Many scordatura (alternate tunings) modify the standard tuning of the lute, especially when playing Renaissance music repertoire originally written for that instrument. Some scordatura drop the pitch of one or more strings, giving access to new lower notes. Some scordatura make it easier to play in unusual keys.
Though a guitar may be played on its own, there are a variety of common accessories used for holding and playing the guitar.
A capo (short for "capotasto") is used to change the pitch of open strings. Capos are clipped onto the fretboard with the aid of spring tension or, in some models, elastic tension. To raise the guitar's pitch by one semitone, the player would clip the capo onto the fretboard just below the first fret. Its use allows players to play in different keys without having to change the chord formations they use. For example, if a folk guitar player wanted to play a song in the key of B Major, they could put a capo on the second fret of the instrument, and then play the song as if it were in the key of A Major, but with the capo the instrument would make the sounds of B Major. This is because with the capo barring the entire second fret, open chords would all sound two semitones (in other words, one tone) higher in pitch. For example, if a guitarist played an open A Major chord (a very common open chord), it would sound like a B Major chord. All of the other open chords would be similarly modified in pitch. Because of the ease with which they allow guitar players to change keys, they are sometimes referred to with pejorative names, such as "cheaters" or the "hillbilly crutch". Despite this negative viewpoint, another benefit of the capo is that it enables guitarists to obtain the ringing, resonant sound of the common keys (C, G, A, etc.) in "harder" and less-commonly used keys. Classical performers are known to use them to enable modern instruments to match the pitch of historical instruments such as the Renaissance music lute.
A slide, (neck of a bottle, knife blade or round metal or glass bar or cylinder) is used in blues and rock to create a glissando or "Hawaiian" effect. The slide is used to fret notes on the neck, instead of using the fretting hand's fingers. The characteristic use of the slide is to move up to the intended pitch by, as the name implies, sliding up the neck to the desired note. The necks of bottles were often used in blues and country music as improvised slides. Modern slides are constructed of glass, plastic, ceramic, chrome, brass or steel bars or cylinders, depending on the weight and tone desired (and the amount of money a guitarist can spend). An instrument that is played exclusively in this manner (using a metal bar) is called a steel guitar or pedal steel. Slide playing to this day is very popular in blues music and country music. Some slide players use a so-called Dobro guitar. Some performers who have become famous for playing slide are Robert Johnson, Elmore James, Ry Cooder, George Harrison, Bonnie Raitt, Derek Trucks, Warren Haynes, Duane Allman, Muddy Waters, Rory Gallagher, and George Thorogood.
A "guitar pick" or "plectrum" is a small piece of hard material generally held between the thumb and first finger of the picking hand and is used to "pick" the strings. Though most classical players pick with a combination of fingernails and fleshy fingertips, the pick is most often used for electric and steel-string acoustic guitars. Though today they are mainly plastic, variations do exist, such as bone, wood, steel or tortoise shell. Tortoise shell was the most commonly used material in the early days of pick-making, but as tortoises and turtles became endangered, the practice of using their shells for picks or anything else was banned. Tortoise-shell picks made before the ban are often coveted for a supposedly superior tone and ease of use, and their scarcity has made them valuable.
Picks come in many shapes and sizes. Picks vary from the small jazz pick to the large bass pick. The thickness of the pick often determines its use. A thinner pick (between 0.2 and 0.5 mm) is usually used for strumming or rhythm playing, whereas thicker picks (between 0.7 and 1.5+ mm) are usually used for single-note lines or lead playing. The distinctive guitar sound of Billy Gibbons is attributed to using a quarter or peso as a pick. Similarly, Brian May is known to use a sixpence coin as a pick, while noted 1970s and early 1980s session musician David Persons is known for using old credit cards, cut to the correct size, as plectrums.
Thumb picks and finger picks that attach to the finger tips are sometimes employed in finger-picking styles on steel strings. These allow the fingers and thumb to operate independently, whereas a flat pick requires the thumb and one or two fingers to manipulate.
A guitar strap is a strip of material with an attachment mechanism on each end, made to hold a guitar via the shoulders at an adjustable length. Guitars have varying accommodations for attaching a strap. The most common are strap buttons, also called strap pins, which are flanged steel posts anchored to the guitar with screws. Two strap buttons come pre-attached to virtually all electric guitars, and many steel-string acoustic guitars. Strap buttons are sometimes replaced with "strap locks", which connect the guitar to the strap more securely.
The lower strap button is usually located at the bottom (bridge end) of the body. The upper strap button is usually located near or at the top (neck end) of the body: on the upper body curve, at the tip of the upper "horn" (on a double cutaway), or at the neck joint (heel). Some electrics, especially those with odd-shaped bodies, have one or both strap buttons on the back of the body. Some Steinberger electric guitars, owing to their minimalist and lightweight design, have both strap buttons at the bottom of the body. Rarely, on some acoustics, the upper strap button is located on the headstock. Some acoustic and classical guitars only have a single strap button at the bottom of the body—the other end must be tied onto the headstock, above the nut and below the machine heads.
Electric guitars and bass guitars have to be used with a guitar amplifier and loudspeaker or a bass amplifier and speaker, respectively, in order to make enough sound to be heard by the performer and audience. Electric guitars and bass guitars almost always use magnetic pickups, which generate an electric signal when the musician plucks, strums or otherwise plays the instrument. The amplifier and speaker strengthen this signal using a power amplifier and a loudspeaker. Acoustic guitars that are equipped with a piezoelectric pickup or microphone can also be plugged into an instrument amplifier, acoustic guitar amp or PA system to make them louder. With electric guitar and bass, the amplifier and speaker are not just used to make the instrument louder; by adjusting the equalizer controls, the preamplifier, and any onboard effects units (reverb, distortion/overdrive, etc.) the player can also modify the tone (also called the timbre or "colour") and sound of the instrument. Acoustic guitar players can also use the amp to change the sound of their instrument, but in general, acoustic guitar amps are used to make the natural acoustic sound of the instrument louder without significantly changing its sound.

</doc>
<doc id="11856" url="https://en.wikipedia.org/wiki?curid=11856" title="Gnutella">
Gnutella

Gnutella is a large peer-to-peer network. It was the first decentralized peer-to-peer network of its kind, leading to other, later networks adopting the model. It celebrated two decades of existence on March 14, 2020, and has a user base in the millions for peer-to-peer file sharing.
In June 2005, Gnutella's population was 1.81 million computers increasing to over three million nodes by January 2006. In late 2007, it was the most popular file-sharing network on the Internet with an estimated market share of more than 40%.
The first client (also called Gnutella) from which the network got its name was developed by Justin Frankel and Tom Pepper of Nullsoft in early 2000, soon after the company's acquisition by AOL. On March 14, the program was made available for download on Nullsoft's servers. The event was prematurely announced on Slashdot, and thousands downloaded the program that day. The source code was to be released later, under the GNU General Public License (GPL); however, the original developers never got the chance to accomplish this purpose.
The next day, AOL stopped the availability of the program over legal concerns and restrained Nullsoft from doing any further work on the project. This did not stop Gnutella; after a few days, the protocol had been reverse engineered, and compatible free and open source clones began to appear. This parallel development of different clients by different groups remains the "modus operandi" of Gnutella development today.
Among the first independent Gnutella pioneers were Gene Kan and Spencer Kimball, they launched the first portal aimed to assemble the open-source community to work on Gnutella, and also developed "GNUbile", one of the first open-source (GNU-GPL) programs to implement the Gnutella protocol.
The Gnutella network is a fully distributed alternative to such semi-centralized systems as FastTrack (KaZaA) and the original Napster. The initial popularity of the network was spurred on by Napster's threatened legal demise in early 2001. This growing surge in popularity revealed the limits of the initial protocol's scalability. In early 2001, variations on the protocol (first implemented in proprietary and closed source clients) allowed an improvement in scalability. Instead of treating every user as client and server, some users were now treated as "ultrapeers", routing search requests and responses for users connected to them.
This allowed the network to grow in popularity. In late 2001, the Gnutella client LimeWire Basic became free and open source. In February 2002, Morpheus, a commercial file sharing group, abandoned its FastTrack-based peer-to-peer software and released a new client based on the free and open source Gnutella client Gnucleus.
The word "Gnutella" today refers not to any one project or piece of software, but to the open protocol used by the various clients.
The name is a portmanteau of "GNU" and "Nutella", the brand name of an Italian hazelnut flavored spread: supposedly, Frankel and Pepper ate a lot of Nutella working on the original project, and intended to license their finished program under the GNU General Public License. Gnutella is not associated with the GNU project or GNU's own peer-to-peer network, GNUnet.
On October 26, 2010, the popular Gnutella client LimeWire was ordered shut down by Judge Kimba Wood of the United States District Court for the Southern District of New York when she signed a Consent Decree to which recording industry plaintiffs and LimeWire had agreed. This event was the likely cause of a notable drop in the size of the network, because, while negotiating the injunction, LimeWire staff had inserted remote-disabling code into the software. As the injunction came into force, users who had installed affected versions (newer than 5.5.10) were cut off from the P2P network. Since LimeWire was free software, nothing had prevented the creation of forks that omitted the disabling code, as long as LimeWire trademarks were not used. The shutdown did not affect, for example, FrostWire, a fork of LimeWire created in 2004 that carries neither the remote-disabling code nor adware.
On November 9, 2010, LimeWire was resurrected by a secret team of developers and named LimeWire Pirate Edition. It was based on LimeWire 5.6 BETA. This version had its server dependencies removed and all the PRO features enabled for free.
To envision how Gnutella originally worked, imagine a large circle of users "(called nodes)," each of whom has Gnutella client software. On initial startup, the client software must bootstrap and find at least one other node. Various methods have been used for this, including a pre-existing address list of possibly working nodes shipped with the software, using updated web caches of known nodes (called "Gnutella Web Caches"), UDP host caches and, rarely, even IRC. Once connected, the client requests a list of working addresses. The client tries to connect to the nodes it was shipped with, as well as nodes it receives from other clients until it reaches a certain quota. It connects to only that many nodes, locally caching the addresses it has not yet tried and discards the addresses it tried that were invalid.
When the user wants to do a search, the client sends the request to each actively connected node. In version 0.4 of the protocol, the number of actively connected nodes for a client was quite small (around 5), so each node then forwarded the request to all its actively connected nodes, and they, in turn, forwarded the request, and so on, until the packet reached a predetermined number of "hops" from the sender (maximum 7).
Since version 0.6 (2002), Gnutella is a composite network made of leaf nodes and ultra nodes (also called ultrapeers). The leaf nodes are connected to a small number of ultrapeers (typically 3) while each ultrapeer is connected to more than 32 other ultrapeers. With this higher outdegree, the maximum number of "hops" a query can travel was lowered to 4.
Leaves and ultrapeers use the Query Routing Protocol to exchange a Query Routing Table (QRT), a table of 64 Ki-slots and up to 2 Mi-slots consisting of hashed keywords. A leaf node sends its QRT to each of the ultrapeers it is connected to, and ultrapeers merge the QRT of all their leaves (downsized to 128 Ki-slots) plus their own QRT (if they share files) and exchange that with their own neighbors. Query routing is then done by hashing the words of the query and seeing whether all of them match in the QRT. Ultrapeers do that check before forwarding a query to a leaf node, and also before forwarding the query to a peer ultra node provided this is the last hop the query can travel.
If a search request turns up a result, the node that has the result contacts the searcher. In the classic Gnutella protocol, response messages were sent back along the route the query came through, as the query itself did not contain identifying information of the node. This scheme was later revised, so that search results now are delivered over User Datagram Protocol (UDP) directly to the node that initiated the search, usually an ultrapeer of the node. Thus, in the current protocol, the queries carry the IP address and port number of either node. This lowers the amount of traffic routed through the Gnutella network, making it significantly more scalable.
If the user decides to download the file, they negotiate the file transfer. If the node which has the requested file is not firewalled, the querying node can connect to it directly. However, if the node is firewalled, stopping the source node from receiving incoming connections, the client wanting to download a file sends it a so-called "push request" to the server for the remote client to initiate the connection instead (to "push" the file). At first, these push requests were routed along the original chain it used to send the query. This was rather unreliable because routes would often break and routed packets are always subject to flow control. Therefore, so-called "push proxies" were introduced. These are usually the ultrapeers of a leaf node and they are announced in search results. The client connects to one of these "push proxies" using an HTTP request and the proxy sends a "push request" to leaf on behalf of the client. Normally, it is also possible to send a push request over UDP to the push proxy which is more efficient than using TCP. Push proxies have two advantages: First, ultrapeer-leaf connections are more stable than routes which makes push requests much more reliable. Second, it reduces the amount of traffic routed through the Gnutella network.
Finally, when a user disconnects, the client software saves the list of nodes that it was actively connected to and those collected from pong packets for use the next time it attempts to connect so that it becomes independent from any kind of bootstrap services.
In practice, this method of searching on the Gnutella network was often unreliable. Each node is a regular computer user; as such, they are constantly connecting and disconnecting, so the network is never completely stable. Also, the bandwidth cost of searching on Gnutella grew exponentially to the number of connected users, often saturating connections and rendering slower nodes useless. Therefore, search requests would often be dropped, and most queries reached only a very small part of the network. This observation identified the Gnutella network as an unscalable distributed system, and inspired the development of distributed hash tables, which are much more scalable but support only exact-match, rather than keyword, search.
To address the problems of bottlenecks, Gnutella developers implemented a tiered system of "ultrapeers" and "leaves". Instead of all nodes being considered equal, nodes entering into the network were kept at the 'edge' of the network as a leaf, not responsible for any routing, and nodes which were capable of routing messages were promoted to ultrapeers, which would accept leaf connections and route searches and network maintenance messages. This allowed searches to propagate further through the network, and allowed for numerous alterations in the topology which have improved the efficiency and scalability greatly.
Additionally, gnutella adopted a number of other techniques to reduce traffic overhead and make searches more efficient. Most notable are Query Routing Protocol (QRP) and Dynamic Querying (DQ). With QRP a search reaches only those clients which are likely to have the files, so rare files searches grow vastly more efficient, and with DQ the search stops as soon as the program has acquired enough search results, which vastly reduces the amount of traffic caused by popular searches. Gnutella For Users has a vast amount of information about these and other improvements to Gnutella in user-friendly style.
One of the benefits of having Gnutella so decentralized is to make it very difficult to shut the network down and to make it a network in which the users are the only ones who can decide which content will be available. Unlike Napster, where the entire network relied on the central server, Gnutella cannot be shut down by shutting down any one node. A decentralized network prevents bad actors from taking control of the contents of the network and/or manipulating data by controlling the central server.
Gnutella did once operate on a purely query flooding-based protocol. The outdated Gnutella version 0.4 network protocol employs five different packet types, namely
These are mainly concerned with searching the Gnutella network. File transfers are handled using HTTP.
The development of the Gnutella protocol is currently led by the Gnutella Developers Forum (The GDF). Many protocol extensions have been and are being developed by the software vendors and free Gnutella developers of the GDF. These extensions include intelligent query routing, SHA-1 checksums, query hit transmission via UDP, querying via UDP, dynamic queries via TCP, file transfers via UDP, XML metadata, source exchange (also termed "the download mesh") and parallel downloading in slices (swarming).
There are efforts to finalize these protocol extensions in the Gnutella 0.6 specification at the Gnutella protocol development website. The Gnutella 0.4 standard, although still being the latest protocol specification since all extensions only exist as proposals so far, is outdated. In fact, it is hard or impossible to connect today with the 0.4 handshakes and according to developers in the GDF, version 0.6 is what new developers should pursue using the work-in-progress specifications.
The Gnutella protocol remains under development and in spite of attempts to make a clean break with the complexity inherited from the old Gnutella 0.4 and to design a clean new message architecture, it is still one of the most successful file-sharing protocols to date.
The following tables compare general and technical information for a number of applications supporting the Gnutella network. The tables do not attempt to give a complete list of Gnutella clients. The tables are limited to clients that can participate in the current Gnutella network.
The Gnutella2 protocol (often referred to as G2), despite its name, is not a successor protocol of Gnutella nor related to the original Gnutella project, but rather is a completely different protocol that forked from the original project and piggybacked on the Gnutella name. A sore point with many Gnutella developers is that the "Gnutella2" name conveys an upgrade or superiority, which led to a flame war. Other criticism included the use of the Gnutella network to bootstrap G2 peers and poor documentation of the G2 protocol. Additionally, the more frequent search retries of the Shareaza client, one of the initial G2 clients, could unnecessarily burden the Gnutella network.
Both protocols have undergone significant changes since the fork in 2002. G2 has advantages and disadvantages compared to Gnutella. An advantage often cited is Gnutella2's hybrid search is more efficient than the original Gnutella's query flooding, which was later replaced by more efficient search methods, starting with Query Routing in 2002, which was proposed in 2001 by Limewire developers. An advantage for Gnutella is that its users number in the millions, whereas the G2 network is approximately an order of magnitude smaller. It is difficult to compare the protocols in their current form; the individual client choice will probably have as much of an effect to an end user on either network.

</doc>
<doc id="11857" url="https://en.wikipedia.org/wiki?curid=11857" title="George Lucas">
George Lucas

George Walton Lucas Jr. (born May 14, 1944) is an American film director, producer, screenwriter, and entrepreneur. Lucas is best known for creating the "Star Wars" and "Indiana Jones" franchises and founding Lucasfilm, LucasArts, and Industrial Light & Magic. He served as chairman of Lucasfilm before selling it to The Walt Disney Company in 2012. Lucas is one of history's most financially successful filmmakers and has been nominated for four Academy Awards. His films are among the 100 highest-grossing movies at the North American box office, adjusted for ticket-price inflation. Lucas is considered a significant figure of the 20th-century New Hollywood movement.
After graduating from the University of Southern California in 1967, Lucas co-founded American Zoetrope with filmmaker Francis Ford Coppola. Lucas wrote and directed "THX 1138" (1971), based on his earlier student short "", which was a critical success but a financial failure. His next work as a writer-director was the film "American Graffiti" (1973), inspired by his youth in the early 1960s Modesto, California, and produced through the newly founded Lucasfilm. The film was critically and commercially successful and received five Academy Award nominations, including Best Picture.
Lucas's next film, the epic space opera "Star Wars" (1977), had a troubled production but was a surprise hit, becoming the highest-grossing film at the time, winning six Academy Awards and sparking a cultural phenomenon. Lucas produced and co-wrote the sequels "The Empire Strikes Back" (1980) and "Return of the Jedi" (1983). With director Steven Spielberg, he created, produced, and co-wrote the "Indiana Jones" films "Raiders of the Lost Ark" (1981), "The Temple of Doom" (1984), "The Last Crusade" (1989) and "The Kingdom of the Crystal Skull" (2008). He also produced and wrote a variety of films and television series through Lucasfilm between the 1970s and the 2010s.
In 1997, Lucas re-released the "Star Wars" Trilogy as part of a special edition featuring several alterations; home media versions with further changes were released in 2004 and 2011. He returned to directing with a "Star Wars" prequel trilogy comprising ' (1999), ' (2002) and ' (2005). He last collaborated on the CGI-animated television series ' (2008–2014, 2020), the war film "Red Tails" (2012), and the CGI film "Strange Magic" (2015).
Lucas was born and raised in Modesto, California, the son of Dorothy Ellinore Lucas (née Bomberger) and George Walton Lucas Sr., and is of German, Swiss-German, English, Scottish, and distant Dutch and French descent. His family attended Disneyland during its opening week in July 1955, and Lucas would remain enthusiastic about the park. He was interested in comics and science fiction, including television programs such as the "Flash Gordon" serials. Long before Lucas began making films, he yearned to be a racecar driver, and he spent most of his high school years racing on the underground circuit at fairgrounds and hanging out at garages. On June 12, 1962, a few days before his high school graduation, Lucas was driving his souped-up Autobianchi Bianchina when another driver broadsided him, flipping his car several times before it crashed into a tree; Lucas's seatbelt had snapped, ejecting him and thereby saving his life. However, his lungs were bruised from severe hemorrhaging and he required emergency medical treatment. This incident caused him to lose interest in racing as a career, but also inspired him to pursue his other interests.
Lucas's father owned a stationery store, and had wanted George to work for him when he turned 18. Lucas had been planning to go to art school, and declared upon leaving home that he would be a millionaire by the age of 30. He attended Modesto Junior College, where he studied anthropology, sociology, and literature, amongst other subjects. He also began shooting with an 8 mm camera, including filming car races.
At this time, Lucas and his friend John Plummer became interested in Canyon Cinema: screenings of underground, avant-garde 16 mm filmmakers like Jordan Belson, Stan Brakhage, and Bruce Conner. Lucas and Plummer also saw classic European films of the time, including Jean-Luc Godard's "Breathless", François Truffaut's "Jules et Jim", and Federico Fellini's "8½". "That's when George really started exploring," Plummer said. Through his interest in autocross racing, Lucas met renowned cinematographer Haskell Wexler, another race enthusiast. Wexler, later to work with Lucas on several occasions, was impressed by Lucas' talent. "George had a very good eye, and he thought visually," he recalled.
At Plummer's recommendation, Lucas then transferred to the University of Southern California (USC) School of Cinematic Arts. USC was one of the earliest universities to have a school devoted to motion picture film. During the years at USC, Lucas shared a dorm room with Randal Kleiser. Along with classmates such as Walter Murch, Hal Barwood, and John Milius, they became a clique of film students known as The Dirty Dozen. He also became good friends with fellow acclaimed student filmmaker and future "Indiana Jones" collaborator, Steven Spielberg. Lucas was deeply influenced by the Filmic Expression course taught at the school by filmmaker Lester Novros which concentrated on the non-narrative elements of Film Form like color, light, movement, space, and time. Another inspiration was the Serbian montagist (and dean of the USC Film Department) Slavko Vorkapić, a film theoretician who made stunning montage sequences for Hollywood studio features at MGM, RKO, and Paramount. Vorkapich taught the autonomous nature of the cinematic art form, emphasizing kinetic energy inherent in motion pictures.
Lucas saw many inspiring films in class, particularly the visual films coming out of the National Film Board of Canada like Arthur Lipsett's "21-87", the French-Canadian cameraman Jean-Claude Labrecque's cinéma vérité "60 Cycles", the work of Norman McLaren, and the documentaries of Claude Jutra. Lucas fell madly in love with pure cinema and quickly became prolific at making 16 mm nonstory noncharacter visual tone poems and cinéma vérité with such titles as "Look at Life", "Herbie", "", "The Emperor", "Anyone Lived in a Pretty (how) Town", "Filmmaker", and "6-18-67". He was passionate and interested in camerawork and editing, defining himself as a filmmaker as opposed to being a director, and he loved making abstract visual films that created emotions purely through cinema.
After graduating with a bachelor of fine arts in film in 1967, he tried joining the United States Air Force as an officer, but he was immediately turned down because of his numerous speeding tickets. He was later drafted by the Army for military service in Vietnam, but he was exempted from service after medical tests showed he had diabetes, the disease that killed his paternal grandfather.
In 1967, Lucas re-enrolled as a USC graduate student in film production. He began working under Verna Fields for the United States Information Agency, where he met his future wife Marcia Griffin. Working as a teaching instructor for a class of U.S. Navy students who were being taught documentary cinematography, Lucas directed the short film "", which won first prize at the 1967–68 National Student film festival. Lucas was awarded a student scholarship by Warner Bros. to observe and work on the making of a film of his choosing. The film he chose was "Finian's Rainbow" (1968) which was being directed by Francis Ford Coppola, who was revered among film school students of the time as a cinema graduate who had "made it" in Hollywood. In 1969, Lucas was one of the camera operators on the classic Rolling Stones concert film "Gimme Shelter".
In 1969, Lucas co-founded the studio American Zoetrope with Coppola, hoping to create a liberating environment for filmmakers to direct outside the perceived oppressive control of the Hollywood studio system. Coppola thought Lucas's "Electronic Labyrinth" could be adapted into his first full-length feature film, which was produced by American Zoetrope as "THX 1138", but was not a success. Lucas then created his own company, Lucasfilm, Ltd., and directed the successful "American Graffiti" (1973).
Lucas then set his sights on adapting Flash Gordon, an adventure serial from his childhood that he fondly remembered. When he was unable to obtain the rights, he set out to write an original space adventure that would eventually become "Star Wars". Despite his success with his previous film, all but one studio turned "Star Wars" down. It was only because Alan Ladd, Jr., at 20th Century Fox liked "American Graffiti" that he forced through a production and distribution deal for the film, which ended up restoring Fox to financial stability after a number of flops. "Star Wars" was significantly influenced by samurai films of Akira Kurosawa, Spaghetti Westerns, as well as classic sword and sorcery fantasy stories.
"Star Wars" quickly became the highest-grossing film of all-time, displaced five years later by Spielberg's "E.T. the Extra-Terrestrial". After the success of "American Graffiti" and prior to the beginning of filming on "Star Wars", Lucas was encouraged to renegotiate for a higher fee for writing and directing "Star Wars" than the $150,000 agreed. He declined to do so, instead negotiating for advantage in some of the as-yet-unspecified parts of his contract with Fox, in particular, ownership of licensing and merchandising rights (for novelizations, clothing, toys, etc.) and contractual arrangements for sequels. Lucasfilm has earned hundreds of millions of dollars from licensed games, toys, and collectibles created for the franchise.
The original "Star Wars" film went through a tumultuous production, and during editing, Lucas suffered chest pains initially feared to be a heart attack, but actually a fit of hypertension and exhaustion.
Following the release of the first "Star Wars" film, Lucas worked extensively as a writer and producer, including on the many "Star Wars" spinoffs made for film, television, and other media. Lucas acted as executive producer for the next two "Star Wars" films, commissioning Irvin Kershner to direct "The Empire Strikes Back", and Richard Marquand to direct "Return of the Jedi", while receiving a story credit on the former and sharing a screenwriting credit with Lawrence Kasdan on the latter. He also acted as story writer and executive producer on all four of the "Indiana Jones" films, which his colleague and good friend Steven Spielberg directed.
Other successful projects where Lucas acted as an executive producer and occasional story writer in this period include Kurosawa's "Kagemusha" (1980), Lawrence Kasdan's "Body Heat" (1981), ' (1984), ' (1985), Jim Henson's "Labyrinth" (1986), Godfrey Reggio's "Powaqqatsi" (1986), Don Bluth's "The Land Before Time" (1988), and the "Indiana Jones" television spinoff "The Young Indiana Jones Chronicles" (1992–96). There were unsuccessful projects, however, including "More American Graffiti" (1979), Willard Huyck's "Howard the Duck" (1986), which was the biggest flop of Lucas's career, Ron Howard's "Willow" (1988), Coppola's "" (1988), and Mel Smith's "Radioland Murders" (1994).
The animation studio Pixar was founded in 1979 as the Graphics Group, one third of the Computer Division of Lucasfilm. Pixar's early computer graphics research resulted in groundbreaking effects in films such as "" and "Young Sherlock Holmes", and the group was purchased in 1986 by Steve Jobs shortly after he left Apple Computer. Jobs paid Lucas US$5 million and put US$5 million as capital into the company. The sale reflected Lucas' desire to stop the cash flow losses from his 7-year research projects associated with new entertainment technology tools, as well as his company's new focus on creating entertainment products rather than tools. As of June 1983, Lucas was worth US$60 million, but he met cash-flow difficulties following his divorce that year, concurrent with the sudden dropoff in revenues from "Star Wars" licenses following the theatrical run of "Return of the Jedi". At this point, Lucas had no desire to return to "Star Wars", and had unofficially canceled the sequel trilogy.
Also in 1983, Lucas and Tomlinson Holman founded the audio company THX Ltd. The company was formerly owned by Lucasfilm, and contains equipment for stereo, digital, and theatrical sound for films, and music. Skywalker Sound and Industrial Light & Magic, are the sound and visual effects subdivisions of Lucasfilm, while Lucasfilm Games, later renamed LucasArts, produces products for the gaming industry.
Having lost much of his fortune in a divorce settlement in 1987, Lucas was reluctant to return to "Star Wars". However, the prequels, which were still only a series of basic ideas partially pulled from his original drafts of "The Star Wars", continued to tantalize him with technical possibilities that would make it worthwhile to revisit his older material. When "Star Wars" became popular once again, in the wake of Dark Horse's comic book line and Timothy Zahn's trilogy of spin-off novels, Lucas realized that there was still a large audience. His children were older, and with the explosion of CGI technology he began to consider directing once again.
By 1993, it was announced, in "Variety" among other sources, that Lucas would be making the prequels. He began penning more to the story, indicating that the series would be a tragic one, examining Anakin Skywalker's fall to the dark side. Lucas also began to change the prequels status relative to the originals; at first, they were supposed to be a "filling-in" of history tangential to the originals, but now he saw that they could form the beginning of one long story that started with Anakin's childhood and ended with his death. This was the final step towards turning the film series into a "Saga". In 1994, Lucas began work on the screenplay of the first prequel, tentatively titled "Episode I: The Beginning".
In 1997, to celebrate the 20th anniversary of "Star Wars," Lucas returned to the original trilogy and made numerous modifications using newly available digital technology, releasing them in theaters as the "Star Wars Special Edition". For DVD releases in 2004 and Blu-ray releases in 2011, the trilogy received further revisions to make them congruent with the prequel trilogy. Besides the additions to the "Star Wars" franchise, Lucas released a "Director's Cut" of "THX 1138" in 2004, with the film re-cut and containing a number of CGI revisions.
The first "Star Wars" prequel was finished and released in 1999 as ', which would be the first film Lucas had directed in over two decades. Following the release of the first prequel, Lucas announced that he would also be directing the next two, and began working on "Episode II". The first draft of "Episode II" was completed just weeks before principal photography, and Lucas hired Jonathan Hales, a writer from "The Young Indiana Jones Chronicles", to polish it. It was completed and released in 2002 as '. The final prequel, ', began production in 2002 and was released in 2005. Numerous fans and critics considered the prequels inferior to the original trilogy, though they were box office successes. From 2003 to 2005, Lucas also served as an executive producer on ', an animated microseries on Cartoon Network created by Genndy Tartakovsky, that bridged the events between "Attack of the Clones" and "Revenge of the Sith".
Lucas collaborated with Jeff Nathanson as a writer of the 2008 film "Indiana Jones and the Kingdom of the Crystal Skull", directed by Steven Spielberg. Like the "Star Wars" prequels, reception was mixed, with numerous fans and critics once again considering it inferior to its predecessors. From 2008 to 2014, Lucas also served as the creator and executive producer and for a second "Star Wars" animated series on Cartoon Network, "" which premiered with a before airing its first episode. The supervising director for this series was Dave Filoni, who was chosen by Lucas and closely collaborated with him on its development. Like the previous series it bridged the events between "Attack of the Clones" and "Revenge of the Sith". The animated series also featured the last "Star Wars" stories on which Lucas was majorly involved.
In 2012, Lucas served as executive producer for "Red Tails", a war film based on the exploits of the Tuskegee Airmen during World War II. He also took over direction of reshoots while director Anthony Hemingway worked on other projects.
In January 2012, Lucas announced his retirement from producing large blockbuster films and instead re-focusing his career on smaller, independently budgeted features.
In June 2012, it was announced that producer Kathleen Kennedy, a long-term collaborator with Steven Spielberg and a producer of the "Indiana Jones" films, had been appointed as co-chair of Lucasfilm Ltd. It was reported that Kennedy would work alongside Lucas, who would remain chief executive and serve as co-chairman for at least one year, after which she would succeed him as the company's sole leader. With the sale of Lucasfilm to Disney, Lucas is currently Disney's second largest single shareholder after the estate of Steve Jobs.
Lucas worked as a creative consultant on the "Star Wars" sequel trilogy's first film, "The Force Awakens". As creative consultant on the film, Lucas' involvement included attending early story meetings; according to Lucas, "I mostly say, 'You can't do this. You can do that.' You know, 'The cars don't have wheels. They fly with antigravity.' There's a million little pieces ... I know all that stuff." Lucas' son Jett told "The Guardian "that his father was "very torn" about having sold the rights to the franchise, despite having hand-picked Abrams to direct, and that his father was "there to guide" but that "he wants to let it go and become its new generation." Among the materials turned over to the production team were rough story treatments Lucas developed when he considered creating episodes "VII"–"IX" himself years earlier; in January 2015, Lucas stated that Disney had discarded his story ideas.
"The Force Awakens", directed by J. J. Abrams, was released on December 18, 2015. Kathleen Kennedy executive produced the film and its sequels. The new sequel trilogy was jointly produced by Lucasfilm and The Walt Disney Company, which had acquired Lucasfilm in 2012. During an interview with talk show host and journalist Charlie Rose that aired on December 24, 2015, Lucas likened his decision to sell Lucasfilm to Disney to a divorce and outlined the creative differences between him and the producers of "The Force Awakens". Lucas described the previous six "Star Wars" films as his "children" and defended his vision for them, while criticizing "The Force Awakens" for having a "retro feel", saying, "I worked very hard to make them completely different, with different planets, with different spaceships – you know, to make it new." Lucas also drew some criticism and subsequently apologized for his remark likening Disney to "white slavers".
In 2015, Lucas wrote the CGI film "Strange Magic", his first musical. The film was produced at Skywalker Ranch. Gary Rydstrom directed the movie. At the same time the sequel trilogy was announced a fifth installment of the "Indiana Jones" series also entered pre-development phase with Harrison Ford and Steven Spielberg set to return. Lucas originally did not specify whether the selling of Lucasfilm would affect his involvement with the film. In October 2016, Lucas announced his decision to not be involved in the story of the film, but would remain an executive producer. In 2016, "", the first film of a "Star Wars" anthology series was released. It told the story of the rebels who stole the plans for the Death Star featured in the original "Star Wars" film, and it was reported that Lucas liked it more than "The Force Awakens". "The Last Jedi", the second film in the sequel trilogy, was released in 2017; Lucas described the film as "beautifully made".
Lucas has had cursory involvement with "" (2018), the "Star Wars" streaming series "The Mandalorian", and the premiere of the eighth season of "Game of Thrones". Lucas met with J. J. Abrams before the latter began writing the script to the sequel trilogy's final film, "The Rise of Skywalker", which was released in 2019.
Lucas has pledged to give half of his fortune to charity as part of an effort called The Giving Pledge led by Bill Gates and Warren Buffett to persuade America's richest individuals to donate their financial wealth to charities.
In 1991, The George Lucas Educational Foundation was founded as a nonprofit operating foundation to celebrate and encourage innovation in schools. The Foundation's content is available under the brand Edutopia, in an award-winning web site, social media and via documentary films. Lucas, through his foundation, was one of the leading proponents of the E-rate program in the universal service fund, which was enacted as part of the Telecommunications Act of 1996. On June 24, 2008, Lucas testified before the United States House of Representatives subcommittee on Telecommunications and the Internet as the head of his Foundation to advocate for a free wireless broadband educational network.
In 2012, Lucas sold Lucasfilm to The Walt Disney Company for a reported sum of $4.05 billion. It was widely reported at the time that Lucas intends to give the majority of the proceeds from the sale to charity. A spokesperson for Lucasfilm said, "George Lucas has expressed his intention, in the event the deal closes, to donate the majority of the proceeds to his philanthropic endeavors." Lucas also spoke on the matter: "For 41 years, the majority of my time and money has been put into the company. As I start a new chapter in my life, it is gratifying that I have the opportunity to devote more time and resources to philanthropy."
By June 2013, Lucas was considering establishing a museum, the Lucas Cultural Arts Museum, to be built on Crissy Field near the Golden Gate Bridge in San Francisco, which would display his collection of illustrations and pop art, with an estimated value of more than $1 billion. Lucas offered to pay the estimated $300 million cost of constructing the museum, and would endow it with $400 million when it opened, eventually adding an additional $400 million to its endowment. After being unable to reach an agreement with The Presidio Trust, Lucas turned to Chicago. A potential lakefront site on Museum Campus in Chicago was proposed in May 2014. By June 2014, Chicago had been selected, pending approval of the Chicago Plan Commission, which was granted. The museum project was renamed the Lucas Museum of Narrative Art. On June 24, 2016, Lucas announced that he was abandoning his plans to locate the museum in Chicago, due to a lawsuit by a local preservation group, Friends of the Parks, and would instead build the museum in California. On January 17, 2017, Lucas announced that the museum will be constructed in Exposition Park, Los Angeles California.
In 2005, Lucas gave US$1 million to help build the Martin Luther King Jr. Memorial on the National Mall in Washington D.C. to commemorate American civil rights leader Martin Luther King Jr.
On September 19, 2006, USC announced that Lucas had donated $175–180 million to his alma mater to expand the film school. It is the largest single donation to USC and the largest gift to a film school anywhere. Previous donations led to the already-existing George Lucas Instructional Building and Marcia Lucas Post-Production building.
In 2013, Lucas and his wife Mellody Hobson donated $25 million to the Chicago-based not-for-profit After School Matters, of which Hobson is the chair.
On April 15, 2016, it was reported that Lucas had donated between $501,000 and $1 million through the Lucas Family Foundation to the Obama Foundation, which is charged with overseeing the construction of the Barack Obama Presidential Center on Chicago's South Side.
In 1969, Lucas married film editor Marcia Lou Griffin, who went on to win an Academy Award for her editing work on the original "Star Wars" film. They adopted a daughter, Amanda Lucas, in 1981, and divorced in 1983. Lucas subsequently adopted two more children as a single parent: daughter Katie Lucas, born in 1988, and son Jett Lucas, born in 1993. His three eldest children all appeared in the three "Star Wars" prequels, as did Lucas himself. Following his divorce, Lucas was in a relationship with singer Linda Ronstadt in the 1980s.
Lucas began dating Mellody Hobson, president of Ariel Investments and chair of DreamWorks Animation, in 2006. Lucas and Hobson announced their engagement in January 2013, and married on June 22, 2013, at Lucas's Skywalker Ranch in Marin County, California. They have one daughter together, born via gestational carrier in August 2013.
Lucas was born and raised in a Methodist family. The religious and mythical themes in "Star Wars" were inspired by Lucas's interest in the writings of mythologist Joseph Campbell, and he would eventually come to identify strongly with the Eastern religious philosophies he studied and incorporated into his films, which were a major inspiration for "the Force". Lucas has come to state that his religion is "Buddhist Methodist". He resides in Marin County.
Lucas is a major collector of the American illustrator and painter Norman Rockwell. A collection of 57 Rockwell paintings and drawings owned by Lucas and fellow Rockwell collector and film director Steven Spielberg were displayed at the Smithsonian American Art Museum from July 2, 2010, to January 2, 2011, in an exhibition titled "Telling Stories".
Lucas has said that he is a fan of Seth MacFarlane's hit TV show "Family Guy". MacFarlane has said that Lucasfilm was extremely helpful when the "Family Guy" crew wanted to .
Lucas supported Democratic candidate Hillary Clinton in the run-up for the 2016 U.S. presidential election.
In 1977, Lucas was awarded the Inkpot Award.
The American Film Institute awarded Lucas its Life Achievement Award on June 9, 2005. This was shortly after the release of "", about which he joked stating that, since he views the entire "Star Wars" series as one film, he could actually receive the award now that he had finally "gone back and finished the movie."
Lucas was nominated for four Academy Awards: Best Directing and Writing for "American Graffiti" and "Star Wars". He received the Academy's Irving G. Thalberg Award in 1991. He appeared at the 79th Academy Awards ceremony in 2007 with Steven Spielberg and Francis Ford Coppola to present the Best Director award to their friend Martin Scorsese. During the speech, Spielberg and Coppola talked about the joy of winning an Oscar, making fun of Lucas, who has not won a competitive Oscar.
The Science Fiction Hall of Fame inducted Lucas in 2006, its second "Film, Television, and Media" contributor, after Spielberg. The Discovery Channel named him one of the 100 "Greatest Americans" in September 2008. Lucas served as Grand Marshal for the Tournament of Roses Parade and made the ceremonial coin toss at the Rose Bowl, New Year's Day 2007. In 2009, he was one of 13 California Hall of Fame inductees in The California Museum's yearlong exhibit.
In July 2013, Lucas was awarded the National Medal of Arts by President Barack Obama for his contributions to American cinema.
In October 2014, Lucas received Honorary Membership of the Society of Motion Picture and Television Engineers.
In August 2015, Lucas was inducted as a Disney Legend, and on December 6, 2015, he was an honoree at the Kennedy Center Honors.
Footnotes
Citations
 

</doc>
<doc id="11861" url="https://en.wikipedia.org/wiki?curid=11861" title="Gothenburg">
Gothenburg

Gothenburg (; abbreviated Gbg; ) is the second-largest city in Sweden, fifth-largest in the Nordic countries, and capital of the Västra Götaland County. It is situated by Kattegat, on the west coast of Sweden, and has a population of approximately 570,000 in the city proper and about 1 million inhabitants in the metropolitan area.
Gothenburg was founded as a heavily fortified, primarily Dutch, trading colony, by royal charter in 1621 by King Gustavus Adolphus. In addition to the generous privileges (e.g. tax relaxation) given to his Dutch allies from the then-ongoing Thirty Years' War, the king also attracted significant numbers of his German and Scottish allies to populate his only town on the western coast. At a key strategic location at the mouth of the Göta älv, where Scandinavia's largest drainage basin enters the sea, the Port of Gothenburg is now the largest port in the Nordic countries.
Gothenburg is home to many students, as the city includes the University of Gothenburg and Chalmers University of Technology. Volvo was founded in Gothenburg in 1927. The original parent Volvo Group and the now separate Volvo Car Corporation are still headquartered on the island of Hisingen in the city. Other key companies are SKF and Astra Zeneca.
Gothenburg is served by Göteborg Landvetter Airport southeast of the city center. The smaller Göteborg City Airport, from the city center, was closed to regular airline traffic in 2015.
The city hosts the Gothia Cup, the world's second largest youth football tournament, and the , Europe's largest youth basketball tournament, alongside some of the largest annual events in Scandinavia. The Gothenburg Film Festival, held in January since 1979, is the leading Scandinavian film festival with over 155,000 visitors each year. In summer, a wide variety of music festivals are held in the city, including the popular Way Out West Festival.
The city was named Göteborg in the city's charter in 1621 and simultaneously given the German and English name Gothenburg. The Swedish name was given after the "Göta älv", called Göta River in English, and other cities ending in "-borg".
Both the Swedish and German/English names were in use before 1621 and had already been used for the previous city founded in 1604 that burned down in 1611. Gothenburg is one of few Swedish cities to still have an official and widely used exonym.
The city council of 1641 consisted of four Swedish, three Dutch, three German, and two Scottish members. In Dutch, Scots, English, and German, all languages with a long history in this trade and maritime-oriented city, the name Gothenburg is or was (in the case of German) used for the city. Variations of the official German/English name Gothenburg in the city's 1621 charter existed or exist in many languages. The French form of the city name is "Gothembourg", but in French texts, the Swedish name "Göteborg" is more frequent. "Gothenburg" can also be seen in some older English texts. In Spanish and Portuguese the city is called Gotemburgo. These traditional forms are sometimes replaced with the use of the Swedish "Göteborg", for example by The Göteborg Opera and the Göteborg Ballet. However, "Göteborgs universitet", previously designated as the Göteborg University in English, changed its name to the University of Gothenburg in 2008. The Gothenburg municipality has also reverted to the use of the English name in international contexts.
In 2009, the city council launched a new logotype for Gothenburg. Since the name "Göteborg" contains the Swedish letter "ö", they planned to make the name more international and "up to date" by turning the "ö" sideways. , the name is spelled "Go:teborg" on a large number of signs in the city.
In the early modern period, the configuration of Sweden's borders made Gothenburg strategically critical as the only Swedish gateway to Skagerrak, the North Sea and Atlantic, situated on the west coast in a very narrow strip of Swedish territory between Danish Halland in the south and Norwegian Bohuslän in the north. After several failed attempts, Gothenburg was successfully founded in 1621 by King Gustavus Adolphus (Gustaf II Adolf).
The site of the first church built in Gothenburg, subsequently destroyed by Danish invaders, is marked by a stone near the north end of the Älvsborg Bridge in the Färjenäs Park. The church was built in 1603 and destroyed in 1611. The city was heavily influenced by the Dutch, Germans, and Scots, and Dutch planners and engineers were contracted to construct the city as they had the skills needed to drain and build in the marshy areas chosen for the city. The town was designed like Dutch cities such as Amsterdam, Batavia (Jakarta) and New Amsterdam (Manhattan). The planning of the streets and canals of Gothenburg closely resembled that of Jakarta, which was built by the Dutch around the same time. The Dutchmen initially won political power, and it was not until 1652, when the last Dutch politician in the city's council died, that Swedes acquired political power over Gothenburg. During the Dutch period, the town followed Dutch town laws and Dutch was proposed as the official language in the town. Robust city walls were built during the 17th century. In 1807, a decision was made to tear down most of the city's wall. The work started in 1810 and was carried out by 150 soldiers from the Bohus regiment.
Along with the Dutch, the town also was heavily influenced by Scots who settled down in Gothenburg. Many became people of high-profile. William Chalmers, the son of a Scottish immigrant, donated his fortunes to set up what later became the Chalmers University of Technology. In 1841, the Scotsman Alexander Keiller founded the Götaverken shipbuilding company that was in business until 1989. His son James Keiller donated Keiller Park to the city in 1906.
The Gothenburg coat of arms was based on the lion of the coat of arms of Sweden, symbolically holding a shield with the national emblem, the Three Crowns, to defend the city against its enemies.
In the Treaty of Roskilde (1658), Denmark–Norway ceded the then Danish province Halland, in the south, and the Norwegian province of Bohus County or "Bohuslän" in the north, leaving Gothenburg less exposed. Gothenburg was able to grow into a significant port and trade centre on the west coast, because it was the only city on the west coast that, along with Marstrand, was granted the rights to trade with merchants from other countries.
In the 18th century, fishing was the most important industry. However, in 1731, the Swedish East India Company was founded, and the city flourished due to its foreign trade with highly profitable commercial expeditions to China.
The harbour developed into Sweden's main harbour for trade towards the west, and when Swedish emigration to the United States increased, Gothenburg became Sweden's main point of departure for these travellers. The impact of Gothenburg as a main port of embarkation for Swedish emigrants is reflected by Gothenburg, Nebraska, a small Swedish settlement in the United States.
With the 19th century, Gothenburg evolved into a modern industrial city that continued on into the 20th century. The population increased tenfold in the century, from 13,000 (1800) to 130,000 (1900). In the 20th century, major companies that developed included SKF (1907) and Volvo (1927).
Gothenburg is located on the west coast, in southwestern Sweden, about halfway between the capitals Copenhagen, Denmark, and Oslo, Norway. The location at the mouth of the Göta älv, which feeds into Kattegatt, an arm of the North Sea, has helped the city grow in significance as a trading city. The archipelago of Gothenburg consists of rough, barren rocks and cliffs, which also is typical for the coast of Bohuslän. Due to the Gulf Stream, the city has a mild climate and moderately heavy precipitation. It is the second-largest city in Sweden after the capital Stockholm.
The Gothenburg Metropolitan Area ("Stor-Göteborg") has 982,360 inhabitants and extends to the municipalities of Ale, Alingsås, Göteborg, Härryda, Kungälv, Lerum, Lilla Edet, Mölndal, Partille, Stenungsund, Tjörn, Öckerö within Västra Götaland County, and Kungsbacka within Halland County.
Angered, a suburb outside Gothenburg, consists of Hjällbo, Eriksbo, Rannebergen, Hammarkullen, Gårdsten, and Lövgärdet. It is a Million Programme part of Gothenburg, like Rosengård in Malmö and Botkyrka in Stockholm. Angered had about 50,000 inhabitants in 2015. It lies north of Gothenburg and is isolated from the rest of the city. Bergsjön is another Million Programme suburb north of Gothenburg, it has 14,000 inhabitants. Biskopsgården is the biggest multicultural suburb on the island of Hisingen, which is a part of Gothenburg but separated from the city by the river.
Gothenburg has a warm-summer humid continental climate using the 0 °C isotherm (Köppen "Dfb") and an oceanic climate using the -3 °C isotherm (Köppen "Cfb") according to the Köppen climate classification. Despite its northern latitude, temperatures are quite mild throughout the year and warmer than places in similar latitude, for example, Stockholm, or even somewhat further south, mainly because of the moderating influence of the warm Gulf Stream. During the summer, daylight extends 18 hours and 5 minutes, but lasts 6 hours and 32 minutes in late December. The climate has become significantly milder in later decades, particularly in summer and winter; July temperatures used to be below Stockholm's 1961–1990 averages, but have since been warmer than that benchmark.
Summers are warm and pleasant with average high temperatures of and lows of , but temperatures of occur on many days during the summer.
Winters are cold and windy with temperatures of around , though it rarely drops below . Precipitation is regular but generally moderate throughout the year. Snow mainly occurs from December to March, but is not unusual in November and April and can sometimes occur even in October and May, in extreme cases even in September.
Gothenburg has several parks and nature reserves ranging in size from tens of square metres to hundreds of hectares. It also has many green areas that are not designated as parks or reserves.
Selection of parks:
Very few houses are left from the 17th century when the city was founded, since all but the military and royal houses were built of wood. A rare exception is the Skansen Kronan.
The first major architecturally interesting period is the 18th century when the East India Company made Gothenburg an important trade city. Imposing stone houses in Neo-Classical style were erected around the canals. One example from this period is the East India House, which today houses the Göteborg City Museum.
In the 19th century, the wealthy bourgeoisie began to move outside the city walls which had protected the city. The style now was an eclectic, academic, somewhat overdecorated style which the middle-class favoured. The working class lived in the overcrowded city district Haga in wooden houses.
In the 19th century, the first comprehensive town plan after the founding of city was created, which led to the construction of the main street, Kungsportsavenyen. Perhaps the most significant type of houses of the city, Landshövdingehusen, were built in the end of the 19th century – three-storey houses with the first floor in stone and the other two in wood.
The early 20th century, characterized by the National Romantic style, was rich in architectural achievements. Masthugg Church is a noted example of the style of this period. In the early 1920s, on the city's 300th anniversary, the Götaplatsen square with its Neoclassical look was built.
After this, the predominant style in Gothenburg and rest of Sweden was Functionalism which especially dominated the suburbs such as Västra Frölunda and Bergsjön. The Swedish functionalist architect Uno Åhrén served as city planner from 1932 through 1943. In the 1950s, the big stadium Ullevi was built when Sweden hosted the 1958 FIFA World Cup.
The modern architecture of the city has been formed by such architects as Gert Wingårdh, who started as a Post-modernist in the 1980s.
Gustaf Adolf Square is a town square located in central Gothenburg. Noted buildings on the square include Gothenburg City Hall (formerly the stock exchange, opened in 1849) and the Nordic Classicism law court. The main canal of Gothenburg also flanks the square.
The Gothenburg Central Station is in the centre of the city, next to Nordstan and Drottningtorget. The building has been renovated and expanded numerous times since the grand opening in October 1858. In 2003, a major reconstruction was finished which brought the 19th-century building into the 21st century expanding the capacity for trains, travellers, and shopping. Not far from the central station is the Skanskaskrapan, or more commonly known as "The Lipstick". It is high with 22 floors and coloured in red-white stripes. The skyscraper was designed by Ralph Erskine and built by Skanska in the late 1980s as the headquarters for the company.
By the shore of the Göta Älv at Lilla Bommen is The Göteborg Opera. It was completed in 1994. The architect Jan Izikowitz was inspired by the landscape and described his vision as "Something that makes your mind float over the squiggling landscape like the wings of a seagull."
Feskekörka, or "Fiskhallen", is an indoor fishmarket by the Rosenlundskanalen in central Gothenburg. Feskekörkan was opened on 1November 1874 and its name from the building's resemblance to a Gothic church. The Gothenburg city hall is in the Beaux-Arts architectural style. The Gothenburg Synagogue at Stora Nygatan, near Drottningtorget, was built in 1855 according to the designs of the German architect August Krüger.
The Gunnebo House is a country house located to the south of Gothenburg, in Mölndal. It was built in a neoclassical architecture towards the end of the 18th century. Created in the early 1900s was the Vasa Church. It is located in Vasastan and is built of granite in a neo-Romanesque style.
Another noted construction is Brudaremossen TV Tower, one of the few partially guyed towers in the world.
The sea, trade, and industrial history of the city are evident in the cultural life of Gothenburg. It is also a popular destination for tourists on the Swedish west coast.
Many of the cultural institutions, as well as hospitals and the university, were created by donations from rich merchants and industrialists, for example the Röhsska Museum. On 29December 2004, the Museum of World Culture opened near Korsvägen. Museums include the Gothenburg Museum of Art, and several museums of sea and navigation history, natural history, the sciences, and East India. Aeroseum, close to the Göteborg City Airport, is an aircraft museum in a former military underground air force base. The Volvo museum has exhibits of the history of Volvo and the development from 1927 until today. Products shown include cars, trucks, marine engines, and buses.
Universeum is a public science centre that opened in 2001, the largest of its kind in Scandinavia. It is divided into six sections, each containing experimental workshops and a collection of reptiles, fish, and insects. Universeum occasionally host debates between Swedish secondary-school students and Nobel Prize laureates or other scholars.
The most noted attraction is the amusement park Liseberg, located in the central part of the city. It is the largest amusement park in Scandinavia by number of rides, and was chosen as one of the top ten amusement parks in the world (2005) by "Forbes". It is the most popular attraction in Sweden by number of visitors per year (more than 3 million).
There are a number of independent theatre ensembles in the city, besides institutions such as Gothenburg City Theatre, Backa Theatre (youth theatre), and Folkteatern.
The main boulevard is called Kungsportsavenyn (commonly known as "Avenyn", "The Avenue"). It is about long and starts at Götaplatsen – which is the location of the Gothenburg Museum of Art, the city's theatre, and the city library, as well as the concert hall – and stretches all the way to Kungsportsplatsen in the old city centre of Gothenburg, crossing a canal and a small park. The "Avenyn" was created in the 1860s and 1870s as a result of an international architecture contest, and is the product of a period of extensive town planning and remodelling. "Avenyn" has Gothenburg's highest concentration of pubs and clubs. Gothenburg's largest shopping centre (8th largest in Sweden), Nordstan, is located in central Gothenburg.
Gothenburg's Haga district is known for its picturesque wooden houses and its cafés serving the well-known "Haga bulle" – a large cinnamon roll similar to the "kanelbulle".
Five Gothenburg restaurants have a star in the 2008 "Michelin Guide": 28 +, Basement, Fond, Kock & Vin, Fiskekrogen, and Sjömagasinet.
The city has a number of star chefs – over the past decade, seven of the Swedish Chef of the Year awards have been won by people from Gothenburg.
The Gustavus Adolphus pastry, eaten every 6November in Sweden, Gustavus Adolphus Day, is especially connected to, and appreciated in, Gothenburg because the city was founded by King Gustavus Adolphus.
One of Gothenburg's most popular natural tourist attractions is the southern Gothenburg archipelago, which is a set of several islands that can be reached by ferry boats mainly operating from Saltholmen. Within the archipelago are the Älvsborg fortress, Vinga and Styrsö islands.
The annual Gothenburg Film Festival, is the largest film festival in Scandinavia. The Gothenburg Book Fair, held each year in September. It is the largest literary festival in Scandinavia, and the second largest book fair in Europe.
The International Science Festival in Gothenburg is an annual festival since April 1997, in central Gothenburg with thought-provoking science activities for the public. The festival is visited by about people each year. This makes it the largest popular-science event in Sweden and one of the leading popular-science events in Europe.
Citing the financial crisis, the International Federation of Library Associations and Institutions moved the 2010 World Library and Information Congress, previously to be held in Brisbane, Australia, to Gothenburg. The event took place on 10–15August 2010.
Gothenburg has a diverse music community—the Gothenburg Symphony Orchestra is the best-known in classical music. Gothenburg also was the birthplace of the Swedish composer Kurt Atterberg. The first internationally successfully Swedish group, instrumental rock group The Spotnicks came from Gothenburg. Bands such as The Soundtrack of Our Lives and Ace of Base are well-known pop representatives of the city. During the 1970s, Gothenburg had strong roots in the Swedish progressive movement (progg) with such groups as Nationalteatern, Nynningen, and Motvind. The record company Nacksving and the editorial office for the magazine Musikens Makt which also were part of the progg movement was located in Gothenburg during this time as well. There is also an active indie scene in Gothenburg. For example, the musician Jens Lekman was born in the suburb of Angered and named his 2007 release "Night Falls Over Kortedala" after another suburb, Kortedala. Other internationally acclaimed indie artists include the electro pop duos Studio, The Knife, Air France, The Tough Alliance, songwriter José González, and pop singer El Perro del Mar, as well as genre-bending quartet Little Dragon fronted by vocalist Yukimi Nagano. Another son of the city is one of Sweden's most popular singers, Håkan Hellström, who often includes many places from the city in his songs. The glam rock group Supergroupies derives from Gothenburg.
Gothenburg's own commercially successful At the Gates, In Flames, and Dark Tranquillity are credited with pioneering melodic death metal. Other well-known bands of the Gothenburg scene are thrash metal band The Haunted, progressive power metal band Evergrey, and power metal bands HammerFall and Dream Evil.
Many music festivals take place in the city every year. The Metaltown Festival is a two-day festival featuring heavy metal music bands, held in Gothenburg. It has been arranged annually since 2004, taking place at the Frihamnen venue. In June 2012, the festival included bands such as In Flames, Marilyn Manson, Slayer, Lamb of God, and Mastodon. Another popular festival, Way Out West, focuses more on rock, electronic, and hip-hop genres.
As in all of Sweden, a variety of sports are followed, including football, ice hockey, basketball, handball, baseball, and figure skating. A varied amateur and professional sports clubs scene exists.
Gothenburg is the birthplace of football in Sweden as the first football match in Sweden was played there in 1892. The city's three major football clubs, IFK Göteborg, Örgryte IS, and GAIS share a total of 34 Swedish championships between them. IFK has also won the UEFA Cup twice. Other notable clubs include BK Häcken (football), Pixbo Wallenstam IBK (floorball), multiple national handball champion Redbergslids IK, and four-time national ice hockey champion Frölunda HC, Gothenburg had a professional basketball team, Gothia Basket, until 2010 when it ceased. The bandy department of GAIS, GAIS Bandy, played the first season in the highest division Elitserien last season. The group stage match between the main rivals Sweden and Russia in the 2013 Bandy World Championship was played at Arena Heden in central Gothenburg.
The city's most notable sports venues are Scandinavium, and Ullevi (multisport) and the newly built Gamla Ullevi (football).
The 2003 World Allround Speed Skating Championships were held in Rudhallen, Sweden's only indoor speed-skating arena. It is a part of Ruddalens IP, which also has a bandy field and several football fields.
The only Swedish heavyweight champion of the world in boxing, Ingemar Johansson, who took the title from Floyd Paterson in 1959, was from Gothenburg.
Gothenburg has hosted a number of international sporting events including the 1958 FIFA World Cup, the 1983 European Cup Winners' Cup Final, an NFL preseason game on 14August 1988 between the Chicago Bears and the Minnesota Vikings, the 1992 European Football Championship, the 1993 and the 2002 World Men's Handball Championship, the 1995 World Championships in Athletics, the 1997 World Championships in Swimming (short track), the 2002 Ice Hockey World Championships, the 2004 UEFA Cup final, the 2006 European Championships in Athletics, and the 2008 World Figure Skating Championships. Annual events held in the city are the Gothia Cup and the Göteborgsvarvet. The annual Gothia Cup, is the world's largest football tournament with regards to the number of participants: in 2011, a total of 35,200 players from 1,567 teams and 72 nations participated.
Gothenburg hosted the XIII FINA World Masters Championships in 2010. Diving, swimming, synchronized swimming and open-water competitions were held on 28July to 7August. The water polo events were played on the neighboring city of Borås.
Gothenburg is also home to the Gothenburg Sharks, a professional baseball team in the Elitserien division of baseball in Sweden.
With around 25,000 sailboats and yachts scattered about the city, sailing is a popular sports activity in the region, particularly because of the nearby Gothenburg archipelago. In June 2015, the Volvo Ocean Race, professional sailing's leading crewed offshore race, concluded in Gothenburg, as well as an event in the 2015–2016 America's Cup World Series in August 2015.
The Gothenburg Amateur Diving Club (Göteborgs amatördykarklubb) Has been operating since October 1938.
Due to Gothenburg's advantageous location in the centre of Scandinavia, trade and shipping have always played a major role in the city's economic history, and they continue to do so. Gothenburg port has come to be the largest harbour in Scandinavia.
Apart from trade, the second pillar of Gothenburg has traditionally been manufacturing and industry, which significantly contributes to the city's wealth. Major companies operating plants in the area include SKF, Volvo (both cars and trucks), and Ericsson. Volvo Cars is the largest employer in Gothenburg, not including jobs in supply companies. The blue-collar industries which have dominated the city for long are still important factors in the city's economy, but they are being gradually replaced by high-tech industries.
Banking and finance are also important, as well as the event and tourist industry.
Gothenburg is the terminus of the Valdemar-Göteborg gas pipeline, which brings natural gas from the North Sea fields to Sweden, through Denmark.
Historically, Gothenburg was home base from the 18th century of the Swedish East India Company. From its founding until the late 1970s, the city was a world leader in shipbuilding, with such shipyards as Eriksbergs Mekaniska Verkstad, Götaverken, Arendalsvarvet, and Lindholmens varv. Gothenburg is classified as a global city by GaWC, with a ranking of Gamma. The city has been ranked as the 12th-most inventive city in the world by "Forbes".
Gothenburg became a city municipality with an elected city council when the first Swedish local government acts were implemented in 1863. The municipality has an assembly consisting of 81 members, elected every fourth year. Political decisions depend on citizens considering them legitimate. Political legitimacy can be based on various factors: legality, due process, and equality before the law, as well as the efficiency and effectiveness of public policy. One method used to achieve greater legitimacy for controversial policy reforms such as congestion charges is to allow citizens to decide or advise on the issue in public referendums. In December 2010 a petition for a local referendum on the congestion tax, signed by 28,000 citizens, was submitted to the City Council. This right to submit so-called “people's initiatives” was inscribed in the Local Government Act, which obliged local governments to hold a local referendum if petitioned by 5% of the citizens unless the issue was deemed to be outside their area of jurisdiction or if a majority in the City Council voted against holding such a referendum. A second petition for a referendum, signed by 57,000 citizens, was submitted to the local government in February 2013. This petition followed a campaign organised by a local newspaper – Göteborgs Tidningen – whose editor-in-chief argued that the paper's involvement was justified by the large public response to a series of articles on the congestion tax, as well as out of concern for the local democracy.
In 2019, approximately 28% (159,342 residents) of the population of Gothenburg were foreign born and approximately 46% (265,019 residents) had at least one parent born abroad. In addition, approximately 12% (69,263 residents) were foreign citizens.
In 2016, 45% of Gothenburg's immigrant population is from other parts of Europe, and 10% of the total population is from another Nordic country.
Gothenburg has two universities, both of which started as colleges founded by private donations in the 19th century. The University of Gothenburg has about 38,000 students and is one of the largest universities in Scandinavia, and one of the most versatile in Sweden. Chalmers University of Technology is a well-known university located in Johanneberg south of the inner city, lately also established at Lindholmen in Norra Älvstranden, Hisingen.
In 2015, there were ten adult education centres in Gothenburg: "Agnesbergs folkhögskola", "Arbetarrörelsens folkhögskola i Göteborg", "Finska folkhögskolan", "Folkhögskolan i Angered", "Göteborgs folkhögskola", "Kvinnofolkhögskolan", "Mo Gård folkhögskola", "S:ta Birgittas folkhögskola", "Västra Götalands folkhögskolor" and "Wendelsbergs folkhögskola".
In 2015, there were 49 high schools Gothenburg. Some of the more notable schools are Hvitfeldtska gymnasiet, Göteborgs Högre Samskola, Sigrid Rudebecks gymnasium and Polhemsgymnasiet. Some high-schools are also connected to large Swedish corporations, such as SKF Technical high-school that is owned by SKF and Gothenburg's technical high-school that is jointly owned by Volvo, Volvo Cars and Gothenburg municipality.
There are two folkhögskola that teach fine arts: Domen and Goteborg Folkhögskola.
With over of double track, the Gothenburg tram network covers most of the city and is the largest tram/light rail network in Scandinavia. Gothenburg also has a bus network. Boat and ferry services connect the Gothenburg archipelago to the mainland. The lack of a subway is due to the soft ground on which Gothenburg is situated. Tunneling is very expensive in such conditions.
The Gothenburg commuter rail with three lines services some nearby cities and towns.
Other major transportation hubs are "Centralstationen" (Gothenburg Central Station) and the Nils Ericson Terminal with trains and buses to various destinations in Sweden, as well as connections to Oslo and Copenhagen (via Malmö).
Gothenburg is served by Göteborg Landvetter Airport , located about 20 km (12 mi) east of the city centre. It is named after nearby locality Landvetter. Flygbussarna offer frequent bus connections to and from Gothenburg with travel time 20–30 minutes. Swebus, Flixbus and Nettbuss also serve the airport with several daily departures to Gothenburg, Borås and other destinations along European route E4. Västtrafik, the local public transport provider in the area, offers additional connections to Landvetter.
The airport is operated by Swedish national airport operator Swedavia, and with 6.8 million passengers served in 2017, it is Sweden's second-largest airport after Stockholm Arlanda. It serves as a base for several domestic and international airlines, e.g. Scandinavian Airlines, Norwegian Air Shuttle and Ryanair. Göteborg Landvetter, however, does not serve as a hub for any airline. In total, there are about 50 destinations with scheduled direct flights to and from Gothenburg, most of them European. An additional 40 destinations are served via charter.
The second airport in the area, Göteborg City Airport , is closed. On 13January 2015, Swedish airport operator Swedavia announced that Göteborg City Airport will not reopen for commercial services following an extensive rebuild of the airport started in November 2014, citing that the cost of making the airport viable for commercial operations again was too high, at 250 million kronor ($31 million). Commercial operations will be gradually wound down. The airport was located northwest of the city centre. It was formerly known as "Säve Flygplats." It is located within the borders of Gothenburg Municipality. In addition to commercial airlines, the airport was also operated by a number of rescue services, including the Swedish Coast Guard, and was used for other general aviation. Most civil air traffic to Göteborg City Airport was via low-cost airlines such as Ryanair and Wizz Air. Those companies have now been relocated to Landvetter Airport.
The Swedish company Stena Line operates between Gothenburg/Frederikshavn in Denmark and Gothenburg/Kiel in Germany.
The "England ferry" ("Englandsfärjan") to Newcastle via Kristiansand (run by the Danish company DFDS Seaways) ceased at the end of October 2006, after being a Gothenburg institution since the 19th century. DFDS Seaways' sister company, DFDS Tor Line, continues to run scheduled cargo ships between Gothenburg and several English ports, and these used to have limited capacity for passengers and their private vehicles. Also freight ships to North America and East Asia leave from the port.
Gothenburg is an intermodal logistics hub and Gothenburg harbour has access to Sweden and Norway via rail and trucks. Gothenburg harbour is the largest port in Scandinavia with a cargo turnover of 36.9 million tonnes per year in 2004.
Two of the noted people from Gothenburg are fictional, but have become synonymous with "people from Gothenburg". They are a working class couple called Kal and Ada, featured in "Gothenburg jokes" ("göteborgsvitsar"), songs, plays and names of events. Each year two persons who have significantly contributed to culture in the city are given the honorary titles of "Kal and Ada". A bronze statue of the couple made by Svenrobert Lundquist, was placed outside the entrance to Liseberg in 1995.
Some of the noted people from Gothenburg are Academy Award Winning actress Alicia Vikander, cookbook author Sofia von Porat, footballer Gunnar Gren, artist Evert Taube, golfer Helen Alfredsson, industrialist Victor Hasselblad, singer-songwriter Björn Ulvaeus, diplomat Jan Eliasson, British Open Winner and professional golfer Henrik Stenson, YouTuber PewDiePie (Felix Kjellberg), the most subscribed-to individual on the platform, with over 100 million subscribers and YouTuber RoomieOfficial (Joel Berghult).
Gothenburg has performed well in international rankings, some of which are mentioned below:
The Global Destination Sustainability Index has named Gothenburg the world's most sustainable destination every year since 2016.
In 2019 Gothenburg was selected by the EU as one of the top 2020 European Capitals of Smart Tourism.
In 2020 Business Region Göteborg received the 'European Entrepreneurial Region Award 2020' (EER Award 2020) from the EU.
The Gothenburg Award is the city's international prize that recognises and supports work to achieve sustainable development – in the Gothenburg region and from a global perspective. The award, which is one million Swedish crowns, is administrated and funded by a coalition of the City of Gothenburg and 12 companies. Past winners of the award have included Kofi Annan, Al Gore, and Michael Biddle.
Gothenburg is twinned with:
With Lyon (France) there is no formal partnership, but "a joint willingness to cooperate".
Gothenburg had signed an agreement with Shanghai in 1986 which was upgraded in 2003 to include exchanges in culture, economics, trade and sport. The agreement was allowed to lapse in 2020.

</doc>
<doc id="11863" url="https://en.wikipedia.org/wiki?curid=11863" title="Gotland County">
Gotland County

Gotland County () is a county or "län" of Sweden. Gotland is located in the Baltic Sea to the east of Öland, and is the largest of Sweden's islands. Counties are usually sub-divided into municipalities, but Gotland County only consists of one municipality: Gotland Municipality. Gotland County is the only county in Sweden that is not governed by a county council. The municipality handles the tasks that are otherwise handled by the county council, mainly health care and public transport. Like other counties, Gotland has a County Administrative Board that oversees implementation of the Swedish state government. Both the County Administrative Board and the municipality have their seat in the largest city Visby, with over 22,000 inhabitants.
The provinces of Sweden are no longer officially administrative units, but are used when reporting population size, politics, etc. In this case the province, the county and the municipality all have identical borders and cover an area of 3151 km² (3151 km² when excluding the lakes and rivers).
Gotland is the only Swedish county that is not administered by a county council. Instead, the municipality is tasked with the responsibilities of a county, including public health care and public transport.
The main aim of the County Administrative Board is to fulfil the goals set in national politics by the Riksdag and the Government, to coordinate the interests and promote the development of the county, to establish regional goals and safeguard the due process of law in the handling of each case. The County Administrative Board is a Government agency headed by a Governor.
During a trial period the County Council provisions for Gotland has been evolved to provisions for a Regional Council, meaning that it has assumed certain tasks from the County Administrative Board. Similar provisions are applicable to the counties of Västra Götaland and Skåne during the trial period.
The five most populous localities of Gotland County in 2010:
SCB have collected statistics on backgrounds of residents since 2002. These tables consist of all who have two foreign-born parents or are born abroad themselves. The chart lists election years and the last year on record alone.
Gotland County inherited its coat of arms from the province of Gotland. When it is shown with a royal crown it represents the County Administrative Board.

</doc>
<doc id="11866" url="https://en.wikipedia.org/wiki?curid=11866" title="Global Positioning System">
Global Positioning System

The Global Positioning System (GPS), originally NAVSTAR GPS, is a satellite-based radionavigation system owned by the United States government and operated by the United States Space Force. It is one of the global navigation satellite systems (GNSS) that provides geolocation and time information to a GPS receiver anywhere on or near the Earth where there is an unobstructed line of sight to four or more GPS satellites. Obstacles such as mountains and buildings block the relatively weak GPS signals.
The GPS does not require the user to transmit any data, and it operates independently of any telephonic or internet reception, though these technologies can enhance the usefulness of the GPS positioning information. The GPS provides critical positioning capabilities to military, civil, and commercial users around the world. The United States government created the system, maintains it, and makes it freely accessible to anyone with a GPS receiver.
The GPS project was started by the U.S. Department of Defense in 1973, with the first prototype spacecraft launched in 1978 and the full constellation of 24 satellites operational in 1993. Originally limited to use by the United States military, civilian use was allowed from the 1980s following an executive order from President Ronald Reagan. Advances in technology and new demands on the existing system have now led to efforts to modernize the GPS and implement the next generation of GPS Block IIIA satellites and Next Generation Operational Control System (OCX). Announcements from Vice President Al Gore and the White House in 1998 initiated these changes. In 2000, the U.S. Congress authorized the modernization effort, GPS III.
During the 1990s, GPS quality was degraded by the United States government in a program called "Selective Availability"; this was discontinued on May 1, 2000 by a law signed by President Bill Clinton.
The GPS service is provided by the United States government, which can selectively deny access to the system, as happened to the Indian military in 1999 during the Kargil War, or degrade the service at any time. As a result, several countries have developed or are in the process of setting up other global or regional satellite navigation systems. The Russian Global Navigation Satellite System (GLONASS) was developed contemporaneously with GPS, but suffered from incomplete coverage of the globe until the mid-2000s. GLONASS can be added to GPS devices, making more satellites available and enabling positions to be fixed more quickly and accurately, to within . China's BeiDou Navigation Satellite System began global services in 2018, and finished its full deployment in 2020.
There are also the European Union Galileo positioning system, and India's NavIC. Japan's Quasi-Zenith Satellite System (QZSS) is a GPS satellite-based augmentation system to enhance GPS's accuracy in Asia-Oceania, with satellite navigation independent of GPS scheduled for 2023.
When selective availability was lifted in 2000, GPS had about a accuracy. The latest stage of accuracy enhancement uses the L5 band and is now fully deployed. GPS receivers released in 2018 that use the L5 band can have much higher accuracy, pinpointing to within .
The GPS project was launched in the United States in 1973 to overcome the limitations of previous navigation systems, integrating ideas from several predecessors, including classified engineering design studies from the 1960s. The U.S. Department of Defense developed the system, which originally used 24 satellites. It was initially developed for use by the United States military and became fully operational in 1995. Civilian use was allowed from the 1980s. Roger L. Easton of the Naval Research Laboratory, Ivan A. Getting of The Aerospace Corporation, and Bradford Parkinson of the Applied Physics Laboratory are credited with inventing it. The work of Gladys West is credited as instrumental in the development of computational techniques for detecting satellite positions with the precision needed for GPS.
The design of GPS is based partly on similar ground-based radio-navigation systems, such as LORAN and the Decca Navigator, developed in the early 1940s.
In 1955, Friedwardt Winterberg proposed a test of general relativity – detecting time slowing in a strong gravitational field using accurate atomic clocks placed in orbit inside artificial satellites.
Special and general relativity predict that the clocks on the GPS satellites would be seen by the Earth's observers to run 38 microseconds faster per day than the clocks on the Earth. The GPS calculated positions would quickly drift into error, accumulating to . This was corrected for in the design of GPS.
When the Soviet Union launched the first artificial satellite (Sputnik 1) in 1957, two American physicists, William Guier and George Weiffenbach, at Johns Hopkins University's Applied Physics Laboratory (APL) decided to monitor its radio transmissions. Within hours they realized that, because of the Doppler effect, they could pinpoint where the satellite was along its orbit. The Director of the APL gave them access to their UNIVAC to do the heavy calculations required.
Early the next year, Frank McClure, the deputy director of the APL, asked Guier and Weiffenbach to investigate the inverse problem—pinpointing the user's location, given the satellite's. (At the time, the Navy was developing the submarine-launched Polaris missile, which required them to know the submarine's location.) This led them and APL to develop the TRANSIT system. In 1959, ARPA (renamed DARPA in 1972) also played a role in TRANSIT.
TRANSIT was first successfully tested in 1960. It used a constellation of five satellites and could provide a navigational fix approximately once per hour.
In 1967, the U.S. Navy developed the Timation satellite, which proved the feasibility of placing accurate clocks in space, a technology required for GPS.
In the 1970s, the ground-based OMEGA navigation system, based on phase comparison of signal transmission from pairs of stations, became the first worldwide radio navigation system. Limitations of these systems drove the need for a more universal navigation solution with greater accuracy.
Although there were wide needs for accurate navigation in military and civilian sectors, almost none of those was seen as justification for the billions of dollars it would cost in research, development, deployment, and operation of a constellation of navigation satellites. During the Cold War arms race, the nuclear threat to the existence of the United States was the one need that did justify this cost in the view of the United States Congress. This deterrent effect is why GPS was funded. It is also the reason for the ultra-secrecy at that time. The nuclear triad consisted of the United States Navy's submarine-launched ballistic missiles (SLBMs) along with United States Air Force (USAF) strategic bombers and intercontinental ballistic missiles (ICBMs). Considered vital to the nuclear deterrence posture, accurate determination of the SLBM launch position was a force multiplier.
Precise navigation would enable United States ballistic missile submarines to get an accurate fix of their positions before they launched their SLBMs. The USAF, with two thirds of the nuclear triad, also had requirements for a more accurate and reliable navigation system. The Navy and Air Force were developing their own technologies in parallel to solve what was essentially the same problem.
To increase the survivability of ICBMs, there was a proposal to use mobile launch platforms (comparable to the Soviet SS-24 and SS-25) and so the need to fix the launch position had similarity to the SLBM situation.
In 1960, the Air Force proposed a radio-navigation system called MOSAIC (MObile System for Accurate ICBM Control) that was essentially a 3-D LORAN. A follow-on study, Project 57, was worked in 1963 and it was "in this study that the GPS concept was born." That same year, the concept was pursued as Project 621B, which had "many of the attributes that you now see in GPS" and promised increased accuracy for Air Force bombers as well as ICBMs.
Updates from the Navy TRANSIT system were too slow for the high speeds of Air Force operation. The Naval Research Laboratory continued making advances with their Timation (Time Navigation) satellites, first launched in 1967, with the third one in 1974 carrying the first atomic clock into orbit.
Another important predecessor to GPS came from a different branch of the United States military. In 1964, the United States Army orbited its first Sequential Collation of Range (SECOR) satellite used for geodetic surveying. The SECOR system included three ground-based transmitters at known locations that would send signals to the satellite transponder in orbit. A fourth ground-based station, at an undetermined position, could then use those signals to fix its location precisely. The last SECOR satellite was launched in 1969.
With these parallel developments in the 1960s, it was realized that a superior system could be developed by synthesizing the best technologies from 621B, Transit, Timation, and SECOR in a multi-service program. Satellite orbital position errors, induced by variations in the gravity field and radar refraction among others, had to be resolved. A team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970–1973, used real-time data assimilation and recursive estimation to do so, reducing systematic and residual errors to a manageable level to permit accurate navigation.
During Labor Day weekend in 1973, a meeting of about twelve military officers at the Pentagon discussed the creation of a "Defense Navigation Satellite System (DNSS)". It was at this meeting that the real synthesis that became GPS was created. Later that year, the DNSS program was named "Navstar." Navstar is often erroneously considered an acronym for "NAVigation System Using Timing and Ranging" but was never considered as such by the GPS Joint Program Office (TRW may have once advocated for a different navigational system that used that acronym). With the individual satellites being associated with the name Navstar (as with the predecessors Transit and Timation), a more fully encompassing name was used to identify the constellation of Navstar satellites, "Navstar-GPS". Ten "Block I" prototype satellites were launched between 1978 and 1985 (an additional unit was destroyed in a launch failure).
The effect of the ionosphere on radio transmission was investigated in a geophysics laboratory of Air Force Cambridge Research Laboratory, renamed to Air Force Geophysical Research Lab (AFGRL) in 1974. AFGRL developed the Klobuchar model for computing ionospheric corrections to GPS location. Of note is work done by Australian space scientist Elizabeth Essex-Cohen at AFGRL in 1974. She was concerned with the curving of the paths of radio waves (atmospheric refraction) traversing the ionosphere from NavSTAR satellites.
After Korean Air Lines Flight 007, a Boeing 747 carrying 269 people, was shot down in 1983 after straying into the USSR's prohibited airspace, in the vicinity of Sakhalin and Moneron Islands, President Ronald Reagan issued a directive making GPS freely available for civilian use, once it was sufficiently developed, as a common good. The first Block II satellite was launched on February 14, 1989, and the 24th satellite was launched in 1994. The GPS program cost at this point, not including the cost of the user equipment but including the costs of the satellite launches, has been estimated at US$5 billion (then-year dollars).
Initially, the highest-quality signal was reserved for military use, and the signal available for civilian use was intentionally degraded, in a policy known as Selective Availability. This changed with President Bill Clinton signing on May 1, 2000 a policy directive to turn off Selective Availability to provide the same accuracy to civilians that was afforded to the military. The directive was proposed by the U.S. Secretary of Defense, William Perry, in view of the widespread growth of differential GPS services by private industry to improve civilian accuracy. Moreover, the U.S. military was actively developing technologies to deny GPS service to potential adversaries on a regional basis.
Since its deployment, the U.S. has implemented several improvements to the GPS service, including new signals for civil use and increased accuracy and integrity for all users, all the while maintaining compatibility with existing GPS equipment. Modernization of the satellite system has been an ongoing initiative by the U.S. Department of Defense through a series of satellite acquisitions to meet the growing needs of the military, civilians, and the commercial market.
As of early 2015, high-quality, FAA grade, Standard Positioning Service (SPS) GPS receivers provided horizontal accuracy of better than , although many factors such as receiver quality and atmospheric issues can affect this accuracy.
GPS is owned and operated by the United States government as a national resource. The Department of Defense is the steward of GPS. The "Interagency GPS Executive Board (IGEB)" oversaw GPS policy matters from 1996 to 2004. After that, the National Space-Based Positioning, Navigation and Timing Executive Committee was established by presidential directive in 2004 to advise and coordinate federal departments and agencies on matters concerning the GPS and related systems. The executive committee is chaired jointly by the Deputy Secretaries of Defense and Transportation. Its membership includes equivalent-level officials from the Departments of State, Commerce, and Homeland Security, the Joint Chiefs of Staff and NASA. Components of the executive office of the president participate as observers to the executive committee, and the FCC chairman participates as a liaison.
The U.S. Department of Defense is required by law to "maintain a Standard Positioning Service (as defined in the federal radio navigation plan and the standard positioning service signal specification) that will be available on a continuous, worldwide basis," and "develop measures to prevent hostile use of GPS and its augmentations without unduly disrupting or degrading civilian uses."
On February 10, 1993, the National Aeronautic Association selected the GPS Team as winners of the 1992 Robert J. Collier Trophy, the US's most prestigious aviation award. This team combines researchers from the Naval Research Laboratory, the USAF, the Aerospace Corporation, Rockwell International Corporation, and IBM Federal Systems Company. The citation honors them "for the most significant development for safe and efficient navigation and surveillance of air and spacecraft since the introduction of radio navigation 50 years ago."
Two GPS developers received the National Academy of Engineering Charles Stark Draper Prize for 2003:
GPS developer Roger L. Easton received the National Medal of Technology on February 13, 2006.
Francis X. Kane (Col. USAF, ret.) was inducted into the U.S. Air Force Space and Missile Pioneers Hall of Fame at Lackland A.F.B., San Antonio, Texas, March 2, 2010 for his role in space technology development and the engineering design concept of GPS conducted as part of Project 621B.
In 1998, GPS technology was inducted into the Space Foundation Space Technology Hall of Fame.
On October 4, 2011, the International Astronautical Federation (IAF) awarded the Global Positioning System (GPS) its 60th Anniversary Award, nominated by IAF member, the American Institute for Aeronautics and Astronautics (AIAA). The IAF Honors and Awards Committee recognized the uniqueness of the GPS program and the exemplary role it has played in building international collaboration for the benefit of humanity.
Gladys West was inducted into the Air Force Space and Missile Pioneers Hall of Fame in 2018 for recognition of her computational work which led to breakthroughs for GPS technology.
On February 12, 2019, four founding members of the project were awarded the Queen Elizabeth Prize for Engineering with the chair of the awarding board stating "Engineering is the foundation of civilisation; there is no other foundation; it makes things happen. And that's exactly what today's Laureates have done - they've made things happen. They've re-written, in a major way, the infrastructure of our world." 
The GPS concept is based on time and the known position of GPS specialized satellites. The satellites carry very stable atomic clocks that are synchronized with one another and with the ground clocks. Any drift from time maintained on the ground is corrected daily. In the same manner, the satellite locations are known with great precision. GPS receivers have clocks as well, but they are less stable and less precise.
Each GPS satellite continuously transmits a radio signal containing the current time and data about its position. Since the speed of radio waves is constant and independent of the satellite speed, the time delay between when the satellite transmits a signal and the receiver receives it is proportional to the distance from the satellite to the receiver. A GPS receiver monitors multiple satellites and solves equations to determine the precise position of the receiver and its deviation from true time. At a minimum, four satellites must be in view of the receiver for it to compute four unknown quantities (three position coordinates and clock deviation from satellite time).
Each GPS satellite continually broadcasts a signal (carrier wave with modulation) that includes:
Conceptually, the receiver measures the TOAs (according to its own clock) of four satellite signals. From the TOAs and the TOTs, the receiver forms four time of flight (TOF) values, which are (given the speed of light) approximately equivalent to receiver-satellite ranges plus time difference between the receiver and GPS satellites multiplied by speed of light, which are called as pseudo-ranges. The receiver then computes its three-dimensional position and clock deviation from the four TOFs.
In practice the receiver position (in three dimensional Cartesian coordinates with origin at the Earth's center) and the offset of the receiver clock relative to the GPS time are computed simultaneously, using the navigation equations to process the TOFs.
The receiver's Earth-centered solution location is usually converted to latitude, longitude and height relative to an ellipsoidal Earth model. The height may then be further converted to height relative to the geoid, which is essentially mean sea level. These coordinates may be displayed, such as on a moving map display, or recorded or used by some other system, such as a vehicle guidance system.
Although usually not formed explicitly in the receiver processing, the conceptual time differences of arrival (TDOAs) define the measurement geometry. Each TDOA corresponds to a hyperboloid of revolution (see Multilateration). The line connecting the two satellites involved (and its extensions) forms the axis of the hyperboloid. The receiver is located at the point where three hyperboloids intersect.
It is sometimes incorrectly said that the user location is at the intersection of three spheres. While simpler to visualize, this is the case only if the receiver has a clock synchronized with the satellite clocks (i.e., the receiver measures true ranges to the satellites rather than range differences). There are marked performance benefits to the user carrying a clock synchronized with the satellites. Foremost is that only three satellites are needed to compute a position solution. If it were an essential part of the GPS concept that all users needed to carry a synchronized clock, a smaller number of satellites could be deployed, but the cost and complexity of the user equipment would increase.
The description above is representative of a receiver start-up situation. Most receivers have a track algorithm, sometimes called a "tracker", that combines sets of satellite measurements collected at different times—in effect, taking advantage of the fact that successive receiver positions are usually close to each other. After a set of measurements are processed, the tracker predicts the receiver location corresponding to the next set of satellite measurements. When the new measurements are collected, the receiver uses a weighting scheme to combine the new measurements with the tracker prediction. In general, a tracker can (a) improve receiver position and time accuracy, (b) reject bad measurements, and (c) estimate receiver speed and direction.
The disadvantage of a tracker is that changes in speed or direction can be computed only with a delay, and that derived direction becomes inaccurate when the distance traveled between two position measurements drops below or near the random error of position measurement. GPS units can use measurements of the Doppler shift of the signals received to compute velocity accurately. More advanced navigation systems use additional sensors like a compass or an inertial navigation system to complement GPS.
GPS requires four or more satellites to be visible for accurate navigation. The solution of the navigation equations gives the position of the receiver along with the difference between the time kept by the receiver's on-board clock and the true time-of-day, thereby eliminating the need for a more precise and possibly impractical receiver based clock. Applications for GPS such as time transfer, traffic signal timing, and synchronization of cell phone base stations, make use of this cheap and highly accurate timing. Some GPS applications use this time for display, or, other than for the basic position calculations, do not use it at all.
Although four satellites are required for normal operation, fewer apply in special cases. If one variable is already known, a receiver can determine its position using only three satellites. For example, a ship or aircraft may have known elevation. Some GPS receivers may use additional clues or assumptions such as reusing the last known altitude, dead reckoning, inertial navigation, or including information from the vehicle computer, to give a (possibly degraded) position when fewer than four satellites are visible.
The current GPS consists of three major segments. These are the space segment, a control segment, and a user segment. The U.S. Space Force develops, maintains, and operates the space and control segments. GPS satellites broadcast signals from space, and each GPS receiver uses these signals to calculate its three-dimensional location (latitude, longitude, and altitude) and the current time.
The space segment (SS) is composed of 24 to 32 satellites, or Space Vehicles (SV), in medium Earth orbit, and also includes the payload adapters to the boosters required to launch them into orbit. The GPS design originally called for 24 SVs, eight each in three approximately circular orbits, but this was modified to six orbital planes with four satellites each. The six orbit planes have approximately 55° inclination (tilt relative to the Earth's equator) and are separated by 60° right ascension of the ascending node (angle along the equator from a reference point to the orbit's intersection). The orbital period is one-half a sidereal day, i.e., 11 hours and 58 minutes so that the satellites pass over the same locations or almost the same locations every day. The orbits are arranged so that at least six satellites are always within line of sight from everywhere on the Earth's surface (see animation at right). The result of this objective is that the four satellites are not evenly spaced (90°) apart within each orbit. In general terms, the angular difference between satellites in each orbit is 30°, 105°, 120°, and 105° apart, which sum to 360°.
Orbiting at an altitude of approximately ; orbital radius of approximately , each SV makes two complete orbits each sidereal day, repeating the same ground track each day. This was very helpful during development because even with only four satellites, correct alignment means all four are visible from one spot for a few hours each day. For military operations, the ground track repeat can be used to ensure good coverage in combat zones.
, there are 31 satellites in the GPS constellation, 27 of which are in use at a given time with the rest allocated as stand-bys. A 32nd was launched in 2018. , this last is still in evaluation. More decommissioned satellites are in orbit and available as spares. The additional satellites over 24 improve the precision of GPS receiver calculations by providing redundant measurements. With the increased number of satellites, the constellation was changed to a nonuniform arrangement. Such an arrangement was shown to improve accuracy but also improves reliability and availability of the system, relative to a uniform system, when multiple satellites fail. With the expanded constellation, nine satellites are usually visible from any point on the ground at any one time, ensuring considerable redundancy over the minimum four satellites needed for a position.
The control segment (CS) is composed of:
The MCS can also access U.S. Air Force Satellite Control Network (AFSCN) ground antennas (for additional command and control capability) and NGA (National Geospatial-Intelligence Agency) monitor stations. The flight paths of the satellites are tracked by dedicated U.S. Space Force monitoring stations in Hawaii, Kwajalein Atoll, Ascension Island, Diego Garcia, Colorado Springs, Colorado and Cape Canaveral, along with shared NGA monitor stations operated in England, Argentina, Ecuador, Bahrain, Australia and Washington DC. The tracking information is sent to the MCS at Schriever Air Force Base ESE of Colorado Springs, which is operated by the 2nd Space Operations Squadron (2 SOPS) of the U.S. Space Force. Then 2 SOPS contacts each GPS satellite regularly with a navigational update using dedicated or shared (AFSCN) ground antennas (GPS dedicated ground antennas are located at Kwajalein, Ascension Island, Diego Garcia, and Cape Canaveral). These updates synchronize the atomic clocks on board the satellites to within a few nanoseconds of each other, and adjust the ephemeris of each satellite's internal orbital model. The updates are created by a Kalman filter that uses inputs from the ground monitoring stations, space weather information, and various other inputs.
Satellite maneuvers are not precise by GPS standards—so to change a satellite's orbit, the satellite must be marked "unhealthy", so receivers don't use it. After the satellite maneuver, engineers track the new orbit from the ground, upload the new ephemeris, and mark the satellite healthy again.
The operation control segment (OCS) currently serves as the control segment of record. It provides the operational capability that supports GPS users and keeps the GPS operational and performing within specification.
OCS successfully replaced the legacy 1970s-era mainframe computer at Schriever Air Force Base in September 2007. After installation, the system helped enable upgrades and provide a foundation for a new security architecture that supported U.S. armed forces.
OCS will continue to be the ground control system of record until the new segment, Next Generation GPS Operation Control System (OCX), is fully developed and functional. The new capabilities provided by OCX will be the cornerstone for revolutionizing GPS's mission capabilities, enabling U.S. Space Force to greatly enhance GPS operational services to U.S. combat forces, civil partners and myriad domestic and international users. The GPS OCX program also will reduce cost, schedule and technical risk. It is designed to provide 50% sustainment cost savings through efficient software architecture and Performance-Based Logistics. In addition, GPS OCX is expected to cost millions less than the cost to upgrade OCS while providing four times the capability.
The GPS OCX program represents a critical part of GPS modernization and provides significant information assurance improvements over the current GPS OCS program.
On September 14, 2011, the U.S. Air Force announced the completion of GPS OCX Preliminary Design Review and confirmed that the OCX program is ready for the next phase of development.
The GPS OCX program has missed major milestones and is pushing its launch into 2021, 5 years past the original deadline. According to the Government Accounting Office, even this new deadline looks shaky.
The user segment (US) is composed of hundreds of thousands of U.S. and allied military users of the secure GPS Precise Positioning Service, and tens of millions of civil, commercial and scientific users of the Standard Positioning Service (see GPS navigation devices). In general, GPS receivers are composed of an antenna, tuned to the frequencies transmitted by the satellites, receiver-processors, and a highly stable clock (often a crystal oscillator). They may also include a display for providing location and speed information to the user. A receiver is often described by its number of channels: this signifies how many satellites it can monitor simultaneously. Originally limited to four or five, this has progressively increased over the years so that, , receivers typically have between 12 and 20 channels. Though there are many receiver manufacturers, they almost all use one of the chipsets produced for this purpose.
GPS receivers may include an input for differential corrections, using the RTCM SC-104 format. This is typically in the form of an RS-232 port at 4,800 bit/s speed. Data is actually sent at a much lower rate, which limits the accuracy of the signal sent using RTCM. Receivers with internal DGPS receivers can outperform those using external RTCM data. , even low-cost units commonly include Wide Area Augmentation System (WAAS) receivers.
Many GPS receivers can relay position data to a PC or other device using the NMEA 0183 protocol. Although this protocol is officially defined by the National Marine Electronics Association (NMEA), references to this protocol have been compiled from public records, allowing open source tools like gpsd to read the protocol without violating intellectual property laws. Other proprietary protocols exist as well, such as the SiRF and MTK protocols. Receivers can interface with other devices using methods including a serial connection, USB, or Bluetooth.
While originally a military project, GPS is considered a dual-use technology, meaning it has significant civilian applications as well.
GPS has become a widely deployed and useful tool for commerce, scientific uses, tracking, and surveillance. GPS's accurate time facilitates everyday activities such as banking, mobile phone operations, and even the control of power grids by allowing well synchronized hand-off switching.
Many civilian applications use one or more of GPS's three basic components: absolute location, relative movement, and time transfer.
The U.S. government controls the export of some civilian receivers. All GPS receivers capable of functioning above above sea level and , or designed or modified for use with unmanned missiles and aircraft, are classified as munitions (weapons)—which means they require State Department export licenses.
This rule applies even to otherwise purely civilian units that only receive the L1 frequency and the C/A (Coarse/Acquisition) code.
Disabling operation above these limits exempts the receiver from classification as a munition. Vendor interpretations differ. The rule refers to operation at both the target altitude and speed, but some receivers stop operating even when stationary. This has caused problems with some amateur radio balloon launches that regularly reach .
These limits only apply to units or components exported from the United States. A growing trade in various components exists, including GPS units from other countries. These are expressly sold as ITAR-free.
As of 2009, military GPS applications include:
GPS type navigation was first used in war in the 1991 Persian Gulf War, before GPS was fully developed in 1995, to assist Coalition Forces to navigate and perform maneuvers in the war. The war also demonstrated the vulnerability of GPS to being jammed, when Iraqi forces installed jamming devices on likely targets that emitted radio noise, disrupting reception of the weak GPS signal.
GPS's vulnerability to jamming is a threat that continues to grow as jamming equipment and experience grows. GPS signals have been reported to have been jammed many times over the years for military purposes. Russia seems to have several objectives for this behavior, such as intimidating neighbors while undermining confidence in their reliance on American systems, promoting their GLONASS alternative, disrupting Western military exercises, and protecting assets from drones. China uses jamming to discourage US surveillance aircraft near the contested Spratly Islands. North Korea has mounted several major jamming operations near its border with South Korea and offshore, disrupting flights, shipping and fishing operations.
While most clocks derive their time from Coordinated Universal Time (UTC), the atomic clocks on the satellites are set to GPS time (GPST; see the page of United States Naval Observatory). The difference is that GPS time is not corrected to match the rotation of the Earth, so it does not contain leap seconds or other corrections that are periodically added to UTC. GPS time was set to match UTC in 1980, but has since diverged. The lack of corrections means that GPS time remains at a constant offset with International Atomic Time (TAI) (TAI − GPS = 19 seconds). Periodic corrections are performed to the on-board clocks to keep them synchronized with ground clocks.
The GPS navigation message includes the difference between GPS time and UTC. GPS time is 18 seconds ahead of UTC because of the leap second added to UTC on December 31, 2016. Receivers subtract this offset from GPS time to calculate UTC and specific time zone values. New GPS units may not show the correct UTC time until after receiving the UTC offset message. The GPS-UTC offset field can accommodate 255 leap seconds (eight bits).
GPS time is theoretically accurate to about 14 nanoseconds, due to the clock drift that atomic clocks experience in GPS transmitters, relative to International Atomic Time. Most receivers lose accuracy in the interpretation of the signals and are only accurate to 100 nanoseconds.
As opposed to the year, month, and day format of the Gregorian calendar, the GPS date is expressed as a week number and a seconds-into-week number. The week number is transmitted as a ten-bit field in the C/A and P(Y) navigation messages, and so it becomes zero again every 1,024 weeks (19.6 years). GPS week zero started at 00:00:00 UTC (00:00:19 TAI) on January 6, 1980, and the week number became zero again for the first time at 23:59:47 UTC on August 21, 1999 (00:00:19 TAI on August 22, 1999). It happened the second time at 23:59:42 UTC on April 6, 2019. To determine the current Gregorian date, a GPS receiver must be provided with the approximate date (to within 3,584 days) to correctly translate the GPS date signal. To address this concern in the future the modernized GPS civil navigation (CNAV) message will use a 13-bit field that only repeats every 8,192 weeks (157 years), thus lasting until 2137 (157 years after GPS week zero).
The navigational signals transmitted by GPS satellites encode a variety of information including satellite positions, the state of the internal clocks, and the health of the network. These signals are transmitted on two separate carrier frequencies that are common to all satellites in the network. Two different encodings are used: a public encoding that enables lower resolution navigation, and an encrypted encoding used by the U.S. military.
Each GPS satellite continuously broadcasts a "navigation message" on L1 (C/A and P/Y) and L2 (P/Y) frequencies at a rate of 50 bits per second (see bitrate). Each complete message takes 750 seconds (12 1/2 minutes) to complete. The message structure has a basic format of a 1500-bit-long frame made up of five subframes, each subframe being 300 bits (6 seconds) long. Subframes 4 and 5 are subcommutated 25 times each, so that a complete data message requires the transmission of 25 full frames. Each subframe consists of ten words, each 30 bits long. Thus, with 300 bits in a subframe times 5 subframes in a frame times 25 frames in a message, each message is 37,500 bits long. At a transmission rate of 50-bit/s, this gives 750 seconds to transmit an entire almanac message (GPS). Each 30-second frame begins precisely on the minute or half-minute as indicated by the atomic clock on each satellite.
The first subframe of each frame encodes the week number and the time within the week, as well as the data about the health of the satellite. The second and the third subframes contain the "ephemeris" – the precise orbit for the satellite. The fourth and fifth subframes contain the "almanac", which contains coarse orbit and status information for up to 32 satellites in the constellation as well as data related to error correction. Thus, to obtain an accurate satellite location from this transmitted message, the receiver must demodulate the message from each satellite it includes in its solution for 18 to 30 seconds. To collect all transmitted almanacs, the receiver must demodulate the message for 732 to 750 seconds or 12 1/2 minutes.
All satellites broadcast at the same frequencies, encoding signals using unique code division multiple access (CDMA) so receivers can distinguish individual satellites from each other. The system uses two distinct CDMA encoding types: the coarse/acquisition (C/A) code, which is accessible by the general public, and the precise (P(Y)) code, which is encrypted so that only the U.S. military and other NATO nations who have been given access to the encryption code can access it.
The ephemeris is updated every 2 hours and is generally valid for 4 hours, with provisions for updates every 6 hours or longer in non-nominal conditions. The almanac is updated typically every 24 hours. Additionally, data for a few weeks following is uploaded in case of transmission updates that delay data upload.
All satellites broadcast at the same two frequencies, 1.57542 GHz (L1 signal) and 1.2276 GHz (L2 signal). The satellite network uses a CDMA spread-spectrum technique where the low-bitrate message data is encoded with a high-rate pseudo-random (PRN) sequence that is different for each satellite. The receiver must be aware of the PRN codes for each satellite to reconstruct the actual message data. The C/A code, for civilian use, transmits data at 1.023 million chips per second, whereas the P code, for U.S. military use, transmits at 10.23 million chips per second. The actual internal reference of the satellites is 10.22999999543 MHz to compensate for relativistic effects that make observers on the Earth perceive a different time reference with respect to the transmitters in orbit. The L1 carrier is modulated by both the C/A and P codes, while the L2 carrier is only modulated by the P code. The P code can be encrypted as a so-called P(Y) code that is only available to military equipment with a proper decryption key. Both the C/A and P(Y) codes impart the precise time-of-day to the user.
The L3 signal at a frequency of 1.38105 GHz is used to transmit data from the satellites to ground stations. This data is used by the United States Nuclear Detonation (NUDET) Detection System (USNDS) to detect, locate, and report nuclear detonations (NUDETs) in the Earth's atmosphere and near space. One usage is the enforcement of nuclear test ban treaties.
The L4 band at 1.379913 GHz is being studied for additional ionospheric correction.
The L5 frequency band at 1.17645 GHz was added in the process of GPS modernization. This frequency falls into an internationally protected range for aeronautical navigation, promising little or no interference under all circumstances. The first Block IIF satellite that provides this signal was launched in May 2010. On February 5th 2016, the 12th and final Block IIF satellite was launched. The L5 consists of two carrier components that are in phase quadrature with each other. Each carrier component is bi-phase shift key (BPSK) modulated by a separate bit train. "L5, the third civil GPS signal, will eventually support safety-of-life applications for aviation and provide improved availability and accuracy."
In 2011, a conditional waiver was granted to LightSquared to operate a terrestrial broadband service near the L1 band. Although LightSquared had applied for a license to operate in the 1525 to 1559 band as early as 2003 and it was put out for public comment, the FCC asked LightSquared to form a study group with the GPS community to test GPS receivers and identify issue that might arise due to the larger signal power from the LightSquared terrestrial network. The GPS community had not objected to the LightSquared (formerly MSV and SkyTerra) applications until November 2010, when LightSquared applied for a modification to its Ancillary Terrestrial Component (ATC) authorization. This filing (SAT-MOD-20101118-00239) amounted to a request to run several orders of magnitude more power in the same frequency band for terrestrial base stations, essentially repurposing what was supposed to be a "quiet neighborhood" for signals from space as the equivalent of a cellular network. Testing in the first half of 2011 has demonstrated that the impact of the lower 10 MHz of spectrum is minimal to GPS devices (less than 1% of the total GPS devices are affected). The upper 10 MHz intended for use by LightSquared may have some impact on GPS devices. There is some concern that this may seriously degrade the GPS signal for many consumer uses. "Aviation Week" magazine reports that the latest testing (June 2011) confirms "significant jamming" of GPS by LightSquared's system.
Because all of the satellite signals are modulated onto the same L1 carrier frequency, the signals must be separated after demodulation. This is done by assigning each satellite a unique binary sequence known as a Gold code. The signals are decoded after demodulation using addition of the Gold codes corresponding to the satellites monitored by the receiver.
If the almanac information has previously been acquired, the receiver picks the satellites to listen for by their PRNs, unique numbers in the range 1 through 32. If the almanac information is not in memory, the receiver enters a search mode until a lock is obtained on one of the satellites. To obtain a lock, it is necessary that there be an unobstructed line of sight from the receiver to the satellite. The receiver can then acquire the almanac and determine the satellites it should listen for. As it detects each satellite's signal, it identifies it by its distinct C/A code pattern. There can be a delay of up to 30 seconds before the first estimate of position because of the need to read the ephemeris data.
Processing of the navigation message enables the determination of the time of transmission and the satellite position at this time. For more information see Demodulation and Decoding, Advanced.
The receiver uses messages received from satellites to determine the satellite positions and time sent. The "x, y," and "z" components of satellite position and the time sent are designated as ["x, y, z, s"] where the subscript "i" denotes the satellite and has the value 1, 2, ..., "n", where "n" ≥ 4. When the time of message reception indicated by the on-board receiver clock is "t̃", the true reception time is , where "b" is the receiver's clock bias from the much more accurate GPS clocks employed by the satellites. The receiver clock bias is the same for all received satellite signals (assuming the satellite clocks are all perfectly synchronized). The message's transit time is , where "s" is the satellite time. Assuming the message traveled at the speed of light, "c", the distance traveled is .
For n satellites, the equations to satisfy are:
where "d" is the geometric distance or range between receiver and satellite "i" (the values without subscripts are the "x, y," and "z" components of receiver position):
Defining "pseudoranges" as formula_3, we see they are biased versions of the true range:
Since the equations have four unknowns ["x, y, z, b"]—the three components of GPS receiver position and the clock bias—signals from at least four satellites are necessary to attempt solving these equations. They can be solved by algebraic or numerical methods. Existence and uniqueness of GPS solutions are discussed by Abell and Chaffee. When "n" is greater than 4 this system is overdetermined and a fitting method must be used.
The amount of error in the results varies with the received satellites' locations in the sky, since certain configurations (when the received satellites are close together in the sky) cause larger errors. Receivers usually calculate a running estimate of the error in the calculated position. This is done by multiplying the basic resolution of the receiver by quantities called the geometric dilution of position (GDOP) factors, calculated from the relative sky directions of the satellites used. The receiver location is expressed in a specific coordinate system, such as latitude and longitude using the WGS 84 geodetic datum or a country-specific system.
The GPS equations can be solved by numerical and analytical methods. Geometrical interpretations can enhance the understanding of these solution methods.
The measured ranges, called pseudoranges, contain clock errors. In a simplified idealization in which the ranges are synchronized, these true ranges represent the radii of spheres, each centered on one of the transmitting satellites. The solution for the position of the receiver is then at the intersection of the surfaces of these spheres. Signals from at minimum three satellites are required, and their three spheres would typically intersect at two points. One of the points is the location of the receiver, and the other moves rapidly in successive measurements and would not usually be on Earth's surface.
In practice, there are many sources of inaccuracy besides clock bias, including random errors as well as the potential for precision loss from subtracting numbers close to each other if the centers of the spheres are relatively close together. This means that the position calculated from three satellites alone is unlikely to be accurate enough. Data from more satellites can help because of the tendency for random errors to cancel out and also by giving a larger spread between the sphere centers. But at the same time, more spheres will not generally intersect at one point. Therefore, a near intersection gets computed, typically via least squares. The more signals available, the better the approximation is likely to be.
If the pseudorange between the receiver and satellite "i" and the pseudorange between the receiver and satellite "j" are subtracted, , the common receiver clock bias ("b") cancels out, resulting in a difference of distances . The locus of points having a constant difference in distance to two points (here, two satellites) is a hyperbola on a plane and a hyperboloid of revolution in 3D space (see Multilateration). Thus, from four pseudorange measurements, the receiver can be placed at the intersection of the surfaces of three hyperboloids each with foci at a pair of satellites. With additional satellites, the multiple intersections are not necessarily unique, and a best-fitting solution is sought instead.
The receiver position can be interpreted as the center of an inscribed sphere (insphere) of radius "bc", given by the receiver clock bias "b" (scaled by the speed of light "c"). The insphere location is such that it touches other spheres (see Problem of Apollonius#Applications). The circumscribing spheres are centered at the GPS satellites, whose radii equal the measured pseudoranges "p". This configuration is distinct from the one described in section #Spheres, in which the spheres' radii were the unbiased or geometric ranges "d".
The clock in the receiver is usually not of the same quality as the ones in the satellites and will not be accurately synchronised to them. This produces large errors in the computed distances to the satellites. Therefore, in practice, the time difference between the receiver clock and the satellite time is defined as an unknown clock bias "b". The equations are then solved simultaneously for the receiver position and the clock bias. The solution space ["x, y, z, b"] can be seen as a four-dimensional geometric space, and signals from at minimum four satellites are needed. In that case each of the equations describes a spherical cone, with the cusp located at the satellite, and the base a sphere around the satellite. The receiver is at the intersection of four or more of such cones.
When more than four satellites are available, the calculation can use the four best, or more than four simultaneously (up to all visible satellites), depending on the number of receiver channels, processing capability, and geometric dilution of precision (GDOP).
Using more than four involves an over-determined system of equations with no unique solution; such a system can be solved by a least-squares or weighted least squares method.
Both the equations for four satellites, or the least squares equations for more than four, are non-linear and need special solution methods. A common approach is by iteration on a linearized form of the equations, such as the Gauss–Newton algorithm.
The GPS was initially developed assuming use of a numerical least-squares solution method—i.e., before closed-form solutions were found.
One closed-form solution to the above set of equations was developed by S. Bancroft. Its properties are well known; in particular, proponents claim it is superior in low-GDOP situations, compared to iterative least squares methods.
Bancroft's method is algebraic, as opposed to numerical, and can be used for four or more satellites. When four satellites are used, the key steps are inversion of a 4x4 matrix and solution of a single-variable quadratic equation. Bancroft's method provides one or two solutions for the unknown quantities. When there are two (usually the case), only one is a near-Earth sensible solution.
When a receiver uses more than four satellites for a solution, Bancroft uses the generalized inverse (i.e., the pseudoinverse) to find a solution. A case has been made that iterative methods, such as the Gauss–Newton algorithm approach for solving over-determined non-linear least squares (NLLS) problems, generally provide more accurate solutions.
Leick et al. (2015) states that "Bancroft's (1985) solution is a very early, if not the first, closed-form solution."
Other closed-form solutions were published afterwards, although their adoption in practice is unclear.
GPS error analysis examines error sources in GPS results and the expected size of those errors. GPS makes corrections for receiver clock errors and other effects, but some residual errors remain uncorrected. Error sources include signal arrival time measurements, numerical calculations, atmospheric effects (ionospheric/tropospheric delays), ephemeris and clock data, multipath signals, and natural and artificial interference. Magnitude of residual errors from these sources depends on geometric dilution of precision. Artificial errors may result from jamming devices and threaten ships and aircraft or from intentional signal degradation through selective availability, which limited accuracy to ≈ , but has been switched off since May 1, 2000.
Integrating external information into the calculation process can materially improve accuracy. Such augmentation systems are generally named or described based on how the information arrives. Some systems transmit additional error information (such as clock drift, ephemera, or ionospheric delay), others characterize prior errors, while a third group provides additional navigational or vehicle information.
Examples of augmentation systems include the Wide Area Augmentation System (WAAS), European Geostationary Navigation Overlay Service (EGNOS), Differential GPS (DGPS), inertial navigation systems (INS) and Assisted GPS. The standard accuracy of about can be augmented to with DGPS, and to about with WAAS.
Accuracy can be improved through precise monitoring and measurement of existing GPS signals in additional or alternative ways.
The largest remaining error is usually the unpredictable delay through the ionosphere. The spacecraft broadcast ionospheric model parameters, but some errors remain. This is one reason GPS spacecraft transmit on at least two frequencies, L1 and L2. Ionospheric delay is a well-defined function of frequency and the total electron content (TEC) along the path, so measuring the arrival time difference between the frequencies determines TEC and thus the precise ionospheric delay at each frequency.
Military receivers can decode the P(Y) code transmitted on both L1 and L2. Without decryption keys, it is still possible to use a "codeless" technique to compare the P(Y) codes on L1 and L2 to gain much of the same error information. This technique is slow, so it is currently available only on specialized surveying equipment. In the future, additional civilian codes are expected to be transmitted on the L2 and L5 frequencies (see GPS modernization). All users will then be able to perform dual-frequency measurements and directly compute ionospheric delay errors.
A second form of precise monitoring is called "Carrier-Phase Enhancement" (CPGPS). This corrects the error that arises because the pulse transition of the PRN is not instantaneous, and thus the correlation (satellite–receiver sequence matching) operation is imperfect. CPGPS uses the L1 carrier wave, which has a period of formula_6, which is about one-thousandth of the C/A Gold code bit period of formula_7, to act as an additional clock signal and resolve the uncertainty. The phase difference error in the normal GPS amounts to of ambiguity. CPGPS working to within 1% of perfect transition reduces this error to of ambiguity. By eliminating this error source, CPGPS coupled with DGPS normally realizes between of absolute accuracy.
"Relative Kinematic Positioning" (RKP) is a third alternative for a precise GPS-based positioning system. In this approach, determination of range signal can be resolved to a precision of less than . This is done by resolving the number of cycles that the signal is transmitted and received by the receiver by using a combination of differential GPS (DGPS) correction data, transmitting GPS signal phase information and ambiguity resolution techniques via statistical tests—possibly with processing in real-time (real-time kinematic positioning, RTK).
Another method that is used in surveying applications is carrier phase tracking. The period of the carrier frequency multiplied by the speed of light gives the wavelength, which is about for the L1 carrier. Accuracy within 1% of wavelength in detecting the leading edge reduces this component of pseudorange error to as little as . This compares to for the C/A code and for the P code.
Triple differencing followed by numerical root finding, and a mathematical technique called least squares can estimate the position of one receiver given the position of another. First, compute the difference between satellites, then between receivers, and finally between epochs. Other orders of taking differences are equally valid. Detailed discussion of the errors is omitted.
The satellite carrier total phase can be measured with ambiguity as to the number of cycles. Let formula_8 denote the phase of the carrier of satellite "j" measured by receiver "i" at time formula_9. This notation shows the meaning of the subscripts "i, j," and "k." The receiver ("r"), satellite ("s"), and time ("t") come in alphabetical order as arguments of formula_10 and to balance readability and conciseness, let formula_11 be a concise abbreviation. Also we define three functions, :formula_12, which return differences between receivers, satellites, and time points, respectively. Each function has variables with three subscripts as its arguments. These three functions are defined below. If formula_13 is a function of the three integer arguments, "i, j," and "k" then it is a valid argument for the functions, :formula_12, with the values defined as
Also if formula_18 are valid arguments for the three functions and "a" and "b" are constants then
formula_19 is a valid argument with values defined as
Receiver clock errors can be approximately eliminated by differencing the phases measured from satellite 1 with that from satellite 2 at the same epoch. This difference is designated as formula_23
Double differencing computes the difference of receiver 1's satellite difference from that of receiver 2. This approximately eliminates satellite clock errors. This double difference is:
Triple differencing subtracts the receiver difference from time 1 from that of time 2. This eliminates the ambiguity associated with the integral number of wavelengths in carrier phase provided this ambiguity does not change with time. Thus the triple difference result eliminates practically all clock bias errors and the integer ambiguity. Atmospheric delay and satellite ephemeris errors have been significantly reduced. This triple difference is:
Triple difference results can be used to estimate unknown variables. For example, if the position of receiver 1 is known but the position of receiver 2 unknown, it may be possible to estimate the position of receiver 2 using numerical root finding and least squares. Triple difference results for three independent time pairs may be sufficient to solve for receiver 2's three position components. This may require a numerical procedure. An approximation of receiver 2's position is required to use such a numerical method. This initial value can probably be provided from the navigation message and the intersection of sphere surfaces. Such a reasonable estimate can be key to successful multidimensional root finding. Iterating from three time pairs and a fairly good initial value produces one observed triple difference result for receiver 2's position. Processing additional time pairs can improve accuracy, overdetermining the answer with multiple solutions. Least squares can estimate an overdetermined system. Least squares determines the position of receiver 2 that best fits the observed triple difference results for receiver 2 positions under the criterion of minimizing the sum of the squares.
In the United States, GPS receivers are regulated under the Federal Communications Commission's (FCC) Part 15 rules. As indicated in the manuals of GPS-enabled devices sold in the United States, as a Part 15 device, it "must accept any interference received, including interference that may cause undesired operation." With respect to GPS devices in particular, the FCC states that GPS receiver manufacturers, "must use receivers that reasonably discriminate against reception of signals outside their allocated spectrum." For the last 30 years, GPS receivers have operated next to the Mobile Satellite Service band, and have discriminated against reception of mobile satellite services, such as Inmarsat, without any issue.
The spectrum allocated for GPS L1 use by the FCC is 1559 to 1610 MHz, while the spectrum allocated for satellite-to-ground use owned by Lightsquared is the Mobile Satellite Service band. Since 1996, the FCC has authorized licensed use of the spectrum neighboring the GPS band of 1525 to 1559 MHz to the Virginia company LightSquared. On March 1, 2001, the FCC received an application from LightSquared's predecessor, Motient Services, to use their allocated frequencies for an integrated satellite-terrestrial service. In 2002, the U.S. GPS Industry Council came to an out-of-band-emissions (OOBE) agreement with LightSquared to prevent transmissions from LightSquared's ground-based stations from emitting transmissions into the neighboring GPS band of 1559 to 1610 MHz. In 2004, the FCC adopted the OOBE agreement in its authorization for LightSquared to deploy a ground-based network ancillary to their satellite system – known as the Ancillary Tower Components (ATCs) – "We will authorize MSS ATC subject to conditions that ensure that the added terrestrial component remains ancillary to the principal MSS offering. We do not intend, nor will we permit, the terrestrial component to become a stand-alone service." This authorization was reviewed and approved by the U.S. Interdepartment Radio Advisory Committee, which includes the U.S. Department of Agriculture, U.S. Space Force, U.S. Army, U.S. Coast Guard, Federal Aviation Administration, National Aeronautics and Space Administration, Interior, and U.S. Department of Transportation.
In January 2011, the FCC conditionally authorized LightSquared's wholesale customers—such as Best Buy, Sharp, and C Spire—to only purchase an integrated satellite-ground-based service from LightSquared and re-sell that integrated service on devices that are equipped to only use the ground-based signal using LightSquared's allocated frequencies of 1525 to 1559 MHz. In December 2010, GPS receiver manufacturers expressed concerns to the FCC that LightSquared's signal would interfere with GPS receiver devices although the FCC's policy considerations leading up to the January 2011 order did not pertain to any proposed changes to the maximum number of ground-based LightSquared stations or the maximum power at which these stations could operate. The January 2011 order makes final authorization contingent upon studies of GPS interference issues carried out by a LightSquared led working group along with GPS industry and Federal agency participation. On February 14, 2012, the FCC initiated proceedings to vacate LightSquared's Conditional Waiver Order based on the NTIA's conclusion that there was currently no practical way to mitigate potential GPS interference.
GPS receiver manufacturers design GPS receivers to use spectrum beyond the GPS-allocated band. In some cases, GPS receivers are designed to use up to 400 MHz of spectrum in either direction of the L1 frequency of 1575.42 MHz, because mobile satellite services in those regions are broadcasting from space to ground, and at power levels commensurate with mobile satellite services. As regulated under the FCC's Part 15 rules, GPS receivers are not warranted protection from signals outside GPS-allocated spectrum. This is why GPS operates next to the Mobile Satellite Service band, and also why the Mobile Satellite Service band operates next to GPS. The symbiotic relationship of spectrum allocation ensures that users of both bands are able to operate cooperatively and freely.
The FCC adopted rules in February 2003 that allowed Mobile Satellite Service (MSS) licensees such as LightSquared to construct a small number of ancillary ground-based towers in their licensed spectrum to "promote more efficient use of terrestrial wireless spectrum." In those 2003 rules, the FCC stated "As a preliminary matter, terrestrial [Commercial Mobile Radio Service (“CMRS”)] and MSS ATC are expected to have different prices, coverage, product acceptance and distribution; therefore, the two services appear, at best, to be imperfect substitutes for one another that would be operating in predominantly different market segments... MSS ATC is unlikely to compete directly with terrestrial CMRS for the same customer base...". In 2004, the FCC clarified that the ground-based towers would be ancillary, noting that "We will authorize MSS ATC subject to conditions that ensure that the added terrestrial component remains ancillary to the principal MSS offering. We do not intend, nor will we permit, the terrestrial component to become a stand-alone service." In July 2010, the FCC stated that it expected LightSquared to use its authority to offer an integrated satellite-terrestrial service to "provide mobile broadband services similar to those provided by terrestrial mobile providers and enhance competition in the mobile broadband sector." GPS receiver manufacturers have argued that LightSquared's licensed spectrum of 1525 to 1559 MHz was never envisioned as being used for high-speed wireless broadband based on the 2003 and 2004 FCC ATC rulings making clear that the Ancillary Tower Component (ATC) would be, in fact, ancillary to the primary satellite component. To build public support of efforts to continue the 2004 FCC authorization of LightSquared's ancillary terrestrial component vs. a simple ground-based LTE service in the Mobile Satellite Service band, GPS receiver manufacturer Trimble Navigation Ltd. formed the "Coalition To Save Our GPS."
The FCC and LightSquared have each made public commitments to solve the GPS interference issue before the network is allowed to operate. According to Chris Dancy of the Aircraft Owners and Pilots Association, airline pilots with the type of systems that would be affected "may go off course and not even realize it." The problems could also affect the Federal Aviation Administration upgrade to the air traffic control system, United States Defense Department guidance, and local emergency services including 911.
On February 14, 2012, the U.S. Federal Communications Commission (FCC) moved to bar LightSquared's planned national broadband network after being informed by the National Telecommunications and Information Administration (NTIA), the federal agency that coordinates spectrum uses for the military and other federal government entities, that "there is no practical way to mitigate potential interference at this time". LightSquared is challenging the FCC's action.
Other notable satellite navigation systems in use or various states of development include:

</doc>
<doc id="11867" url="https://en.wikipedia.org/wiki?curid=11867" title="Germany">
Germany

Germany (, ), officially the Federal Republic of Germany (, ), is a country in Central and Western Europe. Covering an area of , it lies between the Baltic and North seas to the north, and the Alps to the south. It borders Denmark to the north, Poland and the Czech Republic to the east, Austria and Switzerland to the south, and France, Luxembourg, Belgium and the Netherlands to the west.
Various Germanic tribes have inhabited the northern parts of modern Germany since classical antiquity. A region named Germania was documented before AD 100. Beginning in the 10th century, German territories formed a central part of the Holy Roman Empire. During the 16th century, northern German regions became the centre of the Protestant Reformation. Following the Napoleonic Wars and the dissolution of the Holy Roman Empire in 1806, the German Confederation was formed in 1815. In 1871, Germany became a nation state when most of the German states unified into the Prussian-dominated German Empire. After World War I and the German Revolution of 1918–1919, the Empire was replaced by the semi-presidential Weimar Republic. The Nazi seizure of power in 1933 led to the establishment of a dictatorship, World War II, and the Holocaust. After the end of World War II in Europe and a period of Allied occupation, two new German states were founded: the Federal Republic of Germany, generally known as West Germany, and the German Democratic Republic, East Germany. The Federal Republic of Germany was a founding member of the European Economic Community and the European Union, while the German Democratic Republic was a communist Eastern Bloc state and member of the Warsaw Pact. After the fall of communism, German reunification saw the former East German states join the Federal Republic of Germany on 3 October 1990.
Today, Germany is a federal parliamentary republic led by a chancellor. With over 83 million inhabitants of its 16 constituent states, it is the second-most populous country in Europe after Russia, as well as the most populous member state of the European Union. Its capital and largest city is Berlin, and its financial centre is Frankfurt; the largest urban area is the Ruhr.
Germany is a great power with a strong economy; it has the largest economy in Europe, the world's fourth-largest economy by nominal GDP, and the fifth-largest by PPP. As a global leader in several industrial and technological sectors, it is both the world's third-largest exporter and importer of goods. A highly developed country with a very high standard of living, it offers social security and a universal health care system, environmental protections, and a tuition-free university education. Germany is also a member of the United Nations, NATO, the G7, the G20, and the OECD. Known for its long and rich cultural history, Germany has many World Heritage sites and is among the top tourism destinations in the world.
The English word "Germany" derives from the Latin , which came into use after Julius Caesar adopted it for the peoples east of the Rhine. The German term , originally ("the German lands") is derived from , descended from Old High German "of the people" (from or "people"), originally used to distinguish the language of the common people from Latin and its Romance descendants. This in turn descends from Proto-Germanic "of the people" (see also the Latinised form ), derived from , descended from Proto-Indo-European *"" "people", from which the word "Teutons" also originates.
Ancient humans were present in Germany at least 600,000 years ago. The first non-modern human fossil (the Neanderthal) was discovered in the Neander Valley. Similarly dated evidence of modern humans has been found in the Swabian Jura, including 42,000-year-old flutes which are the oldest musical instruments ever found, the 40,000-year-old Lion Man, and the 35,000-year-old Venus of Hohle Fels. The Nebra sky disk, created during the European Bronze Age, is attributed to a German site.
The Germanic tribes are thought to date from the Nordic Bronze Age or the Pre-Roman Iron Age. From southern Scandinavia and north Germany, they expanded south, east, and west, coming into contact with the Celtic, Iranian, Baltic, and Slavic tribes.
Under Augustus, Rome began to invade Germania. In 9 AD, three Roman legions were defeated by Arminius. By 100 AD, when Tacitus wrote "Germania", Germanic tribes had settled along the Rhine and the Danube (the Limes Germanicus), occupying most of modern Germany. However, Baden Württemberg, southern Bavaria, southern Hesse and the western Rhineland had been incorporated into Roman provinces. Around 260, Germanic peoples broke into Roman-controlled lands. After the invasion of the Huns in 375, and with the decline of Rome from 395, Germanic tribes moved farther southwest: the Franks established the Frankish Kingdom and pushed east to subjugate Saxony and Bavaria, and areas of what is today eastern Germany were inhabited by Western Slavic tribes.
Charlemagne founded the Carolingian Empire in 800; it was divided in 843 and the Holy Roman Empire emerged from the eastern portion. The territory initially known as East Francia stretched from the Rhine in the west to the Elbe River in the east and from the North Sea to the Alps. The Ottonian rulers (919–1024) consolidated several major duchies. In 996 Gregory V became the first German Pope, appointed by his cousin Otto III, whom he shortly after crowned Holy Roman Emperor. The Holy Roman Empire absorbed northern Italy and Burgundy under the Salian emperors (1024–1125), although the emperors lost power through the Investiture controversy.
Under the Hohenstaufen emperors (1138–1254), German princes encouraged German settlement to the south and east "(Ostsiedlung)". Members of the Hanseatic League, mostly north German towns, prospered in the expansion of trade. Population declined starting with the Great Famine in 1315, followed by the Black Death of 1348–50. The Golden Bull issued in 1356 provided the constitutional structure of the Empire and codified the election of the emperor by seven prince-electors.
Johannes Gutenberg introduced moveable-type printing to Europe, laying the basis for the democratization of knowledge. In 1517, Martin Luther incited the Protestant Reformation; the 1555 Peace of Augsburg tolerated the "Evangelical" faith (Lutheranism), but also decreed that the faith of the prince was to be the faith of his subjects ("cuius regio, eius religio"). From the Cologne War through the Thirty Years' Wars (1618–1648), religious conflict devastated German lands and significantly reduced the population.
The Peace of Westphalia ended religious warfare among the Imperial Estates; their mostly German-speaking rulers were able to choose Roman Catholicism, Lutheranism, or the Reformed faith as their official religion. The legal system initiated by a series of Imperial Reforms (approximately 1495–1555) provided for considerable local autonomy and a stronger Imperial Diet. The House of Habsburg held the imperial crown from 1438 until the death of Charles VI in 1740. Following the War of Austrian Succession and the Treaty of Aix-la-Chapelle, Charles VI's daughter Maria Theresa ruled as Empress Consort when her husband, Francis I, became Emperor.
From 1740, dualism between the Austrian Habsburg Monarchy and the Kingdom of Prussia dominated German history. In 1772, 1793, and 1795, Prussia and Austria, along with the Russian Empire, agreed to the Partitions of Poland. During the period of the French Revolutionary Wars, the Napoleonic era and the subsequent final meeting of the Imperial Diet, most of the Free Imperial Cities were annexed by dynastic territories; the ecclesiastical territories were secularised and annexed. In 1806 the "Imperium" was dissolved; France, Russia, Prussia and the Habsburgs (Austria) competed for hegemony in the German states during the Napoleonic Wars.
Following the fall of Napoleon, the Congress of Vienna founded the German Confederation, a loose league of 39 sovereign states. The appointment of the Emperor of Austria as the permanent president reflected the Congress's rejection of Prussia's rising influence. Disagreement within restoration politics partly led to the rise of liberal movements, followed by new measures of repression by Austrian statesman Klemens von Metternich. The "Zollverein", a tariff union, furthered economic unity. In light of revolutionary movements in Europe, intellectuals and commoners started the revolutions of 1848 in the German states, raising the German Question. King Frederick William IV of Prussia was offered the title of Emperor, but with a loss of power; he rejected the crown and the proposed constitution, a temporary setback for the movement.
King William I appointed Otto von Bismarck as the Minister President of Prussia in 1862. Bismarck successfully concluded the war with Denmark in 1864; the subsequent decisive Prussian victory in the Austro-Prussian War of 1866 enabled him to create the North German Confederation which excluded Austria. After the defeat of France in the Franco-Prussian War, the German princes proclaimed the founding of the German Empire in 1871. Prussia was the dominant constituent state of the new empire; the King of Prussia ruled as its Kaiser, and Berlin became its capital.
In the period following the unification of Germany, Bismarck's foreign policy as Chancellor of Germany secured Germany's position as a great nation by forging alliances and avoiding war. However, under Wilhelm II, Germany took an imperialistic course, leading to friction with neighbouring countries. A dual alliance was created with the multinational realm of Austria-Hungary; the Triple Alliance of 1882 included Italy. Britain, France and Russia also concluded alliances to protect against Habsburg interference with Russian interests in the Balkans or German interference against France. At the Berlin Conference in 1884, Germany claimed several colonies including German East Africa, German South West Africa, Togoland, and Kamerun. Later, Germany further expanded its colonial empire to include holdings in the Pacific and China. The colonial government in South West Africa (present-day Namibia), from 1904 to 1907, carried out the annihilation of the local Herero and Namaqua peoples as punishment for an uprising; this was the 20th century's first genocide.
The assassination of Austria's crown prince on 28 June 1914 provided the pretext for Austria-Hungary to attack Serbia and trigger World War I. After four years of warfare, in which approximately two million German soldiers were killed, a general armistice ended the fighting. In the German Revolution (November 1918), Emperor Wilhelm II and the ruling princes abdicated their positions and Germany was declared a federal republic. Germany's new leadership signed the Treaty of Versailles in 1919, accepting defeat by the Allies. Germans perceived the treaty as humiliating, which was seen by historians as influential in the rise of Adolf Hitler. Germany lost around 13% of its European territory and ceded all of its colonial possessions in Africa and the South Sea.
On 11 August 1919, President Friedrich Ebert signed the democratic Weimar Constitution. In the subsequent struggle for power, communists seized power in Bavaria, but conservative elements elsewhere attempted to overthrow the Republic in the Kapp Putsch. Street fighting in the major industrial centres, the occupation of the Ruhr by Belgian and French troops, and a period of hyperinflation followed. A debt restructuring plan and the creation of a new currency in 1924 ushered in the Golden Twenties, an era of artistic innovation and liberal cultural life.
The worldwide Great Depression hit Germany in 1929. Chancellor Heinrich Brüning's government pursued a policy of fiscal austerity and deflation which caused unemployment of nearly 30% by 1932. The Nazi Party led by Adolf Hitler won a special election in 1932 and Hindenburg appointed Hitler as Chancellor of Germany on 30 January 1933. After the Reichstag fire, a decree abrogated basic civil rights and the first Nazi concentration camp opened. The Enabling Act gave Hitler unrestricted legislative power, overriding the constitution; his government established a centralised totalitarian state, withdrew from the League of Nations, and dramatically increased the country's rearmament. A government-sponsored programme for economic renewal focused on public works, the most famous of which was the autobahn.
In 1935, the regime withdrew from the Treaty of Versailles and introduced the Nuremberg Laws which targeted Jews and other minorities. Germany also reacquired control of the Saarland in 1935, remilitarised the Rhineland in 1936, annexed Austria in 1938, annexed the Sudetenland in 1938 with the Munich Agreement, and in violation of the agreement occupied Czechoslovakia in March 1939. "Kristallnacht (Night of Broken Glass)" saw the burning of synagogues, the destruction of Jewish businesses, and mass arrests of Jewish people.
In August 1939, Hitler's government negotiated the Molotov–Ribbentrop pact that divided Eastern Europe into German and Soviet spheres of influence. On 1 September 1939, Germany invaded Poland, beginning World War II in Europe; Britain and France declared war on Germany on 3 September. In the spring of 1940, Germany conquered Denmark and Norway, the Netherlands, Belgium, Luxembourg, and France, forcing the French government to sign an armistice. The British repelled German air attacks in the Battle of Britain in the same year. In 1941, German troops invaded Yugoslavia, Greece and the Soviet Union. By 1942, Germany and her allies controlled most of continental Europe and North Africa, but following the Soviet victory at the Battle of Stalingrad, the allies' reconquest of North Africa and invasion of Italy in 1943, German forces suffered repeated military defeats. In 1944, the Soviets pushed into Eastern Europe; the Western allies landed in France and entered Germany despite a final German counteroffensive. Following Hitler's suicide during the Battle of Berlin, Germany surrendered on 8 May 1945, ending World War II in Europe. Following the end of the war, surviving Nazi officials were tried for war crimes at the Nuremberg trials.
In what later became known as the Holocaust, the German government persecuted minorities, including interning them in concentration and death camps across Europe. In total 17 million people were systematically murdered, including 6 million Jews, at least 130,000 Romani, 275,000 persons with disabilities, thousands of Jehovah's Witnesses, thousands of homosexuals, and hundreds of thousands of political and religious opponents. Nazi policies in German-occupied countries resulted in the deaths of an estimated 2.7 million Poles, 1.3 million Ukrainians, 1 million Belarusians and 3.5 million Soviet prisoners of war. German military casualties have been estimated at 5.3 million, and around 900,000 German civilians died. Around 12 million ethnic Germans were expelled from across Eastern Europe, and Germany lost roughly one-quarter of its pre-war territory.
After Nazi Germany surrendered, the Allies partitioned Berlin and Germany's remaining territory into four occupation zones. The western sectors, controlled by France, the United Kingdom, and the United States, were merged on 23 May 1949 to form the Federal Republic of Germany ("Bundesrepublik Deutschland (BRD)"); on 7 October 1949, the Soviet Zone became the German Democratic Republic ("Deutsche Demokratische Republik (DDR)"). They were informally known as West Germany and East Germany. East Germany selected East Berlin as its capital, while West Germany chose Bonn as a provisional capital, to emphasise its stance that the two-state solution was temporary.
West Germany was established as a federal parliamentary republic with a "social market economy". Starting in 1948 West Germany became a major recipient of reconstruction aid under the Marshall Plan. Konrad Adenauer was elected the first Federal Chancellor of Germany in 1949. The country enjoyed prolonged economic growth ("Wirtschaftswunder") beginning in the early 1950s. West Germany joined NATO in 1955 and was a founding member of the European Economic Community.
East Germany was an Eastern Bloc state under political and military control by the USSR via occupation forces and the Warsaw Pact. Although East Germany claimed to be a democracy, political power was exercised solely by leading members ("Politbüro") of the communist-controlled Socialist Unity Party of Germany, supported by the Stasi, an immense secret service. While East German propaganda was based on the benefits of the GDR's social programmes and the alleged threat of a West German invasion, many of its citizens looked to the West for freedom and prosperity. The Berlin Wall, built in 1961, prevented East German citizens from escaping to West Germany, becoming a symbol of the Cold War.
Tensions between East and West Germany were reduced in the late 1960s by Chancellor Willy Brandt's . In 1989, Hungary decided to dismantle the Iron Curtain and open its border with Austria, causing the emigration of thousands of East Germans to West Germany via Hungary and Austria. This had devastating effects on the GDR, where regular mass demonstrations received increasing support. In an effort to help retain East Germany as a state, the East German authorities eased border restrictions, but this actually led to an acceleration of the "Wende" reform process culminating in the "Two Plus Four Treaty" under which Germany regained full sovereignty. This permitted German reunification on 3 October 1990, with the accession of the five re-established states of the former GDR. The fall of the Wall in 1989 became a symbol of the Fall of Communism, the Dissolution of the Soviet Union, German Reunification and "Die Wende".
United Germany was considered the enlarged continuation of West Germany so it retained its memberships in international organisations. Based on the Berlin/Bonn Act (1994), Berlin again became the capital of Germany, while Bonn obtained the unique status of a "Bundesstadt" (federal city) retaining some federal ministries. The relocation of the government was completed in 1999, and modernisation of the east German economy was scheduled to last until 2019.
Since reunification, Germany has taken a more active role in the European Union, signing the Maastricht Treaty in 1992 and the Lisbon Treaty in 2007, and co-founding the Eurozone. Germany sent a peacekeeping force to secure stability in the Balkans and sent German troops to Afghanistan as part of a NATO effort to provide security in that country after the ousting of the Taliban.
In the 2005 elections, Angela Merkel became the first female chancellor. In 2009 the German government approved a €50 billion stimulus plan. Among the major German political projects of the early 21st century are the advancement of European integration, the energy transition ("Energiewende") for a sustainable energy supply, the "Debt Brake" for balanced budgets, measures to increase the fertility rate (pronatalism), and high-tech strategies for the transition of the German economy, summarised as Industry 4.0. Germany was affected by the European migrant crisis in 2015: the country took in over a million migrants and developed a quota system which redistributed migrants around its federal states.
Germany is in Western and Central Europe, bordering Denmark to the north, Poland and the Czech Republic to the east, Austria to the southeast, and Switzerland to the south-southwest. France, Luxembourg and Belgium are situated to the west, with the Netherlands to the northwest. Germany is also bordered by the North Sea and, at the north-northeast, by the Baltic Sea. German territory covers , consisting of of land and of water. It is the seventh largest country by area in Europe and the 62nd largest in the world.
Elevation ranges from the mountains of the Alps (highest point: the Zugspitze at ) in the south to the shores of the North Sea ("Nordsee") in the northwest and the Baltic Sea ("Ostsee") in the northeast. The forested uplands of central Germany and the lowlands of northern Germany (lowest point: in the municipality Neuendorf-Sachsenbande, Wilstermarsch at below sea level) are traversed by such major rivers as the Rhine, Danube and Elbe. Significant natural resources include iron ore, coal, potash, timber, lignite, uranium, copper, natural gas, salt, and nickel.
Most of Germany has a temperate climate, ranging from oceanic in the north to continental in the east and southeast. Winters range from cold in the southern Alps to mild and are generally overcast with limited precipitation, while summers can vary from hot and dry to cool and rainy. The northern regions have prevailing westerly winds that bring in moist air from the North Sea, moderating the temperature and increasing precipitation. Conversely, the southeast regions have more extreme temperatures.
From February 2019–2020, average monthly temperatures in Germany ranged from a low of in January 2020 to a high of in June 2019. Average monthly precipitation ranged from 30 litres per square metre in February and April 2019 to 125 litres per square metre in February 2020. Average monthly hours of sunshine ranged from 45 in November 2019 to 300 in June 2019. The highest temperature ever recorded in Germany was 42.6 °C on 25 July, 2020 in Lingen and the lowest was -37.8 °C on 12 February, 1929 in Wolznach.
The territory of Germany can be divided into two ecoregions: European-Mediterranean montane mixed forests and Northeast-Atlantic shelf marine. 51% of Germany's land area is devoted to agriculture, while 30% is forested and 14% is covered by settlements or infrastructure.
Plants and animals include those generally common to Central Europe. According to the National Forest Inventory, beeches, oaks, and other deciduous trees constitute just over 40% of the forests; roughly 60% are conifers, particularly spruce and pine. There are many species of ferns, flowers, fungi, and mosses. Wild animals include roe deer, wild boar, mouflon (a subspecies of wild sheep), fox, badger, hare, and small numbers of the Eurasian beaver. The blue cornflower was once a German national symbol.
The 16 national parks in Germany include the Jasmund National Park, the Vorpommern Lagoon Area National Park, the Müritz National Park, the Wadden Sea National Parks, the Harz National Park, the Hainich National Park, the Black Forest National Park, the Saxon Switzerland National Park, the Bavarian Forest National Park and the Berchtesgaden National Park. In addition, there are 17 Biosphere Reserves and 105 nature parks. More than 400 zoos and animal parks operate in Germany. The Berlin Zoo, which opened in 1844, is the oldest in Germany, and claims the most comprehensive collection of species in the world.
Germany is a federal, parliamentary, representative democratic republic. Federal legislative power is vested in the parliament consisting of the "Bundestag" (Federal Diet) and "Bundesrat" (Federal Council), which together form the legislative body. The "Bundestag" is elected through direct elections using the mixed-member proportional representation system. The members of the "Bundesrat" represent and are appointed by the governments of the sixteen federated states. The German political system operates under a framework laid out in the 1949 constitution known as the "Grundgesetz" (Basic Law). Amendments generally require a two-thirds majority of both the "Bundestag" and the "Bundesrat"; the fundamental principles of the constitution, as expressed in the articles guaranteeing human dignity, the separation of powers, the federal structure, and the rule of law, are valid in perpetuity.
The president, currently Frank-Walter Steinmeier, is the head of state and invested primarily with representative responsibilities and powers. He is elected by the "Bundesversammlung" (federal convention), an institution consisting of the members of the "Bundestag" and an equal number of state delegates. The second-highest official in the German order of precedence is the "Bundestagspräsident" (president of the "Bundestag"), who is elected by the "Bundestag" and responsible for overseeing the daily sessions of the body. The third-highest official and the head of government is the chancellor, who is appointed by the "Bundespräsident" after being elected by the party or coalition with the most seats in the "Bundestag". The chancellor, currently Angela Merkel, is the head of government and exercises executive power through their Cabinet.
Since 1949, the party system has been dominated by the Christian Democratic Union and the Social Democratic Party of Germany. So far every chancellor has been a member of one of these parties. However, the smaller liberal Free Democratic Party and the Alliance '90/The Greens have also been junior partners in coalition governments. Since 2007, the left-wing populist party The Left has been a staple in the German "Bundestag", though they have never been part of the federal government. In the 2017 German federal election, the right-wing populist Alternative for Germany gained enough votes to attain representation in the parliament for the first time.
Germany comprises sixteen federal states which are collectively referred to as "Bundesländer". Each state has its own state constitution, and is largely autonomous in regard to its internal organisation. Germany is divided into 401 districts ("Kreise") at a municipal level; these consist of 294 rural districts and 107 urban districts.
Germany has a civil law system based on Roman law with some references to Germanic law. The "Bundesverfassungsgericht" (Federal Constitutional Court) is the German Supreme Court responsible for constitutional matters, with power of judicial review. Germany's supreme court system is specialised: for civil and criminal cases, the highest court of appeal is the inquisitorial Federal Court of Justice, and for other affairs the courts are the Federal Labour Court, the Federal Social Court, the Federal Finance Court and the Federal Administrative Court.
Criminal and private laws are codified on the national level in the "Strafgesetzbuch" and the "Bürgerliches Gesetzbuch" respectively. The German penal system seeks the rehabilitation of the criminal and the protection of the public. Except for petty crimes, which are tried before a single professional judge, and serious political crimes, all charges are tried before mixed tribunals on which lay judges ("") sit side by side with professional judges.
Germany has a low murder rate with 1.18 murders per 100,000 . In 2018, the overall crime rate fell to its lowest since 1992.
Germany has a network of 227 diplomatic missions abroad and maintains relations with more than 190 countries. Germany is a member of NATO, the OECD, the G8, the G20, the World Bank and the IMF. It has played an influential role in the European Union since its inception and has maintained a strong alliance with France and all neighbouring countries since 1990. Germany promotes the creation of a more unified European political, economic and security apparatus. The governments of Germany and the United States are close political allies. Cultural ties and economic interests have crafted a bond between the two countries resulting in Atlanticism.
The development policy of Germany is an independent area of foreign policy. It is formulated by the Federal Ministry for Economic Cooperation and Development and carried out by the implementing organisations. The German government sees development policy as a joint responsibility of the international community. It was the world's second biggest aid donor in 2019 after the United States.
Germany's military, the "Bundeswehr", is organised into the "Heer" (Army and special forces KSK), "Marine" (Navy), "Luftwaffe" (Air Force), "Zentraler Sanitätsdienst der Bundeswehr" (Joint Medical Service) and "Streitkräftebasis" (Joint Support Service) branches. In absolute terms, German military expenditure is the 8th highest in the world. In 2018, military spending was at $49.5 billion, about 1.2% of the country's GDP, well below the NATO target of 2%.
, the "Bundeswehr" has a strength of 184,001 active soldiers and 80,947 civilians. Reservists are available to the armed forces and participate in defence exercises and deployments abroad. Until 2011, military service was compulsory for men at age 18, but this has been officially suspended and replaced with a voluntary service. Since 2001 women may serve in all functions of service without restriction. According to SIPRI, Germany was the fourth largest exporter of major arms in the world from 2014 to 2018.
In peacetime, the "Bundeswehr" is commanded by the Minister of Defence. In state of defence, the Chancellor would become commander-in-chief of the "Bundeswehr". The role of the "Bundeswehr" is described in the Constitution of Germany as defensive only. But after a ruling of the Federal Constitutional Court in 1994 the term "defence" has been defined to not only include protection of the borders of Germany, but also crisis reaction and conflict prevention, or more broadly as guarding the security of Germany anywhere in the world. , the German military has about 3,600 troops stationed in foreign countries as part of international peacekeeping forces, including about 1,200 supporting operations against Daesh, 980 in the NATO-led Resolute Support Mission in Afghanistan, and 800 in Kosovo.
Germany has a social market economy with a highly skilled labour force, a low level of corruption, and a high level of innovation. It is the world's third largest exporter of goods, and has the largest national economy in Europe which is also the world's fourth largest by nominal GDP and the fifth by PPP. Its GDP per capita measured in purchasing power standards amounts to 121% of the EU27 average (100%). The service sector contributes approximately 69% of the total GDP, industry 31%, and agriculture 1% . The unemployment rate published by Eurostat amounts to 3.2% , which is the fourth-lowest in the EU.
Germany is part of the European single market which represents more than 450 million consumers. In 2017, the country accounted for 28% of the Eurozone economy according to the International Monetary Fund. Germany introduced the common European currency, the Euro, in 2002. Its monetary policy is set by the European Central Bank, which is headquartered in Frankfurt.
Being home to the modern car, the automotive industry in Germany is regarded as one of the most competitive and innovative in the world, and is the fourth largest by production. The top 10 exports of Germany are vehicles, machinery, chemical goods, electronic products, electrical equipments, pharmaceuticals, transport equipments, basic metals, food products, and rubber and plastics. Germany is one of the largest exporters globally.
Of the world's 500 largest stock-market-listed companies measured by revenue in 2019, the Fortune Global 500, 29 are headquartered in Germany. 30 major Germany-based companies are included in the DAX, the German stock market index which is operated by Frankfurt Stock Exchange. Well-known international brands include Mercedes-Benz, BMW, Volkswagen, Audi, Siemens, Allianz, Adidas, Porsche, Bosch and Deutsche Telekom. Berlin is a hub for startup companies and has become the leading location for venture capital funded firms in the European Union. Germany is recognised for its large portion of specialised small and medium enterprises, known as the "Mittelstand" model. These companies represent 48% global market leaders in their segments, labelled Hidden Champions.
Research and development efforts form an integral part of the German economy. In 2018 Germany ranked fourth globally in terms of number of science and engineering research papers published. Research institutions in Germany include the Max Planck Society, the Helmholtz Association, and the Fraunhofer Society and the Leibniz Association. Germany is the largest contributor to the European Space Agency.
With its central position in Europe, Germany is a transport hub for the continent. Its road network is among the densest in Europe. The motorway (Autobahn) is widely known for having no federally mandated speed limit for some classes of vehicles. The InterCityExpress or "ICE" train network serves major German cities as well as destinations in neighbouring countries with speeds up to . The largest German airports are Frankfurt Airport and Munich Airport. The Port of Hamburg is one of the top twenty largest container ports in the world.
, Germany was the world's seventh-largest consumer of energy. The government and the nuclear power industry agreed to phase out all nuclear power plants by 2021. It meets the country's power demands using 40% renewable sources. Germany is committed to the Paris Agreement and several other treaties promoting biodiversity, low emission standards, and water management. The country's household recycling rate is among the highest in the world—at around 65%. Nevertheless, the country's total greenhouse gas emissions were the highest in the EU . The German energy transition ("Energiewende") is the recognised move to a sustainable economy by means of energy efficiency and renewable energy.
Germany is the ninth most visited country in the world , with 37.4 million visits. Berlin has become the third most visited city destination in Europe. Domestic and international travel and tourism combined directly contribute over €105.3 billion to German GDP. Including indirect and induced impacts, the industry supports 4.2 million jobs.
Germany's most visited and popular landmarks include Cologne Cathedral, the Brandenburg Gate, the Reichstag, the Dresden Frauenkirche, Neuschwanstein Castle, Heidelberg Castle, the Wartburg, and Sanssouci Palace. The Europa-Park near Freiburg is Europe's second most popular theme park resort.
With a population of 80.2 million according to the 2011 census, rising to 83.1 million , Germany is the most populous country in the European Union, the second most populous country in Europe after Russia, and the 19th most populous country in the world. Its population density stands at 227 inhabitants per square kilometre (588 per square mile). The overall life expectancy in Germany at birth is 80.19 years (77.93 years for males and 82.58 years for females). The fertility rate of 1.41 children born per woman (2011 estimates) is below the replacement rate of 2.1 and is one of the lowest fertility rates in the world. Since the 1970s, Germany's death rate has exceeded its birth rate. However, Germany is witnessing increased birth rates and migration rates since the beginning of the 2010s, particularly a rise in the number of well-educated migrants. Germany has the third oldest population in the world, with the average age of 47.4 years.
Four sizeable groups of people are referred to as "national minorities" because their ancestors have lived in their respective regions for centuries: There is a Danish minority in the northernmost state of Schleswig-Holstein; the Sorbs, a Slavic population, are in the Lusatia region of Saxony and Brandenburg.; the Roma and Sinti live throughout the country; and the Frisians are concentrated in Schleswig-Holstein's western coast and in the north-western part of Lower Saxony.
After the United States, Germany is the second most popular immigration destination in the world. The majority of migrants live in western Germany, in particular in urban areas. Of the country's residents, 18.6 million people (22.5%) were of immigrant or partially immigrant descent in 2016 (including persons descending or partially descending from ethnic German repatriates). In 2015, the Population Division of the United Nations Department of Economic and Social Affairs listed Germany as host to the second-highest number of international migrants worldwide, about 5% or 12 million of all 244 million migrants. , Germany ranks fifth amongst EU countries in terms of the percentage of migrants in the country's population, at 12.9%.
Germany has a number of large cities. There are 11 officially recognised metropolitan regions. The country's largest city is Berlin, while its largest urban area is the Ruhr.
The 2011 German Census showed Christianity as the largest religion in Germany, with 66.8% identified themselves as Christian, with 3.8% of those not being church members. 31.7% declared themselves as Protestants, including members of the Evangelical Church in Germany (which encompasses Lutheran, Reformed and administrative or confessional unions of both traditions) and the free churches (); 31.2% declared themselves as Roman Catholics, and Orthodox believers constituted 1.3%. According to data from 2016, the Catholic Church and the Evangelical Church claimed 28.5% and 27.5%, respectively, of the population. Islam is the second largest religion in the country. In the 2011 census, 1.9% of the census population (1.52 million people) gave their religion as Islam, but this figure is deemed unreliable because a disproportionate number of adherents of this religion (and other religions, such as Judaism) are likely to have made use of their right not to answer the question. Most of the Muslims are Sunnis and Alevites from Turkey, but there are a small number of Shi'ites, Ahmadiyyas and other denominations. Other religions comprise less than one percent of Germany's population.
A study in 2018 estimated that 38% of the population are not members of any religious organization or denomination, though up to a third may still consider themselves religious. Irreligion in Germany is strongest in the former East Germany, which used to be predominantly Protestant before the enforcement of state atheism, and in major metropolitan areas.
German is the official and predominant spoken language in Germany. It is one of 24 official and working languages of the European Union, and one of the three procedural languages of the European Commission. German is the most widely spoken first language in the European Union, with around 100 million native speakers.
Recognised native minority languages in Germany are Danish, Low German, Low Rhenish, Sorbian, Romany, North Frisian and Saterland Frisian; they are officially protected by the European Charter for Regional or Minority Languages. The most used immigrant languages are Turkish, Arabic, Kurdish, Polish, the Balkan languages and Russian. Germans are typically multilingual: 67% of German citizens claim to be able to communicate in at least one foreign language and 27% in at least two.
Responsibility for educational supervision in Germany is primarily organised within the individual federal states. Optional kindergarten education is provided for all children between three and six years old, after which school attendance is compulsory for at least nine years. Primary education usually lasts for four to six years. Secondary schooling is divided into tracks based on whether students pursue academic or vocational education. A system of apprenticeship called "Duale Ausbildung" leads to a skilled qualification which is almost comparable to an academic degree. It allows students in vocational training to learn in a company as well as in a state-run trade school. This model is well regarded and reproduced all around the world.
Most of the German universities are public institutions, and students traditionally study without fee payment. The general requirement for university is the "Abitur". According to an OECD report in 2014, Germany is the world's third leading destination for international study. The established universities in Germany include some of the oldest in the world, with Heidelberg University (established in 1386) being the oldest. The Humboldt University of Berlin, founded in 1810 by the liberal educational reformer Wilhelm von Humboldt, became the academic model for many Western universities. In the contemporary era Germany has developed eleven Universities of Excellence.
Germany's system of hospitals, called "Krankenhäuser", dates from medieval times, and today, Germany has the world's oldest universal health care system, dating from Bismarck's social legislation of the 1880s. Since the 1880s, reforms and provisions have ensured a balanced health care system. The population is covered by a health insurance plan provided by statute, with criteria allowing some groups to opt for a private health insurance contract. According to the World Health Organization, Germany's health care system was 77% government-funded and 23% privately funded . In 2014, Germany spent 11.3% of its GDP on health care.
Germany ranked 20th in the world in 2013 in life expectancy with 77 years for men and 82 years for women, and it had a very low infant mortality rate (4 per 1,000 live births). , the principal cause of death was cardiovascular disease, at 37%. Obesity in Germany has been increasingly cited as a major health issue. A 2014 study showed that 52 percent of the adult German population was overweight or obese.
Culture in German states has been shaped by major intellectual and popular currents in Europe, both religious and secular. Historically, Germany has been called "Das Land der Dichter und Denker" ("the land of poets and thinkers"), because of the major role its writers and philosophers have played in the development of Western thought. A global opinion poll for the BBC revealed that Germany is recognised for having the most positive influence in the world in 2013 and 2014.
Germany is well known for such folk festival traditions as Oktoberfest and Christmas customs, which include Advent wreaths, Christmas pageants, Christmas trees, Stollen cakes, and other practices. UNESCO inscribed 41 properties in Germany on the World Heritage List. There are a number of public holidays in Germany determined by each state; 3 October has been a national day of Germany since 1990, celebrated as the "Tag der Deutschen Einheit" (German Unity Day).
German classical music includes works by some of the world's most well-known composers. Dieterich Buxtehude, Johann Sebastian Bach and Georg Friedrich Händel were influential composers of the Baroque period. Ludwig van Beethoven was a crucial figure in the transition between the Classical and Romantic eras. Carl Maria von Weber, Felix Mendelssohn, Robert Schumann and Johannes Brahms were significant Romantic composers. Richard Wagner was known for his operas. Richard Strauss was a leading composer of the late Romantic and early modern eras. Karlheinz Stockhausen and Wolfgang Rihm are important composers of the 20th and early 21st centuries.
As of 2013, Germany was the second largest music market in Europe, and fourth largest in the world. German popular music of the 20th and 21st centuries includes the movements of Neue Deutsche Welle, pop, Ostrock, heavy metal/rock, punk, pop rock, indie and schlager pop. German electronic music gained global influence, with Kraftwerk and Tangerine Dream pioneering in this genre. DJs and artists of the techno and house music scenes of Germany have become well known (e.g. Paul van Dyk, Paul Kalkbrenner, and Scooter).
German painters have influenced western art. Albrecht Dürer, Hans Holbein the Younger, Matthias Grünewald and Lucas Cranach the Elder were important German artists of the Renaissance, Peter Paul Rubens and Johann Baptist Zimmermann of the Baroque, Caspar David Friedrich and Carl Spitzweg of Romanticism, Max Liebermann of Impressionism and Max Ernst of Surrealism. Several German art groups formed in the 20th century; "Die Brücke" (The Bridge) and "Der Blaue Reiter" (The Blue Rider) influenced the development of expressionism in Munich and Berlin. The New Objectivity arose in response to expressionism during the Weimar Republic. After World War II, broad trends in German art include neo-expressionism and the New Leipzig School.
Architectural contributions from Germany include the Carolingian and Ottonian styles, which were precursors of Romanesque. Brick Gothic is a distinctive medieval style that evolved in Germany. Also in Renaissance and Baroque art, regional and typically German elements evolved (e.g. Weser Renaissance). Vernacular architecture in Germany is often identified by its timber framing ("Fachwerk") traditions and varies across regions, and among carpentry styles. When industrialisation spread across Europe, Classicism and a distinctive style of historism developed in Germany, sometimes referred to as "Gründerzeit style". Expressionist architecture developed in the 1910s in Germany and influenced Art Deco and other modern styles. Germany was particularly important in the early modernist movement: it is the home of Werkbund initiated by Hermann Muthesius (New Objectivity), and of the Bauhaus movement founded by Walter Gropius. Ludwig Mies van der Rohe became one of the world's most renowned architects in the second half of the 20th century; he conceived of the glass façade skyscraper. Renowned contemporary architects and offices include Pritzker Prize winners Gottfried Böhm and Frei Otto.
German designers became early leaders of modern product design. The Berlin Fashion Week and the fashion trade fair Bread & Butter are held twice a year.
German literature can be traced back to the Middle Ages and the works of writers such as Walther von der Vogelweide and Wolfram von Eschenbach. Well-known German authors include Johann Wolfgang von Goethe, Friedrich Schiller, Gotthold Ephraim Lessing and Theodor Fontane. The collections of folk tales published by the Brothers Grimm popularised German folklore on an international level. The Grimms also gathered and codified regional variants of the German language, grounding their work in historical principles; their "Deutsches Wörterbuch", or German Dictionary, sometimes called the Grimm dictionary, was begun in 1838 and the first volumes published in 1854.
Influential authors of the 20th century include Gerhart Hauptmann, Thomas Mann, Hermann Hesse, Heinrich Böll and Günter Grass. The German book market is the third largest in the world, after the United States and China. The Frankfurt Book Fair is the most important in the world for international deals and trading, with a tradition spanning over 500 years. The Leipzig Book Fair also retains a major position in Europe.
German philosophy is historically significant: Gottfried Leibniz's contributions to rationalism; the enlightenment philosophy by Immanuel Kant; the establishment of classical German idealism by Johann Gottlieb Fichte, Georg Wilhelm Friedrich Hegel and Friedrich Wilhelm Joseph Schelling; Arthur Schopenhauer's composition of metaphysical pessimism; the formulation of communist theory by Karl Marx and Friedrich Engels; Friedrich Nietzsche's development of perspectivism; Gottlob Frege's contributions to the dawn of analytic philosophy; Martin Heidegger's works on Being; Oswald Spengler's historical philosophy; the development of the Frankfurt School has been particularly influential.
The largest internationally operating media companies in Germany are the Bertelsmann enterprise, Axel Springer SE and ProSiebenSat.1 Media. Germany's television market is the largest in Europe, with some 38 million TV households. Around 90% of German households have cable or satellite TV, with a variety of free-to-view public and commercial channels. There are more than 300 public and private radio stations in Germany; Germany's national radio network is the Deutschlandradio and the public Deutsche Welle is the main German radio and television broadcaster in foreign languages. Germany's print market of newspapers and magazines is the largest in Europe. The papers with the highest circulation are "Bild", "Süddeutsche Zeitung", "Frankfurter Allgemeine Zeitung" and "Die Welt". The largest magazines include "ADAC Motorwelt" and "Der Spiegel". Germany has a large video gaming market, with over 34 million players nationwide.
German cinema has made major technical and artistic contributions to film. The first works of the Skladanowsky Brothers were shown to an audience in 1895. The renowned Babelsberg Studio in Potsdam was established in 1912, thus being the first large-scale film studio in the world. Early German cinema was particularly influential with German expressionists such as Robert Wiene and Friedrich Wilhelm Murnau. Director Fritz Lang's "Metropolis" (1927) is referred to as the first major science-fiction film. After 1945, many of the films of the immediate post-war period can be characterised as "Trümmerfilm" (rubble film). East German film was dominated by state-owned film studio DEFA, while the dominant genre in West Germany was the "Heimatfilm" ("homeland film"). During the 1970s and 1980s, New German Cinema directors such as Volker Schlöndorff, Werner Herzog, Wim Wenders, and Rainer Werner Fassbinder brought West German auteur cinema to critical acclaim.
The Academy Award for Best Foreign Language Film ("Oscar") went to the German production "Die Blechtrommel (The Tin Drum)" in 1979, to "Nirgendwo in Afrika (Nowhere in Africa)" in 2002, and to "Das Leben der Anderen (The Lives of Others)" in 2007. Various Germans won an Oscar for their performances in other films. The annual European Film Awards ceremony is held every other year in Berlin, home of the European Film Academy. The Berlin International Film Festival, known as "Berlinale", awarding the "Golden Bear" and held annually since 1951, is one of the world's leading film festivals. The "Lolas" are annually awarded in Berlin, at the German Film Awards.
German cuisine varies from region to region and often neighbouring regions share some culinary similarities (e.g. the southern regions of Bavaria and Swabia share some traditions with Switzerland and Austria). International varieties such as pizza, sushi, Chinese food, Greek food, Indian cuisine and doner kebab are also popular.
Bread is a significant part of German cuisine and German bakeries produce about 600 main types of bread and 1,200 types of pastries and rolls ("Brötchen"). German cheeses account for about 22% of all cheese produced in Europe. In 2012 over 99% of all meat produced in Germany was either pork, chicken or beef. Germans produce their ubiquitous sausages in almost 1,500 varieties, including Bratwursts and Weisswursts. Although wine is becoming more popular in many parts of Germany, especially close to German wine regions, the national alcoholic drink is beer. German beer consumption per person stands at in 2013 and remains among the highest in the world. German beer purity regulations date back to the 16th century.
The 2018 Michelin Guide awarded eleven restaurants in Germany three stars, giving the country a cumulative total of 300 stars.
Football is the most popular sport in Germany. With more than 7 million official members, the German Football Association ("Deutscher Fußball-Bund") is the largest single-sport organisation worldwide, and the German top league, the Bundesliga, attracts the second highest average attendance of all professional sports leagues in the world. The German men's national football team won the FIFA World Cup in 1954, 1974, 1990, and 2014, the UEFA European Championship in 1972, 1980 and 1996, and the FIFA Confederations Cup in 2017.
Germany is one of the leading motor sports countries in the world. Constructors like BMW and Mercedes are prominent manufacturers in motor sport. Porsche has won the 24 Hours of Le Mans race 19 times, and Audi 13 times (). The driver Michael Schumacher has set many motor sport records during his career, having won seven Formula One World Drivers' Championships. Sebastian Vettel is also among the top five most successful Formula One drivers of all time.
Historically, German athletes have been successful contenders in the Olympic Games, ranking third in an all-time Olympic Games medal count (when combining East and West German medals). Germany was the last country to host both the summer and winter games in the same year, in 1936: the Berlin Summer Games and the Winter Games in Garmisch-Partenkirchen. Munich hosted the Summer Games of 1972.

</doc>
<doc id="11874" url="https://en.wikipedia.org/wiki?curid=11874" title="Guatemala City">
Guatemala City

Guatemala City (), locally known as Guatemala or Guate, officially Ciudad de Guatemala (art. 231 of the Political Constitution of the Republic of Guatemala) , is the capital and largest city of Guatemala, and the most populous urban area in Central America. The city is located in the south-central part of the country, nestled in a mountain valley called Valle de la Ermita (). It is estimated that its population is about 1 million. Guatemala City is also the capital of the Municipality of Guatemala and of the Guatemala Department.
Guatemala City is the site of the Mayan city of Kaminaljuyu, founded around 1500 BC. Following the Spanish conquest, a new town was established, and in 1776 it was made capital of the Kingdom of Guatemala. In 1821, Guatemala City was the scene of the declaration of independence of Central America from Spain, after which it became the capital of the newly established United Provinces of Central America (later the Federal Republic of Central America). In 1847, Guatemala declared itself an independent republic, with Guatemala City as its capital. The city was almost completely destroyed by the 1917–18 earthquakes. Reconstructions following the earthquakes have resulted in a more modern architectural landscape.
Today, Guatemala City is the political, cultural, and economic center of Guatemala. It is served by La Aurora International Airport.
Human settlement on the present site of Guatemala City began with the Maya who built a city at Kaminaljuyu. The Spanish colonists established a small town, which was made a capital city in 1775. At this period the Central Square with the Cathedral and Royal Palace were constructed. After Central American independence from Spain the city became the capital of the United Provinces of Central America in 1821.
The 19th century saw the construction of the monumental Carrera Theater in the 1850s, and the Presidential Palace in the 1890s. At this time the city was expanding around the "30 de junio" Boulevard and elsewhere, displacing native settlements from the ancient site. Earthquakes in 1917–1918 destroyed many historic structures. Under Jorge Ubico in the 1930s a hippodrome and many new public buildings were constructed, although peripheral poor neighborhoods that formed after the 1917–1918 earthquakes continued to lack basic amenities.
During the Guatemalan Civil War, terror attacks beginning with the burning of the Spanish Embassy in 1980 led to severe destruction and loss of life in the city. In May 2010 two disasters struck: the eruption of the Pacaya volcano, and two days later Tropical Storm Agatha.
Guatemala City serves as the economic, governmental, and cultural epicenter of the nation of Guatemala. The city also functions as Guatemala's main transportation hub, hosting an international airport, La Aurora International Airport, and serving as the origination or end points for most of Guatemala's major highways. The city, with its robust economy, attracts hundreds of thousands of rural migrants from Guatemala's interior hinterlands and serves as the main entry point for most foreign immigrants seeking to settle in Guatemala.
In addition to a wide variety of restaurants, hotels, shops, and a modern BRT transport system (Transmetro), the city is home to many art galleries, theaters, sports venues and museums (including some fine collections of Pre-Columbian art) and provides a growing number of cultural offerings. Guatemala City not only possesses a history and culture unique to the Central American region, it also furnishes all the modern amenities of a world class city, ranging from an IMAX Theater to the Ícaro film festival (Festival Ícaro), where independent films produced in Guatemala and Central America are debuted.
Guatemala City is located in the mountainous regions of the country, between the Pacific coastal plain to the south and the northern lowlands of the Peten region.
The city's metropolitan area has recently grown very rapidly and has absorbed most of the neighboring municipalities of Villa Nueva, San Miguel Petapa, Mixco, San Juan Sacatepequez, San José Pinula, Santa Catarina Pinula, Fraijanes, San Pedro Ayampuc, Amatitlán, Villa Canales, Palencia and Chinautla forming what is now known as the Guatemala City Metropolitan Area.
The city is subdivided into 22 zones ("Zonas") designed by the urban engineering of Raúl Aguilar Batres, each one with its own streets ("Calles"). avenues ("Avenidas") and sometimes "Diagonal" Streets, making it pretty easy to find addresses in the city. Zones are numbered 1–25 with Zones 20, 22 and 23 not existing as they would have fallen in two other municipalities' territory. Addresses are assigned according to the street or avenue number, followed by a dash and the number of metres it is away from the intersection.
For example, the INGUAT Office on "7a Av. 1-17, Zona 4" is a building which is located on Avenida 7, 17 meters away from the intersection with Calle 1, toward Calle 2 in zone 4.
7a Av. 1-17, Zona 4; and 7a Av. 1-17, Zona 10, are two radically different addresses.
Short streets/avenues do not get new sequenced number, for example, 6A Calle is a short street between 6a and 7a.
Some "avenidas" or "Calles" have a name in addition to their number, if it is very wide, for example Avenida la Reforma is an avenue which separates Zone 9 and 10 and Calle Montúfar is Calle 12 in Zone 9.
Calle 1 Avenida 1 Zona 1 is the center of every city in Guatemala.
Zone One is the Historic Center, (Centro Histórico), lying in the very heart of the city, the location of many important historic buildings including the Palacio Nacional de la Cultura (National Palace of Culture), the Metropolitan Cathedral, the National Congress, the Casa Presidencial (Presidential House), the National Library and Plaza de la Constitución (Constitution Plaza, old Central Park). Efforts to revitalize this important part of the city have been undertaken by the municipal government.
Besides the parks, the city offers a portfolio of entertainment in the region, focused on the so-called Zona Viva and the Calzada Roosevelt as well as four degrees North. Casino activity is considerable, with several located in different parts of the Zona Viva. The area around the East market is being redeveloped.
Within the financial district are the tallest buildings in the country including: Club Premier, Tinttorento, Atlantis building, Atrium, Tikal Futura, Building of Finances, Towers Building Batteries, Torres Botticelli, Tadeus, building of the INTECAP, Royal Towers, Towers Geminis, Industrial Bank towers, Holiday Inn Hotel, Premier of the Americas, among many others to be used for offices, apartments etc. Also included are projects such as Zona Pradera and Interamerica's World Financial Center.
One of the most outstanding mayors was the engineer Martin Prado Vélez, who took over in 1949, and ruled the city during the reformist Presidents Juan José Arévalo and Jacobo Arbenz Guzman, although he was not a member of the ruling party at the time and was elected due his well-known capabilities. Of cobanero origin, married with Marta Cobos, he studied at the University of San Carlos; under his tenure, among other modernist works of the city, infrastructure projects included El Incienso bridge, the construction of the Roosevelt Avenue, the main road axis from East to West of the city, the town hall building, and numerous road works which meant the widening of the colonial city, its order in the cardinal points and the generation of a ring road with the first cloverleaf interchange in the city.
In an attempt to control the rapid growth of the city, the municipal government (Municipalidad de Guatemala) headed by longtime Mayor Álvaro Arzú, has implemented a plan to focus growth along important arterial roads and apply Transit-oriented development (TOD) characteristics. This plan denominated POT (Plan de Ordenamiento Territorial) aims to allow taller building structures of mixed uses to be built next to large arterial roads and gradually decline in height and density moving away from such. It is also worth mentioning, that due to the airport being in the south of the city, height limits based on aeronautical considerations have been applied to the construction code. This limits the maximum height for a building, at in Zone 10, up to in Zone 1.
Despite its location in the tropics, Guatemala City's relatively high altitude moderates average temperatures. The city has a tropical savanna climate (Köppen "Aw") bordering on a subtropical highland climate ("Cwb"). Guatemala City is generally very warm, almost springlike, throughout the course of the year. It occasionally gets hot during the dry season, but not as hot and humid as in Central American cities at sea level. The hottest month is April. The rainy season extends from May to October, coinciding with the tropical storm and hurricane season in the western Atlantic Ocean and Caribbean Sea, while the dry season extends from November to April. The city can at times be windy, which also leads to lower ambient temperatures.
The average annual temperature ranges from during the day, and at night.
Average morning relative humidity: 82%, evening relative humidity: 58%. Average dew point is .
Four stratovolcanoes are visible from the city, two of them active. The nearest and most active is Pacaya, which at times erupts a considerable amount of ash. These volcanoes lie to the south of the Valle de la Ermita, providing a natural barrier between Guatemala City and the Pacific lowlands that define the southern regions of Guatemala. Agua, Fuego, Pacaya and Acatenango comprise a line of 33 stratovolcanoes that stretches across the breadth of Guatemala, from the Salvadorian border to the Mexican border.
Lying on the Ring of Fire, the Guatemalan highlands and the Valle de la Ermita are frequently shaken by large earthquakes. The last large tremor to hit the Guatemala City region occurred in the 1976, on the Motagua Fault, a left-lateral strike-slip fault that forms the boundary between the Caribbean Plate and the North American Plate. The 1976 event registered 7.5 on the moment magnitude scale. Smaller, less severe tremors are frequently felt in Guatemala City and environs.
Torrential downpours, similar to the more famous monsoons, occur frequently in the Valle de la Ermita during the rainy season, leading to flash floods that sometimes inundate the city. Due to these heavy rainfalls, some of the slums perched on the steep edges of the canyons that criss-cross the Valle de la Ermita are washed away and buried under mudslides, as in October 2005. Tropical waves, tropical storms and hurricanes sometimes strike the Guatemalan highlands, which also bring torrential rains to the Guatemala City region and trigger these deadly mudslides.
In February 2007, a very large, deep circular hole with vertical walls opened in northeastern Guatemala City (), killing five people. This sinkhole, which is classified by geologists as either a "piping feature" or "piping pseudokarst", was deep, and apparently was created by fluid from a sewer eroding the loose volcanic ash, limestone, and other pyroclastic deposits that underlie Guatemala City. As a result, one thousand people were evacuated from the area. This piping feature has since been mitigated by City Hall by providing proper maintenance to the sewerage collection system and plans to develop the site have been proposed. However, critics believe municipal authorities have neglected needed maintenance on the city's aging sewerage system, and have speculated that more dangerous piping features are likely to develop unless action is taken.
3 years later the 2010 Guatemala City sinkhole arose.
It is estimated that the population of Guatemala City proper is about 1 million, while its urban area is almost 3 million. The growth of the city's population has been robust since then, abetted by the mass migration of Guatemalans from the rural hinterlands to the largest and most vibrant regional economy in Guatemala. The inhabitants of Guatemala City are incredibly diverse given the size of the city, with those of Spanish and Mestizo descent being the most numerous. Guatemala City also has sizable indigenous populations, divided among the 23 distinct Mayan groups present in Guatemala. The numerous Mayan languages are now spoken in certain quarters of Guatemala City, making the city a linguistically rich area. Foreigners and foreign immigrants comprise the final distinct group of Guatemala City inhabitants, representing a very small minority among the city's citizens.
Due to mass migration from impoverished rural districts wracked with political instability, Guatemala City's population has exploded since the 1970s, severely straining the existing bureaucratic and physical infrastructure of the city. As a result, chronic traffic congestion, shortages of safe potable water in some areas of the city, and a sudden and prolonged surge in crime have become perennial problems. The infrastructure, although continuing to grow and improve in some areas, it is lagging in relation to the increasing population of those less fortunate. Guatemala City is not unique in facing and tackling problems all too common among rapidly expanding cities around the world.
Guatemala City is headquarters to many communications and telecom companies, among them Tigo, Claro-Telgua, and Movistar-Telefónica. These companies also offer cable television, internet services and telephone access. Due to Guatemala City's large and concentrated consumer base in comparison to the rest of the country, these telecom and communications companies provide most of their services and offerings within the confines of the city. There are also seven local television channels, in addition to numerous international channels. The international channels range from children's programming, like Nickelodeon and the Disney Channel, to more adult offerings, such as E! and HBO. While international programming is dominated by entertainment from the United States, domestic programming is dominated by shows from Mexico. Due to its small and relatively income-restricted domestic market, Guatemala City produces very little in the way of its own programming outside of local news and sports.
Guatemala City, as the capital, is home to Guatemala's central bank, from which Guatemala's monetary and fiscal policies are formulated and promulgated. Guatemala City is also headquarters to numerous regional private banks, among them CitiBank, Banco Agromercantil, Banco Promerica, Banco Industrial, Banco GyT Continental, Banco de Antigua, Banco Reformador, Banrural, Grupo Financiero de Occidente, BAC Credomatic, and Banco Internacional. By far the richest and most powerful regional economy within Guatemala, Guatemala City is the largest market for goods and services, which provides the greatest number of investment opportunities for public and private investors in all of Guatemala. Financing for these investments is provided by the regional private banks, as well as by foreign direct and capital investment, mostly from the United States. Guatemala City's ample consumer base and sophisticated service sector is represented by the large department store chains present in the city, among them Siman, Hiper Paiz & Paiz (Walmart), Price Smart, ClubCo, Cemaco, Sears and Office Depot.
Guatemala City is divided into 22 zones in accordance with the urban layout plan designed by Raúl Aguilar Batres. Each zone has its own streets and avenues, facilitating navigation within the city. Zones are numbered 1 through 25. However, numbers 20, 22 and 23 have not been designated to zones, thus these zones do not exist within the city proper.
Traditional buses are now required to discharge passengers at transfer stations at the city's edge to board the Transmetro. This is being implemented as new Transmetro lines become established. In conjunction with the new mass transit implementation in the city, there is also a prepaid bus card system called Transurbano that is being implemented in the metro area to limit cash handling for the transportation system. A new fleet of buses tailored for this system has been purchased from a Brazilian firm.
A light rail line known as Metro Riel is proposed.
Guatemala City is home to ten universities, among them the oldest institution of higher education in Central America, the University of San Carlos of Guatemala. Founded in 1676, the Universidad de San Carlos is older than all North American universities except for Harvard University.
The other nine institutions of higher education to be found in Guatemala City include the Universidad Mariano Gálvez, the Universidad Panamericana, the Universidad Mesoamericana, the Universidad Rafael Landivar, the Universidad Francisco Marroquín, the Universidad del Valle, the Universidad del Istmo, Universidad Galileo, Universidad da Vinci and the Universidad Rural. Whereas these nine named universities are private, the Universidad de San Carlos remains the only public institution of higher learning.
Guatemala City possesses several sportsgrounds and is home to many sports clubs. Football is the most popular sport, with CSD Municipal, Aurora F.C. and Comunicaciones being the main clubs. The Estadio Mateo Flores, located in the Zone 5 of the city, is the largest stadium in the country, followed in capacity by the Estadio Cementos Progreso, Estadio del Ejército & Estadio El Trébol. An important multi-functional hall is the Domo Polideportivo de la CDAG.
The city has hosted several promotional functions and some international sports events: in 1950 it hosted the VI Central American and Caribbean Games, and in 2000 the FIFA Futsal World Championship. On 4 July 2007 the International Olympic Committee gathered in Guatemala City and voted Sochi to become the host for the 2014 Winter Olympics and Paralympics. In April 2010, it hosted the XIVth Pan-American Mountain Bike Championships.
Guatemala City hosted the 2008 edition of the CONCACAF Futsal Championship, played at the Domo Polideportivo from 2 to 8 June 2008.
Guatemala City is twinned with:

</doc>
<doc id="11875" url="https://en.wikipedia.org/wiki?curid=11875" title="GNU">
GNU

GNU () is an extensive collection of wholly free software, which gave rise to the family of operating systems popularly known as Linux. GNU is also the project within which the free software concept originated. Most of GNU is licensed under the GNU Project's own General Public License (GPL).
Richard Stallman, the founder of the project, views GNU as a "technical means to a social end". Relatedly, Lawrence Lessig states in his introduction to the second edition of Stallman's book "Free Software, Free Society" that in it Stallman has written about "the social aspects of software and how Free Software can create community and social justice".
"GNU" is a recursive acronym for "GNU's Not Unix!", chosen because GNU's design is Unix-like, but differs from Unix by being free software and containing no Unix code.
Development of the GNU operating system was initiated by Richard Stallman while he worked at MIT Artificial Intelligence Laboratory. It was called the GNU Project, and was publicly announced on September 27, 1983, on the net.unix-wizards and net.usoft newsgroups by Stallman. Software development began on January 5, 1984, when Stallman quit his job at the Lab so that they could not claim ownership or interfere with distributing GNU components as free software. Richard Stallman chose the name by using various plays on words, including the song "The Gnu".
The goal was to bring a completely free software operating system into existence. Stallman wanted computer users to be free to study the source code of the software they use, share software with other people, modify the behavior of software, and publish their modified versions of the software. This philosophy was later published as the GNU Manifesto in March 1985.
Richard Stallman's experience with the Incompatible Timesharing System (ITS), an early operating system written in assembly language that became obsolete due to discontinuation of PDP-10, the computer architecture for which ITS was written, led to a decision that a portable system was necessary. It was thus decided that the development would be started using C and Lisp as system programming languages, and that GNU would be compatible with Unix. At the time, Unix was already a popular proprietary operating system. The design of Unix was modular, so it could be reimplemented piece by piece.
Much of the needed software had to be written from scratch, but existing compatible third-party free software components were also used such as the TeX typesetting system, the X Window System, and the Mach microkernel that forms the basis of the GNU Mach core of GNU Hurd (the official kernel of GNU). With the exception of the aforementioned third-party components, most of GNU has been written by volunteers; some in their spare time, some paid by companies, educational institutions, and other non-profit organizations. In October 1985, Stallman set up the Free Software Foundation (FSF). In the late 1980s and 1990s, the FSF hired software developers to write the software needed for GNU.
As GNU gained prominence, interested businesses began contributing to development or selling GNU software and technical support. The most prominent and successful of these was Cygnus Solutions, now part of Red Hat.
The system's basic components include the GNU Compiler Collection (GCC), the GNU C library (glibc), and GNU Core Utilities (coreutils), but also the GNU Debugger (GDB), GNU Binary Utilities (binutils), the GNU Bash shell. GNU developers have contributed to Linux ports of GNU applications and utilities, which are now also widely used on other operating systems such as BSD variants, Solaris and macOS.
Many GNU programs have been ported to other operating systems, including proprietary platforms such as Microsoft Windows and macOS. GNU programs have been shown to be more reliable than their proprietary Unix counterparts.
As of November 2015, there are a total of 466 GNU packages (including decommissioned, 383 excluding) hosted on the official GNU development site.
The GNU collection is sometimes referred to as an operating system. In its original meaning, and one still common in hardware engineering, the operating system is a basic set of functions to control the hardware and manage things like task scheduling and system calls. In modern terminology used by software developers, the collection of these functions is usually referred to as a kernel. While the GNU project does develop such kernels, their inclusion is not mandatory, so that 'GNU' does not necessarily include everything required to operate a computer 
The original kernel of GNU Project is the GNU Hurd microkernel, which was the original focus of the Free Software Foundation (FSF).
With the April 30, 2015 release of the Debian GNU/Hurd 2015 distro, GNU now provides all required components to assemble an operating system that users can install and use on a computer.
However, the Hurd kernel is not yet considered production-ready but rather a base for further development and non-critical application usage.
As of 2012, a fork of the Linux kernel became officially part of the GNU Project in the form of Linux-libre, a variant of Linux with all proprietary components removed.
Because of the development status of Hurd, GNU is usually paired with other kernels such as Linux or FreeBSD. Whether the combination of GNU libraries with external kernels is a GNU operating system with a kernel (e.g. GNU with Linux), because the GNU collection renders the kernel into a usable operating system as understood in modern software development, or whether the kernel is an operating system unto itself with a GNU layer on top (i.e. Linux with GNU), because the kernel can operate a machine without GNU, is a matter of ongoing debate. The FSF maintains that an operating system built using the Linux kernel and GNU tools and utilities should be considered a variant of GNU, and promotes the term "GNU/Linux" for such systems (leading to the GNU/Linux naming controversy). The GNU Project has endorsed Linux distributions, such as gNewSense, Trisquel and Parabola GNU/Linux-libre.
Other GNU variants which do not use the Hurd as a kernel include Debian GNU/kFreeBSD and Debian GNU/NetBSD, bringing to fruition the early plan of GNU on a BSD kernel.
The GNU Project recommends that contributors assign the copyright for GNU packages to the Free Software Foundation, though the Free Software Foundation considers it acceptable to release small changes to an existing project to the public domain. However, this is not required; package maintainers may retain copyright to the GNU packages they maintain, though since only the copyright holder may enforce the license used (such as the GNU GPL), the copyright holder in this case enforces it rather than the Free Software Foundation.
For the development of needed software, Stallman wrote a license called the GNU General Public License (first called Emacs General Public License), with the goal to guarantee users freedom to share and change free software. Stallman wrote this license after his experience with James Gosling and a program called UniPress, over a controversy around software code use in the GNU Emacs program. For most of the 80s, each GNU package had its own license: the Emacs General Public License, the GCC General Public License, etc. In 1989, FSF published a single license they could use for all their software, and which could be used by non-GNU projects: the GNU General Public License (GPL).
This license is now used by most of GNU software, as well as a large number of free software programs that are not part of the GNU Project; it also historically has been the most commonly used free software license (though recently challenged by the MIT license). It gives all recipients of a program the right to run, copy, modify and distribute it, while forbidding them from imposing further restrictions on any copies they distribute. This idea is often referred to as copyleft.
In 1991, the GNU Lesser General Public License (LGPL), then known as the Library General Public License, was written for the GNU C Library to allow it to be linked with proprietary software. 1991 also saw the release of version 2 of the GNU GPL. The GNU Free Documentation License (FDL), for documentation, followed in 2000. The GPL and LGPL were revised to version 3 in 2007, adding clauses to protect users against hardware restrictions that prevent users from running modified software on their own devices.
Besides GNU's packages, the GNU Project's licenses are used by many unrelated projects, such as the Linux kernel, often used with GNU software. A minority of the software used by most of Linux distributions, such as the X Window System, is licensed under permissive free software licenses.
The logo for GNU is a gnu head. Originally drawn by Etienne Suvasa, a bolder and simpler version designed by Aurelio Heckert is now preferred. It appears in GNU software and in printed and electronic documentation for the GNU Project, and is also used in Free Software Foundation materials.
The image shown here is a modified version of the official logo. It was created by the Free Software Foundation in September 2013 to commemorate the 30th anniversary of the GNU Project.

</doc>
<doc id="11877" url="https://en.wikipedia.org/wiki?curid=11877" title="Gradualism">
Gradualism

Gradualism, from the Latin "gradus" ("step"), is a hypothesis, a theory or a tenet assuming that change comes about gradually or that variation is gradual in nature and happens over time as opposed to in large steps. Uniformitarianism, incrementalism, and reformism are similar concepts.
In the natural sciences, gradualism is the theory which holds that profound change is the cumulative product of slow but continuous processes, often contrasted with catastrophism. The theory was proposed in 1795 by James Hutton, a Scottish geologist, and was later incorporated into Charles Lyell's theory of uniformitarianism. Tenets from both theories were applied to biology and formed the basis of early evolutionary theory.
Charles Darwin was influenced by Lyell's "Principles of Geology", which explained both uniformitarian methodology and theory. Using uniformitarianism, which states that one cannot make an appeal to any force or phenomenon which cannot presently be observed (see catastrophism), Darwin theorized that the evolutionary process must occur gradually, not in saltations, since saltations are not presently observed, and extreme deviations from the usual phenotypic variation would be more likely to be selected against.
Gradualism is often confused with the concept of phyletic gradualism. It is a term coined by Stephen Jay Gould and Niles Eldredge to contrast with their model of punctuated equilibrium, which is gradualist itself, but argues that most evolution is marked by long periods of evolutionary stability (called stasis), which is punctuated by rare instances of branching evolution.
Phyletic gradualism is a model of evolution which theorizes that most speciation is slow, uniform and gradual. When evolution occurs in this mode, it is usually by the steady transformation of a whole species into a new one (through a process called anagenesis). In this view no clear line of demarcation exists between an ancestral species and a descendant species, unless splitting occurs.
Punctuated gradualism is a microevolutionary hypothesis that refers to a species that has "relative stasis over a considerable part of its total duration [and] underwent periodic, relatively rapid, morphologic change that did not lead to lineage branching". It is one of the three common models of evolution. While the traditional model of palaeontology, the phylogenetic model, states that features evolved slowly without any direct association with speciation, the relatively newer and more controversial idea of punctuated equilibrium claims that major evolutionary changes don't happen over a gradual period but in localized, rare, rapid events of branching speciation. Punctuated gradualism is considered to be a variation of these models, lying somewhere in between the phyletic gradualism model and the punctuated equilibrium model. It states that speciation is not needed for a lineage to rapidly evolve from one equilibrium to another but may show rapid transitions between long-stable states.
In politics, gradualism is the hypothesis that social change can be achieved in small, discrete increments rather than in abrupt strokes such as revolutions or uprisings. Gradualism is one of the defining features of political liberalism and reformism. Machiavellian politics pushes politicians to espouse gradualism.
In socialist politics and within the socialist movement, the concept of gradualism is frequently distinguished from reformism, with the former insisting that short-term goals need to be formulated and implemented in such a way that they inevitably lead into long-term goals. It is most commonly associated with the libertarian socialist concept of dual power and is seen as a middle way between reformism and revolutionism.
Martin Luther King Jr. was opposed to the idea of gradualism as a method of eliminating segregation. The government wanted to try to integrate African-Americans and European-Americans slowly into the same society, but many believed it was a way for the government to put off actually doing anything about racial segregation:
In linguistics, language change is seen as gradual, the product of chain reactions and subject to cyclic drift. The view that creole languages are the product of catastrophism is heavily disputed.
Gradualism is the approach of certain schools of Buddhism and other Eastern philosophies (e.g. Theravada or Yoga), that enlightenment can be achieved step by step, through an arduous practice. The opposite approach, that insight is attained all at once, is called subitism. The debate on the issue was very important to the history of the development of Zen, which rejected gradualism, and to the establishment of the opposite approach within the Tibetan Buddhism, after the Debate of Samye. It was continued in other schools of Indian and Chinese philosophy.
Contradictorial gradualism is the paraconsistent treatment of fuzziness developed by Lorenzo Peña which regards true contradictions as situations wherein a state of affairs enjoys only partial existence.
Gradualism in social change implemented through reformist means is a moral principle to which the Fabian Society is committed. In a more general way, reformism is the assumption that gradual changes through and within existing institutions can ultimately change a society's fundamental economic system and political structures; and that an accumulation of reforms can lead to the emergence of an entirely different economic system and form of society than present-day capitalism. That hypothesis of social change grew out of opposition to revolutionary socialism, which contends that revolution is necessary for fundamental structural changes to occur.
In the terminology of NWO-related speculations, gradualism refers to the gradual implementation of a totalitarian world government.

</doc>
<doc id="11882" url="https://en.wikipedia.org/wiki?curid=11882" title="Greek">
Greek

Greek may refer to: 
Anything of, from, or related to Greece, a country in Southern Europe:

</doc>
<doc id="11883" url="https://en.wikipedia.org/wiki?curid=11883" title="Germanic languages">
Germanic languages

The Germanic languages are a branch of the Indo-European language family spoken natively by a population of about 515 million people mainly in Europe, North America, Oceania and Southern Africa. The most widely spoken Germanic language, English, is the world's most widely spoken language with an estimated 2 billion speakers. All Germanic languages are derived from Proto-Germanic, spoken in Iron Age Scandinavia.
The West Germanic languages include the three most widely spoken Germanic languages: English with around 360–400 million native speakers; German, with over 100 million native speakers; and Dutch, with 24 million native speakers. Other West Germanic languages include Afrikaans, an offshoot of Dutch, with over 7.1 million native speakers; Low German, considered a separate collection of unstandardized dialects, with roughly 0.3 million native speakers and probably 6.7–10 million people who can understand it (at least 5 million in Germany and 1.7 million in the Netherlands); Yiddish, once used by approximately 13 million Jews in pre-World War II Europe, and Scots, both with 1.5 million native speakers; Limburgish varieties with roughly 1.3 million speakers along the Dutch–Belgian–German border; and the Frisian languages with over 0.5 million native speakers in the Netherlands and Germany.
The largest North Germanic languages are Danish, Norwegian and Swedish, which are mutually intelligible and have a combined total of about 20 million native speakers in the Nordic countries and an additional five million second language speakers; since the middle ages these languages have however been strongly influenced by the West Germanic language Middle Low German, and Low German words account for about 30–60% of their vocabularies according to various estimates. Other North Germanic languages are Faroese and Icelandic, which are more conservative languages with no significant Low German influence, more complex grammar and limited mutual intelligibility with the others today.
The East Germanic branch included Gothic, Burgundian, and Vandalic, all of which are now extinct. The last to die off was Crimean Gothic, spoken until the late 18th century in some isolated areas of Crimea.
The SIL "Ethnologue" lists 48 different living Germanic languages, 41 of which belong to the Western branch and six to the Northern branch; it places Riograndenser Hunsrückisch German in neither of the categories, but it is often considered a German dialect by linguists. The total number of Germanic languages throughout history is unknown as some of them, especially the East Germanic languages, disappeared during or after the Migration Period. Some of the West Germanic languages also did not survive past the Migration Period, including Lombardic. As a result of World War II, the German language suffered a significant loss of "Sprachraum", as well as moribundity and extinction of several of its dialects. In the 21st century, its dialects are dying out due to Standard German gaining primacy.
The common ancestor of all of the languages in this branch is called Proto-Germanic, also known as Common Germanic, which was spoken in about the middle of the 1st millennium BC in Iron Age Scandinavia. Proto-Germanic, along with all of its descendants, is characterised by a number of unique linguistic features, most famously the consonant change known as Grimm's law. Early varieties of Germanic entered history with the Germanic tribes moving south from Scandinavia in the 2nd century BC, to settle in the area of today's northern Germany and southern Denmark.
English is an official language of Belize, Canada, Nigeria, Falkland Islands, Malta, New Zealand, Ireland, South Africa, Philippines, Jamaica, Dominica, Guyana, Trinidad and Tobago, American Samoa, Palau, St. Lucia, Grenada, Barbados, St. Vincent and the Grenadines, Puerto Rico, Guam, Hong Kong, Singapore, Pakistan, India, Papua New Guinea, Namibia, Vanuatu, the Solomon Islands and former British colonies in Asia, Africa and Oceania. Furthermore, it is the "de facto" language of the United Kingdom, the United States and Australia. It is also a recognised language in Nicaragua and Malaysia. American English-speakers make up the majority of all native Germanic speakers, including also making up the bulk of West Germanic speakers.
German is a language of Austria, Belgium, Germany, Liechtenstein, Luxembourg and Switzerland and has regional status in Italy, Poland, Namibia and Denmark. German also continues to be spoken as a minority language by immigrant communities in North America, South America, Central America, Mexico and Australia. A German dialect, Pennsylvania German, is still present amongst Anabaptist populations in Pennsylvania in the United States.
Dutch is an official language of Aruba, Belgium, Curaçao, the Netherlands, Sint Maarten, and Suriname. The Netherlands also colonised Indonesia, but Dutch was scrapped as an official language after Indonesian independence and today it is only used by older or traditionally educated people. Dutch was until 1984 an official language in South Africa but evolved in and was replaced by Afrikaans, a partially mutually intelligible daughter language of Dutch.
Afrikaans is one of the 11 official languages in South Africa and is a "lingua franca" of Namibia. It is used in other Southern African nations, as well.
Low German is a collection of very diverse dialects spoken in the northeast of the Netherlands and northern Germany.
Scots is spoken in Lowland Scotland and parts of Ulster (where the local dialect is known as Ulster Scots).
Frisian is spoken among half a million people who live on the southern fringes of the North Sea in the Netherlands, Germany, and Denmark.
Luxembourgish is a Moselle Franconian dialect that is spoken mainly in the Grand Duchy of Luxembourg, where it is considered to be an official language. Similar varieties of Moselle Franconian are spoken in small parts of Belgium, France, and Germany.
Yiddish, once a native language of some 11 to 13 million people, remains in use by some 1.5 million speakers in Jewish communities around the world, mainly in North America, Europe, Israel, and other regions with Jewish populations.
Limburgish varieties are spoken in the Limburg and Rhineland regions, along the Dutch–Belgian–German border.
In addition to being the official language in Sweden, Swedish is also spoken natively by the Swedish-speaking minority in Finland, which is a large part of the population along the coast of western and southern Finland. Swedish is also one of the two official languages in Finland, along with Finnish, and the only official language in the Åland Islands. Swedish is also spoken by some people in Estonia.
Danish is an official language of Denmark and in its overseas territory of the Faroe Islands, and it is a "lingua franca" and language of education in its other overseas territory of Greenland, where it was one of the official languages until 2009. Danish is also spoken natively by the Danish minority in the German state of Schleswig-Holstein, where it is recognised as a minority language.
Norwegian is the official language of Norway. Norwegian is also the official language in the overseas territories of Norway such as Svalbard, Jan Mayen, Bouvet island, Queen Maud land and Peter 1 island
Icelandic is the official language of Iceland.
Faroese is the official language of the Faroe Islands, and it is also spoken by some people in Denmark.
All Germanic languages are thought to be descended from a hypothetical Proto-Germanic, united by subjection to the sound shifts of Grimm's law and Verner's law. These probably took place during the Pre-Roman Iron Age of Northern Europe from c. 500 BC. Proto-Germanic itself was likely spoken after c. 500 BC, and Proto-Norse from the 2nd century AD and later is still quite close to reconstructed Proto-Germanic, but other common innovations separating Germanic from Proto-Indo-European suggest a common history of pre-Proto-Germanic speakers throughout the Nordic Bronze Age.
From the time of their earliest attestation, the Germanic varieties are divided into three groups: West, East, and North Germanic. Their exact relation is difficult to determine from the sparse evidence of runic inscriptions.
The western group would have formed in the late Jastorf culture, and the eastern group may be derived from the 1st-century variety of Gotland, leaving southern Sweden as the original location of the northern group. The earliest period of Elder Futhark (2nd to 4th centuries) predates the division in regional script variants, and linguistically essentially still reflect the Common Germanic stage. The Vimose inscriptions include some of the oldest datable Germanic inscriptions, starting in c. 160 AD.
The earliest coherent Germanic text preserved is the 4th-century Gothic translation of the New Testament by Ulfilas. Early testimonies of West Germanic are in Old Frankish/Old Dutch (the 5th-century Bergakker inscription), Old High German (scattered words and sentences 6th century and coherent texts 9th century), and Old English (oldest texts 650, coherent texts 10th century). North Germanic is only attested in scattered runic inscriptions, as Proto-Norse, until it evolves into Old Norse by about 800.
Longer runic inscriptions survive from the 8th and 9th centuries (Eggjum stone, Rök stone), longer texts in the Latin alphabet survive from the 12th century (Íslendingabók), and some skaldic poetry dates back to as early as the 9th century.
By about the 10th century, the varieties had diverged enough to make inter-comprehensibility difficult. The linguistic contact of the Viking settlers of the Danelaw with the Anglo-Saxons left traces in the English language and is suspected to have facilitated the collapse of Old English grammar that resulted in Middle English from the 12th century.
The East Germanic languages were marginalized from the end of the Migration Period. The Burgundians, Goths, and Vandals became linguistically assimilated by their respective neighbors by about the 7th century, with only Crimean Gothic lingering on until the 18th century.
During the early Middle Ages, the West Germanic languages were separated by the insular development of Middle English on one hand and by the High German consonant shift on the continent on the other, resulting in Upper German and Low Saxon, with graded intermediate Central German varieties. By early modern times, the span had extended into considerable differences, ranging from Highest Alemannic in the South to Northern Low Saxon in the North, and, although both extremes are considered German, they are hardly mutually intelligible. The southernmost varieties had completed the second sound shift, while the northern varieties remained unaffected by the consonant shift.
The North Germanic languages, on the other hand, remained unified until well past 1000 AD, and in fact the mainland Scandinavian languages still largely retain mutual intelligibility into modern times. The main split in these languages is between the mainland languages and the island languages to the west, especially Icelandic, which has maintained the grammar of Old Norse virtually unchanged, while the mainland languages have diverged greatly.
Germanic languages possess a number of defining features compared with other Indo-European languages.
Some of the most well-known are the following:
Other significant characteristics are:
Note that some of the above characteristics were not present in Proto-Germanic but developed later as areal features that spread from language to language:
Roughly speaking, Germanic languages differ in how conservative or how progressive each language is with respect to an overall trend toward analyticity. Some, such as Icelandic and, to a lesser extent, German, have preserved much of the complex inflectional morphology inherited from Proto-Germanic (and in turn from Proto-Indo-European). Others, such as English, Swedish, and Afrikaans, have moved toward a largely analytic type.
The subgroupings of the Germanic languages are defined by shared innovations. It is important to distinguish innovations from cases of linguistic conservatism. That is, if two languages in a family share a characteristic that is not observed in a third language, that is evidence of common ancestry of the two languages "only if" the characteristic is an innovation compared to the family's proto-language.
The following innovations are common to the Northwest Germanic languages (all but Gothic):
The following innovations are also common to the Northwest Germanic languages but represent areal changes:
The following innovations are common to the West Germanic languages:
The following innovations are common to the Ingvaeonic subgroup of the West Germanic languages, which includes English, Frisian, and in a few cases Dutch and Low German, but not High German:
The following innovations are common to the Anglo-Frisian subgroup of the Ingvaeonic languages:
The oldest Germanic languages all share a number of features, which are assumed to be inherited from Proto-Germanic. Phonologically, it includes the important sound changes known as Grimm's Law and Verner's Law, which introduced a large number of fricatives; late Proto-Indo-European had only one, /s/.
The main vowel developments are the merging (in most circumstances) of long and short /a/ and /o/, producing short /a/ and long /ō/. That likewise affected the diphthongs, with PIE /ai/ and /oi/ merging into /ai/ and PIE /au/ and /ou/ merging into /au/. PIE /ei/ developed into long /ī/. PIE long /ē/ developed into a vowel denoted as /ē/ (often assumed to be phonetically ), while a new, fairly uncommon long vowel /ē/ developed in varied and not completely understood circumstances. Proto-Germanic had no front rounded vowels, but all Germanic languages except for Gothic subsequently developed them through the process of i-umlaut.
Proto-Germanic developed a strong stress accent on the first syllable of the root, but remnants of the original free PIE accent are visible due to Verner's Law, which was sensitive to this accent. That caused a steady erosion of vowels in unstressed syllables. In Proto-Germanic, that had progressed only to the point that absolutely-final short vowels (other than /i/ and /u/) were lost and absolutely-final long vowels were shortened, but all of the early literary languages show a more advanced state of vowel loss. This ultimately resulted in some languages (like Modern English) losing practically all vowels following the main stress and the consequent rise of a very large number of monosyllabic words.
The following table shows the main outcomes of Proto-Germanic vowels and consonants in the various older languages. For vowels, only the outcomes in stressed syllables are shown. Outcomes in unstressed syllables are quite different, vary from language to language and depend on a number of other factors (such as whether the syllable was medial or final, whether the syllable was open or closed and (in some cases) whether the preceding syllable was light or heavy).
Notes:
The oldest Germanic languages have the typical complex inflected morphology of old Indo-European languages, with four or five noun cases; verbs marked for person, number, tense and mood; multiple noun and verb classes; few or no articles; and rather free word order. The old Germanic languages are famous for having only two tenses (present and past), with three PIE past-tense aspects (imperfect, aorist, and perfect/stative) merged into one and no new tenses (future, pluperfect, etc.) developing. There were three moods: indicative, subjunctive (developed from the PIE optative mood) and imperative. Gothic verbs had a number of archaic features inherited from PIE that were lost in the other Germanic languages with few traces, including dual endings, an inflected passive voice (derived from the PIE mediopassive voice), and a class of verbs with reduplication in the past tense (derived from the PIE perfect). The complex tense system of modern English (e.g. "In three months, the house will still be being built" or "If you had not acted so stupidly, we would never have been caught") is almost entirely due to subsequent developments (although paralleled in many of the other Germanic languages).
Among the primary innovations in Proto-Germanic are the preterite present verbs, a special set of verbs whose present tense looks like the past tense of other verbs and which is the origin of most modal verbs in English; a past-tense ending (in the so-called "weak verbs", marked with "-ed" in English) that appears variously as /d/ or /t/, often assumed to be derived from the verb "to do"; and two separate sets of adjective endings, originally corresponding to a distinction between indefinite semantics ("a man", with a combination of PIE adjective and pronoun endings) and definite semantics ("the man", with endings derived from PIE "n"-stem nouns).
Note that most modern Germanic languages have lost most of the inherited inflectional morphology as a result of the steady attrition of unstressed endings triggered by the strong initial stress. (Contrast, for example, the Balto-Slavic languages, which have largely kept the Indo-European pitch accent and consequently preserved much of the inherited morphology.) Icelandic and to a lesser extent modern German best preserve the Proto–Germanic inflectional system, with four noun cases, three genders, and well-marked verbs. English and Afrikaans are at the other extreme, with almost no remaining inflectional morphology.
The following shows a typical masculine "a"-stem noun, Proto-Germanic "*fiskaz" ("fish"), and its development in the various old literary languages:
Originally, adjectives in Proto-Indo-European followed the same declensional classes as nouns. The most common class (the "o/ā" class) used a combination of "o"-stem endings for masculine and neuter genders and "ā"-stems ending for feminine genders, but other common classes (e.g. the "i" class and "u" class) used endings from a single vowel-stem declension for all genders, and various other classes existed that were based on other declensions. A quite different set of "pronominal" endings was used for pronouns, determiners, and words with related semantics (e.g., "all", "only").
An important innovation in Proto-Germanic was the development of two separate sets of adjective endings, originally corresponding to a distinction between indefinite semantics ("a man") and definite semantics ("the man"). The endings of indefinite adjectives were derived from a combination of pronominal endings with one of the common vowel-stem adjective declensions – usually the "o/ā" class (often termed the "a/ō" class in the specific context of the Germanic languages) but sometimes the "i" or "u" classes. Definite adjectives, however, had endings based on "n"-stem nouns. Originally both types of adjectives could be used by themselves, but already by Proto-Germanic times a pattern evolved whereby definite adjectives had to be accompanied by a determiner with definite semantics (e.g., a definite article, demonstrative pronoun, possessive pronoun, or the like), while indefinite adjectives were used in other circumstances (either accompanied by a word with indefinite semantics such as "a", "one", or "some" or unaccompanied).
In the 19th century, the two types of adjectives – indefinite and definite – were respectively termed "strong" and "weak", names which are still commonly used. These names were based on the appearance of the two sets of endings in modern German. In German, the distinctive case endings formerly present on nouns have largely disappeared, with the result that the load of distinguishing one case from another is almost entirely carried by determiners and adjectives. Furthermore, due to regular sound change, the various definite ("n"-stem) adjective endings coalesced to the point where only two endings ("-e" and "-en") remain in modern German to express the sixteen possible inflectional categories of the language (masculine/feminine/neuter/plural crossed with nominative/accusative/dative/genitive – modern German merges all genders in the plural). The indefinite ("a/ō"-stem) adjective endings were less affected by sound change, with six endings remaining ("-, -e, -es, -er, -em, -en"), cleverly distributed in a way that is capable of expressing the various inflectional categories without too much ambiguity. As a result, the definite endings were thought of as too "weak" to carry inflectional meaning and in need of "strengthening" by the presence of an accompanying determiner, while the indefinite endings were viewed as "strong" enough to indicate the inflectional categories even when standing alone. (This view is enhanced by the fact that modern German largely uses weak-ending adjectives when accompanying an indefinite article, and hence the indefinite/definite distinction no longer clearly applies.) By analogy, the terms "strong" and "weak" were extended to the corresponding noun classes, with "a"-stem and "ō"-stem nouns termed "strong" and "n"-stem nouns termed "weak".
However, in Proto-Germanic – and still in Gothic, the most conservative Germanic language – the terms "strong" and "weak" are not clearly appropriate. For one thing, there were a large number of noun declensions. The "a"-stem, "ō"-stem, and "n"-stem declensions were the most common and represented targets into which the other declensions were eventually absorbed, but this process occurred only gradually. Originally the "n"-stem declension was not a single declension but a set of separate declensions (e.g., "-an", "-ōn", "-īn") with related endings, and these endings were in no way any "weaker" than the endings of any other declensions. (For example, among the eight possible inflectional categories of a noun — singular/plural crossed with nominative/accusative/dative/genitive — masculine "an"-stem nouns in Gothic include seven endings, and feminine "ōn"-stem nouns include six endings, meaning there is very little ambiguity of "weakness" in these endings and in fact much less than in the German "strong" endings.) Although it is possible to group the various noun declensions into three basic categories — vowel-stem, "n"-stem, and other-consonant-stem (a.k.a. "minor declensions") — the vowel-stem nouns do not display any sort of unity in their endings that supports grouping them together with each other but separate from the "n"-stem endings.
It is only in later languages that the binary distinction between "strong" and "weak" nouns become more relevant. In Old English, the "n"-stem nouns form a single, clear class, but the masculine "a"-stem and feminine "ō"-stem nouns have little in common with each other, and neither has much similarity to the small class of "u"-stem nouns. Similarly, in Old Norse, the masculine "a"-stem and feminine "ō"-stem nouns have little in common with each other, and the continuations of the masculine "an"-stem and feminine "ōn/īn"-stem nouns are also quite distinct. It is only in Middle Dutch and modern German that the various vowel-stem nouns have merged to the point that a binary strong/weak distinction clearly applies.
As a result, newer grammatical descriptions of the Germanic languages often avoid the terms "strong" and "weak" except in conjunction with German itself, preferring instead to use the terms "indefinite" and "definite" for adjectives and to distinguish nouns by their actual stem class.
In English, both two sets of adjective endings were lost entirely in the late Middle English period.
Note that divisions between and among subfamilies of Germanic are rarely precisely defined; most form continuous clines, with adjacent varieties being mutually intelligible and more separated ones not. Within the Germanic language family are East Germanic, West Germanic, and North Germanic. However, East Germanic languages became extinct several centuries ago.
The table below shows the succession of the significant historical stages of each language (horizontally) and their approximate groupings in subfamilies (vertically). Vertical sequence within each group does not imply a measure of greater or lesser similarity.
All living Germanic languages belong either to the West Germanic or to the North Germanic branch.
The West Germanic group is the larger by far, further subdivided into Anglo-Frisian on one hand and Continental West Germanic on the other. Anglo-Frisian notably includes English and all its variants, while Continental West Germanic includes German (standard register and dialects), as well as Dutch (standard register and dialects).
Modern classification looks like this. For a full classification, see List of Germanic languages.
The earliest evidence of Germanic languages comes from names recorded in the 1st century by Tacitus (especially from his work "Germania"), but the earliest Germanic writing occurs in a single instance in the 2nd century BC on the Negau helmet.
From roughly the 2nd century AD, certain speakers of early Germanic varieties developed the Elder Futhark, an early form of the runic alphabet. Early runic inscriptions also are largely limited to personal names and difficult to interpret. The Gothic language was written in the Gothic alphabet developed by Bishop Ulfilas for his translation of the Bible in the 4th century. Later, Christian priests and monks who spoke and read Latin in addition to their native Germanic varieties began writing the Germanic languages with slightly modified Latin letters. However, throughout the Viking Age, runic alphabets remained in common use in Scandinavia.
In addition to the standard Latin script, many Germanic languages use a variety of accent marks and extra letters, including the ß ("Eszett"), Ĳ, Ø, Æ, Å, Ä, Ü, Ö, Ð, Ȝ, and the Latinized runes Þ and Ƿ (with its Latin counterpart W). In print, German used to be prevalently set in blackletter typefaces (e.g., fraktur or schwabacher) until the 1940s, when "Kurrent" and, since the early 20th century, "Sütterlin" were used for German handwriting.
Yiddish is written using an adapted Hebrew alphabet.
Several of the terms in the table below have had semantic drift. For example, the form "sterben" and other terms for "die" are cognates with the English word "starve". There are also at least three examples of a common borrowing from a non-Germanic source ("ounce" and "devil" and their cognates from Latin, "church" and its cognates from Greek).

</doc>
<doc id="11884" url="https://en.wikipedia.org/wiki?curid=11884" title="German language">
German language

German (, ) is a West Germanic language that is mainly spoken in Central Europe. It is the most widely spoken and official or co-official language in Germany, Austria, Switzerland, South Tyrol in Italy, the German-speaking Community of Belgium, and Liechtenstein. It is one of the three official languages of Luxembourg and a co-official language in the Opole Voivodeship in Poland. The German language is most similar to other languages within the West Germanic language branch, including Afrikaans, Dutch, English, the Frisian languages, Low German/Low Saxon, Luxembourgish, and Yiddish. It also contains close similarities in vocabulary to Danish, Norwegian and Swedish, although they belong to the North Germanic group. German is the second most widely spoken Germanic language, after English.
One of the major languages of the world, German is a native language to almost 100 million people worldwide and the most widely spoken native language in the European Union. German is the third most commonly spoken foreign language in the EU after English and French, making it the second biggest language in the EU in terms of overall speakers. German is also the second most widely taught foreign language in the EU after English at primary school level (but third after English and French at lower secondary level), the fourth most widely taught non-English language in the US (after Spanish, French and American Sign Language), the second most commonly used scientific language and the third most widely used language on websites after English and Russian. The German-speaking countries are ranked fifth in terms of annual publication of new books, with one tenth of all books (including e-books) in the world being published in German. In the United Kingdom, German and French are the most sought-after foreign languages for businesses (with 49% and 50% of businesses identifying these two languages as the most useful, respectively).
German is an inflected language, with four cases for nouns, pronouns, and adjectives (nominative, accusative, genitive, dative); three genders (masculine, feminine, neuter); and two numbers (singular, plural). It also has strong and weak verbs. It derives the majority of its vocabulary from the ancient Germanic branch of the Indo-European language family. Some of its vocabulary is derived from Latin and Greek, and fewer words are borrowed from French and Modern English. German is a pluricentric language, with its standardized variants being German, Austrian, and Swiss Standard German. It is also notable for its broad spectrum of dialects, with many varieties existing in Europe and other parts of the world. Italy recognizes all the German-speaking minorities in its territory as national historic minorities and protects the varieties of German spoken in several regions of Northern Italy besides South Tyrol. Due to the limited intelligibility between certain varieties and Standard German, as well as the lack of an undisputed, scientific distinction between a "dialect" and a "language", some German varieties or dialect groups (e.g. Low German or Plautdietsch) can be described as either "languages" or "dialects".
Modern Standard German is a West Germanic language in the Germanic branch of the Indo-European languages. The Germanic languages are traditionally subdivided into three branches: North Germanic, East Germanic, and West Germanic. The first of these branches survives in modern Danish, Swedish, Norwegian, Faroese, and Icelandic, all of which are descended from Old Norse. The East Germanic languages are now extinct, and Gothic is the only language in this branch which survives in written texts. The West Germanic languages, however, have undergone extensive dialectal subdivision and are now represented in modern languages such as English, German, Dutch, Yiddish, Afrikaans, and others. 
Within the West Germanic language dialect continuum, the Benrath and Uerdingen lines (running through Düsseldorf-Benrath and Krefeld-Uerdingen, respectively) serve to distinguish the Germanic dialects that were affected by the High German consonant shift (south of Benrath) from those that were not (north of Uerdingen). The various regional dialects spoken south of these lines are grouped as High German dialects "(nos. 29–34 on the map)", while those spoken to the north comprise the Low German/Low Saxon "(nos. 19–24)" and Low Franconian "(no. 25)" dialects. As members of the West Germanic language family, High German, Low German, and Low Franconian can be further distinguished historically as Irminonic, Ingvaeonic, and Istvaeonic, respectively. This classification indicates their historical descent from dialects spoken by the Irminones (also known as the Elbe group), Ingvaeones (or North Sea Germanic group), and Istvaeones (or Weser-Rhine group).
Standard German is based on a combination of Thuringian-Upper Saxon and Upper Franconian and Bavarian dialects, which are Central German and Upper German dialects, belonging to the Irminonic High German dialect group "(nos. 29–34)". German is therefore closely related to the other languages based on High German dialects, such as Luxembourgish (based on Central Franconian dialects – "no. 29"), and Yiddish. Also closely related to Standard German are the Upper German dialects spoken in the southern German-speaking countries, such as Swiss German (Alemannic dialects – "no. 34"), and the various Germanic dialects spoken in the French region of Grand Est, such as Alsatian (mainly Alemannic, but also Central- and Upper Franconian "(no. 32)" dialects) and Lorraine Franconian (Central Franconian – "no. 29").
After these High German dialects, standard German is less closely related to languages based on Low Franconian dialects (e.g. Dutch and Afrikaans), Low German or Low Saxon dialects (spoken in northern Germany and southern Denmark), neither of which underwent the High German consonant shift. As has been noted, the former of these dialect types is Istvaeonic and the latter Ingvaeonic, whereas the High German dialects are all Irminonic; the differences between these languages and standard German are therefore considerable. Also related to German are the Frisian languages—North Frisian (spoken in Nordfriesland – "no. 28"), Saterland Frisian (spoken in Saterland – "no. 27"), and West Frisian (spoken in Friesland – "no. 26")—as well as the Anglic languages of English and Scots. These Anglo-Frisian dialects are all members of the Ingvaeonic family of West Germanic languages, which did not take part in the High German consonant shift.
The history of the German language begins with the High German consonant shift during the migration period, which separated Old High German dialects from Old Saxon. This sound shift involved a drastic change in the pronunciation of both voiced and voiceless stop consonants ("b", "d", "g", and "p", "t", "k", respectively). The primary effects of the shift were the following below.
While there is written evidence of the Old High German language in several Elder Futhark inscriptions from as early as the sixth century AD (such as the Pforzen buckle), the Old High German period is generally seen as beginning with the "Abrogans" (written c. 765–775), a Latin-German glossary supplying over 3,000 OHG words with their Latin equivalents. After the "Abrogans", the first coherent works written in Old High German appear in the ninth century, chief among them being the "Muspilli", the "Merseburg Charms", and the "Hildebrandslied", and other religious texts (the "Georgslied", the "Ludwigslied", the "Evangelienbuch", and translated hymns and prayers). The "Muspilli" is a Christian poem written in a Bavarian dialect offering an account of the soul after the Last Judgment, and the "Merseburg Charms" are transcriptions of spells and charms from the pagan Germanic tradition. Of particular interest to scholars, however, has been the "Hildebrandslied", a secular epic poem telling the tale of an estranged father and son unknowingly meeting each other in battle. Linguistically this text is highly interesting due to the mixed use of Old Saxon and Old High German dialects in its composition. The written works of this period stem mainly from the Alamanni, Bavarian, and Thuringian groups, all belonging to the Elbe Germanic group (Irminones), which had settled in what is now southern-central Germany and Austria between the 2nd and 6th centuries during the great migration.
In general, the surviving texts of OHG show a wide range of dialectal diversity with very little written uniformity. The early written tradition of OHG survived mostly through monasteries and scriptoria as local translations of Latin originals; as a result, the surviving texts are written in highly disparate regional dialects and exhibit significant Latin influence, particularly in vocabulary. At this point monasteries, where most written works were produced, were dominated by Latin, and German saw only occasional use in official and ecclesiastical writing.
The German language through the OHG period was still predominantly a spoken language, with a wide range of dialects and a much more extensive oral tradition than a written one. Having just emerged from the High German consonant shift, OHG was also a relatively new and volatile language still undergoing a number of phonetic, phonological, morphological, and syntactic changes. The scarcity of written work, instability of the language, and widespread illiteracy of the time explain the lack of standardization up to the end of the OHG period in 1050.
While there is no complete agreement over the dates of the Middle High German (MHG) period, it is generally seen as lasting from 1050 to 1350. This was a period of significant expansion of the geographical territory occupied by Germanic tribes, and consequently of the number of German speakers. Whereas during the Old High German period the Germanic tribes extended only as far east as the Elbe and Saale rivers, the MHG period saw a number of these tribes expanding beyond this eastern boundary into Slavic territory (known as the "Ostsiedlung"). With the increasing wealth and geographic spread of the Germanic groups came greater use of German in the courts of nobles as the standard language of official proceedings and literature. A clear example of this is the "mittelhochdeutsche Dichtersprache" employed in the Hohenstaufen court in Swabia as a standardized supra-dialectal written language. While these efforts were still regionally bound, German began to be used in place of Latin for certain official purposes, leading to a greater need for regularity in written conventions.
While the major changes of the MHG period were socio-cultural, German was still undergoing significant linguistic changes in syntax, phonetics, and morphology as well (e.g. diphthongization of certain vowel sounds: "hus" (OHG "house")"→haus" (MHG), and weakening of unstressed short vowels to schwa [ə]: "taga" (OHG "days")→"tage" (MHG)).
A great wealth of texts survives from the MHG period. Significantly, these texts include a number of impressive secular works, such as the "Nibelungenlied", an epic poem telling the story of the dragon-slayer Siegfried ( 13th century), and the "Iwein," an Arthurian verse poem by Hartmann von Aue ( 1203), as well as several lyric poems and courtly romances such as "Parzival" and "Tristan". Also noteworthy is the "Sachsenspiegel", the first book of laws written in Middle "Low" German ( 1220). The abundance and especially the secular character of the literature of the MHG period demonstrate the beginnings of a standardized written form of German, as well as the desire of poets and authors to be understood by individuals on supra-dialectal terms.
The Middle High German period is generally seen as ending when the 1346-53 Black Death decimated Europe's population.
Modern German begins with the Early New High German (ENHG) period, which the influential German philologist Wilhelm Scherer dates 1350–1650, terminating with the end of the Thirty Years' War. This period saw the further displacement of Latin by German as the primary language of courtly proceedings and, increasingly, of literature in the German states. While these states were still under the control of the Holy Roman Empire and far from any form of unification, the desire for a cohesive written language that would be understandable across the many German-speaking principalities and kingdoms was stronger than ever. As a spoken language German remained highly fractured throughout this period, with a vast number of often mutually incomprehensible regional dialects being spoken throughout the German states; the invention of the printing press 1440 and the publication of Luther's vernacular translation of the Bible in 1534, however, had an immense effect on standardizing German as a supra-dialectal written language.
The ENHG period saw the rise of several important cross-regional forms of chancery German, one being "gemeine tiutsch," used in the court of the Holy Roman Emperor Maximilian I, and the other being "Meißner Deutsch", used in the Electorate of Saxony in the Duchy of Saxe-Wittenberg.
Alongside these courtly written standards, the invention of the printing press led to the development of a number of printers' languages ("Druckersprachen") aimed at making printed material readable and understandable across as many diverse dialects of German as possible. The greater ease of production and increased availability of written texts brought about increased standardization in the written form of German.One of the central events in the development of ENHG was the publication of Luther's translation of the Bible into German (the New Testament was published in 1522; the Old Testament was published in parts and completed in 1534). Luther based his translation primarily on the "Meißner Deutsch" of Saxony, spending much time among the population of Saxony researching the dialect so as to make the work as natural and accessible to German speakers as possible. Copies of Luther's Bible featured a long list of glosses for each region, translating words which were unknown in the region into the regional dialect. Luther said the following concerning his translation method:One who would talk German does not ask the Latin how he shall do it; he must ask the mother in the home, the children on the streets, the common man in the market-place and note carefully how they talk, then translate accordingly. They will then understand what is said to them because it is German. When Christ says 'ex abundantia cordis os loquitur,' I would translate, if I followed the papists, "aus dem Überflusz des Herzens redet der Mund". But tell me is this talking German? What German understands such stuff? No, the mother in the home and the plain man would say, "Wesz das Herz voll ist, des gehet der Mund über".With Luther's rendering of the Bible in the vernacular, German asserted itself against the dominance of Latin as a legitimate language for courtly, literary, and now ecclesiastical subject-matter. Furthermore, his Bible was ubiquitous in the German states: nearly every household possessed a copy. Nevertheless, even with the influence of Luther's Bible as an unofficial written standard, a widely accepted standard for written German did not appear until the middle of the 18th century.
German was the language of commerce and government in the Habsburg Empire, which encompassed a large area of Central and Eastern Europe. Until the mid-19th century, it was essentially the language of townspeople throughout most of the Empire. Its use indicated that the speaker was a merchant or someone from an urban area, regardless of nationality.
Some cities, such as Prague () and Budapest (Buda, ), were gradually Germanized in the years after their incorporation into the Habsburg domain. Others, such as Pozsony (, now Bratislava), were originally settled during the Habsburg period and were primarily German at that time. Prague, Budapest and Bratislava, as well as cities like Zagreb () and Ljubljana (), contained significant German minorities.
In the eastern provinces of Banat, Bukovina, and Transylvania (), German was the predominant language not only in the larger towns – such as (Timișoara), (Sibiu) and (Brașov) – but also in many smaller localities in the surrounding areas.
The most comprehensive guide to the vocabulary of the German language is found within the . This dictionary was created by the Brothers Grimm and is composed of 16 parts which were issued between 1852 and 1860. In 1872, grammatical and orthographic rules first appeared in the "Duden Handbook".
In 1901, the 2nd Orthographical Conference ended with a complete standardization of the German language in its written form and the "Duden Handbook" was declared its standard definition. The (literally, German stage language) had established conventions for German pronunciation in theatres (Bühnendeutsch) three years earlier; however, this was an artificial standard that did not correspond to any traditional spoken dialect. Rather, it was based on the pronunciation of Standard German in Northern Germany, although it was subsequently regarded often as a general prescriptive norm, despite differing pronunciation traditions especially in the Upper-German-speaking regions that still characterize the dialect of the area today – especially the pronunciation of the ending as [ɪk] instead of [ɪç]. In Northern Germany, Standard German was a foreign language to most inhabitants, whose native dialects were subsets of Low German. It was usually encountered only in writing or formal speech; in fact, most of Standard German was a written language, not identical to any spoken dialect, throughout the German-speaking area until well into the 19th century.
Official revisions of some of the rules from 1901 were not issued until the controversial German orthography reform of 1996 was made the official standard by governments of all German-speaking countries. Media and written works are now almost all produced in Standard German (often called , "High German") which is understood in all areas where German is spoken.
Due to the German diaspora as well as German being the second most widely spoken language in Europe and the third most widely taught foreign language in the US and the EU (in upper secondary education) amongst others, the geographical distribution of German speakers (or "Germanophones") spans all inhabited continents. As for the number of speakers of any language worldwide, an assessment is always compromised by the lack of sufficient, reliable data. For an exact, global number of native German speakers, this is further complicated by the existence of several varieties whose status as separate "languages" or "dialects" is disputed for political and/or linguistic reasons, including quantitatively strong varieties like certain forms of Alemannic (e.g., Alsatian) and Low German/Plautdietsch. Depending on the inclusion or exclusion of certain varieties, it is estimated that approximately 90–95 million people speak German as a first language, 10–25 million as a second language, and 75–100 million as a foreign language. This would imply the existence of approximately 175–220 million German speakers worldwide. It is estimated that including every person studying German, regardless of their actual proficiency, would amount to about 280 million people worldwide with at least some knowledge of German.
In Europe, German is the second most widely spoken mother tongue (after Russian) and the second biggest language in terms of overall speakers (after English). The area in central Europe where the majority of the population speaks German as a first language and has German as a (co-)official language is called the "German "Sprachraum"". It comprises an estimated 88 million native speakers and 10 million who speak German as a second language (e.g. immigrants). Excluding regional minority languages, German is the only official language of the following countries:
German is a co-official language of the following countries:
Although expulsions and (forced) assimilation after the two World Wars greatly diminished them, minority communities of mostly bilingual German native speakers exist in areas both adjacent to and detached from the Sprachraum.
Within Europe and Asia, German is a recognized minority language in the following countries:
In France, the High German varieties of Alsatian and Moselle Franconian are identified as "regional languages", but the European Charter for Regional and Minority Languages of 1998 has not yet been ratified by the government. In the Netherlands, the Limburgish, Frisian, and Low German languages are protected regional languages according to the European Charter for Regional and Minority Languages; however, they are widely considered separate languages and neither German nor Dutch dialects.
Namibia was a colony of the German Empire from 1884 to 1919. Mostly descending from German settlers who immigrated during this time, 25–30,000 people still speak German as a native tongue today. The period of German colonialism in Namibia also led to the evolution of a Standard German-based pidgin language called "Namibian Black German", which became a second language for parts of the indigenous population. Although it is nearly extinct today, some older Namibians still have some knowledge of it.
German, along with English and Afrikaans, was a co-official language of Namibia from 1984 until its independence from South Africa in 1990. At this point, the Namibian government perceived Afrikaans and German as symbols of apartheid and colonialism, and decided English would be the sole official language, stating that it was a "neutral" language as there were virtually no English native speakers in Namibia at that time. German, Afrikaans and several indigenous languages became "national languages" by law, identifying them as elements of the cultural heritage of the nation and ensuring that the state acknowledged and supported their presence in the country. Today, German is used in a wide variety of spheres, especially business and tourism, as well as the churches (most notably the German-speaking Evangelical Lutheran Church in Namibia (GELK)), schools (e.g. the ), literature (German-Namibian authors include Giselher W. Hoffmann), radio (the Namibian Broadcasting Corporation produces radio programs in German), and music (e.g. artist EES). The is one of the three biggest newspapers in Namibia and the only German-language daily in Africa.
Mostly originating from different waves of immigration during the 19th and 20th centuries, an estimated 12,000 people speak German or a German variety as a first language in South Africa. One of the largest communities consists of the speakers of "Nataler Deutsch", a variety of Low German concentrated in and around Wartburg. The small town of Kroondal in the North-West Province also has a mostly German-speaking population. The South African constitution identifies German as a "commonly used" language and the Pan South African Language Board is obligated to promote and ensure respect for it. The community is strong enough that several German International schools are supported, such as the Deutsche Schule Pretoria.
In the United States, the states of North Dakota and South Dakota are the only states where German is the most common language spoken at home after English. German geographical names can be found throughout the Midwest region of the country, such as New Ulm and many other towns in Minnesota; Bismarck (North Dakota's state capital), Munich, Karlsruhe, and Strasburg (named after a town near Odessa in Ukraine) in North Dakota; New Braunfels, Fredericksburg, Weimar, and Muenster in Texas; Corn (formerly Korn), Kiefer and Berlin in Oklahoma; and Kiel, Berlin, and Germantown in Wisconsin.
In Brazil, the largest concentrations of German speakers are in the states of Rio Grande do Sul (where Riograndenser Hunsrückisch developed), Santa Catarina, Paraná, São Paulo and Espírito Santo.
There are important concentrations of German-speaking descendants in Argentina, Chile, Paraguay, Venezuela, Peru, and Bolivia.
The impact of nineteenth century German immigration to southern Chile was such that Valdivia was for a while a Spanish-German bilingual city with "German signboards and placards alongside the Spanish". The prestige the German language caused it to acquire qualities of a superstratum in southern Chile. The word for blackberry, a ubiquitous plant in southern Chile, is "murra", instead of the ordinary Spanish words "mora" and "zarzamora", from Valdivia to the Chiloé Archipelago and in some towns in the Aysén Region. The use of "rr" is an adaptation of guttural sounds found in German but difficult to pronounce in Spanish. Similarly the name for marbles, a traditional children's game, is different in Southern Chile compared to areas further north. From Valdivia to the Aysén Region this game is called "bochas", in contrast to the word "bolitas" used further north. The word "bocha" is likely a derivative of the German "Bocciaspiel".
In Australia, the state of South Australia experienced a pronounced wave of immigration in the 1840s from Prussia (particularly the Silesia region). With the prolonged isolation from other German speakers and contact with Australian English, a unique dialect known as Barossa German developed, spoken predominantly in the Barossa Valley near Adelaide. Usage of German sharply declined with the advent of World War I, due to the prevailing anti-German sentiment in the population and related government action. It continued to be used as a first language into the 20th century, but its use is now limited to a few older speakers.
German migration to New Zealand in the 19th century was less pronounced than migration from Britain, Ireland, and perhaps even Scandinavia. Despite this there were significant pockets of German-speaking communities which lasted until the first decades of the 20th century. German speakers settled principally in Puhoi, Nelson, and Gore. At the last census (2013), 36,642 people in New Zealand spoke German, making it the third most spoken European language after English and French and overall the ninth most spoken language.
There is also an important German creole being studied and recovered, named , spoken in the former German colony of German New Guinea, across Micronesia and in northern Australia (i.e. coastal parts of Queensland and Western Australia) by a few elderly people. The risk of its extinction is serious and efforts to revive interest in the language are being implemented by scholars.
Like French and Spanish, German has become a standard second foreign language in the western world. German ranks second (after English) among the best known foreign languages in the EU (on a par with French) as well as in Russia. In terms of student numbers across all levels of education, German ranks third in the EU (after English and French) as well as in the United States (after Spanish and French). In 2015, approximately 15.4 million people were in the process of learning German across all levels of education worldwide. As this number remained relatively stable since 2005 (± 1 million), roughly 75–100 million people able to communicate in German as a foreign language can be inferred, assuming an average course duration of three years and other estimated parameters. According to a 2012 survey, 47 million people within the EU (i.e., up to two-thirds of the 75–100 million worldwide) claimed to have sufficient German skills to have a conversation. Within the EU, not counting countries where it is an official language, German as a foreign language is most popular in Eastern and northern Europe, namely the Czech Republic, Croatia, Denmark, the Netherlands, Slovakia, Hungary, Slovenia, Sweden and Poland. German was once, and to some extent still is, a lingua franca in those parts of Europe.
The basis of Standard German is the Luther Bible, which was translated by Martin Luther and which had originated from the Saxon court language (it being a convenient norm). However, there are places where the traditional regional dialects have been replaced by new vernaculars based on standard German; that is the case in large stretches of Northern Germany but also in major cities in other parts of the country. It is important to note, however, that the colloquial standard German differs greatly from the formal written language, especially in grammar and syntax, in which it has been influenced by dialectal speech.
Standard German differs regionally among German-speaking countries in vocabulary and some instances of pronunciation and even grammar and orthography. This variation must not be confused with the variation of local dialects. Even though the regional varieties of standard German are only somewhat influenced by the local dialects, they are very distinct. German is thus considered a pluricentric language.
In most regions, the speakers use a continuum from more dialectal varieties to more standard varieties depending on the circumstances.
In German linguistics, German dialects are distinguished from varieties of standard German.
The "varieties of standard German" refer to the different local varieties of the pluricentric standard German. They differ only slightly in lexicon and phonology. In certain regions, they have replaced the traditional German dialects, especially in Northern Germany.
In the German-speaking parts of Switzerland, mixtures of dialect and standard are very seldom used, and the use of Standard German is largely restricted to the written language. About 11% of the Swiss residents speak "High German" (Standard German) at home, but this is mainly due to German immigrants. This situation has been called a "medial diglossia". Swiss Standard German is used in the Swiss education system, while Austrian Standard German is officially used in the Austrian education system.
A mixture of dialect and standard does not normally occur in Northern Germany either. The traditional varieties there are Low German, whereas Standard German is a High German "variety". Because their linguistic distance is greater, they do not mesh with Standard German the way that High German dialects (such as Bavarian, Swabian, and Hessian) can.
The German dialects are the traditional local varieties of the language; many of them are not mutually intelligibile with standard German, and they have great differences in lexicon, phonology, and syntax. If a narrow definition of language based on mutual intelligibility is used, many German dialects are considered to be separate languages (for instance in the "Ethnologue"). However, such a point of view is unusual in German linguistics.
The German dialect continuum is traditionally divided most broadly into High German and Low German, also called Low Saxon. However, historically, High German dialects and Low Saxon/Low German dialects do not belong to the same language. Nevertheless, in today's Germany, Low Saxon/Low German is often perceived as a dialectal variation of Standard German on a functional level even by many native speakers. The same phenomenon is found in the eastern Netherlands, as the traditional dialects are not always identified with their Low Saxon/Low German origins, but with Dutch.
The variation among the German dialects is considerable, with often only neighbouring dialects being mutually intelligible. Some dialects are not intelligible to people who know only Standard German. However, all German dialects belong to the dialect continuum of High German and Low Saxon.
Middle Low German was the lingua franca of the Hanseatic League. It was the predominant language in Northern Germany until the 16th century. In 1534, the Luther Bible was published. The translation is considered to be an important step towards the evolution of the Early New High German. It aimed to be understandable to a broad audience and was based mainly on Central and Upper German varieties. The Early New High German language gained more prestige than Low German and became the language of science and literature. Around the same time, the Hanseatic League, based around northern ports, lost its importance as new trade routes to Asia and the Americas were established, and the most powerful German states of that period were located in Middle and Southern Germany.
The 18th and 19th centuries were marked by mass education in Standard German in schools. Gradually, Low German came to be politically viewed as a mere dialect spoken by the uneducated. Today, Low Saxon can be divided in two groups: Low Saxon varieties with a reasonable level of Standard German influence and varieties of Standard German with a Low Saxon influence known as . Sometimes, Low Saxon and Low Franconian varieties are grouped together because both are unaffected by the High German consonant shift. However, the proportion of the population who can understand and speak it has decreased continuously since World War II. The largest cities in the Low German area are Hamburg and Dortmund.
The Low Franconian dialects are the dialects that are more closely related to Dutch than to Low German. Most of the Low Franconian dialects are spoken in the Netherlands and Belgium, where they are considered as dialects of Dutch, which is itself a Low Franconian language. In Germany, Low Franconian dialects are spoken in the northwest of North Rhine-Westphalia, along the Lower Rhine. The Low Franconian dialects spoken in Germany are referred to as Meuse-Rhenish or Low Rhenish. In the north of the German Low Franconian language area, North Low Franconian dialects (also referred to as Cleverlands or as dialects of South Guelderish) are spoken. These dialects are more closely related to Dutch (also North Low Franconian) than the South Low Franconian dialects (also referred to as East Limburgish and, east of the Rhine, Bergish), which are spoken in the south of the German Low Franconian language area. The South Low Franconian dialects are more closely related to Limburgish than to Dutch, and are transitional dialects between Low Franconian and Ripuarian (Central Franconian). The East Bergish dialects are the easternmost Low Franconian dialects, and are transitional dialects between North- and South Low Franconian, and Westphalian (Low German), with most of their features being North Low Franconian. The largest cities in the German Low Franconian area are Düsseldorf and Duisburg.
The High German dialects consist of the Central German, High Franconian, and Upper German dialects. The High Franconian dialects are transitional dialects between Central and Upper German. The High German varieties spoken by the Ashkenazi Jews have several unique features and are considered as a separate language, Yiddish, written with the Hebrew alphabet.
The Central German dialects are spoken in Central Germany, from Aachen in the west to Görlitz in the east. They consist of Franconian dialects in the west (West Central German) and non-Franconian dialects in the east (East Central German). Modern Standard German is mostly based on Central German dialects.
The Franconian, West Central German dialects are the Central Franconian dialects (Ripuarian and Moselle Franconian) and the Rhine Franconian dialects (Hessian and Palatine). These dialects are considered as
Luxembourgish as well as the Transylvanian Saxon dialect spoken in Transylvania are based on Moselle Franconian dialects. The largest cities in the Franconian Central German area are Cologne and Frankfurt.
Further east, the non-Franconian, East Central German dialects are spoken (Thuringian, Upper Saxon, Ore Mountainian, and Lusatian-New Markish, and earlier, in the then German-speaking parts of Silesia also Silesian, and in then German southern East Prussia also High Prussian). The largest cities in the East Central German area are Berlin and Leipzig.
The High Franconian dialects are transitional dialects between Central and Upper German. They consist of the East and South Franconian dialects.
The East Franconian dialect branch is one of the most spoken dialect branches in Germany. These dialects are spoken in the region of Franconia and in the central parts of Saxon Vogtland. Franconia consists of the Bavarian districts of Upper, Middle, and Lower Franconia, the region of South Thuringia (Thuringia), and the eastern parts of the region of Heilbronn-Franken (Tauber Franconia and Hohenlohe) in Baden-Württemberg. The largest cities in the East Franconian area are Nuremberg and Würzburg.
South Franconian is mainly spoken in northern Baden-Württemberg in Germany, but also in the northeasternmost part of the region of Alsace in France. While these dialects are considered as dialects of German in Baden-Württemberg, they are considered as dialects of Alsatian in Alsace (most Alsatian dialects are Low Alemannic, however). The largest cities in the South Franconian area are Karlsruhe and Heilbronn.
The Upper German dialects are the Alemannic dialects in the west and the Bavarian dialects in the east.
Alemannic dialects are spoken in Switzerland (High Alemannic in the densely populated Swiss Plateau, in the south also Highest Alemannic, and Low Alemannic in Basel), Baden-Württemberg (Swabian and Low Alemannic, in the southwest also High Alemannic), Bavarian Swabia (Swabian, in the southwesternmost part also Low Alemannic), Vorarlberg (Low, High, and Highest Alemannic), Alsace (Low Alemannic, in the southernmost part also High Alemannic), Liechtenstein (High and Highest Alemannic), and in the Tyrolean district of Reutte (Swabian). The Alemannic dialects are considered as Alsatian in Alsace. The largest cities in the Alemannic area are Stuttgart and Zürich.
Bavarian dialects are spoken in Austria (Vienna, Lower and Upper Austria, Styria, Carinthia, Salzburg, Burgenland, and in most parts of Tyrol), Bavaria (Upper and Lower Bavaria as well as Upper Palatinate), South Tyrol, southwesternmost Saxony (Southern Vogtlandian), and in the Swiss village of Samnaun. The largest cities in the Bavarian area are Vienna and Munich.
German is a fusional language with a moderate degree of inflection, with three grammatical genders; as such, there can be a large number of words derived from the same root.
German nouns inflect by case, gender, and number:
This degree of inflection is considerably less than in Old High German and other old Indo-European languages such as Latin, Ancient Greek, and Sanskrit, and it is also somewhat less than, for instance, Old English, modern Icelandic, or Russian. The three genders have collapsed in the plural. With four cases and three genders plus plural, there are 16 permutations of case and gender/number of the article (not the nouns), but there are only six forms of the definite article, which together cover all 16 permutations. In nouns, inflection for case is required in the singular for strong masculine and neuter nouns only in the genitive and in the dative (only in fixed or archaic expressions), and even this is losing ground to substitutes in informal speech. The singular dative noun ending is considered archaic or at least old-fashioned in almost all contexts and is almost always dropped in writing, except in poetry, songs, proverbs, and other petrified forms. Weak masculine nouns share a common case ending for genitive, dative, and accusative in the singular. Feminine nouns are not declined in the singular. The plural has an inflection for the dative. In total, seven inflectional endings (not counting plural markers) exist in German: .
In German orthography, nouns and most words with the syntactical function of nouns are capitalised to make it easier for readers to determine the function of a word within a sentence ( – "On Friday I went shopping.";  – "One day he finally showed up.") This convention is almost unique to German today (shared perhaps only by the closely related Luxembourgish language and several insular dialects of the North Frisian language), but it was historically common in other languages such as Danish (which abolished the capitalization of nouns in 1948) and English.
Like the other Germanic languages, German forms noun compounds in which the first noun modifies the category given by the second: ("dog hut"; specifically: "dog kennel"). Unlike English, whose newer compounds or combinations of longer nouns are often written "open" with separating spaces, German (like some other Germanic languages) nearly always uses the "closed" form without spaces, for example: ("tree house"). Like English, German allows arbitrarily long compounds in theory (see also English compounds). The longest German word verified to be actually in (albeit very limited) use is , which, literally translated, is "beef labelling supervision duties assignment law" [from (cattle), (meat), (labelling), (supervision), (duties), (assignment), (law)]. However, examples like this are perceived by native speakers as excessively bureaucratic, stylistically awkward, or even satirical.
The inflection of standard German verbs includes:
The meaning of basic verbs can be expanded and sometimes radically changed through the use of a number of prefixes. Some prefixes have a specific meaning; the prefix ' refers to destruction, as in (to tear apart), (to break apart), (to cut apart). Other prefixes have only the vaguest meaning in themselves; ' is found in a number of verbs with a large variety of meanings, as in (to try) from (to seek), (to interrogate) from (to take), (to distribute) from (to share), (to understand) from (to stand).
Other examples include the following:
Many German verbs have a separable prefix, often with an adverbial function. In finite verb forms, it is split off and moved to the end of the clause and is hence considered by some to be a "resultative particle". For example, , meaning "to go along", would be split, giving (Literal: "Go you with?"; Idiomatic: "Are you going along?").
Indeed, several parenthetical clauses may occur between the prefix of a finite verb and its complement (ankommen = to arrive, er kam an = he arrived, er ist angekommen = he has arrived):
A selectively literal translation of this example to illustrate the point might look like this:
German word order is generally with the V2 word order restriction and also with the SOV word order restriction for main clauses. For polar questions, exclamations, and wishes, the finite verb always has the first position. In subordinate clauses, the verb occurs at the very end.
German requires a verbal element (main verb or auxiliary verb) to appear second in the sentence. The verb is preceded by the topic of the sentence. The element in focus appears at the end of the sentence. For a sentence without an auxiliary, these are several possibilities:
The position of a noun in a German sentence has no bearing on its being a subject, an object or another argument. In a declarative sentence in English, if the subject does not occur before the predicate, the sentence could well be misunderstood.
However, German's flexible word order allows one to emphasise specific words:
Normal word order:
Object in front:
Adverb of time in front:
Both time expressions in front:
Another possibility:
Swapped adverbs:
Swapped object:
The flexible word order also allows one to use language "tools" (such as poetic meter and figures of speech) more freely.
When an auxiliary verb is present, it appears in second position, and the main verb appears at the end. This occurs notably in the creation of the perfect tense. Many word orders are still possible:
The main verb may appear in first position to put stress on the action itself. The auxiliary verb is still in second position.
Sentences using modal verbs place the infinitive at the end. For example, the English sentence "Should he go home?" would be rearranged in German to say "Should he (to) home go?" (). Thus, in sentences with several subordinate or relative clauses, the infinitives are clustered at the end. Compare the similar clustering of prepositions in the following (highly contrived) English sentence: "What did you bring that book that I do not like to be read to out of up for?"
German subordinate clauses have all verbs clustered at the end. Given that auxiliaries encode future, passive, modality, and the perfect, very long chains of verbs at the end of the sentence can occur. In these constructions, the past participle formed with is often replaced by the infinitive.
The order at the end of such strings is subject to variation, but the second one in the last example is unusual.
Most German vocabulary is derived from the Germanic branch of the Indo-European language family. However, there is a significant amount of loanwords from other languages, in particular Latin, Greek, Italian, French, and most recently English. In the early 19th century, Joachim Heinrich Campe estimated that one fifth of the total German vocabulary was of French or Latin origin.
Latin words were already imported into the predecessor of the German language during the Roman Empire and underwent all the characteristic phonetic changes in German. Their origin is thus no longer recognizable for most speakers (e.g. , , , , from Latin , , , , ). Borrowing from Latin continued after the fall of the Roman Empire during Christianization, mediated by the church and monasteries. Another important influx of Latin words can be observed during Renaissance humanism. In a scholarly context, the borrowings from Latin have continued until today, in the last few decades often indirectly through borrowings from English. During the 15th to 17th centuries, the influence of Italian was great, leading to many Italian loanwords in the fields of architecture, finance, and music. The influence of the French language in the 17th to 19th centuries resulted in an even greater import of French words. The English influence was already present in the 19th century, but it did not become dominant until the second half of the 20th century.
Thus, Notker Labeo was able to translate Aristotelian treatises into pure (Old High) German in the decades after the year 1000. The tradition of loan translation was revitalized in the 18th century with linguists like Joachim Heinrich Campe, who introduced close to 300 words that are still used in modern German. Even today, there are movements that try to promote the (substitution) of foreign words that are deemed unnecessary with German alternatives. It is claimed that this would also help in spreading modern or scientific notions among the less educated and as well democratise public life.
As in English, there are many pairs of synonyms due to the enrichment of the Germanic vocabulary with loanwords from Latin and Latinized Greek. These words often have different connotations from their Germanic counterparts and are usually perceived as more scholarly.
The size of the vocabulary of German is difficult to estimate. The ("German Dictionary") initiated by Jacob and Wilhelm Grimm already contained over 330,000 headwords in its first edition. The modern German scientific vocabulary is estimated at nine million words and word groups (based on the analysis of 35 million sentences of a corpus in Leipzig, which as of July 2003 included 500 million words in total).
The Duden is the "de facto" official dictionary of the German language, first published by Konrad Duden in 1880. The Duden is updated regularly, with new editions appearing every four or five years. , it was in its 27th edition and in 12 volumes, each covering different aspects such as loanwords, etymology, pronunciation, synonyms, and so forth.The first of these volumes, (German Orthography), has long been the prescriptive source for the spelling of German. The "Duden" has become the bible of the German language, being the definitive set of rules regarding grammar, spelling and usage of German.
The ("Austrian Dictionary"), abbreviated , is the official dictionary of the German language in the Republic of Austria. It is edited by a group of linguists under the authority of the Austrian Federal Ministry of Education, Arts and Culture (). It is the Austrian counterpart to the German "Duden" and contains a number of terms unique to Austrian German or more frequently used or differently pronounced there. A considerable amount of this "Austrian" vocabulary is also common in Southern Germany, especially Bavaria, and some of it is used in Switzerland as well. Since the 39th edition in 2001 the orthography of the has been adjusted to the German spelling reform of 1996. The dictionary is also officially used in the Italian province of South Tyrol.
This is a selection of cognates in both English and German. Instead of the usual infinitive ending "-en", German verbs are indicated by a hyphen after their stems. Words that are written with capital letters in German are nouns.
German is written in the Latin alphabet. In addition to the 26 standard letters, German has three vowels with an umlaut mark, namely "ä", "ö" and "ü", as well as the eszett or (sharp s): "ß". In Switzerland and Liechtenstein, "ss" is used instead of "ß". Since "ß" can never occur at the beginning of a word, it has no traditional uppercase form.
Written texts in German are easily recognisable as such by distinguishing features such as umlauts and certain orthographical features – German is the only major language that capitalizes all nouns, a relic of a widespread practice in Northern Europe in the early modern era (including English for a while, in the 1700s) – and the frequent occurrence of long compounds. Because legibility and convenience set certain boundaries, compounds consisting of more than three or four nouns are almost exclusively found in humorous contexts. (In contrast, although English can also string nouns together, it usually separates the nouns with spaces. For example, "toilet bowl cleaner".)
Before the German orthography reform of 1996, "ß" replaced "ss" after long vowels and diphthongs and before consonants, word-, or partial-word endings. In reformed spelling, "ß" replaces "ss" only after long vowels and diphthongs.
Since there is no traditional capital form of "ß", it was replaced by "SS" when capitalization was required. For example, (tape measure) became in capitals. An exception was the use of ß in legal documents and forms when capitalizing names. To avoid confusion with similar names, lower case "ß" was maintained (thus " instead of "). Capital ß (ẞ) was ultimately adopted into German orthography in 2017, ending a long orthographic debate (thus " and ").
Umlaut vowels (ä, ö, ü) are commonly transcribed with ae, oe, and ue if the umlauts are not available on the keyboard or other medium used. In the same manner ß can be transcribed as ss. Some operating systems use key sequences to extend the set of possible characters to include, amongst other things, umlauts; in Microsoft Windows this is done using Alt codes. German readers understand these transcriptions (although they appear unusual), but they are avoided if the regular umlauts are available, because they are a makeshift and not proper spelling. (In Westphalia and Schleswig-Holstein, city and family names exist where the extra e has a vowel lengthening effect, e.g. "Raesfeld" , "Coesfeld" and "Itzehoe" , but this use of the letter e after a/o/u does not occur in the present-day spelling of words other than proper nouns.)
There is no general agreement on where letters with umlauts occur in the sorting sequence. Telephone directories treat them by replacing them with the base vowel followed by an e. Some dictionaries sort each umlauted vowel as a separate letter after the base vowel, but more commonly words with umlauts are ordered immediately after the same word without umlauts. As an example in a telephone book occurs after but before (because Ä is replaced by Ae). In a dictionary comes after , but in some dictionaries and all other words starting with "Ä" may occur after all words starting with "A". In some older dictionaries or indexes, initial "Sch" and "St" are treated as separate letters and are listed as separate entries after "S", but they are usually treated as S+C+H and S+T.
Written German also typically uses an alternative opening inverted comma (quotation mark) as in .
Until the early 20th century, German was mostly printed in blackletter typefaces (mostly in Fraktur, but also in Schwabacher) and written in corresponding handwriting (for example Kurrent and Sütterlin). These variants of the Latin alphabet are very different from the serif or sans-serif Antiqua typefaces used today, and the handwritten forms in particular are difficult for the untrained to read. The printed forms, however, were claimed by some to be more readable when used for Germanic languages. (Often, foreign names in a text were printed in an Antiqua typeface even though the rest of the text was in Fraktur.) The Nazis initially promoted Fraktur and Schwabacher because they were considered Aryan, but they abolished them in 1941, claiming that these letters were Jewish. It is also believed that the Nazi régime had banned this script as they realized that Fraktur would inhibit communication in the territories occupied during World War II.
The Fraktur script however remains present in everyday life in pub signs, beer brands and other forms of advertisement, where it is used to convey a certain rusticality and antiquity.
A proper use of the long s (), ſ, is essential for writing German text in Fraktur typefaces. Many Antiqua typefaces also include the long s. A specific set of rules applies for the use of long s in German text, but nowadays it is rarely used in Antiqua typesetting. Any lower case "s" at the beginning of a syllable would be a long s, as opposed to a terminal s or short s (the more common variation of the letter s), which marks the end of a syllable; for example, in differentiating between the words (guard-house) and (tube of polish/wax). One can easily decide which "s" to use by appropriate hyphenation, ( vs. ). The long s only appears in lower case.
The orthography reform of 1996 led to public controversy and considerable dispute. The states () of North Rhine-Westphalia and Bavaria refused to accept it. At one point, the dispute reached the highest court, which quickly dismissed it, claiming that the states had to decide for themselves and that only in schools could the reform be made the official rule – everybody else could continue writing as they had learned it. After 10 years, without any intervention by the federal parliament, a major revision was installed in 2006, just in time for the coming school year. In 2007, some traditional spellings were finally invalidated; however, in 2008, many of the old comma rules were again put in force.
The most noticeable change was probably in the use of the letter "ß", called ("Sharp S") or (pronounced "ess-tsett"). Traditionally, this letter was used in three situations:
Examples are , , and . Currently, only the first rule is in effect, making the correct spellings , , and . The word 'foot' has the letter "ß" because it contains a long vowel, even though that letter occurs at the end of a syllable. The logic of this change is that an 'ß' is a single letter whereas 'ss' are two letters, so the same distinction applies as (for example) between the words and .
In German, vowels (excluding diphthongs; see below) are either "short" or "long", as follows:
Short is realized as in stressed syllables (including secondary stress), but as in unstressed syllables. Note that stressed short can be spelled either with "e" or with "ä" (for instance, 'would have' and 'chain' rhyme). In general, the short vowels are open and the long vowels are close. The one exception is the open sound of long "Ä"; in some varieties of standard German, and have merged into , removing this anomaly. In that case, pairs like 'bears/berries' or 'spike (of wheat)/honour' become homophonous (see: Captain Bluebear).
In many varieties of standard German, an unstressed is not pronounced but vocalised to .
Whether any particular vowel letter represents the long or short phoneme is not completely predictable, although the following regularities exist:
Both of these rules have exceptions (e.g. "has" is short despite the first rule; "moon" is long despite the second rule). For an "i" that is neither in the combination "ie" (making it long) nor followed by a double consonant or cluster (making it short), there is no general rule. In some cases, there are regional differences. In central Germany (Hesse), the "o" in the proper name "Hoffmann" is pronounced long, whereas most other Germans would pronounce it short. The same applies to the "e" in the geographical name "Mecklenburg" for people in that region. The word "cities" is pronounced with a short vowel by some (Jan Hofer, ARD Television) and with a long vowel by others (Marietta Slomka, ZDF Television). Finally, a vowel followed by "ch" can be short ( "compartment", "kitchen") or long ( "search", "books") almost at random. Thus, is homographous between "puddle" and "manner of laughing" (colloquial) or "laugh!" (imperative).
German vowels can form the following digraphs (in writing) and diphthongs (in pronunciation); note that the pronunciation of some of them (ei, äu, eu) is very different from what one would expect when considering the component letters:
Additionally, the digraph "ie" generally represents the phoneme , which is not a diphthong. In many varieties, an at the end of a syllable is vocalised. However, a sequence of a vowel followed by such a vocalised is not a phonemic diphthong: "bear", "he", "we", "gate", "short", "words".
In most varieties of standard German, syllables that begin with a vowel are preceded by a glottal stop .
With approximately 26 phonemes, the German consonant system exhibits an average number of consonants in comparison with other languages. One of the more noteworthy ones is the unusual affricate . The consonant inventory of the standard language is shown below.
German does not have any dental fricatives (as English th). The th sound, which the English language still has, disappeared on the continent in German with the consonant shifts between the 8th and 10th centuries. It is sometimes possible to find parallels between English and German by replacing the English th with d in German: "Thank" → in German , "this" and "that" → and , "thou" (old 2nd person singular pronoun) → , "think" → , "thirsty" → and many other examples.
Likewise, the gh in Germanic English words, pronounced in several different ways in modern English (as an f or not at all), can often be linked to German ch: "to laugh" → , "through" → , "high" → , "naught" → , "light" → or , "sight" → , "daughter" → , "neighbour" → .
The German language is used in German literature and can be traced back to the Middle Ages, with the most notable authors of the period being Walther von der Vogelweide and Wolfram von Eschenbach.
The , whose author remains unknown, is also an important work of the epoch. The fairy tales collected and published by Jacob and Wilhelm Grimm in the 19th century became famous throughout the world.
Reformer and theologian Martin Luther, who was the first to translate the Bible into German, is widely credited for having set the basis for the modern "High German" language. Among the best-known poets and authors in German are Lessing, Goethe, Schiller, Kleist, Hoffmann, Brecht, Heine, and Kafka. Fourteen German-speaking people have won the Nobel Prize in literature: Theodor Mommsen, Rudolf Christoph Eucken, Paul von Heyse, Gerhart Hauptmann, Carl Spitteler, Thomas Mann, Nelly Sachs, Hermann Hesse, Heinrich Böll, Elias Canetti, Günter Grass, Elfriede Jelinek, Herta Müller and Peter Handke, making it the second most awarded linguistic region (together with French) after English.
English has taken many loanwords from German, often without any change of spelling (aside from frequently eliminating umlauts and not capitalizing nouns):
Several organisations promote the use and learning of the German language:
The government-backed (named after Johann Wolfgang von Goethe) aims to enhance the knowledge of German culture and language within Europe and the rest of the world. This is done by holding exhibitions and conferences with German-related themes, and providing training and guidance in the learning and use of the German language. For example, the teaches the German language qualification.
The Dortmund-based , founded in 1997, supports the German language and is the largest language association of citizens in the world. The VDS has more than thirty-five thousand members in over seventy countries. Its founder, statistics professor Dr. Walter Krämer, has remained chairperson of the association from its formation.
The German state broadcaster provides radio and television broadcasts in German and 30 other languages across the globe. Its German language services are spoken slowly and thus tailored for learners. also provides an website for teaching German.

</doc>
<doc id="11887" url="https://en.wikipedia.org/wiki?curid=11887" title="Greek language">
Greek language

Greek (, "Elliniká") is an independent branch of the Indo-European family of languages, native to Greece, Cyprus, Albania, other parts of the Eastern Mediterranean and the Black Sea. It has the longest documented history of any living Indo-European language, spanning at least 3,500 years of written records. Its writing system has been the Greek alphabet for the major part of its history; other systems, such as Linear B and the Cypriot syllabary, were used previously. The alphabet arose from the Phoenician script and was in turn the basis of the Latin, Cyrillic, Armenian, Coptic, Gothic, and many other writing systems.
The Greek language holds an important place in the history of the Western world and Christianity; the canon of ancient Greek literature includes works in the Western canon such as the epic poems "Iliad" and "Odyssey". Greek is also the language in which many of the foundational texts in science, especially astronomy, mathematics and logic and Western philosophy, such as the Platonic dialogues and the works of Aristotle, are composed; the New Testament of the Christian Bible was written in Koiné Greek. Together with the Latin texts and traditions of the Roman world, the study of the Greek texts and society of antiquity constitutes the discipline of Classics.
During antiquity, Greek was a widely spoken lingua franca in the Mediterranean world, West Asia and many places beyond. It would eventually become the official parlance of the Byzantine Empire and develop into Medieval Greek. In its modern form, Greek is the official language in two countries, Greece and Cyprus, a recognized minority language in seven other countries, and is one of the 24 official languages of the European Union. The language is spoken by at least 13.4 million people today in Greece, Cyprus, Italy, Albania, and Turkey and by the Greek diaspora.
Greek roots are often used to coin new words for other languages; Greek and Latin are the predominant sources of international scientific vocabulary.
Greek has been spoken in the Balkan peninsula since around the 3rd millennium BC, or possibly earlier. The earliest written evidence is a Linear B clay tablet found in Messenia that dates to between 1450 and 1350 BC, making Greek the world's oldest recorded living language. Among the Indo-European languages, its date of earliest written attestation is matched only by the now-extinct Anatolian languages.
The Greek language is conventionally divided into the following periods:
In the modern era, the Greek language entered a state of diglossia: the coexistence of vernacular and archaizing written forms of the language. What came to be known as the Greek language question was a polarization between two competing varieties of Modern Greek: Dimotiki, the vernacular form of Modern Greek proper, and Katharevousa, meaning 'purified', a compromise between Dimotiki and Ancient Greek, which was developed in the early 19th century, and was used for literary and official purposes in the newly formed Greek state. In 1976, Dimotiki was declared the official language of Greece, having incorporated features of Katharevousa and giving birth to Standard Modern Greek, which is used today for all official purposes and in education.
The historical unity and continuing identity between the various stages of the Greek language are often emphasized. Although Greek has undergone morphological and phonological changes comparable to those seen in other languages, never since classical antiquity has its cultural, literary, and orthographic tradition been interrupted to the extent that one can speak of a new language emerging. Greek speakers today still tend to regard literary works of ancient Greek as part of their own rather than a foreign language. It is also often stated that the historical changes have been relatively slight compared with some other languages. According to one estimation, "Homeric Greek is probably closer to Demotic than 12-century Middle English is to modern spoken English".
Greek is spoken today by at least 13 million people, principally in Greece and Cyprus along with a sizable Greek-speaking minority in Albania near the Greek-Albanian border. A significant percentage of Albania's population has some basic knowledge of the Greek language due in part to the Albanian wave of immigration to Greece in the 1980s and '90s. Prior to the Greco-Turkish War and the resulting population exchange in 1923 a very large population of Greek-speakers also existed in Turkey, though very few remain today. A small Greek-speaking community is also found in Bulgaria near the Greek-Bulgarian border. Greek is also spoken worldwide by the sizable Greek diaspora which as notable communities in the United States, Australia, Canada, South Africa, Chile, Brazil, Argentina, Russia, Ukraine, the United Kingdom, and throughout the European Union, especially in Germany.
Historically, significant Greek-speaking communities and regions were found throughout the Eastern Mediterranean, in what are today Southern Italy, Turkey, Cyprus, Syria, Lebanon, Israel, Egypt, and Libya; in the area of the Black Sea, in what are today Turkey, Bulgaria, Romania, Ukraine, Russia, Georgia, Armenia, and Azerbaijan; and, to a lesser extent, in the Western Mediterranean in and around colonies such as Massalia, Monoikos, and Mainake.
Greek, in its modern form, is the official language of Greece, where it is spoken by almost the entire population. It is also the official language of Cyprus (nominally alongside Turkish). Because of the membership of Greece and Cyprus in the European Union, Greek is one of the organization's 24 official languages. Furthermore, Greek is officially recognized as official in Dropull and Himara (Albania), and as a minority language all over Albania, as well as in parts of Italy, Armenia, Romania, and Ukraine as a regional or minority language in the framework of the European Charter for Regional or Minority Languages. Greeks are also a recognized ethnic minority in Hungary.
The phonology, morphology, syntax, and vocabulary of the language show both conservative and innovative tendencies across the entire attestation of the language from the ancient to the modern period. The division into conventional periods is, as with all such periodizations, relatively arbitrary, especially because at all periods, Ancient Greek has enjoyed high prestige, and the literate borrowed heavily from it.
Across its history, the syllabic structure of Greek has varied little: Greek shows a mixed syllable structure, permitting complex syllabic onsets but very restricted codas. It has only oral vowels and a fairly stable set of consonantal contrasts. The main phonological changes occurred during the Hellenistic and Roman period (see Koine Greek phonology for details):
In all its stages, the morphology of Greek shows an extensive set of productive derivational affixes, a limited but productive system of compounding and a rich inflectional system. Although its morphological categories have been fairly stable over time, morphological changes are present throughout, particularly in the nominal and verbal systems. The major change in the nominal morphology since the classical stage was the disuse of the dative case (its functions being largely taken over by the genitive). The verbal system has lost the infinitive, the synthetically-formed future, and perfect tenses and the optative mood. Many have been replaced by periphrastic (analytical) forms.
Pronouns show distinctions in person (1st, 2nd, and 3rd), number (singular, dual, and plural in the ancient language; singular and plural alone in later stages), and gender (masculine, feminine, and neuter), and decline for case (from six cases in the earliest forms attested to four in the modern language). Nouns, articles, and adjectives show all the distinctions except for a person. Both attributive and predicative adjectives agree with the noun.
The inflectional categories of the Greek verb have likewise remained largely the same over the course of the language's history but with significant changes in the number of distinctions within each category and their morphological expression. Greek verbs have synthetic inflectional forms for:
Many aspects of the syntax of Greek have remained constant: verbs agree with their subject only, the use of the surviving cases is largely intact (nominative for subjects and predicates, accusative for objects of most verbs and many prepositions, genitive for possessors), articles precede nouns, adpositions are largely prepositional, relative clauses follow the noun they modify and relative pronouns are clause-initial. However, the morphological changes also have their counterparts in the syntax, and there are also significant differences between the syntax of the ancient and that of the modern form of the language. Ancient Greek made great use of participial constructions and of constructions involving the infinitive, and the modern variety lacks the infinitive entirely (instead of having a raft of new periphrastic constructions) and uses participles more restrictively. The loss of the dative led to a rise of prepositional indirect objects (and the use of the genitive to directly mark these as well). Ancient Greek tended to be verb-final, but neutral word order in the modern language is VSO or SVO.
Modern Greek inherits most of its vocabulary from Ancient Greek, which in turn is an Indo-European language, but also includes a number of borrowings from the languages of the populations that inhabited Greece before the arrival of Proto-Greeks, some documented in Mycenaean texts; they include a large number of Greek toponyms. The form and meaning of many words have evolved. Loanwords (words of foreign origin) have entered the language, mainly from Latin, Venetian, and Turkish. During the older periods of Greek, loanwords into Greek acquired Greek inflections, thus leaving only a foreign root word. Modern borrowings (from the 20th century on), especially from French and English, are typically not inflected; other modern borrowings are derived from South Slavic (Macedonian/Bulgarian) and Eastern Romance languages (Aromanian and Megleno-Romanian).
Greek words have been widely borrowed into other languages, including English: "mathematics", "physics", "astronomy", "democracy", "philosophy", "athletics, theatre, rhetoric", "baptism", "evangelist", etc. Moreover, Greek words and word elements continue to be productive as a basis for coinages: "anthropology", "photography", "telephony", "isomer", "biomechanics", "cinematography", etc. and form, with Latin words, the foundation of international scientific and technical vocabulary like all words ending with "–logy" ("discourse"). There are many English words of Greek origin.
Greek is an independent branch of the Indo-European language family. The ancient language most closely related to it may be ancient Macedonian, which many scholars suggest may have been a dialect of Greek itself, but it is so poorly attested that it is difficult to conclude anything about it. Independently of the Macedonian question, some scholars have grouped Greek into Graeco-Phrygian, as Greek and the extinct Phrygian share features that are not found in other Indo-European languages. Among living languages, some Indo-Europeanists suggest that Greek may be most closely related to Armenian (see Graeco-Armenian) or the Indo-Iranian languages (see Graeco-Aryan), but little definitive evidence has been found for grouping the living branches of the family. In addition, Albanian has also been considered somewhat related to Greek and Armenian by some linguists. If proven and recognized, the three languages would form a new Balkan sub-branch with other dead European languages.
Linear B, attested as early as the late 15th century BC, was the first script used to write Greek. It is basically a syllabary, which was finally deciphered by Michael Ventris and John Chadwick in the 1950s (its precursor, Linear A, has not been deciphered and most likely encodes a non-Greek language). The language of the Linear B texts, Mycenaean Greek, is the earliest known form of Greek.
Another similar system used to write the Greek language was the Cypriot syllabary (also a descendant of Linear A via the intermediate Cypro-Minoan syllabary), which is closely related to Linear B but uses somewhat different syllabic conventions to represent phoneme sequences. The Cypriot syllabary is attested in Cyprus from the 11th century BC until its gradual abandonment in the late Classical period, in favor of the standard Greek alphabet.
Greek has been written in the Greek alphabet since approximately the 9th century BC. It was created by modifying the Phoenician alphabet, with the innovation of adopting certain letters to represent the vowels. The variant of the alphabet in use today is essentially the late Ionic variant, introduced for writing classical Attic in 403 BC. In classical Greek, as in classical Latin, only upper-case letters existed. The lower-case Greek letters were developed much later by medieval scribes to permit a faster, more convenient cursive writing style with the use of ink and quill.
The Greek alphabet consists of 24 letters, each with an uppercase (majuscule) and lowercase (minuscule) form. The letter sigma has an additional lowercase form (ς) used in the final position:
In addition to the letters, the Greek alphabet features a number of diacritical signs: three different accent marks (acute, grave, and circumflex), originally denoting different shapes of pitch accent on the stressed vowel; the so-called breathing marks (rough and smooth breathing), originally used to signal presence or absence of word-initial /h/; and the diaeresis, used to mark the full syllabic value of a vowel that would otherwise be read as part of a diphthong. These marks were introduced during the course of the Hellenistic period. Actual usage of the grave in handwriting saw a rapid decline in favor of uniform usage of the acute during the late 20th century, and it has only been retained in typography.
After the writing reform of 1982, most diacritics are no longer used. Since then, Greek has been written mostly in the simplified monotonic orthography (or monotonic system), which employs only the acute accent and the diaeresis. The traditional system, now called the polytonic orthography (or polytonic system), is still used internationally for the writing of Ancient Greek.
In Greek, the question mark is written as the English semicolon, while the functions of the colon and semicolon are performed by a raised point (•), known as the "ano teleia" (). In Greek the comma also functions as a silent letter in a handful of Greek words, principally distinguishing ("ó,ti", 'whatever') from ("óti", 'that').
Ancient Greek texts often used "scriptio continua" ('continuous writing'), which means that ancient authors and scribes would write word after word with no spaces or punctuation between words to differentiate or mark boundaries. Boustrophedon, or bi-directional text, was also used in Ancient Greek.
Greek has occasionally been written in the Latin script, especially in areas under Venetian rule or by Greek Catholics. The term / applies when the Latin script is used to write Greek in the cultural ambit of Catholicism (because / is an older Greek term for West-European dating to when most of (Roman Catholic Christian) West Europe was under the control of the Frankish Empire). / (meaning 'Catholic Chiot') alludes to the significant presence of Catholic missionaries based on the island of Chios. Additionally, the term Greeklish is often used when the Greek language is written in a Latin script in online communications.
The Latin script is nowadays used by the Greek-speaking communities of Southern Italy.
The Yevanic dialect was written by Romaniote and Constantinopolitan Karaite Jews using the Hebrew Alphabet.
Some Greek Muslims from Crete wrote their Cretan Greek in the Arabic alphabet.
The same happened among Epirote Muslims in Ioannina.
This usage is sometimes called aljamiado as when Romance languages are written in the Arabic alphabet.

</doc>
<doc id="11888" url="https://en.wikipedia.org/wiki?curid=11888" title="Golem">
Golem

In Jewish folklore, a golem ( ; ) is an animated anthropomorphic being that is created entirely from inanimate matter (usually clay or mud). The word was used to mean an amorphous, unformed material in Psalms and medieval writing.
The most famous golem narrative involves Judah Loew ben Bezalel, the late-16th-century rabbi of Prague. Many tales differ on how the golem was brought to life and afterward controlled. According to "Moment Magazine", "the golem is a highly mutable metaphor with seemingly limitless symbolism. It can be a victim or villain, Jew or non-Jew, man or woman—or sometimes both. Over the centuries it has been used to connote war, community, isolation, hope, and despair."
The word "golem" occurs once in the Bible in , which uses the word ("golmi"; my golem), that means "my light form", "raw" material, connoting the unfinished human being before God's eyes. The Mishnah uses the term for an uncultivated person: "Seven characteristics are in an uncultivated person, and seven in a learned one," (שבעה דברים בגולם) (Pirkei Avot 5:10 in the Hebrew text; English translations vary). In Modern Hebrew, "golem" is used to mean "dumb" or "helpless". Similarly, it is often used today as a metaphor for a mindless lunk or entity who serves a man under controlled conditions but is hostile to him under others. "Golem" passed into Yiddish as "goylem" to mean someone who is lethargic or beneath a stupor.
The oldest stories of golems date to early Judaism. In the Talmud (Tractate Sanhedrin 38b), Adam was initially created as a golem (גולם) when his dust was "kneaded into a shapeless husk." Like Adam, all golems are created from mud by those close to divinity, but no anthropogenic golem is fully human. Early on, the main disability of the golem was its inability to speak. Sanhedrin 65b describes Rava creating a man ("gavra"). He sent the man to Rav Zeira. Rav Zeira spoke to him, but he did not answer. Rav Zeira said, "You were created by the sages; return to your dust".
During the Middle Ages, passages from the "Sefer Yetzirah" ("Book of Creation") were studied as a means to create and animate a golem, although there is little in the writings of Jewish mysticism that supports this belief. It was believed that golems could be activated by an ecstatic experience induced by the ritualistic use of various letters of the Hebrew Alphabet forming a "shem" (any one of the Names of God), wherein the "shem" was written on a piece of paper and inserted in the mouth or in the forehead of the golem.
A golem is inscribed with Hebrew words in some tales (for example, some versions of Chełm and Prague, as well as in Polish tales and versions of Brothers Grimm), such as the word "emet" (אמת, "truth" in Hebrew) written on its forehead. The golem could then be deactivated by removing the aleph (א) in "emet", thus changing the inscription from "truth" to "death" ("met" מת, meaning "dead"). 
Rabbi Jacob Ben Shalom arrived at Barcelona from Germany in 1325 and remarked that the law of destruction is the reversal of the law of creation.
One source credits 11th century Solomon ibn Gabirol with creating a golem, possibly female, for household chores.
Joseph Delmedigo informs us in 1625 that "many legends of this sort are current, particularly in Germany."
The earliest known written account of how to create a golem can be found in "Sodei Razayya" by Eleazar ben Judah of Worms of the late 12th and early 13th century.
The oldest description of the creation of a golem by a historical figure is included in a tradition connected to Rabbi Eliyahu of Chełm (1550–1583).
A Polish Kabbalist, writing in about 1630–1650, reported the creation of a golem by Rabbi Eliyahu thus: "And I have heard, in a certain and explicit way, from several respectable persons that one man [living] close to our time, whose name is R. Eliyahu, the master of the name, who made a creature out of matter [Heb. "Golem"] and form [Heb. "tzurah"] and it performed hard work for him, for a long period, and the name of "emet" was hanging upon his neck until he finally removed it for a certain reason, the name from his neck and it turned to dust." A similar account was reported by a Christian author, Christoph Arnold, in 1674.
Rabbi Jacob Emden (d. 1776) elaborated on the story in a book published in 1748: "As an aside, I'll mention here what I heard from my father's holy mouth regarding the Golem created by his ancestor, the Gaon R. Eliyahu Ba'al Shem of blessed memory. When the Gaon saw that the Golem was growing larger and larger, he feared that the Golem would destroy the universe. He then removed the Holy Name that was embedded on his forehead, thus causing him to disintegrate and return to dust. Nonetheless, while he was engaged in extracting the Holy Name from him, the Golem injured him, scarring him on the face."
According to the Polish Kabbalist, "the legend was known to several persons, thus allowing us to speculate that the legend had indeed circulated for some time before it was committed to writing and, consequently, we may assume that its origins are to be traced to the generation immediately following the death of R. Eliyahu, if not earlier."
The most famous golem narrative involves Judah Loew ben Bezalel, the late 16th century rabbi of Prague, also known as the Maharal, who reportedly "created a golem out of clay from the banks of the Vltava River and brought it to life through rituals and Hebrew incantations to defend the Prague ghetto from anti-Semitic attacks" and pogroms. Depending on the version of the legend, the Jews in Prague were to be either expelled or killed under the rule of Rudolf II, the Holy Roman Emperor. The Golem was called Josef and was known as Yossele. It was said that he could make himself invisible and summon spirits from the dead. Rabbi Loew deactivated the Golem on Friday evenings by removing the "shem" before the Sabbath (Saturday) began, so as to let it rest on Sabbath. One Friday evening Rabbi Loew forgot to remove the "shem", and feared that the Golem would desecrate the Sabbath. A different story tells of a golem that fell in love, and when rejected, became the violent monster seen in most accounts. Some versions have the golem eventually going on a murderous rampage.
The rabbi then managed to pull the "shem" from his mouth and immobilize him in front of the synagogue, whereupon the golem fell in pieces. The Golem's body was stored in the attic "genizah" of the Old New Synagogue, where it would be restored to life again if needed. According to legend, the body of Rabbi Loew's Golem still lies in the synagogue's attic. When the attic was renovated in 1883, no evidence of the Golem was found. Some versions of the tale state that the Golem was stolen from the "genizah" and entombed in a graveyard in Prague's Žižkov district, where the Žižkov Television Tower now stands. A recent legend tells of a Nazi agent ascending to the synagogue attic during World War II and trying to stab the Golem, but he died instead. The attic is not open to the general public.
Some Orthodox Jews believe that the Maharal did actually create a golem. The evidence for this belief has been analyzed from an Orthodox Jewish perspective by Shnayer Z. Leiman.
The general view of historians and critics is that the story of the Golem of Prague was a German literary invention of the early 19th century. According to John Neubauer, the first writers on the Prague Golem were:
However, there are in fact a couple of slightly earlier examples, in 1834 and 1836.
All of these early accounts of the Golem of Prague are in German by Jewish writers. It has been suggested that they emerged as part of a Jewish folklore movement parallel with the contemporary German folklore movement.
The origins of the story have been obscured by attempts to exaggerate its age and to pretend that it dates from the time of the Maharal. It has been said that Rabbi Yudel Rosenberg (1859–1935) of Tarłów (before moving to Canada where he became one of its most prominent rabbis) originated the idea that the narrative dates from the time of the Maharal. Rosenberg published "Nifl'os Maharal" ("Wonders of Maharal") (Piotrków, 1909) which purported to be an eyewitness account by the Maharal's son-in-law, who had helped to create the Golem. Rosenberg claimed that the book was based upon a manuscript that he found in the main library in Metz. "Wonders of Maharal" "is generally recognized in academic circles to be a literary hoax". Gershom Sholem observed that the manuscript "contains not ancient legends but modern fiction". Rosenberg's claim was further disseminated in Chayim Bloch's (1881–1973) "The Golem: Legends of the Ghetto of Prague" (English edition 1925).
The "Jewish Encyclopedia" of 1906 cites the historical work "Zemach David" by David Gans, a disciple of the Maharal, published in 1592. In it, Gans writes of an audience between the Maharal and Rudolph II: "Our lord the emperor ... Rudolph ... sent for and called upon our master Rabbi Low ben Bezalel and received him with a welcome and merry expression, and spoke to him face to face, as one would to a friend. The nature and quality of their words are mysterious, sealed and hidden." But it has been said of this passage, "Even when [the Maharal is] eulogized, whether in David Gans' "Zemach David" or on his epitaph …, not a word is said about the creation of a golem. No Hebrew work published in the 16th, 17th, and 18th centuries (even in Prague) is aware that the Maharal created a golem." Furthermore, the Maharal himself did not refer to the Golem in his writings. Rabbi Yedidiah Tiah Weil (1721–1805), a Prague resident, who described the creation of golems, including those created by Rabbis Avigdor Kara of Prague (died 1439) and Eliyahu of Chelm, did not mention the Maharal, and Rabbi Meir Perils' biography of the Maharal published in 1718 does not mention a golem.
There is a similar tradition relating to the Vilna Gaon or "the saintly genius from Vilnius" (1720–1797). Rabbi Chaim Volozhin (Lithuania 1749–1821) reported in an introduction to "Sifra de Tzeniuta" that he once presented to his teacher, the Vilna Gaon, ten different versions of a certain passage in the "Sefer Yetzira" and asked the Gaon to determine the correct text. The Gaon immediately identified one version as the accurate rendition of the passage. The amazed student then commented to his teacher that, with such clarity, he should easily be able to create a live human. The Gaon affirmed Rabbi Chaim's assertion and said that he once began to create a person when he was a child, under the age of 13, but during the process, he received a sign from Heaven ordering him to desist because of his tender age.
The existence of a golem is sometimes a mixed blessing. Golems are not intelligent, and if commanded to perform a task, they will perform the instructions literally. In many depictions, Golems are inherently perfectly obedient. In its earliest known modern form, the Golem of Chełm became enormous and uncooperative. In one version of this story, the rabbi had to resort to trickery to deactivate it, whereupon it crumbled upon its creator and crushed him. There is a similar hubris theme in "Frankenstein", "The Sorcerer's Apprentice", and some other stories in popular culture, such as "The Terminator". The theme also manifests itself in "R.U.R. (Rossum's Universal Robots)", Karel Čapek's 1921 play which coined the term robot; the play was written in Prague, and while Čapek denied that he modeled the robot after the Golem, there are many similarities in the plot.
The Golem is a popular figure in the Czech Republic. There are several restaurants and other businesses whose names make reference to the creature, a Czech strongman (René Richter) goes by the nickname "Golem", and a Czech monster truck outfit calls itself the "Golem Team."
Abraham Akkerman preceded his article on human automatism in the contemporary city with a short satirical poem on a pair of golems turning human.
A Yiddish and Slavic folktale is the Clay Boy, which combines elements of the Golem and "The Gingerbread Man", in which a lonely couple makes a child out of clay, with disastrous or comical consequences. In one common Russian version, an older couple, whose children have left home, makes a boy out of clay and dries him by their hearth. The Clay Boy comes to life; at first, the couple is delighted and treats him like a real child, but the Clay Boy does not stop growing and eats all their food, then all their livestock, and then the Clay Boy eats his parents. The Clay Boy rampages through the village until he is smashed by a quick-thinking goat.

</doc>
