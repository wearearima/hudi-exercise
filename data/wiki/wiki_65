<doc id="8212" url="https://en.wikipedia.org/wiki?curid=8212" title="Disc golf">
Disc golf

Disc golf is a flying disc sport in which players throw a disc at a target; it is played using rules similar to golf. It is usually played on a course with 9 or 18 holes. Players complete a hole by throwing a disc from a tee area toward a target, throwing again from where the previous throw landed, until the target is reached. Usually, the number of throws a player uses to reach each target is tallied (often in relation to par), and players seek to complete each hole in the lowest number of total throws.
The game is played in about 40 countries and, as of 2019, there are active members of the PDGA worldwide.
Disc golf was first invented in the early 1900s. The first game was held in Bladworth, Saskatchewan, Canada in 1927. Ronald Franklin Gibson and a group of his Bladworth Elementary School buddies played a game of throwing tin lids into 4-foot wide circles drawn into sandy patches on their school grounds. They called the game Tin Lid Golf and played on a fairly regular basis. However, after they grew older and went their separate ways, the game came to an end. It was not until the 1970s that modern disc golf would be introduced to Canadians at the Canadian Open Frisbee Championships in Toronto and Vancouver, BC.
Modern disc golf started in the early 1960s, but there is debate over who came up with the idea first. The consensus is that multiple groups of people played independently throughout the 1960s. Students at Rice University in Houston, Texas, for example, held tournaments with trees as targets as early as 1964, and in the early 1960s, players in Pendleton King Park in Augusta, Georgia would toss Frisbees into 50-gallon barrel trash cans designated as targets. In 1968 Frisbee Golf was also played in Alameda Park in Santa Barbara, California by teenagers in the Anacapa and Sola street areas. Gazebos, water fountains, lamp posts, and trees were all part of the course. This took place for several years and an Alameda Park collectors edition disc still exists, though rare, as few were made. Clifford Towne from this group went on to hold a National Time Aloft record.
Most disc golf courses have 9 or 18 holes, and exceptions most often have holes in multiples of three. Courses with 6, 12, 21, 24 or 27 holes are not uncommon. The PDGA recommends that courses average per hole, with holes no shorter than . The longest holes in the world measure more than long. Course designers use trees, bushes, elevation changes, water hazards, and distance variation, along with out-of-bounds zones and mandatory flight paths to make each hole challenging and unique. Many courses include multiple tee positions or multiple target positions to cater to players of different ability levels.
Most disc golf courses are built in more natural and less manicured environments than golf and require minimal maintenance. Professional course designers consider safety a critical factor in course design, and are careful to minimize the danger of being hit by a flying disc while providing designs that create strategy in play and variety in shots for enjoyment. Holes are designed to require a range of different throws to challenge players with different strengths or particular skills. Many courses are central organizing points for local disc golf clubs, and some include shops selling disc golf equipment. More than 80% of the courses listed on Disc Golf Course Review are listed as public and free to play.
Three countries account for 85% of all disc golf courses worldwide: the United States (75%), Finland (7%) and Canada (3%). Other notable countries include Sweden, and Estonia, which has the highest density of disc golf courses per km of dry land of any country and the second-highest number of courses per capita, between Iceland and Finland, which have 150 and 111 courses per million inhabitants, respectively. Outside North American and European strongholds, Japan, Australia, New Zealand, and South Korea have the most courses. There are disc golf courses on every continent, including 24 in Latin America, 8 in Africa, and one in Antarctica. The Åland Islands have been defined as the world's largest single disc golf park, with one course in each of the 16 municipalities of Åland.
A disc golf tee (commonly referred to as a tee box or the box) is the starting position of a hole. The PDGA recommends that the tee box be no smaller than 1.2 meters wide by 3 meters long. The tee box is usually a pad of concrete, asphalt, rubber, gravel, or artificial turf. Some courses have natural turf with only the front of the tee position marked or no tee boxes at all and players begin from a general location based on the course layout.
Established courses have tee signs near each tee position. Signs may depict a simple map of the hole including the tee, target, expected disc flight, out-of-bounds areas, water hazards, trees, and mandatory paths. Signs typically include the distance to the hole, and par. Some courses include a unique name for the hole and may have sponsor logos. They are often supplemented with a larger sign near the course entrance which has a map of the entire course.
Although early courses were played using trees, fence posts, or park equipment as the target, standard disc golf baskets are by far the most common type of target on modern courses. Some courses feature tone targets that are designed to make a distinctive sound when hit with a disc. Disc golf baskets are constructed with a central pole holding a basket under an assembly of hanging chains. When a disc hits the chains, it is often, but not always, deflected into the basket. Per PDGA rules, in order to complete a hole with a basket target, the disc must "enter the target above the top of the tray and below the bottom of the chain support, and come to rest supported by the target. There are many different brands of baskets made by numerous manufacturers.
The sport of disc golf is set up similar to a game of golf. A "round" is played on a disc golf course consisting of a number of "holes", usually 9 or 18. Each hole includes a tee position for starting play and a disc golf target some distance away, often with obstacles such as trees, hills or bodies of water in between. Players begin by throwing a disc from the tee, without crossing over the front of the tee prior to releasing the disc when throwing. This could lead to a fault similar to the foot fault in bowling. Players then navigate the hole by picking up the disc where it lands and throwing again until they reach the target. The object of the game is to get through the course with the lowest number of total throws. Play is usually in groups of five or fewer, with each player taking turn at the tee box, then progressing with the player furthest from the hole throwing first, while the other players stand aside. 
Each course is unique, so each course requires a different combination of throws to complete, with the best players aiming to shape the flight of the disc to account for distance, terrain, obstacles and weather. In order to facilitate making different shots, players carry a variety of discs with different flight characteristics, choosing an appropriate disc for each throw. Some players also carry a mini marker disc, used to accurately mark the throwing position before each throw.  Use of mini marker discs is particularly prevalent in formal competitive play.
Many courses include out-of-bounds areas, commonly called "OB zones" or just "OB". If the disc lands in these areas, the player is usually required to add a penalty throw onto his or her score and continue play from near where the disc entered the out-of-bounds zone. Some courses include out-of-bounds areas with special rules requiring the player the resume play from a specified area called a drop zone, or requiring the player to restart the hole from the tee. Some courses also include Mandatories (also called "Mandos") which require the path of the disc to be above, below or to one side of a specific line indicated by a sign.
By tradition, players throw from the tee box in the order of their score on the previous hole, with the lowest scorer throwing first. Most players also follow a loose code of courtesy while playing, which includes norms such as standing out of the sight line of the throwing player and avoiding making distracting noises. Because a thrown disc could injure someone, the Professional Disc Golf Association recommends that players "Never throw into a blind area or when spectators, pedestrians or facility users are within range."
Formal competitive play is governed by the PDGA Official Rules of Disc Golf and the PDGA Competition Manual for Disc Golf events.
Disc golf discs are smaller than Ultimate flying discs or general-purpose recreational frisbees. They typically measure in diameter and weigh . All PDGA-approved discs measure in diameter and weigh no more than . Discs used for disc golf are designed and shaped for control, speed, and accuracy, while general-purpose flying discs, such as those used for playing guts or ultimate, have a more traditional shape, similar to a catch disc. There is a wide variety of discs used in disc golf and they are generally divided into three categories: drivers, mid-range discs, and putters.
Drivers are recognized by their sharp, beveled edge and have most of their mass concentrated on the outer rim of the disc rather than distributed equally throughout. They are optimized for aerodynamics and designed to travel maximum distances at high speeds. They are typically thrown by experienced players during tee-off and other long distance fairway throws.
Some disc brands further sub-divide their drivers into different categories. For example, Innova has "Distance Drivers" and "Fairway Drivers", with a fairway driver being somewhere between a distance driver and a mid-range disc. Discraft has three categories of drivers: "Long Drivers", "Extra Long Drivers", and "Maximum Distance Drivers". Another type of driver, used less frequently, is a roller. As the name indicates, it has an edge designed to roll rather than fly. (Although any disc can be used for a roller, some behave quite differently than others.)
Because the physics of a disc require "snap" or "flick", which means putting spin on the disc, new players generally find that throwing a distance driver accurately can be somewhat difficult and will require experience with golf disc response. This is why it is better for players to begin with fairway drivers, long drivers, or even mid-ranges, and incorporate maximum distance drivers as their strength and disc control increases. Most players that are starting off will be most likely throwing lighter discs.
The world record distance for a golf disc is , thrown by Simon Lizotte on October 25, 2014.
Mid-range discs feature a dull, beveled edge and a moderate rim width. They offer more control than drivers, but they have a smaller range. Mid-range discs are typically used as approach discs. Beginner players will often use mid-ranges instead of drivers at tee-off, as they require less strength and technique to fly straight than higher speed drivers.
Putters are similar to the discs used in simple games of catch, such as the Wham-O brand Frisbee. They are designed to fly straight, predictably, and very slowly compared to mid-range discs and drivers. They are typically used for tight, controlled shots that are close to the basket, although some players use them for short drives where trees or other obstacles come into play. Usually a pro carries 1–7 putters depending on their flight characteristics. As a beginner it is suggested that you only use a putter or mid-range while building fundamentals such as proper follow-through, disc throw positioning, and hyzer/anhyzer technique. Additionally, higher speed discs won't fly properly without a fast enough release snap, so a putter or mid-range with lower snap requirements is more forgiving and will behave in a more regular way.
Stability is the measurement of a disc's tendency to bank laterally during its flight. A disc that is over-stable will tend to track left (for a right handed, backhand throw), whereas a disc that is under-stable will tend to track right (also for a right handed, backhand throw). The stability rating of the discs differs depending on the manufacturer of the disc. Innova Discs rate stability as "turn" and "fade". "Turn" references how the disc will fly at high speed during the beginning and middle of its flight, and is rated on a scale of +1 to −5, where +1 is the most overstable and −5 is the most understable. "Fade" references how the disc will fly at lower speeds towards the end of its flight, and is rated on a scale of 0 to 5, where 0 has the least fade, and 5 has the most fade. For example, a disc with a turn of −5 and fade of +1 will fly to the right for (right handed, backhand throw) the majority of its flight then curl back minimally left at the end. A disc with a turn of −1 and a fade of +3 will turn slightly right during the middle of its flight and turn hard left as it slows down. These ratings can be found on the discs themselves or from the manufacturer's web site. Discraft prints the stability rating on all discs and also provides this information on their web site. The stability ranges from 3 to −2 for Discraft discs; however Discraft's ratings are more of a combination of turn and fade with the predominance being fade.
Spin (rotation) has little influence on lift and drag forces but impacts a disc's stability during flight. Imagine a spinning top. A gentle nudge will knock it off its axis of rotation for a second, but it will not topple over because spin adds gyroscopic stability. In the same way, a flying disc resists rolling (flipping over) because spin adds gyroscopic stability. A flying disc will maintain its spin rate even as it loses velocity. Toward the end of a disc's flight, when the spin and velocity lines cross, a flying disc will predictably begin to fade. The degree to which a disc will fade depends on its pitch angle and design.
There are a variety of different discs, each with a specific plastic made with them. Plastics such as DX, J-Pro, Pro-D, X-Line, D-line, retro, and R-Pro from Innova (https://www.innovadiscs.com/), Latitude 64°, Discmania, and Discraft are some of the less durable plastics, but good for beginners due to their lower prices, compared to the higher end plastics. Plastics such as Champion, Titanium, FLX, GStar, Gold Line, Tournament Plastic, Fuzion and Star plastics, which are the best offered from the same companies, offering the best quality, durability and flight compared to the other types available. There are also plastics that provide additional functionality, specifically glow in the dark plastic and plastic that allows the disc to float in water. Most companies also offer a line of plastic that is much lighter than the maximum throwing weight (normally filled with air bubbles) which is conducive to beginners or players with less arm speed. Players might prefer bright colored discs to contrast most green flora and recover their disc easier.
While there are many different grips and styles to throwing the disc, there are two basic throwing techniques: backhand and forehand (or sidearm). These techniques vary in effectiveness under different circumstances. Their understanding and mastery can greatly improve a player's game, and offer diverse options in maneuvering the disc to the basket with greater efficacy. Many players use what is referred to as a run-up during their drive. This is practiced to build more forward disc momentum and distance. Throwing styles vary from player to player, and there is no standard throwing style.
All discs when thrown will naturally fall to a certain direction determined by the rotation direction of the disc when released, this direction is termed "Hyzer", the natural fall of the disc, or "Anhyzer", making the disc fall against its natural flight pattern. For a right-handed backhand throw (RHBH), the disc will naturally fall to the left. For a right-handed forehand throw (RHFH), the disc will naturally fall to the right. For a left-handed, backhand throw (LHBH), the disc will naturally fall to the right. For a left-handed, forehand throw (LHFH), the disc will naturally fall to the left.
To perform this throw, the disc is rapidly drawn from across the front of the body, and released towards a forward aimpoint. Due to the potential snap available with this technique, one can expect greater distance than with a forehand throw. It is important to initiate momentum from the feet and allow it to travel up the body, hips and shoulders, culminating in the transfer of energy to the disc.
The forehand (sidearm) throw is performed by drawing the disc from behind and partially across the front of the body: similar to a sidearm throw in baseball. The term sidearm actually predates the term forehand, which is seemingly in use today as a simpler means to communicate the technique, equating to a tennis forehand.
The following examples of throws may be used to better deliver a disc where the former common two throws would be impeded by obstacles such as bushes, trees, boulders, or artificial structures.
Common alternative styles
Other alternative styles
Stroke play is the most common scoring method used in the sport but there are many other forms. These include match play, skins, speed golf and captain's choice, which in disc golf is referred to as "doubles" (not to be confused with partner or team play).
Regardless of which form of play the participants choose, the main objectives of disc golf are conceptually the same as traditional golf in the sense that players follow the same scorekeeping technique.
Scoring terms for a single hole:
Doubles play is a unique style of play that many local courses offer on a weekly basis. In this format, teams of two golfers are determined. Sometimes this is done by random draw, and other times it is a pro-am format. On the course, it is a "best-disc" scramble, meaning both players throw their tee shot; and then decide which lie they would like to play. Both players then play from the same lie, again choosing which lie is preferable. The World Amateur Doubles Format includes best shot, alternate shot, best score (players play singles and take the best result from the hole) and worst shot (both players must sink the putt).
Tournaments are held nationwide and year long in the United States. Sanctioned Tournament play is communicated through the Professional Disc Golf Association Membership. The PDGA provides international, professional, and amateur disc golf tournaments as well as communicates event results, opinions and other information beneficial to the sport via electronic and printed media. In 1982 the PDGA hosted the first World Championship Tournament. Since then, the World Championships have been held in 17 different American states, as well as Toronto, Ontario.
Disc golf tournaments are popular around the world. As with traditional golf, there are many championship tournaments. One of the largest is the United States Disc Golf Championship.
Every year, the largest teams tournament in the world is held in Austin, Texas, by John Houck.
To prove the year-round sustainability of the sport, annual winter tournaments known as Ice Bowls are held at courses around the world. Using the motto "No Wimps, No Whiners", Ice Bowls collectively are designed to create sport awareness, and are considered charity events that typically benefit a food bank local to a given tournament location. The official Web site reports that the 2010 Ice Bowls raised over $250,000 and donated over 67,000 pounds of food in the 222 tournaments for the year. Ice Bowl HQ | Other charitable tournaments include the annual St. Jude Disc Golf Tournament which first started in 2017 and has raised over $100,000 for St. Jude Children's Research Hospital.
Disc golf is a rapidly growing sport worldwide, and is the 4th fastest growing sport in United States, behind mixed martial arts, roller derby, and parkour. DGCourseReview.com, which tracks courses worldwide along with opening dates, shows a rapid increase in installed permanent courses with an average of more than 400 new courses added each year between 2007 and 2017. The site lists 6800 courses worldwide (in May 2017).
Although most players play on a casual, amateur level, the professional disc golf scene is also growing rapidly, with the top professionals playing full-time and earning their livings through tournament winnings and sponsorship from equipment manufacturers. Online viewership of major tournaments and events has increased rapidly, with coverage of several tournaments in 2016 achieving more than 75,000 views on YouTube. and a clip focused on a single throw by professional Philo Braithwaite achieving more than 1.2 million views.
While there are more male than female players, the Women's Disc Golf Association exists to encourage female players and arrange women's tournaments. A PDGA survey states that out of its 35,662 active members in 2016, 7.6% are female, or about 2,728.
Several companies have started programs and websites to help attract women to the sport. The PDGA Women's Committee is "Dedicated to Attract, Encourage, and Retain Female Participation in Organized Disc Golf Events". The PDGA Women's Committee set historical records on 12 May 2012 by running the Inaugural Women's Global Event that attracted 636 female players in 24 states and 4 countries. The Women's Global Event was expected to take place every two years from 2014, with hopes of increasing the number of participants.
There are also Disc golf companies such as Disc-Diva, that have started up with a primary, though not exclusive, focus on women in the sport, promoting accessories geared towards women and using catch phrases like "you wish you threw like a girl". Sassy Pants is another group that focuses on getting more involvement from women in the sport, advocating for sponsorship of women to enter tournaments.
Women's disc golf teams are involved in the National Collegiate Disc Golf Championship, and the Mississippi State Women's Team were the inaugural champions.
Inductees:

</doc>
<doc id="8214" url="https://en.wikipedia.org/wiki?curid=8214" title="Decimal">
Decimal

The decimal numeral system (also called base-ten positional numeral system, and occasionally called denary or decanary) is the standard system for denoting integer and non-integer numbers. It is the extension to non-integer numbers of the Hindu–Arabic numeral system. The way of denoting numbers in the decimal system is often referred to as "decimal notation".
A "decimal numeral", or just "decimal", or casually "decimal number", refers generally to the notation of a number in the decimal numeral system. Decimals may sometimes be identified by a decimal separator (usually "." or "," as in or ). Decimal may also refer specifically to the digits after the decimal separator, such as in " is the approximation of to "two decimals"".
The numbers that may be represented in the decimal system are the decimal fractions. That is, fractions of the form , where is an integer, and is a non-negative integer.
The decimal system has been extended to "infinite decimals" for representing any real number, by using an infinite sequence of digits after the decimal separator (see Decimal representation). In this context, the decimal numerals with a finite number of non–zero places after the decimal separator are sometimes called "terminating decimals". A repeating decimal is an infinite decimal that after some place, repeats indefinitely the same sequence of digits (e.g., ). An infinite decimal represents a rational number if and only if it is a repeating decimal or has a finite number of nonzero digits.
Many numeral systems of ancient civilizations use ten and its powers for representing numbers, possibly because there are ten fingers on two hands and people started counting by using their fingers. Examples are Brahmi numerals, Greek numerals, Hebrew numerals, Roman numerals, and Chinese numerals. Very large numbers were difficult to represent in these old numeral systems, and only the best mathematicians were able to multiply or divide large numbers. These difficulties were completely solved with the introduction of the Hindu–Arabic numeral system for representing integers. This system has been extended to represent some non-integer numbers, called "decimal fractions" or "decimal numbers", for forming the "decimal numeral system".
For writing numbers, the decimal system uses ten decimal digits, a decimal mark, and, for negative numbers, a minus sign "−". The decimal digits are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9; the decimal separator is the dot "" in many countries, but also a comma "" in other countries.
For representing a non-negative number, a decimal consists of
If , it is generally assumed that the first digit is not zero, but in some circumstances, it may be useful to have one or more 0's on the left. This does not change the value represented by the decimal. For example, . Similarly, if , it may be removed, and conversely, trailing zeros may be added without changing the represented number: for example, and . Sometimes the extra zeros are used for indicating the accuracy of a measurement. For example, 15.00 m may indicate that the measurement error is less than one centimeter (0.01 m), while 15 m may mean that the length is roughly fifteen meters, and that the error may exceed 10 cm.
For representing a negative number, a minus sign is placed before .
The numeral formula_2 represents the number
The "integer part" or "integral part" of a decimal is the integer written to the left of the decimal separator (see also truncation). For a non-negative decimal, it is the largest integer that is not greater than the decimal. The part from the decimal separator to the right is the "fractional part", which equals the difference between the numeral and its integer part.
When the integral part of a numeral is zero, it may occur, typically in computing, that the integer part is not written (for example , instead of ). In normal writing, this is generally avoided, because of the risk of confusion between the decimal mark and other punctuation.
In brief, the contribution of each digit to the value of a number depends on its position in the numeral. That is, the decimal system is a positional numeral system.
The numbers that are represented by decimal numerals are the decimal fractions (sometimes called decimal numbers), that is, the rational numbers that may be expressed as a fraction whose denominator is a power of ten. For example, the numerals formula_5 represent the fractions , , , and . More generally, a decimal with digits after the separator represents the fraction with denominator , whose numerator is the integer obtained by removing the separator.
Expressed as a fully reduced fraction, the decimal numbers are those whose denominator is a product of a power of 2 and a power of 5. Thus the smallest denominators of decimal numbers are
Decimal numerals do not allow an exact representation for all real numbers, e.g. for the real number . Nevertheless, they allow approximating every real number with any desired accuracy, e.g., the decimal 3.14159 approximates the real , being less than 10 off; and so decimals are widely used in science, engineering and everyday life.
More precisely, for every real number "x", and every positive integer "n", there are two decimals "L" and "u", with at most "n" digits after the decimal mark, such that "L" ≤ "x" ≤ "u" and ("u" – "L") = 10.
Numbers are very often obtained as the result of a measurement. As measurements are generally afflicted with some measurement error with a known upper bound, the result of a measurement is well represented by a decimal with digits after the decimal mark, as soon as the absolute measurement error is bounded from above by 10. In practice, measurement results are often given with a certain number of digits after the decimal point, which indicate the error bounds. For example, although 0.080 and 0.08 denote the same decimal number, the numeral 0.080 suggests a measurement with an error less than 0.001, while the numeral 0.08 indicates an absolute error bounded by 0.01. In both cases, the true value of the measured quantity could be, for example, 0.0803 or 0.0796 (see also significant figures).
For a real number "x" and an integer "n" ≥ 0, let ["x"] denote the (finite) decimal expansion of the greatest number that is not greater than "x", which has exactly "n" digits after the decimal mark. Let "d" denote the last digit of ["x"]. It is straightforward to see that ["x"] may be obtained by appending "d" to the right of ["x"]. This way one has
and the difference of ["x"] and ["x"] amounts to
which is either 0, if "d" = 0, or gets arbitrarily small, when "n" tends to infinity. According to the definition of a limit, "x" is the limit of ["x"] when "n" tends to infinity. This is written as formula_7 or
which is called an infinite decimal expansion of "x".
Conversely, for any integer ["x"] and any sequence of digits formula_8 the (infinite) expression is an "infinite decimal expansion" of a real number "x". This expansion is unique if neither all "d" are equal to 9 nor all "d" are equal to 0 for "n" large enough (for all "n" greater than some natural number "N").
If all "d" for "n" > "N" equal to 9 and the limit of the sequence formula_9 is the decimal fraction obtained by replacing the last digit that is not a 9, i.e.: "d", by "d" + 1, and replacing all subsequent 9s by 0s (see 0.999...).
Any such decimal fraction, i.e., "d" = 0 for "n" > "N", may be converted to its equivalent infinite decimal expansion by replacing "d" by "d" − 1, and replacing all subsequent 0s by 9s (see 0.999...).
In summary, every real number that is not a decimal fraction has a unique infinite decimal expansion. Each decimal fraction has exactly two infinite decimal expansions, one containing only 0s after some place, which is obtained by the above definition of ["x"], and the other containing only 9s after some place, which is obtained by defining ["x"] as the greatest number that is "less" than "x", having exactly "n" digits after the decimal mark.
Long division allows computing the infinite decimal expansion of a rational number. If the rational number is a decimal fraction, the division stops eventually, producing a decimal numeral, which may be prolongated into an infinite expansion by adding infinitely many zeros. If the rational number is not a decimal fraction, the division may continue indefinitely. However, as all successive remainders are less than the divisor, there are only a finite number of possible remainders, and after some place, the same sequence of digits must be repeated indefinitely in the quotient. That is, one has a "repeating decimal". For example,
Conversely, every eventually repeating sequence of digits is the infinite decimal expansion of a rational number. This is a consequence of the fact that the recurring part of a decimal representation is, in fact, an infinite geometric series which will sum to a rational number. For example,
Most modern computer hardware and software systems commonly use a binary representation internally (although many early computers, such as the ENIAC or the IBM 650, used decimal representation internally).
For external use by computer specialists, this binary representation is sometimes presented in the related octal or hexadecimal systems.
For most purposes, however, binary values are converted to or from the equivalent decimal values for presentation to or input from humans; computer programs express literals in decimal by default. (123.1, for example, is written as such in a computer program, even though many computer languages are unable to encode that number precisely.)
Both computer hardware and software also use internal representations which are effectively decimal for storing decimal values and doing arithmetic. Often this arithmetic is done on data which are encoded using some variant of binary-coded decimal, especially in database implementations, but there are other decimal representations in use (including decimal floating point such as in newer revisions of the IEEE 754 Standard for Floating-Point Arithmetic).
Decimal arithmetic is used in computers so that decimal fractional results of adding (or subtracting) values with a fixed length of their fractional part always are computed to this same length of precision. This is especially important for financial calculations, e.g., requiring in their results integer multiples of the smallest currency unit for book keeping purposes. This is not possible in binary, because the negative powers of formula_11 have no finite binary fractional representation; and is generally impossible for multiplication (or division). See Arbitrary-precision arithmetic for exact calculations.
Many ancient cultures calculated with numerals based on ten, sometimes argued due to human hands typically having ten digits. Standardized weights used in the Indus Valley Civilization () were based on the ratios: 1/20, 1/10, 1/5, 1/2, 1, 2, 5, 10, 20, 50, 100, 200, and 500, while their standardized ruler – the "Mohenjo-daro ruler" – was divided into ten equal parts. Egyptian hieroglyphs, in evidence since around 3000 BCE, used a purely decimal system, as did the Cretan hieroglyphs () of the Minoans whose numerals are closely based on the Egyptian model. The decimal system was handed down to the consecutive Bronze Age cultures of Greece, including Linear A (c. 18th century BCE−1450 BCE) and Linear B (c. 1375−1200 BCE) – the number system of classical Greece also used powers of ten, including, Roman numerals, an intermediate base of 5. Notably, the polymath Archimedes (c. 287–212 BCE) invented a decimal positional system in his Sand Reckoner which was based on 10 and later led the German mathematician Carl Friedrich Gauss to lament what heights science would have already reached in his days if Archimedes had fully realized the potential of his ingenious discovery. Hittite hieroglyphs (since 15th century BCE) were also.strictly decimal.
Some non-mathematical ancient texts like the Vedas, dating back to 1900–1700 BCE make use of decimals and mathematical decimal fractions.
The Egyptian hieratic numerals, the Greek alphabet numerals, the Hebrew alphabet numerals, the Roman numerals, the Chinese numerals and early Indian Brahmi numerals are all non-positional decimal systems, and required large numbers of symbols. For instance, Egyptian numerals used different symbols for 10, 20, to 90, 100, 200, to 900, 1000, 2000, 3000, 4000, to 10,000.
The world's earliest positional decimal system was the Chinese rod calculus.
Decimal fractions were first developed and used by the Chinese in the end of 4th century BCE, and then spread to the Middle East and from there to Europe. The written Chinese decimal fractions were non-positional. However, counting rod fractions were positional.
Qin Jiushao in his book Mathematical Treatise in Nine Sections (1247) denoted 0.96644 by
J. Lennart Berggren notes that positional decimal fractions appear for the first time in a book by the Arab mathematician Abu'l-Hasan al-Uqlidisi written in the 10th century. The Jewish mathematician Immanuel Bonfils used decimal fractions around 1350, anticipating Simon Stevin, but did not develop any notation to represent them. The Persian mathematician Jamshīd al-Kāshī claimed to have discovered decimal fractions himself in the 15th century. Al Khwarizmi introduced fraction to Islamic countries in the early 9th century; a Chinese author has alleged that his fraction presentation was an exact copy of traditional Chinese mathematical fraction from Sunzi Suanjing. This form of fraction with numerator on top and denominator at bottom without a horizontal bar was also used by al-Uqlidisi and by al-Kāshī in his work "Arithmetic Key".
A forerunner of modern European decimal notation was introduced by Simon Stevin in the 16th century.
A method of expressing every possible natural number using a set of ten symbols emerged in India. Several Indian languages show a straightforward decimal system. Many Indo-Aryan and Dravidian languages have numbers between 10 and 20 expressed in a regular pattern of addition to 10.
The Hungarian language also uses a straightforward decimal system. All numbers between 10 and 20 are formed regularly (e.g. 11 is expressed as "tizenegy" literally "one on ten"), as with those between 20 and 100 (23 as "huszonhárom" = "three on twenty").
A straightforward decimal rank system with a word for each order (10 , 100 , 1000 , 10,000 ), and in which 11 is expressed as "ten-one" and 23 as "two-ten-three", and 89,345 is expressed as 8 (ten thousands) 9 (thousand) 3 (hundred) 4 (tens) 5 is found in Chinese, and in Vietnamese with a few irregularities. Japanese, Korean, and Thai have imported the Chinese decimal system. Many other languages with a decimal system have special words for the numbers between 10 and 20, and decades. For example, in English 11 is "eleven" not "ten-one" or "one-teen".
Incan languages such as Quechua and Aymara have an almost straightforward decimal system, in which 11 is expressed as "ten with one" and 23 as "two-ten with three".
Some psychologists suggest irregularities of the English names of numerals may hinder children's counting ability.
Some cultures do, or did, use other bases of numbers.

</doc>
<doc id="8216" url="https://en.wikipedia.org/wiki?curid=8216" title="Dorians">
Dorians

The Dorians (; , "Dōrieîs", singular , "Dōrieús") were one of the four major ethnic groups among which the Hellenes (or Greeks) of Classical Greece considered themselves divided (along with the Aeolians, Achaeans, and Ionians). They are almost always referred to as just "the Dorians", as they are called in the earliest literary mention of them in the "Odyssey", where they already can be found inhabiting the island of Crete.
They were diverse in way of life and social organization, varying from the populous trade center of the city of Corinth, known for its ornate style in art and architecture, to the isolationist, military state of Sparta. And yet, all Hellenes knew which localities were Dorian, and which were not. Dorian states at war could more likely, but not always, count on the assistance of other Dorian states. Dorians were distinguished by the Doric Greek dialect and by characteristic social and historical traditions.
In the 5th century BCE, Dorians and Ionians were the two most politically important Greek "ethnoi", whose ultimate clash resulted in the Peloponnesian War. The degree to which fifth-century Hellenes self-identified as "Ionian" or "Dorian" has itself been disputed. At one extreme Édouard Will concludes that there was no true ethnic component in fifth-century Greek culture, in spite of anti-Dorian elements in Athenian propaganda. At the other extreme John Alty reinterprets the sources to conclude that ethnicity did motivate fifth-century actions. Moderns viewing these ethnic identifications through the 5th and 4th century BCE literary tradition have been profoundly influenced by their own social politics. Also, according to E.N. Tigerstedt, nineteenth-century European admirers of virtues they considered "Dorian" identified themselves as "Laconophile" and found responsive parallels in the culture of their day as well; their biases contribute to the traditional modern interpretation of "Dorians".
Accounts vary as to the Dorians' place of origin. One theory, widely believed in ancient times, is that they originated in the northern mountainous regions of Greece, ancient Macedonia and Epirus, and obscure circumstances brought them south into the Peloponnese, to certain Aegean islands, Magna Graecia, Lapithos and Crete. Mythology gave them a Greek origin and eponymous founder, Dorus son of Hellen, the mythological patriarch of the Hellenes.
The origin of the Dorians is a multifaceted concept. In modern scholarship, the term has often meant the location of the population disseminating the Doric Greek dialect within a hypothetical Proto-Greek speaking population. The dialect is known from records of classical northwestern Greece, the Peloponnesus and Crete and some of the islands. The geographic and ethnic information found in the west's earliest known literary work, the "Iliad", combined with the administrative records of the former Mycenaean states, prove to universal satisfaction that East Greek speakers were once dominant in the Peloponnesus but suffered a setback there and were replaced at least in official circles by West Greek speakers. An historical event is associated with the overthrow, called anciently the "Return of the Heracleidai" and by moderns the Dorian Invasion.
This theory of a return or invasion presupposes that West Greek speakers resided in northwest Greece but overran the Peloponnesus replacing the East Greek there with their own dialect. No records other than Mycenaean ones are known to have existed in the Bronze Age so a West Greek of that time and place can be neither proved nor disproved. West Greek speakers were in western Greece in classical times. Unlike the East Greeks, they are not associated with any evidence of displacement events. That provides circumstantial evidence that the Doric dialect disseminated among the Hellenes of northwest Greece, a highly-mountainous and somewhat-isolated region.
The Dorian invasion is a modern historical concept attempting to account for:
On the whole, none of the objectives has been met, but the investigations served to rule out various speculative hypotheses. Most scholars doubt that the Dorian invasion was the main cause of the collapse of the Mycenean civilization. The source of the West Greek speakers in the Peloponnese remains unattested by any solid evidence.
Though most of the Doric invaders settled in the Peloponnese, they also settled on Rhodes and Sicily and in what is now Southern Italy. In Asia Minor existed the Dorian Hexapolis (the six great Dorian cities): Halikarnassos (Halicarnassus) and Knidos (Cnidus) in Asia Minor, Kos, and Lindos, Kameiros, and Ialyssos on the island of Rhodes. The six cities would later become rivals with the Ionian cities of Asia Minor. The Dorians also invaded Crete. The origin traditions remained strong into classical times: Thucydides saw the Peloponnesian War in part as "Ionians fighting against Dorians" and reported the tradition that the Syracusans in Sicily were of Dorian descent. Other such "Dorian" colonies, originally from Corinth, Megara, and the Dorian islands, dotted the southern coasts of Sicily from Syracuse to Selinus. Also Taras was a Spartan colony.
A man's name, "Dōrieus", occurs in the Linear B tablets at Pylos, one of the regions later invaded and subjugated by the Dorians. Pylos tablet Fn867 records it in the dative case as "do-ri-je-we", *Dōriēwei, a third- or consonant-declension noun with stem ending in w. An unattested nominative plural, "*Dōriēwes", would have become "Dōrieis" by loss of the w and contraction. The tablet records the grain rations issued to the servants of "religious dignitaries" celebrating a religious festival of Potnia, the mother goddess.
The nominative singular, "Dōrieus", remained the same in the classical period. Many Linear B names of servants were formed from their home territory or the places where they came into Mycenaean ownership. Carl Darling Buck sees the "-eus" suffix as very productive. One of its uses was to convert a toponym to an anthroponym; for example, Megareus, "Megarian", from Megara.
A "Dōrieus" would be from Dōris, the only classical Greek state to serve as the basis for the name of the Dorians. The state was a small one in the mountains of west central Greece. However, classical Doris may not have been the same as Mycenaean Doris.
A number of credible etymologies by noted scholars have been proposed. Julius Pokorny derives Δωριεύς, "Dōrieus" from δωρίς, "dōris", "woodland" (which can also mean upland). The "dōri-" segment is from the o-grade (either "ō" or "o") of Proto-Indo-European "*deru-", "tree", which also gives the Homeric Δούρειος Ἵππος ("Doureios Hippos", "Wooden Horse"). This derivation has the advantage of naming the people after their wooded, mountainous country.
A second popular derivation was given by the French linguist, Émile Boisacq, from the same root, but from Greek ("doru") 'spear-shaft' (which was made of wood); i.e., "the people of the spear" or "spearmen." In this case the country would be named after the people, as in Saxony from the Saxons. However, R. S. P. Beekes doubted the validity of this derivation and asserted that no good etymology exists.
It sometimes happens that different derivations of an Indo-European word exploit similar-sounding Indo-European roots. Greek "doru", "lance," is from the o-grade of Indo-European *"deru", "solid," in the sense of wood. It is similar to an extended form, *"dō-ro-", of "*dō-", (give), as can be seen in the modern Greek imperative δώσε ("dose", "give [sing.]!") appearing in Greek as δῶρον ("dōron", "gift"). This is the path taken by Jonathan Hall, relying on elements taken from the myth of the Return of the Herakleidai.
Hall cites the tradition, based on a fragment of the poet, Tyrtaeus, that "Sparta is a divine gift granted by Zeus and Hera" to the Heracleidae. In another version, Tyndareus gives his kingdom to Heracles in gratitude for restoring him to the throne, but Heracles "asks the Spartan king to safeguard the gift until his descendants might claim it."
Hall, therefore, proposes that the Dorians are the people of the gift. They assumed the name on taking possession of Lacedaemon. Doris was subsequently named after them. Hall makes comparisons of Spartans to Hebrews as a chosen people maintaining a covenant with God and being assigned a Holy Land. To arrive at this conclusion, Hall relies on Herodotus' version of the myth (see below) that the Hellenes under Dorus did not take his name until reaching the Peloponnesus. In other versions the Heracleidae enlisted the help of their Dorian neighbors. Hall does not address the problem of the Dorians not calling Lacedaemon Doris, but assigning that name to some less holy and remoter land. Similarly, he does not mention the Dorian servant at Pylos, whose sacred gift, if such it was, was still being ruled by the Achaean Atreid family at Lacedaemon.
A minor, and perhaps regrettably forgotten, episode in the history of scholarship was the attempt to emphasize the etymology of Doron with the meaning of 'hand'. This in turn was connected to an interpretation of the famous lambda on Spartan shields, which was to rather stand for a hand with outstanding thumb than the initial letter of Lacedaimon. Given the origin of the Spartan shield lambda legend, however, in a fragment by Eupolis, an Athenian comic poet, there has been a recent attempt to suggest that a comic confusion between the letter and the hand image may yet have been intended.
Dorian social structure was characterized by a communal social structure and separation of the sexes. The lives of free men centered around military campaigns. When not abroad, men stayed in all-male residences focusing on military training until the age of 30, regardless of marital status. 
Dorian women had greater freedom and economic power than women of other Greek ethnicities. Unlike other Hellenic women, Dorian women were able to own property, manage their husbands' estate, and delegate many domestic tasks to slaves. Women in ancient Sparta possessed the greatest agency and economic power, likely due to the prolonged absences of men during military campaigns. Dorian women wore the peplos, which was once common to all Hellenes. This tunic was pinned at the shoulders by brooches and had slit skirts which bared the thighs and permitted more freedom of movement than the voluminous Ionian chiton (costume).
The Doric dialect was spoken in northwest Greece, the Peloponnese, Crete, southwest Asia Minor, the southernmost islands of the Aegean Sea, and the various Dorian colonies of Magna Graecia in Southern Italy and Sicily. After the classical period, it was mainly replaced by the Attic dialect upon which the Koine or "common" Greek language of the Hellenistic period was based. The main characteristic of Doric was the preservation of Proto-Indo-European , long , which in Attic-Ionic became , . A famous example is the valedictory phrase uttered by Spartan mothers to their sons before sending them off to war: ἢ τὰν ἢ ἐπὶ τᾶς ("ḕ tàn ḕ epì tâs", literally "either with it or on it": return alive with your shield
or dead upon it) would have been ἢ τὴν ἢ ἐπὶ τῆς ("ḕ tḕn ḕ epì tês") in the Attic-Ionic dialect of an Athenian mother. Tsakonian, a descendant of Doric Greek, is still spoken in some parts of the southern Argolid coast of the Peloponnese, in the modern prefecture of Arcadia.
Culturally, in addition to their Doric dialect of Greek, Doric colonies retained their characteristic Doric calendar that revolved round a cycle of festivals, the Hyacinthia and the Carneia being especially important.
The Dorian mode in music also was attributed to Doric societies and was associated by classical writers with martial qualities.
The Doric order of architecture in the tradition inherited by Vitruvius included the Doric column, noted for its simplicity and strength.
The Dorians seem to have offered the central mainland cultus for Helios. The scattering of cults of the sun god in Sicyon, Argos, Ermioni, Epidaurus and Laconia, and his holy livestock flocks at Taenarum, seem to suggest that the deity was considerably important in Dorian religion, compared to other parts of ancient Greece. Additionally, it may have been the Dorians to import his worship to Rhodes.
In Greek historiography, the Dorians are mentioned by many authors. The chief classical authors to relate their origins are Herodotus, Thucydides and Pausanias. The most copious authors, however, lived in Hellenistic and Roman times, long after the main events. This apparent paradox does not necessarily discredit the later writers, who were relying on earlier works that did not survive. The customs of the Spartan state and its illustrious individuals are detailed at great length in such authors as Plutarch and Diodorus Siculus.
The "Odyssey" has one reference to the Dorians:"There is a land called Crete, in the midst of the wine-dark sea, a fair, rich land, begirt with water, and therein are many men, past counting, and ninety cities. They have not all the same speech, but their tongues are mixed. There dwell Achaeans, there great-hearted native Cretans, there Cydonians, and Dorians of waving plumes, and goodly Pelasgians."
The reference is not compatible with a Dorian invasion that brought Dorians to Crete only after the fall of the Mycenaean states. In the "Odyssey", Odysseus and his relatives visit those states. Two solutions are possible, either the "Odyssey" is anachronistic or Dorians were on Crete in Mycenaean times. The uncertain nature of the Dorian invasion defers a definitive answer until more is known about it. Also, the Messenian town of Dorium is mentioned in the Catalogue of Ships. If its name comes from Dorians, it would imply there were settlements of the latter in Messenia during that time as well.
Tyrtaeus, a Spartan poet, became advisor of the Lacedaemonians in their mid-7th-century war to suppress a rebellion of the Messenians. The latter were a remnant of the Achaeans conquered "two generations before," which suggests a rise to supremacy at the end of the Dark Age rather than during and after the fall of Mycenae. The Messenian population was reduced to serfdom.
Only a few fragments of Tyrtaeus' five books of martial verse survive. His is the earliest mention of the three Dorian tribes: Pamphyli, Hylleis, Dymanes. He also says: "For Cronus' Son Himself, Zeus the husband of fair-crowned Hera, hath given this city to the children of Heracles, with whom we came into the wide isle of Pelops from windy Erineus."
Erineus was a village of Doris. He helped to establish the Spartan constitution, giving the kings and elders, among other powers, the power to dismiss the assembly. He established a rigorous military training program for the young including songs and poems he wrote himself, such as the "Embateria or Songs of the Battle-Charge which are also called Enoplia or Songs-under-Arms." These were chants used to establish the timing of standard drills under arms. He stressed patriotism:"For 'tis a fair thing for a good man to fall and die fighting in the van for his native land, ... let us fight with a will for this land, and die for our children and never spare our lives."
Herodotus was from Halicarnassus, a Dorian colony on the southwest coast of Asia Minor; following the literary tradition of the times he wrote in Ionic Greek, being one of the last authors to do so. He described the Persian Wars, giving a thumbnail account of the histories of the antagonists, Greeks and Persians.
Herodotus gives a general account of the events termed "the Dorian Invasion", presenting them as transfers of population. Their original home was in Thessaly, central Greece. He goes on to expand in mythological terms, giving some of the geographic details of the myth:1.56.2-3 And inquiring he found that the Lacedemonians and the Athenians had the pre-eminence, the first of the Dorian and the others of the Ionian race. For these were the most eminent races in ancient time, the second being a Pelasgian and the first a Hellenic race: and the one never migrated from its place in any direction, while the other was very exceedingly given to wanderings; for in the reign of Deucalion this race dwelt in Pthiotis, and in the time of Doros the son of Hellen in the land lying below Ossa and Olympos, which is called Histiaiotis; and when it was driven from Histiaiotis by the sons of Cadmos, it dwelt in Pindos and was called Makednian; and thence it moved afterwards to Dryopis, and from Dryopis it came finally to Peloponnesus, and began to be called Dorian.
1.57.1-3 What language however the Pelasgians used to speak I am not able with certainty to say. But if one must pronounce judging by those that still remain of the Pelasgians who dwelt in the city of Creston above the Tyrsenians, and who were once neighbours of the race now called Dorian, dwelling then in the land which is now called Thessaliotis, and also by those that remain of the Pelasgians who settled at Plakia and Skylake in the region of the Hellespont, who before that had been settlers with the Athenians, and of the natives of the various other towns which are really Pelasgian, though they have lost the name,—if one must pronounce judging by these, the Pelasgians used to speak a Barbarian language. If therefore all the Pelasgian race was such as these, then the Attic race, being Pelasgian, at the same time when it changed and became Hellenic, unlearnt also its language. For the people of Creston do not speak the same language with any of those who dwell about them, nor yet do the people of Phakia, but they speak the same language one as the other: and by this it is proved that they still keep unchanged the form of language which they brought with them when they migrated to these places.
1.58 As for the Hellenic race, it has used ever the same language, as I clearly perceive, since it first took its rise; but since the time when it parted off feeble at first from the Pelasgian race, setting forth from a small beginning it has increased to that great number of races which we see, and chiefly because many Barbarian races have been added to it besides. Moreover it is true, as I think, of the Pelasgian race also, that so far as it remained Barbarian it never made any great increase. 
Thus, according to Herodotus, the Dorians did not name themselves after Dorus until they had reached Peloponnesus. Herodotus does not explain the contradictions of the myth; for example, how Doris, located outside the Peloponnesus, acquired its name. However, his goal, as he relates in the beginning of the first book, is only to report what he had heard from his sources without judgement. In the myth, the Achaeans displaced from the Peloponnesus gathered at Athens under a leader Ion and became identified as "Ionians".
Herodotus' list of Dorian states is as follows. From northeastern Greece were Phthia, Histiaea and Macedon. In central Greece were Doris (the former Dryopia) and in the south Peloponnesus, specifically the states of Lacedaemon, Corinth, Sicyon, Epidaurus and Troezen. Hermione was not Dorian but had joined the Dorians. Overseas were the islands of Rhodes, Cos, Nisyrus and the Anatolian cities of Cnidus, Halicarnassus, Phaselis and Calydna. Dorians also colonised Crete including founding of such towns as Lato, Dreros and Olous. The Cynurians were originally Ionians but had become Dorian under the influence of their Argive masters.
Thucydides professes little of Greece before the Trojan War except to say that it was full of barbarians and that there was no distinction between barbarians and Greeks. The Hellenes came from Phthiotis. The whole country indulged in and suffered from piracy and was not settled. After the Trojan War, "Hellas was still engaged in removing and settling."
Some 60 years after the Trojan War the Boeotians were driven out of Arne by the Thessalians into Boeotia and 20 years later "the Dorians and the Heraclids became masters of the Peloponnese." So the lines were drawn between the Dorians and the Aeolians (here Boeotians) with the Ionians (former Peloponnesians).
Other than these few brief observations Thucydides names but few Dorians. He does make it clear that some Dorian states aligned or were forced to align with the Athenians while some Ionians went with the Lacedaemonians and that the motives for alignment were not always ethnic but were diverse. Among the Dorians was Lacedaemon, Corcyra, Corinth and Epidamnus, Leucadia, Ambracia, Potidaea, Rhodes, Cythera, Argos, Carystus, Syracuse, Gela, Acragas (later Agrigentum), Acrae, Casmenae.
He does explain with considerable dismay what happened to incite ethnic war after the unity between the Greek states during the Battle of Thermopylae. The Congress of Corinth, formed prior to it, "split into two sections." Athens headed one and Lacedaemon the other:"For a short time the league held together, till the Lacedaemonians and Athenians quarreled, and made war upon each other with their allies, a duel into which all the Hellenes sooner or later were drawn."
He adds: "the real cause I consider to be ... the growth of the power of Athens and the alarm which this inspired in Lacedaemon..."
In the Platonic work "Laws" is mentioned that the Achaeans who fought in the Trojan War, on their return from Troy were driven out from their homes and cities by the young residents, so they migrated under a leader named Dorieus and hence they were renamed "Dorians".
Now during this period of ten years, while the siege lasted, the affairs of each of the besiegers at home suffered much owing to the seditious conduct of the young men. For when the soldiers returned to their own cities and homes, these young people did not receive them fittingly and justly, but in such a way that there ensued a vast number of cases of death, slaughter, and exile. So they, being again driven out, migrated by sea; and because Dorieus was the man who then banded together the exiles, they got the new name of "Dorians", instead of "Achaeans". But as to all the events that follow this, you Lacedaemonians relate them all fully in your traditions.
The "Description of Greece" by Pausanias relates that the Achaeans were driven from their lands by Dorians coming from Oeta, a mountainous region bordering on Thessaly. They were led by Hyllus, a son of Heracles, but were defeated by the Achaeans. Under other leadership they managed to be victorious over the Achaeans and remain in the Peloponnesus, a mythic theme called "the return of the Heracleidae." They had built ships at Naupactus in which to cross the Gulf of Corinth. This invasion is viewed by the tradition of Pausanias as a return of the Dorians to the Peloponnesus, apparently meaning a return of families ruling in Aetolia and northern Greece to a land in which they had once had a share. The return is described in detail: there were "disturbances" throughout the Peloponnesus except in Arcadia, and new Dorian settlers. Pausanias goes on to describe the conquest and resettlement of Laconia, Messenia, Argos and elsewhere, and the emigration from there to Crete and the coast of Asia Minor.
Diodorus is a rich source of traditional information concerning the mythology and history of the Dorians, especially the "Library of History". He does not make any such distinction but the fantastic nature of the earliest material marks it as mythical or legendary. The myths do attempt to justify some Dorian operations, suggesting that they were in part political.
Diodorus quoting from an earlier historian Hecataeus of Abdera details that during the Exodus many Israelites went into the islands of Greece and other places."All the foreigners were forthwith expelled, and the most valiant and noble among them, under some notable leaders, were brought to Greece and other places, as some relate; the most famous of their leaders were Danaus and Cadmus. But the majority of the people descended into a country not far from Egypt, which is now called Judaea and at that time was altogether uninhabited."Heracles was a Perseid, a member of the ruling family of Greece. His mother Alcmene had both Perseids and Pelopids in her ancestry. A princess of the realm, she received Zeus thinking he was Amphitryon. Zeus intended his son to rule Greece but according to the rules of succession Eurystheus, born slightly earlier, preempted the right. Attempts to kill Heracles as a child failed. On adulthood he was forced into the service of Eurystheus, who commanded him to perform 12 labors.
Heracles became a warrior without a home, wandering from place to place assisting the local rulers with various problems. He took a retinue of Arcadians with him acquiring also over time a family of grown sons, the Heraclidae. He continued this mode of life even after completing the 12 labors. The legend has it that he became involved with Achaean Sparta when the family of king Tyndareus was unseated and driven into exile by Hippocoön and his family, who in the process happened to kill the son of a friend of Heracles. The latter and his retinue assaulted Sparta, taking it back from Hippocoön. He recalled Tyndareus, set him up as a guardian regent, and instructed him to turn the kingdom over to any descendants of his that should claim it. Heracles went on with the way of life to which he had become accustomed, which was by today's standards that of a mercenary, as he was being paid for his assistance. Subsequently, he founded a colony in Aetolia, then in Trachis.
After displacing the Dryopes, he went to the assistance of the Dorians, who lived in a land called Hestiaeotis under king Aegimius and were campaigning against the numerically superior Lapithae. The Dorians promised him 1/3 of Doris (which they did not yet possess). He asked Aegimius to keep his share of the land "in trust" until it should be claimed by a descendant. He went on to further adventures but was poisoned by his jealous wife, Deianeira. He immolated himself in full armor dressed for combat and "passed from among men into the company of the gods."
Strabo, who depends of course on the books available to him, goes on to elaborate:
Beside this sole reference to Dorians in Crete, the mention of the "Iliad" of the Heraclid Tlepolemus, a warrior on the side of Achaeans and colonist of three important Dorian cities in Rhodes has been also regarded as a later interpolation.
Language
Mythology
History
List of Dorian states

</doc>
<doc id="8217" url="https://en.wikipedia.org/wiki?curid=8217" title="Declaration of the Rights of Man and of the Citizen">
Declaration of the Rights of Man and of the Citizen

The Declaration of the Rights of Man and of the Citizen (), set by France's National Constituent Assembly in 1789, is a human civil rights document from the French Revolution.
The Declaration was drafted by the Abbé Sieyès and the Marquis de Lafayette, in consultation with Thomas Jefferson. Influenced by the doctrine of "natural right", the rights of man are held to be universal: valid at all times and in every place. It became the basis for a nation of free individuals protected equally by the law. It is included in the beginning of the constitutions of both the Fourth French Republic (1946) and Fifth Republic (1958) and is still current. Inspired by the Enlightenment philosophers, the Declaration was a core statement of the values of the French Revolution and had a major impact on the development of popular conceptions of individual liberty and democracy in Europe and worldwide.
The 1789 Declaration, together with the 1215 Magna Carta, the 1689 English Bill of Rights, the 1776 United States Declaration of Independence, and the 1789 United States Bill of Rights, inspired in large part the 1948 United Nations Universal Declaration of Human Rights.
The content of the document emerged largely from the ideals of the Enlightenment.
The principal drafts were prepared by Lafayette, working at times with his close friend Thomas Jefferson. In August 1789, Honoré Mirabeau played a central role in conceptualizing and drafting the Declaration of the Rights of Man and of the Citizen.
The last article of the Declaration of the Rights of Man and the Citizen was adopted on the 26 of August 1789 by the National Constituent Assembly, during the period of the French Revolution, as the first step toward writing a constitution for France. Inspired by the Enlightenment, the original version of the Declaration was discussed by the representatives on the basis of a 24 article draft proposed by , led by . The draft was later modified during the debates. A second and lengthier declaration, known as the Declaration of the Rights of Man and Citizen of 1793, was written in 1793 but never formally adopted.
The concepts in the Declaration come from the philosophical and political duties of the Enlightenment, such as individualism, the social contract as theorized by the Genevan philosopher Rousseau, and the separation of powers espoused by the Baron de Montesquieu. As can be seen in the texts, the French declaration was heavily influenced by the political philosophy of the Enlightenment and principles of human rights as was the U.S. Declaration of Independence which preceded it (4 July 1776).
According to a legal textbook published in 2007, the declaration is in the spirit of "secular natural law", which does not base itself on religious doctrine or authority, in contrast with traditional natural law theory, which does.
The declaration defines a single set of individual and collective rights for all men. Influenced by the doctrine of natural rights, these rights are held to be universal and valid in all times and places. For example, "Men are born and remain free and equal in rights. Social distinctions may be founded only upon the general good." They have certain natural rights to property, to liberty, and to life. According to this theory, the role of government is to recognize and secure these rights. Furthermore, the government should be carried on by elected representatives.
At the time it was written, the rights contained in the declaration were only awarded to men. Furthermore, the declaration was a statement of vision rather than reality. The declaration was not deeply rooted in either the practice of the West or even France at the time. The declaration emerged in the late 18th century out of war and revolution. It encountered opposition as democracy and individual rights were frequently regarded as synonymous with anarchy and subversion. This declaration embodies ideals and aspirations towards which France pledged to struggle in the future.
The Declaration is introduced by a preamble describing the fundamental characteristics of the rights which are qualified as being "natural, unalienable and sacred" and consisting of "simple and incontestable principles" on which citizens could base their demands. In the second article, "the natural and imprescriptible rights of man" are defined as "liberty, property, security and resistance to oppression". It called for the destruction of aristocratic privileges by proclaiming an end to feudalism and to exemptions from taxation, freedom and equal rights for all "Men", and access to public office based on talent. The monarchy was restricted, and all citizens were to have the right to take part in the legislative process. Freedom of speech and press were declared, and arbitrary arrests outlawed.
The Declaration also asserted the principles of popular sovereignty, in contrast to the divine right of kings that characterized the French monarchy, and social equality among citizens, "All the citizens, being equal in the eyes of the law, are equally admissible to all public dignities, places, and employments, according to their capacity and without distinction other than that of their virtues and of their talents," eliminating the special rights of the nobility and clergy.
Article I – Men are born and remain free and equal in rights. Social distinctions can be founded only on the common good.
Article II – The goal of any political association is the conservation of the natural and imprescriptible rights of man. These rights are liberty, property, safety and resistance against oppression.
Article III – The principle of any sovereignty resides essentially in the Nation. No body, no individual may exercise any authority which does not proceed directly from the nation.
Article IV – Liberty consists of doing anything which does not harm others: thus, the exercise of the natural rights of each man has only those borders which assure other members of the society the fruition of these same rights. These borders can be determined only by the law.
Article V – The law has the right to forbid only actions harmful to society. Anything which is not forbidden by the law cannot be impeded, and no one can be constrained to do what it does not order.
Article VI – The law is the expression of the general will. All the citizens have the right of contributing personally or through their representatives to its formation. It must be the same for all, either that it protects, or that it punishes. All the citizens, being equal in its eyes, are equally admissible to all public dignities, places, and employments, according to their capacity and without distinction other than that of their virtues and of their talents.
Article VII – No man can be accused, arrested nor detained but in the cases determined by the law, and according to the forms which it has prescribed. Those who solicit, dispatch, carry out or cause to be carried out arbitrary orders, must be punished; but any citizen called or seized under the terms of the law must obey at once; he renders himself culpable by resistance.
Article VIII – The law should establish only penalties that are strictly and evidently necessary, and no one can be punished but under a law established and promulgated before the offense and legally applied.
Article IX – Any man being presumed innocent until he is declared culpable if it is judged indispensable to arrest him, any rigor which would not be necessary for the securing of his person must be severely reprimanded by the law.
Article X – No one may be disquieted for his opinions, even religious ones, provided that their manifestation does not trouble the public order established by the law.
Article XI – The free communication of thoughts and of opinions is one of the most precious rights of man: any citizen thus may speak, write, print freely, except to respond to the abuse of this liberty, in the cases determined by the law.
Article XII – The guarantee of the rights of man and of the citizen necessitates a public force: this force is thus instituted for the advantage of all and not for the particular utility of those in whom it is trusted.
Article XIII – For the maintenance of the public force and for the expenditures of administration, a common contribution is indispensable; it must be equally distributed to all the citizens, according to their ability to pay.
Article XIV – Each citizen has the right to ascertain, by himself or through his representatives, the need for a public tax, to consent to it freely, to know the uses to which it is put, and of determining the proportion, basis, collection, and duration.
Article XV – The society has the right of requesting an account from any public agent of its administration.
Article XVI – Any society in which the guarantee of rights is not assured, nor the separation of powers determined, has no Constitution.
Article XVII – Property being an inviolable and sacred right, no one can be deprived of private usage, if it is not when the public necessity, legally noted, evidently requires it, and under the condition of a just and prior indemnity.
While the French Revolution provided rights to a larger portion of the population, there remained a distinction between those who obtained the political rights in the Declaration of the Rights of Man and of the Citizen and those who did not. Those who were deemed to hold these political rights were called active citizens. Active citizenship was granted to men who were French, at least 25 years old, paid taxes equal to three days work, and could not be defined as servants (Thouret). This meant that at the time of the Declaration only male property owners held these rights. The deputies in the National Assembly believed that only those who held tangible interests in the nation could make informed political decisions. This distinction directly affects articles 6, 12, 14, and 15 of the Declaration of the Rights of Man and of the Citizen as each of these rights is related to the right to vote and to participate actively in the government. With the decree of 29 October 1789, the term active citizen became embedded in French politics.
The concept of passive citizens was created to encompass those populations that had been excluded from political rights in the Declaration of the Rights of Man and of the Citizen. Because of the requirements set down for active citizens, the vote was granted to approximately 4.3 million Frenchmen out of a population of around 29 million. These omitted groups included women, slaves, children, and foreigners. As these measures were voted upon by the General Assembly, they limited the rights of certain groups of citizens while implementing the democratic process of the new French Republic (1792–1804). This legislation, passed in 1789, was amended by the creators of the Constitution of the Year III in order to eliminate the label of active citizen. The power to vote was then, however, to be granted solely to substantial property owners.
Tensions arose between active and passive citizens throughout the Revolution. This happened when passive citizens started to call for more rights, or when they openly refused to listen to the ideals set forth by active citizens. This cartoon clearly demonstrates the difference that existed between the active and passive citizens along with the tensions associated with such differences. In the cartoon, a passive citizen is holding a spade and a wealthy landowning active citizen is ordering the passive citizens to go to work. The act appears condescending to the passive citizen and it revisits the reasons why the French Revolution began in the first place.
Women, in particular, were strong passive citizens who played a significant role in the Revolution. Olympe de Gouges penned her Declaration of the Rights of Woman and of the Female Citizen in 1791 and drew attention to the need for gender equality. By supporting the ideals of the French Revolution and wishing to expand them to women, she represented herself as a revolutionary citizen. Madame Roland also established herself as an influential figure throughout the Revolution. She saw women of the French Revolution as holding three roles; "inciting revolutionary action, formulating policy, and informing others of revolutionary events." By working with men, as opposed to working separate from men, she may have been able to further the fight of revolutionary women. As players in the French Revolution, women occupied a significant role in the civic sphere by forming social movements and participating in popular clubs, allowing them societal influence, despite their lack of direct political influence.
The Declaration recognized many rights as belonging to citizens (who could only be male). This was despite the fact that after The March on Versailles on 5 October 1789, women presented the Women's Petition to the National Assembly in which they proposed a decree giving women equal rights. In 1790, Nicolas de Condorcet and Etta Palm d'Aelders unsuccessfully called on the National Assembly to extend civil and political rights to women. Condorcet declared that "he who votes against the right of another, whatever the religion, color, or sex of that other, has henceforth abjured his own". The French Revolution did not lead to a recognition of women's rights and this prompted Olympe de Gouges to publish the Declaration of the Rights of Woman and of the Female Citizen in September 1791.
The Declaration of the Rights of Woman and of the Female Citizen is modeled on the Declaration of the Rights of Man and of the Citizen and is ironic in formulation and exposes the failure of the French Revolution, which had been devoted to equality. It states that:
This revolution will only take effect when all women become fully aware of their deplorable condition, and of the rights, they have lost in society.
The Declaration of the Rights of Woman and of the Female Citizen follows the seventeen articles of the Declaration of the Rights of Man and of the Citizen point for point and has been described by Camille Naish as "almost a parody... of the original document". The first article of the Declaration of the Rights of Man and of the Citizen proclaims that "Men are born and remain free and equal in rights. Social distinctions may be based only on common utility." The first article of Declaration of the Rights of Woman and the Female Citizen replied: "Woman is born free and remains equal to man in rights. Social distinctions may only be based on common utility".
De Gouges also draws attention to the fact that under French law women were fully punishable, yet denied equal rights, declaring "Women have the right to mount the scaffold, they must also have the right to mount the speaker's rostrum".
The declaration did not revoke the institution of slavery, as lobbied for by Jacques-Pierre Brissot's "Les Amis des Noirs" and defended by the group of colonial planters called the Club Massiac because they met at the Hôtel Massiac. Despite the lack of explicit mention of slavery in the Declaration, slave uprisings in Saint-Domingue in the Haitian Revolution were inspired by it, as discussed in C. L. R. James' history of the Haitian Revolution, "The Black Jacobins".
Deplorable conditions for the thousands of slaves in Saint-Domingue, the most profitable slave colony in the world, led to the uprisings which would be known as the first successful slave revolt in the New World. Free persons of color were part of the first wave of revolt, but later former slaves took control. In 1794 the Convention dominated by the Jacobins abolished slavery, including in the colonies of Saint-Domingue and Guadeloupe. However, Napoleon reinstated it in 1802 and attempted to regain control of Saint-Domingue by sending in thousands of troops. After suffering the losses of two-thirds of the men, many to yellow fever, the French withdrew from Saint-Domingue in 1803. Napoleon gave up on North America and agreed to the Louisiana Purchase by the United States. In 1804, the leaders of Saint-Domingue declared it as an independent state, the Republic of Haiti, the second republic of the New World.
The Declaration has also influenced and inspired rights-based liberal democracy throughout the world. It was translated as soon as 1793–1794 by Colombian Antonio Nariño, who published it despite the Inquisition. He was sentenced to 10 years in prison for doing so. In 2003, the document was listed on UNESCO's Memory of the World register.
The 1945 Proclamation of Independence of the Democratic Republic of Vietnam references the opening line of the Declaration.
According to the preamble of the Constitution of the French Fifth Republic (adopted on 4 October 1958, and the current constitution), the principles set forth in the Declaration have constitutional value. Many laws and regulations have been canceled because they did not comply with those principles as interpreted by the Conseil Constitutionnel ("Constitutional Council of France") or by the "Conseil d'État" ("Council of State").

</doc>
<doc id="8218" url="https://en.wikipedia.org/wiki?curid=8218" title="Dennis Ritchie">
Dennis Ritchie

Dennis MacAlistair Ritchie (September 9, 1941 – October 12, 2011) was an American computer scientist. He created the C programming language and, with long-time colleague Ken Thompson, the Unix operating system and B programming language. Ritchie and Thompson were awarded the Turing Award from the ACM in 1983, the Hamming Medal from the IEEE in 1990 and the National Medal of Technology from President Bill Clinton in 1999. Ritchie was the head of Lucent Technologies System Software Research Department when he retired in 2007. He was the "R" in K&R C, and commonly known by his username dmr.
Dennis Ritchie was born in Bronxville, New York. His father was Alistair E. Ritchie, a longtime Bell Labs scientist and co-author of "The Design of Switching Circuits" on switching circuit theory. As a child, Dennis moved with his family to Summit, New Jersey, where he graduated from Summit High School. He graduated from Harvard University with degrees in physics and applied mathematics. After receiving his bachelor's degree he decided with typical modesty that he was not smart enough to be a physicist.
In 1967, Ritchie began working at the Bell Labs Computing Sciences Research Center, and in 1968, he defended his PhD thesis on "Program Structure and Computational Complexity" at Harvard under the supervision of Patrick C. Fischer. However, Ritchie never officially received his PhD degree as he did not submit a bound copy of his dissertation to the Harvard library, a requirement for the degree. In 2020, the Computer History museum worked with Ritchie's family and Fischer's family and found a copy of the lost dissertation. 
During the 1960s, Ritchie and Ken Thompson worked on the Multics operating system at Bell Labs. Thompson then found an old PDP-7 machine and developed his own application programs and operating system from scratch, aided by Ritchie and others. In 1970, Brian Kernighan suggested the name "Unix", a pun on the name "Multics". To supplement assembly language with a system-level programming language, Thompson created B. Later, B was replaced by C, created by Ritchie, who continued to contribute to the development of Unix and C for many years.
During the 1970s, Ritchie collaborated with James Reeds and Robert Morris on a ciphertext-only attack on the M-209 US cipher machine that could solve messages of at least 2000–2500 letters. Ritchie relates that, after discussions with the NSA, the authors decided not to publish it, as they were told that the principle was applicable to machines still in use by foreign governments.
Ritchie was also involved with the development of the Plan 9 and Inferno operating systems, and the programming language Limbo.
As part of an AT&T restructuring in the mid-1990s, Ritchie was transferred to Lucent Technologies, where he retired in 2007 as head of System Software Research Department.
Ritchie is best known as the creator of the C programming language, a key developer of the Unix operating system, and co-author of the book "The C Programming Language"; he was the 'R' in "K&R" (a common reference to the book's authors Kernighan and Ritchie). Ritchie worked together with Ken Thompson, who is credited with writing the original version of Unix; one of Ritchie's most important contributions to Unix was its porting to different machines and platforms. They were so influential on Research Unix that Doug McIlroy later wrote, "The names of Ritchie and Thompson may safely be assumed to be attached to almost everything not otherwise attributed."
Ritchie liked to emphasize that he was just one member of a group. He suggested that many of the improvements he introduced simply "looked like a good thing to do," and that anyone else in the same place at the same time might have done the same thing.
Nowadays, the C language is widely used in application, operating system, and embedded system development, and its influence is seen in most modern programming languages. C fundamentally changed the way computer programs were written. For the first time C enabled the same program to work on different machines. Modern software is written using one of C's more evolved dialects. Apple uses objective C, Microsoft uses C# and Java is the choice of internet applications. Mr. Ritchie and Ken Thompson used C to write UNIX. Unix has been influential establishing computing concepts and principles that have been widely adopted.
In an interview from 1999, Ritchie clarified that he saw Linux and BSD operating systems as a continuation of the basis of the Unix operating system, and as derivatives of Unix:
In the same interview, he stated that he viewed both Unix and Linux as "the continuation of ideas that were started by Ken and me and many others, many years ago."
In 1983, Ritchie and Thompson received the Turing Award "for their development of generic operating systems theory and specifically for the implementation of the UNIX operating system". Ritchie's Turing Award lecture was titled "Reflections on Software Research". In 1990, both Ritchie and Thompson received the IEEE Richard W. Hamming Medal from the Institute of Electrical and Electronics Engineers (IEEE), "for the origination of the UNIX operating system and the C programming language".
In 1997, both Ritchie and Thompson were made Fellows of the Computer History Museum, "for co-creation of the UNIX operating system, and for development of the C programming language."
On April 21, 1999, Thompson and Ritchie jointly received the National Medal of Technology of 1998 from President Bill Clinton for co-inventing the UNIX operating system and the C programming language which, according to the citation for the medal, "led to enormous advances in computer hardware, software, and networking systems and stimulated growth of an entire industry, thereby enhancing American leadership in the Information Age".
In 2005, the Industrial Research Institute awarded Ritchie its Achievement Award in recognition of his contribution to science and technology, and to society generally, with his development of the Unix operating system.
In 2011, Ritchie, along with Thompson, was awarded the Japan Prize for Information and Communications for his work in the development of the Unix operating system.
Ritchie was found dead on October 12, 2011, at the age of 70 at his home in Berkeley Heights, New Jersey, where he lived alone. First news of his death came from his former colleague, Rob Pike. The cause and exact time of death have not been disclosed. He had been in frail health for several years following treatment for prostate cancer and heart disease. News of Ritchie's death was largely overshadowed by the media coverage of the death of Apple co-founder Steve Jobs, which occurred the week before.
Following Ritchie's death, computer historian Paul E. Ceruzzi stated:
In an interview shortly after Ritchie's death, long time colleague Brian Kernighan said Ritchie never expected C to be so significant.
Kernighan told "The New York Times" "The tools that Dennis built—and their direct descendants—run pretty much everything today.” Kernighan reminded readers of how important a role C and Unix had played in the development of later high-profile projects, such as the iPhone. Other testimonials to his influence followed.
Reflecting upon his death, a commentator compared the relative importance of Steve Jobs and Ritchie, concluding that "[Ritchie's] work played a key role in spawning the technological revolution of the last forty years—including technology on which Apple went on to build its fortune." Another commentator said, "Ritchie, on the other hand, invented and co-invented two key software technologies which make up the DNA of effectively every single computer software product we use directly or even indirectly in the modern age. It sounds like a wild claim, but it really is true." Another said, "many in computer science and related fields knew of Ritchie’s importance to the growth and development of, well, everything to do with computing..."
The Fedora 16 Linux distribution, which was released about a month after he died, was dedicated to his memory. FreeBSD 9.0, released January 12, 2012 was also dedicated in his memory.
Asteroid 294727 Dennisritchie, discovered by astronomers Tom Glinos and David H. Levy in 2008, was named in his memory. The official was published by the Minor Planet Center on 7 February 2012 ().

</doc>
<doc id="8219" url="https://en.wikipedia.org/wiki?curid=8219" title="December 16">
December 16


</doc>
<doc id="8220" url="https://en.wikipedia.org/wiki?curid=8220" title="Doctrine and Covenants">
Doctrine and Covenants

The Doctrine and Covenants (sometimes abbreviated and cited as D&C or D. and C.) is a part of the open scriptural canon of several denominations of the Latter Day Saint movement. Originally published in 1835 as Doctrine and Covenants of the Church of the Latter Day Saints: Carefully Selected from the Revelations of God, editions of the book continue to be printed mainly by The Church of Jesus Christ of Latter-day Saints (LDS Church) and the Community of Christ (formerly the Reorganized Church of Jesus Christ of Latter Day Saints [RLDS Church]).
The book originally contained two parts: a sequence of lectures setting forth basic church doctrine, followed by a compilation of revelations, or "covenants" of the church: thus the name "Doctrine and Covenants". The "doctrine" portion of the book, however, has been removed by both the LDS Church and Community of Christ. The remaining portion of the book contains revelations on numerous topics, most of which were dictated by the movement's founder Joseph Smith, supplemented by materials periodically added by each denomination.
Controversy has existed between the two largest denominations of the Latter Day Saint movement over some sections added to the 1876 LDS edition, attributed to founder Smith. Whereas the LDS Church believes these sections to have been revelations to Smith, the RLDS Church traditionally disputed their authenticity.
The Doctrine and Covenants was first published in 1835 as a later version of the Book of Commandments, which had been partially printed in 1833. This earlier book contained 65 early revelations to church leaders, including Joseph Smith and Oliver Cowdery. Before many copies of the book could be printed, the printing press and most of the printed copies were destroyed by a mob in Missouri.
On September 24, 1834, a committee was appointed by the general assembly of the church to organize a new volume containing the most significant revelations. This committee of Presiding Elders, consisting of Smith, Cowdery, Sidney Rigdon, and Frederick G. Williams, began to review and revise numerous revelations for inclusion in the new work. The committee eventually organized the book into two parts: a "Doctrine" part and a "Covenants" part.
The "Doctrine" part of the book consisted of a theological course now called the "Lectures on Faith". The lectures were a series of doctrinal courses used in the School of the Prophets which had recently been completed in Kirtland, Ohio. According to the committee, these lectures were included in the compilation "in consequence of their embracing the important doctrine of salvation."
The "Covenants" part of the book, labeled "Covenants and Commandments of the Lord, to his servants of the church of the Latter Day Saints", contained a total of 103 revelations. These 103 revelations were said to "contain items or principles for the regulation of the church, as taken from the revelations which have been given since its organization, as well as from former ones." Each of the 103 revelations was assigned a "section number"; however, section 66 was mistakenly used twice. Thus, the sections of the original work were numbered only to 102.
On February 17, 1835, after the committee had selected the book's contents, the committee wrote that the resulting work represents "our belief, and when we say this, humbly trust, the faith and principles of this society as a body."
The book was first introduced to the church body in a general conference on August 17, 1835. Smith and Williams, two of the Presiding Elders on the committee, were absent, but Cowdery and Rigdon were present. The church membership at the time had not yet seen the Doctrine and Covenants manuscript as it had been compiled and revised solely by the committee; however, various church members who were familiar with the work "bore record" of the book's truth. At the end of the conference, the church "by a unanimous vote" agreed to accept the compilation as "the doctrine and covenants of their faith" and to make arrangements for its printing.
In 1835, the book was printed and published under the title "Doctrine and Covenants of the Church of the Latter Day Saints: Carefully Selected from the Revelations of God".
A copy of the Doctrine and Covenants from NASA photographer M. Edward Thomas traveled to the moon and back in 1972 with astronaut John Young aboard Apollo 16.
In the LDS Church, The Doctrine and Covenants of The Church of Jesus Christ of Latter-day Saints stands alongside the Bible, the Book of Mormon, and The Pearl of Great Price as scripture. Together the LDS Church's scriptures are referred to as the "standard works". The LDS Church's version of the Doctrine and Covenants is described by the church as "containing revelations given to Joseph Smith, the Prophet, with some additions by his successors in the Presidency of the Church."
The 138 sections and two official declarations in LDS Church's Doctrine and Covenants break down as follows:
The following sections are not revelations, but letters, reports, statements, and other similar documents: 102, 123, 127–131, 134, 135, and Official Declarations 1 and 2.
In 1844, the church added eight sections not included in the 1835 edition. In the current edition, these added sections are numbered 103, 105, 112, 119, 124, 127, 128, and 135.
In 1876, a new LDS Church edition renumbered most of the sections in a roughly chronological order instead of the earlier topical order, and included 26 sections not included in previous editions, now numbered as sections 2, 13, 77, 85, 87, 108–111, 113–118, 120–123, 125, 126, 129–132, and 136. Previous editions had been divided into verses with the early versifications generally following the paragraph structure of the original text. It was with the 1876 edition that the currently used versification was first employed.
During the 1880s, five foreign editions contained two revelations to John Taylor that were received in 1882 and 1883; these revelations "set in order" the priesthood, gave more clarification about the roles of priesthood offices—especially the seventy—and required "men who ... preside over my priesthood" to live plural marriage in order to qualify to hold their church positions. Due to the LDS Church's change in attitude to polygamy in 1890, these sections were not included in future English editions of the Doctrine and Covenants.
In 1930, a small volume edited by apostle James E. Talmage titled "Latter-day Revelations" was published, which was a highly edited selective version of the Doctrine and Covenants. Talmage wrote that the book's purpose was "to make the strictly doctrinal parts of the Doctrine and Covenants of easy access and reduce its bulk" by including only "the sections comprising scriptures of general and enduring value". Ninety-five of the sections of the Doctrine and Covenants were completely omitted—most notably section 132 on plural and celestial marriage—along with parts of 21 others. Twenty complete sections were retained along with parts of 21 others. Fundamentalist Mormons were offended, particularly at the exclusion of section 132, and accused the church of "changing the scriptures." As a result, church president Heber J. Grant ordered the withdrawal of the book from sale with the remaining copies shredded in order to "avoid further conflict with the fundamentalists".
Sections 137 and 138 were added to the LDS Church's 1981 edition of the Doctrine and Covenants, which is the edition currently in use by the church. These were accounts of two visions, one from Joseph Smith in 1837 and the other from his nephew, Joseph F. Smith, in 1918. The revelations were earlier accepted as scripture when added to the Pearl of Great Price in April 1976. No new revelatory sections have been added since 1981.
The LDS Church's 1981 edition contains two "Official Declarations" at the book's conclusion. The 1890 Official Declaration 1 ended the church-authorized practice of plural marriage, and the 1978 Official Declaration 2 announces the opening of priesthood ordination to all worthy male members without regard to race or color. The two Official Declarations are not revelations, but they serve as the formal announcements that a revelation was received. In neither case is the revelation included in the Doctrine and Covenants. The text of Official Declaration 1 has been included in every LDS Church printing of the Doctrine and Covenants since 1908.
In 1876, section 101 from the 1835 edition (and subsequent printings) was removed. Section 101 was a "Statement on Marriage" as adopted by an 1835 conference of the church, and contained the following text: 
This section was removed because it had been superseded by section 132 of the modern LDS edition, recorded in 1843, which contains a revelation received by Joseph Smith on eternal marriage and plural marriage, the principles of which can be dated to as early as 1831.
In 1921, the LDS Church removed the "Lectures on Faith" portion of the book, with an explanation that the lectures "were never presented to nor accepted by the Church as being otherwise than theological lectures or lessons". The lectures contain theology concerning the Godhead and emphasize the importance of faith and works.
Until 1981, editions of the book used code names for certain people and places in those sections that dealt with the United Order. The 1981 LDS edition replaced these with the real names, relegating the code names to footnotes. The Community of Christ edition still uses the code names.
Officials of the Community of Christ (formerly known as the Reorganized Church of Jesus Christ of Latter Day Saints) first published an edition of the Doctrine and Covenants in 1864, based on the previous 1844 edition. A general conference of the church in 1878 approved a resolution that declared that the revelations of the Prophet-President Joseph Smith III had equal standing to those previously included in the work. Since that time, the church has continued to add sections to its edition of the Doctrine and Covenants, containing the revelations of succeeding Prophet–Presidents. The most recent addition was formally authorized on April 14, 2010, after being presented to the church for informal consideration on January 17, 2010. The numbers of the sections and versification differ from the edition published by the LDS Church and both modern editions differ from the original 1835 edition numeration.
Regarding the contents of the Doctrine and Covenants, the church has stated: "As with other books of scripture, the various passages vary in their enduring quality."
The 167 sections of the Community of Christ's Doctrine and Covenants break down as follows:
The following sections are not revelations, but letters, reports, statements, and other similar documents: 99, 108A, 109–113, and 123.
Based on the above, the number of revelations (accounting for sections that are not revelations) presented by each Community of Christ president, are as follows:
The Community of Christ removed the "Lectures on Faith" in 1897. The 1970 World Conference concluded that several sections that had been added between the 1835 and 1844 editions—mainly dealing with the subjects of temple worship and baptism for the dead—had been published without proper approval of a church conference. As a result, the World Conference removed sections 107, 109, 110, 113, and 123 to a historical appendix, which also includes documents that were never published as sections. Of these, only section 107 was a revelation. The World Conference of 1990 subsequently removed the entire appendix from the Doctrine and Covenants. Section 108A contained the minutes of a business meeting, which, because of its historical nature, was moved to the Introduction in the 1970s. After 1990, the Introduction was updated, and what was section 108A was removed entirely.
The ongoing additions to the Community of Christ edition provide a record of the leadership changes and doctrinal developments within the denomination. When W. Grant McMurray became Prophet-President, he declared that instruction specific to leadership changes would no longer be included, so that the focus of the work could be more doctrinal in nature, and less administrative. The record of these leadership changes are still maintained in the form of published "letters of counsel." Prophet–President Stephen M. Veazey has conformed to this pattern. Although these letters are not formally published in the Doctrine and Covenants, they are still deemed to be inspired, and are dealt with in the same manner that revelations are (that is, they must be deliberated and approved by the voting members of a World Conference).
A modern revelation that resulted in some "disaffection" and "led to intense conflict in scattered areas of the RLDS Church" is contained in the Community of Christ version's section 156, presented by Prophet–President Wallace B. Smith and added in 1984, which called for the ordination of women to the priesthood and set out the primary purpose of temples to be "the pursuit of peace". A resulting schism over the legitimacy of these change led to the formation of the Restoration Branches movement, the Restoration Church of Jesus Christ of Latter Day Saints and the Remnant Church of Jesus Christ of Latter Day Saints.
While some of the prose in the new revelations seems designed to guide the denomination on matters of church governance and doctrine, others are seen as inspirational. One such example can be cited from section 161, presented as counsel to the church by W. Grant McMurray in 1996: "Become a people of the Temple—those who see violence but proclaim peace, who feel conflict yet extend the hand of reconciliation, who encounter broken spirits and find pathways for healing."
The Church of Jesus Christ of Latter Day Saints (Strangite) uses the 1846 edition that was published in Nauvoo, Illinois; this version is virtually identical to the 1844 edition. Most recently a facsimile reprint was produced for the church at Voree, Wisconsin by Richard Drew in 1993.
The Church of Christ (Temple Lot) contends that the thousands of changes made to the original revelations as published in the Book of Commandments (including the change of the church's name) are not doctrinal and result from Joseph Smith's fall from his original calling. As a result, the Church of Christ (Temple Lot) prefers to use reprints of the Book of Commandments text.
The Church of Jesus Christ (Cutlerite) accepts the 1844 edition of the Doctrine and Covenants, including the Lectures on Faith, which it insists are as much inspired as the revelations themselves.
The Restoration Branches generally use the older RLDS Church Doctrine and Covenants, typically sections 1–144.
The Remnant Church of Jesus Christ of Latter Day Saints uses the older RLDS Church version of the Doctrine and Covenants up to section 144, and also 19 new revelations from their previous president, Frederick Niels Larsen.
"Remnant" movement, a spiritual movement in schism with the LDS Church, published an online "Restoration" edition of the Doctrine and Covenants in 2017. It includes any sections authored by Joseph Smith. It also: includes a new version of D&C 54, as revised by Denver Snuffer; excludes the Kirtland Temple visitation by Elijah and other angelic beings in D&C 110; excludes portions based on fragmentary teachings by Smith in D&C 129; includes Smith's Lectures on Faith; and includes a new appendix titled, "A Prophet’s Prerogative," by Jeff Savage.
The following chart compares the current editions of the Doctrine and Covenants used by the LDS Church (LDS ed.) and Community of Christ (CofC ed.) with the 1833 Book of Commandments (BofC), the 1835 edition published in Kirtland, and the 1844 edition published in Nauvoo. Unless otherwise specified, the document is styled a "revelation" of the person delivering it.

</doc>
<doc id="8221" url="https://en.wikipedia.org/wiki?curid=8221" title="Death">
Death

Death is the permanent cessation of all biological functions that sustain a living organism. The remains of a living organism begin to decompose shortly after death. It is an inevitable process eventually occurring in all living organisms.
As of the early 21st century, over 150,000 humans die each day.
Many cultures and religions have the idea of an afterlife, and also hold the idea of judgement and reward for good deeds or punishment for sin.
The concept of death is a key to human understanding of the phenomenon. There are many scientific approaches and various interpretations of the concept. Additionally, the advent of life-sustaining therapy and the numerous criteria for defining death from both a medical and legal standpoint, have made it difficult to create a single unifying definition.
One of the challenges in defining death is in distinguishing it from life. As a point in time, death would seem to refer to the moment at which life ends. Determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems. Such determination, therefore, requires drawing precise conceptual boundaries between life and death. This is difficult, due to there being little consensus on how to define life.
It is possible to define life in terms of consciousness. When consciousness ceases, a living organism can be said to have died. One of the flaws in this approach is that there are many organisms that are alive but probably not conscious (for example, single-celled organisms). Another problem is in defining consciousness, which has many different definitions given by modern scientists, psychologists and philosophers. Additionally, many religious traditions, including Abrahamic and Dharmic traditions, hold that death does not (or may not) entail the end of consciousness. In certain cultures, death is more of a process than a single event. It implies a slow shift from one spiritual state to another.
Other definitions for death focus on the character of cessation of something. More specifically, death occurs when a living entity experiences irreversible cessation of all functioning. As it pertains to human life, death is an irreversible process where someone loses their existence as a person.
Historically, attempts to define the exact moment of a human's death have been subjective, or imprecise. Death was once defined as the cessation of heartbeat (cardiac arrest) and of breathing, but the development of CPR and prompt defibrillation have rendered that definition inadequate because breathing and heartbeat can sometimes be restarted. This type of death where circulatory and respiratory arrest happens is known as the circulatory definition of death (DCDD). Proponents of the DCDD believe that this definition is reasonable because a person with permanent loss of circulatory and respiratory function should be considered dead. Critics of this definition state that while cessation of these functions may be permanent, it does not mean the situation is not irreversible, because if CPR was applied, the person could be revived. Thus, the arguments for and against the DCDD boil down to a matter of defining the actual words "permanent" and "irreversible," which further complicates the challenge of defining death. Furthermore, events which were causally linked to death in the past no longer kill in all circumstances; without a functioning heart or lungs, life can sometimes be sustained with a combination of life support devices, organ transplants and artificial pacemakers.
Today, where a definition of the moment of death is required, doctors and coroners usually turn to "brain death" or "biological death" to define a person as being dead; people are considered dead when the electrical activity in their brain ceases. It is presumed that an end of electrical activity indicates the end of consciousness. Suspension of consciousness must be permanent, and not transient, as occurs during certain sleep stages, and especially a coma. In the case of sleep, EEGs can easily tell the difference.
The category of "brain death" is seen as problematic by some scholars. For instance, Dr. Franklin Miller, senior faculty member at the Department of Bioethics, National Institutes of Health, notes: "By the late 1990s... the equation of brain death with death of the human being was increasingly challenged by scholars, based on evidence regarding the array of biological functioning displayed by patients correctly diagnosed as having this condition who were maintained on mechanical ventilation for substantial periods of time. These patients maintained the ability to sustain circulation and respiration, control temperature, excrete wastes, heal wounds, fight infections and, most dramatically, to gestate fetuses (in the case of pregnant "brain-dead" women)."
While "brain death" is viewed as problematic by some scholars, there are certainly proponents of it that believe this definition of death is the most reasonable for distinguishing life from death. The reasoning behind the support for this definition is that brain death has a set of criteria that is reliable and reproducible. Also, the brain is crucial in determining our identity or who we are as human beings. The distinction should be made that "brain death" cannot be equated with one who is in a vegetative state or coma, in that the former situation describes a state that is beyond recovery.
Those people maintaining that only the neo-cortex of the brain is necessary for consciousness sometimes argue that only electrical activity should be considered when defining death. Eventually it is possible that the criterion for death will be the permanent and irreversible loss of cognitive function, as evidenced by the death of the cerebral cortex. All hope of recovering human thought and personality is then gone given current and foreseeable medical technology. At present, in most places the more conservative definition of death – irreversible cessation of electrical activity in the whole brain, as opposed to just in the neo-cortex – has been adopted (for example the Uniform Determination Of Death Act in the United States). In 2005, the Terri Schiavo case brought the question of brain death and artificial sustenance to the front of American politics.
Even by whole-brain criteria, the determination of brain death can be complicated. EEGs can detect spurious electrical impulses, while certain drugs, hypoglycemia, hypoxia, or hypothermia can suppress or even stop brain activity on a temporary basis. Because of this, hospitals have protocols for determining brain death involving EEGs at widely separated intervals under defined conditions.
In the past, adoption of this whole-brain definition was a conclusion of the President's Commission for the Study of Ethical Problems in Medicine and Biomedical and Behavioral Research in 1980. They concluded that this approach to defining death sufficed in reaching a uniform definition nationwide. A multitude of reasons were presented to support this definition including: uniformity of standards in law for establishing death; consumption of a family's fiscal resources for artificial life support; and legal establishment for equating brain death with death in order to proceed with organ donation.
Aside from the issue of support of or dispute against brain death, there is another inherent problem in this categorical definition: the variability of its application in medical practice. In 1995, the American Academy of Neurology (AAN), established a set of criteria that became the medical standard for diagnosing neurologic death. At that time, three clinical features had to be satisfied in order to determine “irreversible cessation” of the total brain including: coma with clear etiology, cessation of breathing, and lack of brainstem reflexes. This set of criteria was then updated again most recently in 2010, but substantial discrepancies still remain across hospitals and medical specialties.
The problem of defining death is especially imperative as it pertains to the dead donor rule, which could be understood as one of the following interpretations of the rule: there must be an official declaration of death in a person before starting organ procurement or that organ procurement cannot result in death of the donor. A great deal of controversy has surrounded the definition of death and the dead donor rule. Advocates of the rule believe the rule is legitimate in protecting organ donors while also countering against any moral or legal objection to organ procurement. Critics, on the other hand, believe that the rule does not uphold the best interests of the donors and that the rule does not effectively promote organ donation.
Signs of death or strong indications that a warm-blooded animal is no longer alive are:
The stages that follow after death are:
The death of a person has legal consequences that may vary between different jurisdictions.
A death certificate is issued in most jurisdictions, either by a doctor, or by an administrative office upon presentation of a doctor's declaration of death.
There are many anecdotal references to people being declared dead by physicians and then "coming back to life", sometimes days later in their own coffin, or when embalming procedures are about to begin. From the mid-18th century onwards, there was an upsurge in the public's fear of being mistakenly buried alive, and much debate about the uncertainty of the signs of death. Various suggestions were made to test for signs of life before burial, ranging from pouring vinegar and pepper into the corpse's mouth to applying red hot pokers to the feet or into the rectum. Writing in 1895, the physician J.C. Ouseley claimed that as many as 2,700 people were buried prematurely each year in England and Wales, although others estimated the figure to be closer to 800.
In cases of electric shock, cardiopulmonary resuscitation (CPR) for an hour or longer can allow stunned nerves to recover, allowing an apparently dead person to survive. People found unconscious under icy water may survive if their faces are kept continuously cold until they arrive at an emergency room. This "diving response", in which metabolic activity and oxygen requirements are minimal, is something humans share with cetaceans called the mammalian diving reflex.
As medical technologies advance, ideas about when death occurs may have to be re-evaluated in light of the ability to restore a person to vitality after longer periods of apparent death (as happened when CPR and defibrillation showed that cessation of heartbeat is inadequate as a decisive indicator of death). The lack of electrical brain activity may not be enough to consider someone scientifically dead. Therefore, the concept of information-theoretic death has been suggested as a better means of defining when true death occurs, though the concept has few practical applications outside the field of cryonics.
There have been some scientific attempts to bring dead organisms back to life, but with limited success. In science fiction scenarios where such technology is readily available, real death is distinguished from reversible death.
The leading cause of human death in developing countries is infectious disease. The leading causes in developed countries are atherosclerosis (heart disease and stroke), cancer, and other diseases related to obesity and aging. By an extremely wide margin, the largest unifying cause of death in the developed world is biological aging, leading to various complications known as aging-associated diseases. These conditions cause loss of homeostasis, leading to cardiac arrest, causing loss of oxygen and nutrient supply, causing irreversible deterioration of the brain and other tissues. Of the roughly 150,000 people who die each day across the globe, about two thirds die of age-related causes. In industrialized nations, the proportion is much higher, approaching 90%. With improved medical capability, dying has become a condition to be managed. Home deaths, once commonplace, are now rare in the developed world.
In developing nations, inferior sanitary conditions and lack of access to modern medical technology makes death from infectious diseases more common than in developed countries. One such disease is tuberculosis, a bacterial disease which killed 1.8M people in 2015. Malaria causes about 400–900M cases of fever and 1–3M deaths annually. AIDS death toll in Africa may reach 90–100M by 2025.
According to Jean Ziegler (United Nations Special Reporter on the Right to Food, 2000 – Mar 2008), mortality due to malnutrition accounted for 58% of the total mortality rate in 2006. Ziegler says worldwide approximately 62M people died from all causes and of those deaths more than 36M died of hunger or diseases due to deficiencies in micronutrients.
Tobacco smoking killed 100 million people worldwide in the 20th century and could kill 1 billion people around the world in the 21st century, a World Health Organization report warned.
Many leading developed world causes of death can be postponed by diet and physical activity, but the accelerating incidence of disease with age still imposes limits on human longevity. The evolutionary cause of aging is, at best, only just beginning to be understood. It has been suggested that direct intervention in the aging process may now be the most effective intervention against major causes of death.
Selye proposed a unified non-specific approach to many causes of death. He demonstrated that stress decreases adaptability of an organism and proposed to describe the adaptability as a special resource, "adaptation energy". The animal dies when this resource is exhausted. Selye assumed that the adaptability is a finite supply, presented at birth. Later on, Goldstone proposed the concept of a production or income of adaptation energy which may be stored (up to a limit), as a capital reserve of adaptation. In recent works, adaptation energy is considered as an internal coordinate on the "dominant path" in the model of adaptation. It is demonstrated that oscillations of well-being appear when the reserve of adaptability is almost exhausted.
In 2012, suicide overtook car crashes for leading causes of human injury deaths in the U.S., followed by poisoning, falls and murder. Causes of death are different in different parts of the world. In high-income and middle income countries nearly half up to more than two thirds of all people live beyond the age of 70 and predominantly die of chronic diseases. In low-income countries, where less than one in five of all people reach the age of 70, and more than a third of all deaths are among children under 15, people predominantly die of infectious diseases.
An autopsy, also known as a "postmortem examination" or an "obduction", is a medical procedure that consists of a thorough examination of a human corpse to determine the cause and manner of a person's death and to evaluate any disease or injury that may be present. It is usually performed by a specialized medical doctor called a pathologist.
Autopsies are either performed for legal or medical purposes. A forensic autopsy is carried out when the cause of death may be a criminal matter, while a clinical or academic autopsy is performed to find the medical cause of death and is used in cases of unknown or uncertain death, or for research purposes. Autopsies can be further classified into cases where external examination suffices, and those where the body is dissected and an internal examination is conducted. Permission from next of kin may be required for internal autopsy in some cases. Once an internal autopsy is complete the body is generally reconstituted by sewing it back together. Autopsy is important in a medical environment and may shed light on mistakes and help improve practices.
A necropsy, which is not always a medical procedure, was a term previously used to describe an unregulated postmortem examination . In modern times, this term is more commonly associated with the corpses of animals.
Senescence refers to a scenario when a living being is able to survive all calamities, but eventually dies due to causes relating to old age. Animal and plant cells normally reproduce and function during the whole period of natural existence, but the aging process derives from deterioration of cellular activity and ruination of regular functioning. Aptitude of cells for gradual deterioration and mortality means that cells are naturally sentenced to stable and long-term loss of living capacities, even despite continuing metabolic reactions and viability. In the United Kingdom, for example, nine out of ten of all the deaths that occur on a daily basis relates to senescence, while around the world it accounts for two-thirds of 150,000 deaths that take place daily (Hayflick & Moody, 2003).
Almost all animals who survive external hazards to their biological functioning eventually die from biological aging, known in life sciences as "senescence". Some organisms experience negligible senescence, even exhibiting biological immortality. These include the jellyfish "Turritopsis dohrnii", the hydra, and the planarian. Unnatural causes of death include suicide and predation. From all causes, roughly 150,000 people die around the world each day.<ref name="doi10.2202/1941-6008.1011"></ref> Of these, two thirds die directly or indirectly due to senescence, but in industrialized countries – such as the United States, the United Kingdom, and Germany – the rate approaches 90% (i.e., nearly nine out of ten of all deaths are related to senescence).
Physiological death is now seen as a process, more than an event: conditions once considered indicative of death are now reversible. Where in the process a dividing line is drawn between life and death depends on factors beyond the presence or absence of vital signs. In general, clinical death is neither necessary nor sufficient for a determination of legal death. A patient with working heart and lungs determined to be brain dead can be pronounced legally dead without clinical death occurring. As scientific knowledge and medicine advance, formulating a precise medical definition of death becomes more difficult.
Cryonics (from Greek κρύος 'kryos-' meaning 'icy cold') is the low-temperature preservation of animals and humans who cannot be sustained by contemporary medicine, with the hope that healing and resuscitation may be possible in the future.
Cryopreservation of people or large animals is not reversible with current technology. The stated rationale for cryonics is that people who are considered dead by current legal or medical definitions may not necessarily be dead according to the more stringent information-theoretic definition of death.
Some scientific literature is claimed to support the feasibility of cryonics. Medical science and cryobiologists generally regards cryonics with skepticism.
"One of medicine's new frontiers: treating the dead", recognizes that cells that have been without oxygen for more than five minutes die, not from lack of oxygen, but rather when their oxygen supply is resumed. Therefore, practitioners of this approach, e.g., at the Resuscitation Science institute at the University of Pennsylvania, "aim to reduce oxygen uptake, slow metabolism and adjust the blood chemistry for gradual and safe reperfusion."
Life extension refers to an increase in maximum or average lifespan, especially in humans, by slowing down or reversing the processes of aging. Average lifespan is determined by vulnerability to accidents and age or lifestyle-related afflictions such as cancer, or cardiovascular disease. Extension of average lifespan can be achieved by good diet, exercise and avoidance of hazards such as smoking. Maximum lifespan is also determined by the rate of aging for a species inherent in its genes. Currently, the only widely recognized method of extending maximum lifespan is calorie restriction. Theoretically, extension of maximum lifespan can be achieved by reducing the rate of aging damage, by periodic replacement of damaged tissues, or by molecular repair or rejuvenation of deteriorated cells and tissues.
A United States poll found that religious people and irreligious people, as well as men and women and people of different economic classes have similar rates of support for life extension, while Africans and Hispanics have higher rates of support than white people. 38 percent of the polled said they would desire to have their aging process cured.
Researchers of life extension are a subclass of biogerontologists known as "biomedical gerontologists". They try to understand the nature of aging and they develop treatments to reverse aging processes or to at least slow them down, for the improvement of health and the maintenance of youthful vigor at every stage of life. Those who take advantage of life extension findings and seek to apply them upon themselves are called "life extensionists" or "longevists". The primary life extension strategy currently is to apply available anti-aging methods in the hope of living long enough to benefit from a complete cure to aging once it is developed.
Before about 1930, most people in Western countries died in their own homes, surrounded by family, and comforted by clergy, neighbors, and doctors making house calls. By the mid-20th century, half of all Americans died in a hospital. By the start of the 21st century, only about 20–25% of people in developed countries died outside of a medical institution. The shift away from dying at home towards dying in a professional medical environment has been termed the "Invisible Death". This shift occurred gradually over the years, until most deaths now occur outside the home.
Many people are afraid of dying. Discussing, thinking, or planning their own deaths causes them discomfort. This fear may cause them to put off financial planning, preparing a will and testament, or requesting help from a hospice organization.
Different people have different responses to the idea of their own deaths.
Philosopher Galen Strawson writes that the death that many people wish for is an instant, painless, unexperienced annihilation. In this unlikely scenario, the person dies without realizing it and without being able to fear it. One moment the person is walking, eating, or sleeping, and the next moment, the person is dead. Strawson reasons that this type of death would not take anything away from the person, as he believes that a person cannot have a legitimate claim to ownership in the future.
In society, the nature of death and humanity's awareness of its own mortality has for millennia been a concern of the world's religious traditions and of philosophical inquiry. This includes belief in resurrection or an afterlife (associated with Abrahamic religions), reincarnation or rebirth (associated with Dharmic religions), or that consciousness permanently ceases to exist, known as eternal oblivion (associated with Secular humanism).
Commemoration ceremonies after death may include various mourning, funeral practices and ceremonies of honouring the deceased. The physical remains of a person, commonly known as a "corpse" or "body", are usually interred whole or cremated, though among the world's cultures there are a variety of other methods of mortuary disposal. In the English language, blessings directed towards a dead person include "rest in peace", or its initialism RIP.
Death is the center of many traditions and organizations; customs relating to death are a feature of every culture around the world. Much of this revolves around the care of the dead, as well as the afterlife and the disposal of bodies upon the onset of death. The disposal of human corpses does, in general, begin with the last offices before significant time has passed, and ritualistic ceremonies often occur, most commonly interment or cremation. This is not a unified practice; in Tibet, for instance, the body is given a sky burial and left on a mountain top. Proper preparation for death and techniques and ceremonies for producing the ability to transfer one's spiritual attainments into another body (reincarnation) are subjects of detailed study in Tibet. Mummification or embalming is also prevalent in some cultures, to retard the rate of decay.
Legal aspects of death are also part of many cultures, particularly the settlement of the deceased estate and the issues of inheritance and in some countries, inheritance taxation.
Capital punishment is also a culturally divisive aspect of death. In most jurisdictions where capital punishment is carried out today, the death penalty is reserved for premeditated murder, espionage, treason, or as part of military justice. In some countries, sexual crimes, such as adultery and sodomy, carry the death penalty, as do religious crimes such as apostasy, the formal renunciation of one's religion. In many retentionist countries, drug trafficking is also a capital offense. In China, human trafficking and serious cases of corruption are also punished by the death penalty. In militaries around the world courts-martial have imposed death sentences for offenses such as cowardice, desertion, insubordination, and mutiny.
Death in warfare and in suicide attack also have cultural links, and the ideas of "dulce et decorum est pro patria mori", mutiny punishable by death, grieving relatives of dead soldiers and death notification are embedded in many cultures. Recently in the western world, with the increase in terrorism following the September 11 attacks, but also further back in time with suicide bombings, kamikaze missions in World War II and suicide missions in a host of other conflicts in history, death for a cause by way of suicide attack, and martyrdom have had significant cultural impacts.
Suicide is also present in some subcultures, in recent times for example in the emo subculture. The qualitative research has shown emo respondents reported "attitudes including high acceptance for suicidal behavior and self-injury", and concluded: "The identification with the emo youth subculture is considered to be a factor strengthening vulnerability towards risky behaviors."
Suicide in general, and particularly euthanasia, are also points of cultural debate. Both acts are understood very differently in different cultures. In Japan, for example, ending a life with honor by seppuku was considered a desirable death, whereas according to traditional Christian and Islamic cultures, suicide is viewed as a sin. Death is personified in many cultures, with such symbolic representations as the Grim Reaper, Azrael, the Hindu god Yama and Father Time.
In Brazil, a human death is counted officially when it is registered by existing family members at a cartório, a government-authorized registry. Before being able to file for an official death, the deceased must have been registered for an official birth at the cartório. Though a Public Registry Law guarantees all Brazilian citizens the right to register deaths, regardless of their financial means, of their family members (often children), the Brazilian government has not taken away the burden, the hidden costs and fees, of filing for a death. For many impoverished families, the indirect costs and burden of filing for a death lead to a more appealing, unofficial, local, cultural burial, which in turn raises the debate about inaccurate mortality rates.
Talking about death and witnessing it is a difficult issue with most cultures. Western societies may like to treat the dead with the utmost material respect, with an official embalmer and associated rites. Eastern societies (like India) may be more open to accepting it as a "fait accompli", with a funeral procession of the dead body ending in an open-air burning-to-ashes of the same.
Much interest and debate surround the question of what happens to one's consciousness as one's body dies. The belief in the permanent loss of consciousness after death is often called "eternal oblivion". Belief that the stream of consciousness is preserved after physical death is described by the term "afterlife". Neither are likely to ever be confirmed without the ponderer having to actually die.
After death, the remains of an organism become part of the biogeochemical cycle, during which animals may be consumed by a predator or a scavenger. Organic material may then be further decomposed by detritivores, organisms which recycle detritus, returning it to the environment for reuse in the food chain, where these chemicals may eventually end up being consumed and assimilated into the cells of a living organism. Examples of detritivores include earthworms, woodlice and dung beetles.
Microorganisms also play a vital role, raising the temperature of the decomposing matter as they break it down into yet simpler molecules. Not all materials need to be fully decomposed. Coal, a fossil fuel formed over vast tracts of time in swamp ecosystems, is one example.
Contemporary evolutionary theory sees death as an important part of the process of natural selection. It is considered that organisms less adapted to their environment are more likely to die having produced fewer offspring, thereby reducing their contribution to the gene pool. Their genes are thus eventually bred out of a population, leading at worst to extinction and, more positively, making the process possible, referred to as speciation. Frequency of reproduction plays an equally important role in determining species survival: an organism that dies young but leaves numerous offspring displays, according to Darwinian criteria, much greater fitness than a long-lived organism leaving only one.
Extinction is the cessation of existence of a species or group of taxa, reducing biodiversity. The moment of extinction is generally considered to be the death of the last individual of that species (although the capacity to breed and recover may have been lost before this point). Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively. This difficulty leads to phenomena such as Lazarus taxa, where species presumed extinct abruptly "reappear" (typically in the fossil record) after a period of apparent absence. New species arise through the process of speciation, an aspect of evolution. New varieties of organisms arise and thrive when they are able to find and exploit an ecological niche – and species become extinct when they are no longer able to survive in changing conditions or against superior competition.
Inquiry into the evolution of aging aims to explain why so many living things and the vast majority of animals weaken and die with age (exceptions include "Hydra" and the already cited jellyfish "Turritopsis dohrnii", which research shows to be biologically immortal). The evolutionary origin of senescence remains one of the fundamental puzzles of biology. Gerontology specializes in the science of human aging processes.
Organisms showing only asexual reproduction (e.g. bacteria, some protists, like the euglenoids and many amoebozoans) and unicellular organisms with sexual reproduction (colonial or not, like the volvocine algae "Pandorina" and "Chlamydomonas") are "immortal" at some extent, dying only due to external hazards, like being eaten or meeting with a fatal accident. In multicellular organisms (and also in multinucleate ciliates), with a Weismannist development, that is, with a division of labor between mortal somatic (body) cells and "immortal" germ (reproductive) cells, death becomes an essential part of life, at least for the somatic line.
The "Volvox" algae are among the simplest organisms to exhibit that division of labor between two completely different cell types, and as a consequence include death of somatic line as a regular, genetically regulated part of its life history.
In Buddhist doctrine and practice, death plays an important role. Awareness of death was what motivated Prince Siddhartha to strive to find the "deathless" and finally to attain enlightenment. In Buddhist doctrine, death functions as a reminder of the value of having been born as a human being. Being reborn as a human being is considered the only state in which one can attain enlightenment. Therefore, death helps remind oneself that one should not take life for granted. The belief in rebirth among Buddhists does not necessarily remove death anxiety, since all existence in the cycle of rebirth is considered filled with suffering, and being reborn many times does not necessarily mean that one progresses.
Death is part of several key Buddhist tenets, such as the Four Noble Truths and dependent origination.
In the Hindu texts, death is described as the individual eternal spiritual soul (conscious self) exiting the current temporary material body. The soul exits this body when the body can no longer sustain the conscious self (life), which may be due to mental or physical reasons, or more accurately, the inability to act on one's material desires. During conception, the soul enters a compatible new body based on one's remaining karma and the state of one's mind (last thought) at the time of death.
Usually this transition makes one forget all memories of one's previous life. Because nothing really dies and the temporary material body is always changing, both in this life and the next, death simply means forgetfulness of one’s previous experiences (previous material identity).
Material existence is described as being full of miseries arising from birth, disease, old age, death, mind, weather, etc. To conquer the cycle of death and rebirth and become eligible for one of the different types of liberation, one has to first conquer material desires and become self-realized. The human form of life is most suitable for this spiritual journey, especially with the help of sadhu (self-realized saintly persons), sastra (revealed spiritual scriptures), and guru (self-realized spiritual masters), given all three are in agreement.
Death is seen in Judaism as tragic and intimidating. Persons who come into contact with corpses are ritually impure. There are a variety of beliefs about the afterlife within Judaism, but none of them contradict the preference of life over death. This is partially because death puts a cessation to the possibility of fulfilling any commandments.
The word death comes from Old English "dēaþ", which in turn comes from Proto-Germanic *"dauþuz" (reconstructed by etymological analysis). This comes from the Proto-Indo-European stem *"dheu-" meaning the "process, act, condition of dying".
The concept and symptoms of death, and varying degrees of delicacy used in discussion in public forums, have generated numerous scientific, legal, and socially acceptable terms or euphemisms for death. When a person has died, it is also said they have "passed away", "passed on", "expired", or are "gone", among numerous other socially accepted, religiously specific, slang, and irreverent terms.
As a formal reference to a dead person, it has become common practice to use the participle form of "decease", as in "the deceased"; another noun form is "decedent".
Bereft of life, the dead person is then a "corpse", "cadaver", a "body", a "set of remains", and when all flesh has rotted away, a skeleton. The terms "carrion" and "carcass" can also be used, though these more often connote the remains of non-human animals. The ashes left after a cremation are sometimes referred to by the neologism "cremains", a portmanteau of "cremation" and "remains".
Bibliography

</doc>
<doc id="8222" url="https://en.wikipedia.org/wiki?curid=8222" title="Deseret alphabet">
Deseret alphabet

The Deseret alphabet (; Deseret: or ) is a phonemic English-language spelling reform developed between 1847 and 1854 by the board of regents of the University of Deseret under the leadership of Brigham Young, the second president of The Church of Jesus Christ of Latter-day Saints (LDS Church). George D. Watt is reported to have been the most actively involved in the development of the script, as well as being its first serious user.
In public statements, Young claimed the alphabet was intended to replace the traditional Latin alphabet with an alternative, more phonetically accurate alphabet for the English language. This would offer immigrants an opportunity to learn to read and write English, he said, the orthography of which is often less phonetically consistent than those of many other languages. Similar experiments have not been uncommon, the most well-known of which is the Shavian alphabet.
The Deseret alphabet was an outgrowth of Young's, and the early LDS Church's, idealism and utopianism. Young and his Mormon pioneers believed "all aspects of life" were in need of reform, and the Deseret alphabet was just one of many ways they sought to bring about a complete "transformation in society".
Young also prescribed the learning of Deseret to the school system, stating "It will be the means of introducing uniformity in our orthography, and the years that are now required to learn to read and spell can be devoted to other studies."
During the alphabet's heyday between 1854 and 1869, books, newspapers, street signs and correspondence used the new letters, but despite heavy and costly promotion by the early LDS Church, the alphabet never enjoyed prolonged widespread use and has been regarded by historians as a failure.
The Deseret alphabet was developed primarily by a committee made up of the university's board of regents, members of which included church leaders Brigham Young, Parley P. Pratt and Heber C. Kimball. According to Brigham Young University professor Richard G. Moore, most scholars believe that George D. Watt's contribution to the actual form the alphabet took was the greatest; he furthermore "plant[ed] the idea of spelling reform in Brigham Young's mind" through a phonography class he gave after the death of Joseph Smith which Young attended. In addition, William W. Phelps helped "work out the letters" along with Pratt. Before they decided on the Deseret alphabet, the attention of the board of regents was mostly focused on Pitman style alphabets, and in April 1847 Brigham Young nearly purchased of lead type to print books using Pitman's orthography. The University of Deseret was incorporated on 28 February 1850; less than three weeks later, on 20 March, the new board of regents began to discuss spelling reform.
On 29 November 1853, the committee was ready to approve a slightly modified version of the Pitman orthography, when Willard Richards, who had been deathly ill and missed the debate before the vote, saw the proposed alphabet, which spelled the word "phonetic" as "fɷnetic". Richards was quick to condemn it, saying to the committee that in his view "those characters...seem like putting old wine into new bottles...I am inclined to think...we shall...throw away all characters that bear much resemblance to the English characters, and introduce an alphabet that is original."
These words persuaded Brigham Young and the rest of the committee, and Watt then endeavored to create an original alphabet. Less than two months later, on 19 January 1854, the board of regents finally approved the first 38-letter Deseret alphabet. One legacy of Pitman's orthography survived, though: the idea that one letter should equal one sound.
Upon the alphabet's acceptance, its first user was its principal architect, George D. Watt, who began writing the meeting minutes of the early Bishops in a cursive form of it in 1854. Almost immediately after its publication, church members began experimenting with it, and by 1855 travel writers Jules Remy and Julius Brenchley published a chart of the new alphabet which differed heavily from the 1854 version. Some early Mormons, such as Thales Hastings Haskell, began writing their personal journals in the new alphabet. Remy further reported that during his time in Salt Lake City, he saw signs on the street and above shops using the new alphabet.
After its approval by the board of regents, Brigham Young testified before the Utah territorial legislature that the new alphabet should "be thoroughly and extensively taught in all the schools." Some teaching in Utah schools did take place: John B. Milner taught the alphabet in Provo, Lehi, American Fork, and Pleasant Grove, while evening classes were taught in Salt Lake City and Farmington.
After several months' practice writing with the new alphabet, Watt wrote to Brigham Young that he was unhappy with it, and proposed a complete overhaul, which was never followed up on.
Word of the new alphabet soon spread outside Utah, and most press reports in non-Mormon papers were critical. Other writers, however, acquainted with other phonotypic and stenographic alphabets, ranged from neutral descriptions of the new alphabet to praise.
Until this point, all the printed material (mostly just charts of the alphabet and its standard orthography equivalents) had been produced with large wooden type, which was not suitable for printing at small sizes. Because the alphabet was wholly unique, no font existed, so in 1857 the board of regents appointed Erastus Snow to procure metal type from St. Louis-based font foundry Ladew & Peer. However, in May 1857 the Utah War began, and Snow left St. Louis to support the Mormon pioneers. During the war, Ladew & Peer kept working on the type, and the punches and matrices were delivered in the winter of 1858. The first use of the new type was to make a business card for George A. Smith, an early Mormon historian.
In 1859, with the new type in hand, the "Deseret News" began printing with it. It would print one piece per issue in the new alphabet, usually a quotation from "The Book of Mormon" or the New Testament. However, this only lasted for one year, after which the practice stopped; it would start again in May 1864 and stop permanently at the end of that year.Benn Pitman, the brother of Isaac Pitman, was also interested in spelling reform, and by 1864 had published his own orthography, which the board of regents considered adopting. However, they ultimately decided not to and used the opportunity to re-affirm their commitment to the Deseret alphabet.
Brigham Young blamed the failure of this first attempt at reform on the ugliness of the type developed by Ladew & Peer, and so he commissioned Russell's American Steam Printing House, a New York City based font foundry, to design more pleasing type. The result was the Bodoni-esque font (at right) that was used to print all of the books in this period. In an 1868 article, the "Deseret News" wrote that "the characters, to a person unaccustomed to them, may look strange, [but] to the eye to which they are familiar they are beautiful."
At least four books were published in the new alphabet, all transcribed by Orson Pratt and all using the Russell's House font: "The First Deseret Alphabet Reader" (1868), "The Second Deseret Alphabet Reader" (1868), "The Book of Mormon" (1869), and a "Book of Mormon" excerpt called "First Nephi–Omni" (1869).
Considerable non-printed material in the Deseret alphabet was made, including a replica headstone in Cedar City, Utah, some coinage, letters, diaries, and meeting minutes. One of the more curious items found in the Deseret alphabet is an English-Hopi dictionary prepared by two Mormon missionaries. It sat unappreciated in handwritten form at the LDS Church Archives until 2014, when Kenneth R. Beesley, a writing system researcher and computer scientist, noticed its significance and transcribed it into standard written English.
Despite years of heavy promotion, the Deseret alphabet was never widely adopted. This reluctance was partly due to prohibitive costs; the project had already cost the early church $20,000, with $6,000 going to Pratt as remuneration for his transcription effort and most of the rest going to cutting metal type featuring the new alphabet and printing costs. In 1859, Orson Pratt estimated that the cost of supplying all Utah Territory schoolchildren with suitable textbooks would be over US$5,000,000.
According to Beesley, many have written that interest in the Deseret alphabet died with Brigham Young. This, however, is not true; the alphabet was already regarded as a failure during Young's time. Only 500 copies of the full "Book of Mormon" translated into the Deseret alphabet sold for US$2 each, and even Young realized that the venture was too expensive and even the most devout Mormons could not be convinced to purchase and study the Deseret edition books over the books in the traditional orthography. In the winter of 1870, just one year after their publication, advertisements for the Deseret alphabet books were quietly removed from the "Deseret News".
Contemporary writers noted that thousands of copies of the US$0.15 and US$0.20 Deseret primers went unsold, and historian Roby Wentz speculated that the LDS Church at that time had a "cache" of the primers in mint condition, which it was slowly selling off; according to him, one such primer sold for $250 in 1978.
The Mormons had planned to use the profits from sale of the earlier books to fund printing of more books, and in anticipation Orson Pratt had already transcribed the complete Bible, "Doctrine and Covenants", and John Jaques's "Catechism for Children". Pratt had also prepared an apparent sequel to the primers, the "Deseret Phonetic Speller". After the sales failure, however, none of these books were ever published and were thought lost until being rediscovered in a storage area of the LDS Church Archives in Salt Lake City in May 1967.
Ralph Vigoda, a reporter for "The Philadelphia Inquirer", has speculated that the completion of the Transcontinental Railroad may have contributed to the alphabet's downfall: non-Mormons, not loyal to Brigham Young, became a large part of the city, and without the religious motivation it would be difficult indeed to get them to learn a new alphabet. In a retrospective piece, historian A. J. Simmonds claims that the new railroad doomed the alphabet. According to him, easy access to "the whole literature of the English speaking world" rendered the alphabet useless.
In July 1877, Young tried one more time at a spelling reform, ordering lead type designed for the orthography of Benn Pitman (Isaac's brother) with the intention of printing an edition of the "Book of Mormon" and "Doctrine and Covenants" using it. Most of the type had arrived by August, but with Young's death, the translation was never undertaken and the type never used. Young's death thus marked the end of the Mormon experimentation with English spelling reforms.
Modern digital typography has reduced the costs of typesetting substantially, especially for small print runs. As long as a freely licensed Deseret alphabet font and a font of the standard orthography have similar inked surface areas, printing a book in the Deseret alphabet using modern technology would have a similar cost as printing a book in the standard orthography.
Film director Trent Harris used the Deseret alphabet in his 1994 satire of Mormon theology, "Plan 10 from Outer Space", where it features as an alien language used on a mysterious "Plaque of Kolob".
During the 1996 Utah Centennial celebration, an activity book for children was distributed, within which one of the activities was for a child to write their own name in the alphabet. The book says that a child who does this will be "the first kid in 100 years to write [their] name in the Deseret alphabet!"
Also in 1996, "Buffalo River Press" published a reprint of the "Deseret First Book", of which only 10,000 were originally printed. The entire "Book of Mormon" in the Deseret alphabet has been likewise reprinted, as only 500 copies from the original print run exist, and they can sell on eBay for ≈$7,500 (as of 2004). In 1997, John Jenkins uploaded a free three part PDF of the so-called "triple combination", that is, a combined "Book of Mormon", "Doctrine and Covenants" and "Pearl of Great Price".
John Jenkins has gone on to publish many classic pieces of English literature in the Deseret alphabet, such as "Alice in Wonderland," "Pride and Prejudice", and "The Wonderful Wizard of Oz."
Owing to the character set's inclusion in Unicode, most of the original books and many of the original manuscripts have been transcribed into plain text, and, when this is not possible due to discrepancies between the Unicode reference glyphs and the documents, LaTeX.
 The first digital font for the Deseret alphabet, called "Deseret", was designed by Greg Kearney as part of work he was doing for the LDS Church History Department in 1991; the font was used in an exhibit that year. In August 1995, a cleaned up, digitized version of the font in use in the "Deseret Second Book" was created by Salt Lake City graphic designer Edward Bateman, who made the font in Fontographer while working on "Plan 10 from Outer Space".
Although it is not currently available in CTAN, Kenneth R. Beesley purports to have created a Metafont (and thus, LaTeX-compatible) font called in 2004.
All computers running Microsoft's Windows 7 operating system or newer can display the entire Deseret alphabet Unicode range as the glyphs are included in the Segoe UI Symbol font.
Besides maintaining a Deseret input method for Windows, Joshua Erickson, a UCLA alumnus, also maintains a large collection of freeware Unicode fonts for the alphabet, which he collectively terms the "Bee Fonts."
There also exist free software fonts for the Deseret alphabet. Google, through its Noto Sans project, the aim of which is "to support all languages with a harmonious look and feel", has also released a Deseret font under the name "Noto Sans Deseret". George Douros maintains a public domain font called "Analecta" as part of his Unicode Fonts for Ancient Scripts project, which supports the Coptic, Gothic, and Deseret scripts. Deseret glyphs are also available in the popular pan-Unicode fonts Code2001 and Everson Mono (as of version 5.1.5).
Although the Deseret alphabet has letter case, the only difference between the minuscule and majuscule forms is that the majuscule forms are larger.
A degree of free spelling is allowed to accommodate dialectal differences in English. For example, in the Deseret edition of "The Book of Mormon", the word "wherefore" is written as 𐐸𐐶𐐯𐑉𐑁𐐬𐑉 (), which means that the translator of the book did not exhibit the wine–whine merger. Those who do exhibit the merger might instead prefer the spelling 𐐶𐐯𐑉𐑁𐐬𐑉 to match the pronunciation (), or, depending on dialect, perhaps 𐐶𐐯𐑉𐑁𐐫𐑉 ().
The alphabet was designed to be able to write all of the vowels used in the dialect spoken in 19th century Utah. The vowel inventory has also been attributed to the fact that unlike other American pioneers, the Mormon pioneers were from New England as opposed to the American South. As such, many of the vowels in the Deseret alphabet have since merged in the modern era: they are no longer distinguished in many dialects of English.
Speakers who exhibit the father–bother merger no longer distinguish (𐐪) and (𐐱), and so both father and bother would be written with 𐐪: as 𐑁𐐪𐑄𐐲𐑉 and 𐐺𐐪𐑄𐐲𐑉 as opposed to 𐑁𐐪𐑄𐐲𐑉 and 𐐺𐐱𐑄𐐲𐑉. For those with the cot–caught merger, (𐐫) and (𐐱) are no longer distinguished: both cot and caught are thus written by them as 𐐿𐐱𐐻 () in the case of North American English and as 𐐿𐐫𐐻 () in the case of Scottish English. For those exhibiting both mergers, both would be written 𐐿𐐪𐐻 ().
There have been several published versions of the alphabet. Most versions (including the versions used in "The Deseret First Book", "The Deseret Second Book", "The Deseret News" and "The Book of Mormon") had only 38 letters, but some versions contained two ligatures, 𐐧 (ew) and 𐐦 (oi). In place of 𐐮𐐭 or 𐐷𐐭, 𐑏 was to be used; in place of 𐐱𐐮, 𐑎.
In the 23 February 1859 edition of the "Deseret News", the editors announced their approval of the two new letters and eventual intention to use them in the newsletter. However, due to the hot metal typesetting technology in use at the time, casting the new letters for use would have been a considerable expense, so it was never realized.
The Deseret alphabet does not have a distinct symbol for the mid central vowel (, "schwa"). The lack of a schwa has been cited as the biggest "phonological flaw" in the alphabet.
Because of the lack of a schwa, the author must write the sound that would be used if the word was stressed. For example, the word "enough" is commonly pronounced , but when it is stressed (as in a declaration of irritation) it is pronounced . The Deseret spelling of the word, 𐐨𐑌𐐲𐑁, reflects that stressed pronunciation. If does not have an inherent stressed value in a word, as is often the case before , then it is written as 𐐲.
Marion J. Shelton, an early Mormon missionary, proposed the addition of a new glyph to represent the schwa, a simple vertical line of the same height as other Deseret characters with a similar appearance to the Turkish dotless i (ı). The addition of this glyph did not catch on among his contemporaries, however, and no document outside of ones penned by Shelton makes use of it. Shelton used the new glyph in an 1860 letter to Brigham Young reporting on a recently completed mission to the Paiute people.
Each letter in the Deseret alphabet has a name, and when a letter is written on its own it has the value of that name. This allows some short words to be written with a single letter, and is called a letter's "syllabic value". The most common word in English, "the", is written simply 𐑄, as the letter's name is and that is the stressed pronunciation of the word. The consonants with syllabic values are 𐐶 (woo), 𐐷 (yee), 𐐸 (ha), 𐐹 (pee), 𐐺 (be/bee), 𐐻 (tee/tea), 𐐽 (qi), 𐐾 (gee), 𐑀 (gay), and 𐑄 (the/thee).
Syllabic values do not apply within words, although this was formerly the case. In early documents, Watt writes "people" as 𐐹𐐹𐑊 with the expectation that readers will interpret the first 𐐹 as , but the second 𐐹 as . This contextual value switching was soon done away with, so in later documents, while "bee" is written 𐐺, "bees" is written 𐐺𐐨𐑆.
In 40-letter versions of the alphabet which include the letter 𐐧 (ew) which represents , the letter 𐐧 when standing alone can be used to represent the word "you".
The first lesson in the "Deseret First Book" reads simply:
In the "Deseret Second Book", there is a version of "Twinkle, Twinkle, Little Star" on page 19:
There were two main handwritten forms of the Deseret alphabet: a cursive version and a printed version. Over the lifetime of the alphabet, the cursive form fell out of favor among most users of the alphabet and by 1856 no more cursive documents exist. Its impact on the glyphs can however still be plainly seen in the loops of certain characters such as 𐑅, 𐑀 and 𐐼. The earliest surviving versions of the Deseret alphabet, from 1853 (one year before its January 1854 approval), have printed and cursive forms side-by-side, suggesting that a cursive form was part of the plan from the very beginning.
The cursive form of the Deseret alphabet was mainly used by two people: George D. Watt, and James Henry Martineau. Watt, a stenographer, recorded several bishops meetings and wrote other personal documents in this cursive style. A chart of the cursive form appears below. The blue glyphs represent how to write each character, while the top row of printed glyphs shows the corresponding Unicode reference glyph.
The cursive style has many unorthodox characteristics uncommon to Western writing systems. Vowels can be dropped if the writer is in a hurry and feels the word is obvious as in an abjad, letters can be written above or below the base line depending on what precedes them, and 𐐮 is placed on letters after they are already written as in an abugida. Furthermore, unlike the typeset alphabet, the cursive alphabet has no letter case. These characteristics could have arisen because Watt was a local expert in Pitman shorthand, which is written in a similar way.
The table below shows some examples of how the cursive form is written. Dropped vowels are marked in parenthesis.
George D. Watt found his own alphabet cumbersome to write and abandoned it. As he wrote to Brigham Young on 21 August 1854:
His new alphabet closely resembled an 1853 publication of Isaac Pitman, containing only 33 letters. However, at this point, Young was still enamored with the original Deseret alphabet, and so he rejected the proposal and Watt continued to publicly promote the alphabet as part of his job despite his reservations.
After 1855, no more cursive documents appear, and all surviving journals are written in block letters. Marion J. Shelton, an early Mormon missionary who wrote a dictionary of the Hopi language in the alphabet, was a "typical" 40-letter Deseret writer, and his style of writing is shown below.
The Deseret alphabet was purposely designed so as to not have ascenders and descenders. This was envisioned as a practical benefit for the alphabet in an era of metal type: after many uses, the edges of type sorts become dull, and narrow ascenders and descenders are most prone to this effect.
While well intentioned, this lack has been described as a "catastrophic" mistake that makes type look "monotonous" and makes all words look alike. Some have drawn comparisons between the alphabet and the Old Turkic script, saying that writing in the new alphabet could be mistaken from afar as a Turkish tax list.
The Mormon pioneers were apparently aware of the problems caused by its monotony:
Other criticism of the design was harsher still. In an 18 December 1857 editorial in the "Boston Globe", the alphabet was described as being "so arranged and named as to cause the greatest possible annoyance to outsiders" and the design of the letters as "incomprehensible as [...] the hieroglyphics of the [...] Egyptians." On 4 March 1872, "The New York Times" called the alphabet "rude, awkward and cumbersome."
Some modern computer fonts and printed books have attempted to correct this perceived fault: in the books in John Jenkins' "Deseret Alphabet Classics" series, the font used adds a descender to 𐑉 and 𐐻 and an ascender to 𐐼 and 𐑇 among other tweaks.
Officially, the Deseret alphabet was created to simplify the spelling of English words for the benefit of children and English as a second language learners. Some of the alphabet's contemporaries, however, posited an alternative motivation for its development: increasing the isolation of the early Mormons.
The charge that the Deseret alphabet's main purpose was to keep outsiders ("gentiles" in LDS terminology) in the dark was brought almost immediately, as evidenced by the following 1858 "Lyttelton Times" reprint of an unnamed "New York newspaper":
Having obtained a copy of the "Deseret News" in 1859, the "Richmond Dispatch" disparaged it on April 25, writing "The "Deseret News" is filled with a lot of hieroglyphs. It seems to be [an alphabet] which the Mormons alone are to be taught."
Modern historians, however, doubt the veracity of this theory. For one thing, notes Kenneth R. Beesley, the "Deseret News" and every book published in the alphabet prominently feature the key to the alphabet, and anyone without a key could have gotten a copy of "A Journey to Great-Salt-Lake City", or traveled to Salt Lake City themselves and bought one. Contemporary scholars Richard F. Burton and Jules Remy also dismissed the secrecy argument, in 1860 and 1855 respectively.
With the impending completion of the Transcontinental Railroad, the Mormon pioneers would have easy, cheap access to publications from the east, including yellow-backs, penny dreadfuls, pulp magazines, and other often scandalous or dirty publications that were rising to prominence in the 19th century. Indeed, in an article about the benefits of the alphabet, the "Deseret News" proudly wrote:
In another article, the "Deseret News" cited an example of the kind of literature Mormons would benefit from not being able to read: "The Police Gazette". Historians A. J. Simmonds and Roby Wentz contend that while this may have been a tertiary goal of the alphabet, a sort of "happy accident", the main purpose of it was simple orthographic reform. Simmonds notes that the teaching of English to foreigners was not a mere hypothetical to mask isolationist tendencies: 35% of the Utah Territory's population at the time was Scandinavian, with German, Italian and Welsh speaking people also making up a considerable percentage of inhabitants; therefore, communication between the recently baptized and the community was a real problem.
The Deseret alphabet (U+10400–U+1044F) was added to the Unicode Standard in March 2001 with the release of version 3.1, after a request by John H. Jenkins of Apple, making it one of the first scripts to be added outside of the Basic Multilingual Plane. The letters 𐐧 (ew) and 𐐦 (oi) were added to the Unicode Standard in April 2003 with the release of version 4.0.
According to Kenneth R. Beesley, who submitted the proposal to expand the encoding, "Unicode fonts based on the current heterogeneous collection of glyphs will be useless for any practical typesetting of 40-letter Deseret Alphabet documents." This is because the Unicode Consortium chose to use glyphs from 1855 as the reference glyphs, while by 1859 those glyphs were already outmoded and replaced with newer glyphs. Beesley thus recommends using LaTeX along with his Metafont font to typeset Deseret text, but fonts which use the alternate glyphs for the two codepoints in question would also work for transcription of 40-letter Deseret texts written during and after 1859.
On 25 February 2016, the Library of Congress approved an ALA-LC romanization for the Deseret alphabet. The table can be used to display approximations of titles in non-Latin scripts using the Latin alphabet for use in library catalogs that do not support non-Latin alphabets.

</doc>
<doc id="8226" url="https://en.wikipedia.org/wiki?curid=8226" title="Danish">
Danish

Danish may refer to:

</doc>
<doc id="8227" url="https://en.wikipedia.org/wiki?curid=8227" title="Danish language">
Danish language

Danish (; "dansk" , "dansk sprog" ) is a North Germanic language spoken by about six million people, principally in Denmark, Greenland and in the region of Southern Schleswig in northern Germany, where it has minority language status. Also, minor Danish-speaking communities are found in Norway, Sweden, Spain, the United States, Canada, Brazil, and Argentina. Due to immigration and language shift in urban areas, about 15–20% of the population of Greenland speak Danish as their first language.
Along with the other North Germanic languages, Danish is a descendant of Old Norse, the common language of the Germanic peoples who lived in Scandinavia during the Viking Era. Danish, together with Swedish, derives from the East Norse dialect group, while the Middle Norwegian language before the influence of Danish and Norwegian Bokmål are classified as West Norse along with Faroese and Icelandic. A more recent classification based on mutual intelligibility separates modern spoken Danish, Norwegian, and Swedish as "mainland Scandinavian", while Icelandic and Faroese are classified as "insular Scandinavian". Although writing is compatible, spoken Danish is distinctly different from Norwegian and Swedish and thus the degree of mutual intelligibility with either is variable between regions and speakers.
Until the 16th century, Danish was a continuum of dialects spoken from Schleswig to Scania with no standard variety or spelling conventions. With the Protestant Reformation and the introduction of printing, a standard language was developed which was based on the educated Copenhagen dialect. It spread through use in the education system and administration, though German and Latin continued to be the most important written languages well into the 17th century. Following the loss of territory to Germany and Sweden, a nationalist movement adopted the language as a token of Danish identity, and the language experienced a strong surge in use and popularity, with major works of literature produced in the 18th and 19th centuries. Today, traditional Danish dialects have all but disappeared, though regional variants of the standard language exist. The main differences in language are between generations, with youth language being particularly innovative.
Danish has a very large vowel inventory consisting of 27 phonemically distinctive vowels, and its prosody is characterized by the distinctive phenomenon "stød", a kind of laryngeal phonation type. Due to the many pronunciation differences that set apart Danish from its neighboring languages, particularly the vowels, difficult prosody and "weakly" pronounced consonants, it is sometimes considered to be a "difficult language to learn, acquire and understand", and some evidence shows that children are slower to acquire the phonological distinctions of Danish compared to other languages. The grammar is moderately inflective with strong (irregular) and weak (regular) conjugations and inflections. Nouns and demonstrative pronouns distinguish common and neutral gender. Like English, Danish only has remnants of a former case system, particularly in the pronouns. Unlike English, it has lost all person marking on verbs. Its syntax is V2 word order, with the finite verb always occupying the second slot in the sentence.
Danish is a Germanic language of the North Germanic branch. Other names for this group are the Nordic or Scandinavian languages. Along with Swedish, Danish descends from the Eastern dialects of the Old Norse language; Danish and Swedish are also classified as East Scandinavian or East Nordic languages.
Scandinavian languages are often considered a dialect continuum, where no sharp dividing lines are seen between the different vernacular languages.
Like Norwegian and Swedish, Danish was significantly influenced by Low German in the Middle Ages, and has been influenced by English since the turn of the 20th century.
Danish itself can be divided into three main dialect areas: West Danish (Jutlandic), Insular Danish (including the standard variety), and East Danish (including Bornholmian and Scanian). Under the view that Scandinavian is a dialect continuum, East Danish can be considered intermediary between Danish and Swedish, while Scanian can be considered a Swedified East Danish dialect, and Bornholmsk is its closest relative. Contemporary Scanian is fully mutually intelligible with Swedish and less so with Danish since it shares a standardized vocabulary and less distinct prounciations with the rest of Sweden than in the past. Blekinge and Halland, the two other provinces further away from Copenhagen that transitioned to Sweden in the 17th century speak dialects more similar to standard Swedish.
Danish is largely mutually intelligible with Norwegian and Swedish. Proficient speakers of any of the three languages can often understand the others fairly well, though studies have shown that speakers of Norwegian generally understand both Danish and Swedish far better than Swedes or Danes understand each other. Both Swedes and Danes also understand Norwegian better than they understand each other's languages. The reason Norwegian occupies a middle position in terms of intelligibility is because of its shared border with Sweden resulting in a similarity in pronunciation, combined with the long tradition of having Danish as a written language which has led to similarities in vocabulary. Among younger Danes, Copenhageners are worse at understanding Swedish than Danes from the provinces. In general, younger Danes are not as good at understanding the neighboring languages as are Norwegian and Swedish youths.
The Danish philologist Johannes Brøndum-Nielsen divided the history of Danish into a period from 800 AD to 1525 to be "Old Danish", which he subdivided into "Runic Danish" (800-1100), Early Middle Danish (1100–1350) and Late Middle Danish (1350–1525).
By the eighth century, the common Germanic language of Scandinavia, Proto-Norse, had undergone some changes and evolved into Old Norse.
This language was generally called the "Danish tongue" ("Dǫnsk tunga"), or "Norse language" ("Norrœnt mál"). Norse was written in the runic alphabet, first with the elder futhark and from the 9th century with the younger futhark.
From the seventh century, the common Norse language began to undergo changes that did not spread to all of Scandinavia, resulting in the appearance of two dialect areas, Old West Norse (Norway and Iceland) and Old East Norse (Denmark and Sweden). Most of the changes separating East Norse from West Norse started as innovations in Denmark, that spread through Scania into Sweden and by maritime contact to southern Norway. A change that separated Old East Norse (Runic Swedish/Danish) from Old West Norse was the change of the diphthong "æi" (Old West Norse "ei") to the monophthong "e", as in "stæin" to "sten". This is reflected in runic inscriptions where the older read "stain" and the later "stin". Also, a change of "au" as in "dauðr" into "ø" as in "døðr" occurred. This change is shown in runic inscriptions as a change from "tauþr" into "tuþr". Moreover, the "øy" (Old West Norse "ey") diphthong changed into "ø", as well, as in the Old Norse word for "island". This monophthongization started in Jutland and spread eastward, having spread throughout Denmark and most of Sweden by 1100.
Through Danish conquest, Old East Norse was once widely spoken in the northeast counties of England. Many words derived from Norse, such as "gate" () for street, still survive in Yorkshire, the East Midlands and East Anglia, and parts of eastern England colonized by Danish Vikings. The city of York was once the Viking settlement of Jorvik. Several other English words derive from Old East Norse, for example "knife" (), "husband" (), and "egg" (). The suffix "-by" for 'town' is common in place names in Yorkshire and the east Midlands, for example Selby, Whitby, Derby, and Grimsby. The word "dale" meaning valley is common in Yorkshire and Derbyshire placenames.
In the medieval period, Danish emerged as a separate language from Swedish. The main written language was Latin, and the few Danish-language texts preserved from this period are written in the Latin alphabet, although the runic alphabet seems to have lingered in popular usage in some areas. The main text types written in this period are laws, which were formulated in the vernacular language to be accessible also to those who were not Latinate. The Jutlandic Law and Scanian Law were written in vernacular Danish in the early 13th century. Beginning in 1350, Danish began to be used as a language of administration, and new types of literature began to be written in the language, such as royal letters and testaments. The orthography in this period was not standardized nor was the spoken language, and the regional laws demonstrate the dialectal differences between the regions in which they were written.
Throughout this period, Danish was in contact with Low German, and many Low German loan words were introduced in this period. With the Protestant Reformation in 1536, Danish also became the language of religion, which sparked a new interest in using Danish as a literary language. Also in this period, Danish began to take on the linguistic traits that differentiate it from Swedish and Norwegian, such as the "stød", the voicing of many stop consonants, and the weakening of many final vowels to /e/.
The first printed book in Danish dates from 1495, the "Rimkrøniken" ("Rhyming Chronicle"), a history book told in rhymed verses. The first complete translation of the Bible in Danish, the Bible of Christian II translated by Christiern Pedersen, was published in 1550. Pedersen's orthographic choices set the "de facto" standard for subsequent writing in Danish.
Following the first Bible translation, the development of Danish as a written language, as a language of religion, administration, and public discourse accelerated. In the second half of the 17th century, grammarians elaborated grammars of Danish, first among them Rasmus Bartholin's 1657 Latin grammar "De studio lingvæ danicæ"; then Laurids Olufsen Kock's 1660 grammar of the Zealand dialect "Introductio ad lingvam Danicam puta selandicam"; and in 1685 the first Danish grammar written in Danish, "Den Danske Sprog-Kunst" ("The Art of the Danish Language") by Peder Syv. Major authors from this period are Thomas Kingo, poet and psalmist, and Leonora Christina Ulfeldt, whose novel "Jammersminde" ("Remembered Woes") is considered a literary masterpiece by scholars. Orthography was still not standardized and the principles for doing so were vigorously discussed among Danish philologists. The grammar of Jens Pedersen Høysgaard was the first to give a detailed analysis of Danish phonology and prosody, including a description of the "stød". In this period, scholars were also discussing whether it was best to "write as one speaks" or to "speak as one writes", including whether archaic grammatical forms that had fallen out of use in the vernacular, such as the plural form of verbs, should be conserved in writing (i.e. "han er" "he is" vs. "de ere" "they are").
The East Danish provinces were lost to Sweden after the Second Treaty of Brömsebro (1645) after which they were gradually Swedified; just as Norway was politically severed from Denmark, beginning also a gradual end of Danish influence on Norwegian (influence through the shared written standard language remained). With the introduction of absolutism in 1660, the Danish state was further integrated, and the language of the Danish chancellery, a Zealandic variety with German and French influence, became the "de facto" official standard language, especially in writing—this was the original so-called "rigsdansk" ("Danish of the Realm"). Also, beginning in the mid-18th century, the "skarre-R", the uvular R sound (), began spreading through Denmark, likely through influence from Parisian French and German. It affected all of the areas where Danish had been influential, including all of Denmark, Southern Sweden, and coastal southern Norway.
In the 18th century, Danish philology was advanced by Rasmus Rask, who pioneered the disciplines of comparative and historical linguistics, and wrote the first English-language grammar of Danish. Literary Danish continued to develop with the works of Ludvig Holberg, whose plays and historical and scientific works laid the foundation for the Danish literary canon. With the Danish colonization of Greenland by Hans Egede, Danish became the administrative and religious language there, while Iceland and the Faroe Islands had the status of Danish colonies with Danish as an official language until the mid-20th century.
Following the loss of Schleswig to Germany, a sharp influx of German speakers moved into the area, eventually outnumbering the Danish speakers. The political loss of territory sparked a period of intense nationalism in Denmark, coinciding with the so-called "Golden Age" of Danish culture. Authors such as N.F.S. Grundtvig emphasized the role of language in creating national belonging. Some of the most cherished Danish-language authors of this period are existential philosopher Søren Kierkegaard and prolific fairy tale author Hans Christian Andersen. The influence of popular literary role models, together with increased requirements of education did much to strengthen the Danish language, and also started a period of homogenization, whereby the Copenhagen standard language gradually displaced the regional vernacular languages. After the Schleswig referendum in 1920, a number of Danes remained as a minority within German territories. Throughout the 19th century, Danes emigrated, establishing small expatriate communities in the Americas, particularly in the US, Canada, and Argentina, where memory and some use of Danish remains today.
After the occupation of Denmark by Germany in World War II, the 1948 orthography reform dropped the German-influenced rule of capitalizing nouns, and introduced the letter Å/å. Three 20th-century Danish authors have become Nobel Prize laureates in Literature: Karl Gjellerup and Henrik Pontoppidan (joint recipients in 1917) and Johannes V. Jensen (awarded 1944).
With the exclusive use of "rigsdansk", the High Copenhagenian Standard, in national broadcasting, the traditional dialects came under increased pressure. In the 20th century, they have all but disappeared, and the standard language has extended throughout the country. Minor regional pronunciation variation of the standard language, sometimes called "regionssprog" ("regional languages") remain, and are in some cases vital. Today, the major varieties of Standard Danish are High Copenhagenian, associated with elderly, well to-do, and well educated people of the capital, and low-Copenhagenian traditionally associated with the working class, but today adopted as the prestige variety of the younger generations. Also, in the 21st century, the influence of immigration has had linguistic consequences, such as the emergence of a so-called multiethnolect in the urban areas, an immigrant Danish variety (also known as "Perkerdansk"), combining elements of different immigrant languages such as Arabic, Turkish, and Kurdish, as well as English and Danish.
Danish is the national language of Denmark and one of two official languages of the Faroe Islands (alongside Faroese). Until 2009, it had also been one of two official languages of Greenland (alongside Greenlandic). Danish is widely spoken in Greenland now as "lingua franca", and an unknown portion of the native Greenlandic population has Danish as their first language; a large percentage of the native Greenlandic population speaks Danish as a second language since its introduction into the education system as a compulsory language in 1928. Danish was an official language in Iceland until 1944, but is today still widely used and is a mandatory subject in school taught as a second foreign language after English, Iceland was a ruled territory of Denmark-Norway, where Danish was one of the official languages.
In addition, a noticeable community of Danish speakers is in Southern Schleswig, the portion of Germany bordering Denmark, where it is an officially recognized regional language, just as German is north of the border. Furthermore, Danish is one of the official languages of the European Union and one of the working languages of the Nordic Council. Under the Nordic Language Convention, Danish-speaking citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable for any interpretation or translation costs.
The more widespread of the two varieties of written Norwegian, "Bokmål", is very close to Danish, because standard Danish was used as the "de facto" administrative language until 1814 and one of the official languages of Denmark-Norway." Bokmål "is based on Danish, unlike the other variety of Norwegian, "Nynorsk", which is based on the Norwegian dialects, with Old Norwegian as an important reference point.
No law stipulates an official language for Denmark, making Danish the "de facto" language only. The Code of Civil Procedure does, however, lay down Danish as the language of the courts. Since 1997, public authorities have been obliged to observe the official spelling by way of the Orthography Law. In the 21st century, discussions have been held regarding creating a language law that would make Danish the official language of Denmark.
Standard Danish ("rigsdansk") is the language based on dialects spoken in and around the capital, Copenhagen. Unlike Swedish and Norwegian, Danish does not have more than one regional speech norm. More than 25% of all Danish speakers live in the metropolitan area of the capital, and most government agencies, institutions, and major businesses keep their main offices in Copenhagen, which has resulted in a very homogeneous national speech norm.
Danish dialects can be divided into the traditional dialects, which differ from modern Standard Danish in both phonology and grammar, and the Danish accents or regional languages, which are local varieties of the Standard language distinguished mostly by pronunciation and local vocabulary colored by traditional dialects. Traditional dialects are now mostly extinct in Denmark, with only the oldest generations still speaking them.
Danish traditional dialects are divided into three main dialect areas:
Jutlandic is further divided into Southern Jutlandic and Northern Jutlandic, with Northern Jutlandic subdivided into North Jutlandic and West Jutlandic. Insular Danish is divided into Zealand, Funen, Møn, and Lolland-Falster dialect areas―each with addition internal variation. The term "Eastern Danish" is occasionally used for Bornholmian, but including the dialects of Scania (particularly in a historical context)―Jutlandic dialect, Insular Danish, and Bornholmian. Bornholmian is the only Eastern Danish dialect spoken in Denmark, since the other Eastern Danish dialects were spoken in areas ceded to Sweden and subsequently swedified.
Traditional dialects differ in phonology, grammar, and vocabulary from standard Danish. Phonologically, one of the most diagnostic differences is the presence or absence of "stød". Four main regional variants for the realization of stød are known: In Southeastern Jutlandic, Southernmost Funen, Southern Langeland, and Ærø, no "stød" is used, but instead a pitch accent. South of a line ( "The Stød border") going through central South Jutland, crossing Southern Funen and central Langeland and north of Lolland-Falster, Møn, Southern Zealand and Bornholm neither "stød" nor pitch accent exists. Most of Jutland and on Zealand use" stød", and in Zealandic traditional dialects and regional language," stød" occurs more often than in the standard language. In Zealand, the "stød" line divides Southern Zealand (without "stød"), an area which used to be directly under the Crown, from the rest of the Island that used to be the property of various noble estates.
Grammatically, a dialectally significant feature is the number of grammatical genders. Standard Danish has two genders and the definite form of nouns is formed by the use of suffixes, while Western Jutlandic has only one gender and the definite form of nouns uses an article before the noun itself, in the same fashion as West Germanic languages. The Bornholmian dialect has maintained to this day many archaic features, such as a distinction between three grammatical genders. Insular Danish traditional dialects also conserved three grammatical genders. By 1900, Zealand insular dialects had been reduced to two genders under influence from the standard language, but other Insular varieties, such as Funen dialect had not. Besides using three genders, the old Insular or Funen dialect, could also use personal pronouns (like he and she) in certain cases, particularly referring to animals. A classic example in traditional Funen dialect is the sentence: "Katti, han får unger", literally "The cat, he is having kittens", because cat is a masculine noun, thus is referred to as "han" (he), even if it is female cat.
The sound system of Danish is unusual among the world's languages, particularly in its large vowel inventory and in the unusual prosody. In informal or rapid speech, the language is prone to considerable reduction of unstressed syllables, creating many vowel-less syllables with syllabic consonants, as well as reduction of final consonants. Furthermore, the language's prosody does not include many clues about the sentence structure, unlike many other languages, making it relatively more difficult to segment the speech flow into its constituent elements. These factors taken together make Danish pronunciation difficult to master for learners, and Danish children are indicated to take slightly longer in learning to segment speech in early childhood.
Although somewhat depending on analysis, most modern variants of Danish distinguish 12 long vowels, 13 short vowels, and two "schwa" vowels, and that only occur in unstressed syllables. This gives a total of 27 different vowel phonemes – a very large number among the world's languages. At least 19 different diphthongs also occur, all with a short first vowel and the second segment being either , , or . The table below shows the approximate distribution of the vowels as given by in Modern Standard Danish, with the symbols used in . Questions of analysis may give a slightly different inventory, for example based on whether r-colored vowels are considered distinct phonemes. gives 25 "full vowels", not counting the two unstressed "schwa" vowels.
The consonant inventory is comparatively simple. distinguishes 16 non-syllabic consonant phonemes in Danish.
Many of these phonemes have quite different allophones in onset and coda. Phonetically there is no voicing distinction among the stops, rather the distinction is one of aspiration and fortis vs. lenis. are aspirated in onset realized as , but not in coda. The pronunciation of "t", , is in between a simple aspirated and a fully affricated as has happened in German with many words that now contain "z". /v/ is pronounced as a [w] in syllable coda, so e.g. /grav/ ("grave") is pronounced .
The sound is found for example in the word /sjovˀ/ "fun" pronounced and /tjalˀ/ "marijuana" pronounced . Some analyses have posited it as a phoneme, but since it occurs only after or and doesn't occur after these phonemes, it can be analyzed as an allophone of , which is devoiced after voiceless alveolar frication. This makes it unnecessary to postulate a -phoneme in Danish.
In onset is realized as a uvu-pharyngeal approximant, , but in coda it is either realized as a non-syllabic low central vowel, or simply coalesces with the preceding vowel. The phenomenon is comparable to the "r" in German or in non-rhotic pronunciations of English. The Danish pronunciation of as a so-called skarre-r distinguishes the language from those varieties of Norwegian and Swedish that use trilled .
Danish is characterized by a prosodic feature called "stød" (lit. "thrust"). This is a form of laryngealization or creaky voice. Some sources have described it as a glottal stop, but this is a very infrequent realization, and today phoneticians consider it a phonation type or a prosodic phenomenon. It has phonemic status, since it serves as the sole distinguishing feature of words with different meanings in minimal pairs such as "bønder" ("peasants") with stød, versus "bønner" ("beans") without stød. The distribution of stød in the vocabulary is related to the distribution of the common Scandinavian pitch accents found in most dialects of Norwegian and Swedish.
Stress is phonemic and distinguishes words such as "billigst" "cheapest" and "bilist" "car driver".
Similarly to the case of English, modern Danish grammar is the result of a gradual change from a typical Indo-European dependent-marking pattern with a rich inflectional morphology and relatively free word order, to a mostly analytic pattern with little inflection, a fairly fixed SVO word order and a complex syntax. Some traits typical of Germanic languages persist in Danish, such as the distinction between irregularly inflected strong stems inflected through ablaut or umlaut (i.e. changing the vowel of the stem, as in the pairs "tager/tog" ("takes/took") and "fod/fødder" ("foot/feet")) and weak stems inflected through affixation (such as "elsker/elskede" "love/loved", "bil/biler" "car/cars"). Vestiges of the Germanic case and gender system are found in the pronoun system. Typically for an Indo-European language, Danish follows accusative morphosyntactic alignment. Danish distinguishes at least seven major word classes: verbs, nouns, numerals, adjectives, adverbs, articles, prepositions, conjunctions, interjections and ideophones.
Nouns are inflected for number (singular vs. plural) and definiteness, and are classified into two grammatical genders. Only pronouns inflect for case, and the previous genitive case has become an enclitic. A distinctive feature of the Nordic languages, including Danish, is that the definite articles, which also mark noun gender, have developed into suffixes. Typically of Germanic languages plurals are either irregular or "strong" stems inflected through umlaut (i.e. changing the vowel of the stem (e.g. "fod/fødder" "foot/feet", "mand/mænd" "man/men") or "weak" stems inflected through affixation (e.g. "skib/skibe" "ship/ships", "kvinde/kvinder" "woman/women").
Standard Danish has two nominal genders: "common" and "neuter"; the common gender arose as the historical feminine and masculine genders conflated into a single category. Some traditional dialects retain a three-way gender distinction, between masculine, feminine and neuter, and some dialects of Jutland have a masculine/feminine contrast. While the majority of Danish nouns (ca. 75%) have the "common" gender, and "neuter" is often used for inanimate objects, the genders of nouns are not generally predictable and must in most cases be memorized. The gender of a noun determines the form of adjectives that modify it, and the form of the definite suffixes. 
Definiteness is marked by two mutually exclusive articles, a preposed demonstrative article which occurs with nouns that are modified by an adjective or a postposed enclitic. Neuter nouns take the clitic "-et", and common gender nouns take "-en". Indefinite nouns take the articles "en" (common gender) or "et" (neuter). Hence, the common gender noun "en mand" "a man" (indefinite) has the definite form "manden" "the man", whereas the neuter noun "et hus" "a house" (indefinite) has the definite form, "the house" (definite) "huset". 
Indefinite:
Definite with enclitic article:
Definite with preposed demonstrative article:
The plural definite ending is "-(e)ne" (e.g. "drenge" "boys > "drengene" "the boys" and "piger" "girls" > "pigerne" "the girls"), and nouns ending in -"ere" lose the last "-e" before adding the -ne suffix (e.g. "danskere" "Danes" > "danskerne" "the Danes"). When the noun is modified by an adjective, the definiteness is marked by the definite article "den" (common) or "det" (neuter) and the definite/plural form of the adjective: "den store mand" "the big man", "det store hus" "the big house".
There are three different types of regular plurals: Class 1 forms the plural with the suffix -"er" (indefinite) and -"erne" (definite), Class 2 with the suffix "-e" (indefinite) and -"ene" (definite.), and Class 3 takes no suffix for the plural indefinite form and -"ene" for the plural definite.
Most irregular nouns take an ablaut plural (with a change in the stem vowel), or combine ablaut stem-change with the suffix, and some have unique plural forms. Unique forms may be inherited (e.g. the plural of "øje" "eye", which is the old dual form "øjne"), or for loan words they may be borrowed from the donor language (e.g. the word "konto" "account" which is borrowed from Italian and uses the Italian masculine plural form "konti" "accounts").
Possessive phrases are formed with the enclitic -"s", for example "min fars hus" "my father's house" where the noun "far" carries the possessive enclitic. This is however not an example of genitive case marking, because in the case of longer noun phrases the -s attaches to the last word in the phrase, which need not be the head-noun or even a noun at all. For example, the phrases "kongen af Danmark's bolsjefabrik" "the king of Denmark's candy factory", or "det er pigen Uffe bor sammen meds datter" "that is the daughter of the girl that Uffe lives with", where the enclitic attaches to a stranded preposition.
As does English, the Danish pronominal system retains a distinction between subjective and oblique case. The subjective case form of pronouns is used when pronouns occur as grammatical subject of a sentence, and oblique forms are used for all non-subjective occurrences including accusative, dative, predicative, comparative and other types of constructions. The third person singular pronouns also distinguish between and animate masculine ("han" "he"), animate feminine ("hun" "she") forms, as well as inanimate neuter ("det" "it") and inanimate common gender ("den" "it"). 
Possessive pronouns have independent and adjectival forms. The adjectival form is used immediately preceding the possessed noun ("det er min hest" "it is my horse"), whereas the independent possessive pronoun is used in place of the possessed noun ("den er min" "it is mine"). In the third person singular "sin" is used when the owner is also the subject of the sentence, whereas "hans" ("his"), "hendes" (her) and "dens/dets" "its" is used when the owner is different from the grammatical subject.
Like all Germanic languages, Danish forms compound nouns. These are represented in Danish orthography as one word, as in "kvindehåndboldlandsholdet", "the female national handball team". In some cases, nouns are joined with an extra "s", originally possessive in function, like "landsmand" (from "land", "country", and "mand", "man", meaning "compatriot"), but "landmand" (from same roots, meaning "farmer"). Some words are joined with an extra "e", like "gæstebog" (from "gæst" and "bog", meaning "guest book").
Danish verbs are morphologically simple, marking very few grammatical categories. They do not mark person or number of subject, although the marking of plural subjects was still used in writing as late as the 19th century. Verbs have a past, non-past and infinitive form, past and present participle forms, and a passive, and an imperative.
Verbs can be divided into two main classes, the strong/irregular verbs and the regular/weak verbs. The regular verbs are also divided into two classes, those that take the past suffix -"te" and those that take the suffix -"ede".
The infinitive always ends in a vowel, usually -e (pronounced ), infinitive forms are preceded by the article "at" (pronounced ). The non-past or present tense takes the suffix -"r", except for a few strong verbs that have irregular non-past forms. The past form does not necessarily mark past tense, but also counterfactuality or conditionality, and the non-past has many uses besides present tense time reference.
The present participle ends in -"ende" (e.g. "løbende" "running"), and the past participle ends in -"et" (e.g. "løbet" "run"), "-t" (e.g. købt "bought"). Perfect tense is constructed with "at have" ("to have") and participial forms, like in English. But some transitive verbs can also form an imperfective perfect using "at være" ("to be") instead.
The passive form takes the suffix -s: "avisen læses hver dag" ("the newspaper is read every day"). Another passive construction uses the auxiliary verb "at blive" "to become": "avisen bliver læst hver dag".
The imperative mood is formed from the infinitive by removing the final schwa-vowel:
Danish basic constituent order in simple sentences with both a subject and an object is Subject–Verb–Object. However, Danish is also a V2 language, which means that the verb must always be the second constituent of the sentence. Following the Danish grammarian Paul Diderichsen Danish grammar tends to be analyzed as consisting of slots or fields, and in which certain types of sentence material can be moved to the pre-verbal (or "grounding") field to achieve different pragmatic effects. Usually the sentence material occupying the preverbal slot has to be pragmatically marked, usually either new information or topics. There is no rule that subjects must occur in the preverbal slot, but since subject and topic often coincide, they often do. Therefore, whenever any sentence material that is not the subject occurs in the preverbal position the subject is demoted to postverbal position and the sentence order becomes VSO.
but
When there is no pragmatically marked constituents in the sentence to take the preverbal slot (for example when all the information is new), the slot has to take a dummy subject "der".
 describes the basic order of sentence constituents in main clauses as comprising the following 8 positions:
Position 0 is not part of the sentence and can only contain sentential connectors (such as conjunctions or interjections). Position 1 can contain any sentence constituent. Position 2 can only contain the main verb. Position 3 is the subject position, unless the subject is fronted to occur in position 1. Position 4 can only contain light adverbs and the negation. Position 5 is for non-finite verbs, such as auxiliaries. Position 6 is the position of direct and indirect objects, and position 7 is for heavy adverbial constituents.
Questions with wh-words are formed differently from yes/no questions. In wh-questions the question word occupies the preverbal field, regardless of whether its grammatical role is subject or object or adverbial. In yes/no questions the preverbal field is empty, so that the sentence begins with the verb.
Wh-question:
In subordinate clauses, the syntax differs from that of main clauses. In the subordinate clause structure the verb is preceded by the subject and any light adverbial material (e.g. negation). Complement clauses begin with the particle "at" in the "connector field".
Relative clauses are marked by the relative articles "som" or "der" which occupy the preverbal slot:
About 2 000 of Danish non-compound words are derived from the Old Norse language, and ultimately from Proto Indo-European. Of these 2 000 words, 1 200 are nouns, 500 are verbs, 180 are adjectives and the rest belong to other word classes. Danish has also absorbed a large number of loan words, most of which were borrowed from Middle Low German in the late medieval period. Out of the 500 most frequently used words in Danish, 100 are medieval loans from Middle Low German, as Low German is the other official language of Denmark-Norway. In the 17th and 18th centuries, standard German and French superseded Low German influence and in the 20th century English became the main supplier of loan words, especially after World War II. Although many old Nordic words remain, some were replaced with borrowed synonyms, as can be seen with "æde" (to eat) which became less common when the Low German "spise" came into fashion. As well as loan words, new words are freely formed by compounding existing words. In standard texts of contemporary Danish, Middle Low German loans account for about 16‒17% of the vocabulary, Graeco-Latin-loans 4‒8%, French 2‒4% and English about 1%.
Danish and English are both Germanic languages, Danish a North Germanic language descended from Old Norse and English a West Germanic language descended from Old English, and Old Norse exerted a strong influence on Old English in the early medieval period. To see their shared Germanic heritage, one merely has to note the many common words that are very similar in the two languages. For example, commonly used Danish nouns and prepositions such as "have", "over", "under", "for", "give", "flag," "salt," and "kat" are easily recognizable in their written form to English speakers. Similarly, some other words are almost identical to their Scots equivalents, e.g., "kirke" (Scots "kirk", i.e., 'church') or "barn" (Scots "bairn", i.e. 'child'). In addition, the word "by", meaning "village" or "town", occurs in many English place-names, such as "Whitby" and "Selby", as remnants of the Viking occupation. During the latter period, English adopted "are", the third person plural form of the verb "to be", as well as the corresponding personal pronoun form "they" from contemporary Old Norse.
In the word forms of numbers above 20, the units are stated before the tens, so 21 is rendered "enogtyve", literally "one and twenty".
The numeral "halvanden" means 1½ (literally "half second", implying "one plus half of the second one"). The numerals "halvtredje" (2½), "halvfjerde" (3½) and "halvfemte" (4½) are obsolete, but still implicitly used in the vigesimal system described below. Similarly, the temporal designation ("klokken")" halv tre", literally "half three (o'clock)", is half past two.
One peculiar feature of the Danish language is the fact that numerals 50, 60, 70, 80 and 90 are (as are the French numerals from 80 through 99) based on a vigesimal system, meaning that the score (20) is used as a base unit in counting. "Tres" (short for "tre-sinds-tyve", "three times twenty") means 60, while 50 is "halvtreds" (short for "halvtredje-sinds-tyve", "half third times twenty", implying two score plus half of the third score). The ending "sindstyve" meaning "times twenty" is no longer included in cardinal numbers, but may still be used in ordinal numbers. Thus, in modern Danish fifty-two is usually rendered as "tooghalvtreds" from the now obsolete "tooghalvtredsindstyve", whereas 52nd is either "tooghalvtredsende" or "tooghalvtredsindstyvende". Twenty is "tyve" (derived from old Danish "tiughu", a haplology of "tuttiughu", meaning 'two tens'), while thirty is "tredive" (Old Danish "þrjatiughu", "three tens"), and forty is "fyrre" (Old Danish "fyritiughu", "four tens", still used today as the archaism "fyrretyve"). Thus, the suffix "-tyve" should be understood as a plural of "ti" (10), though to modern Danes "tyve" means 20, making it hard to explain why "fyrretyve" is 40 (four tens) and not 80 (four times twenty).
For large numbers (one billion or larger), Danish uses the long scale, so that the short-scale billion (1,000,000,000) is called "milliard", and the short-scale trillion (1,000,000,000,000) is "billion".
The oldest preserved examples of written Danish (from the Iron and Viking Ages) are in the Runic alphabet. The introduction of Christianity also brought the Latin script to Denmark, and at the end of the High Middle Ages Runes had more or less been replaced by Latin letters.
Danish orthography is conservative, using most of the conventions established in the 16th century. The spoken language however has changed a lot since then, creating a gap between the spoken and written languages.
The modern Danish alphabet is similar to the English one, with three additional letters: "æ", "ø", and "å", which come at the end of the alphabet, in that order. The letters c, q, w, x and z are only used in loan words. A spelling reform in 1948 introduced the letter "å", already in use in Norwegian and Swedish, into the Danish alphabet to replace the digraph "aa". The old usage continues to occur in some personal and geographical names; for example, the name of the city of "Aalborg" is spelled with Aa following a decision by the City Council in the 1970s and "Aarhus" decided to go back to Aa in 2011. When representing the "å" sound, "aa" is treated like "å" in alphabetical sorting, though it appears to be two letters. When the letters are not available due to technical limitations, they are often replaced by "ae" (Æ, æ), "oe" or "o" (Ø, ø), and "aa" (Å, å), respectively.
The same spelling reform changed the spelling of a few common words, such as the past tense "vilde" (would), "kunde" (could) and "skulde" (should), to their current forms of "ville", "kunne" and "skulle" (making them identical to the infinitives in writing, as they are in speech). Modern Danish and Norwegian use the same alphabet, though spelling differs slightly, particularly with the phonetic spelling of loanwords; for example the spelling of "station" and "garage" in Danish remains identical to other languages, whereas in Norwegian, they are transliterated as "stasjon" and "garasje".
Realm languages:
Nordic languages:

</doc>
<doc id="8228" url="https://en.wikipedia.org/wiki?curid=8228" title="Decade (Neil Young album)">
Decade (Neil Young album)

Decade is a compilation album by Canadian-American musician Neil Young, originally released in 1977 as a triple album, now available on two compact discs. It contains 35 of Young's songs recorded between 1966 and 1976, among them five tracks that had been unreleased up to that point. It peaked at No. 43 on the "Billboard" Top Pop Albums chart, and was certified platinum by the RIAA in 1986.
Compiled by Young himself, with his hand-written liner notes about each track, "Decade" represents almost every album from his career and various affiliations through 1977 with the exception of "4 Way Street" and "Time Fades Away". Of the previously unreleased songs, "Down to the Wire" features the New Orleans pianist Dr. John with Buffalo Springfield on an item from their shelved "Stampede" album; "Love Is a Rose" was a minor hit for Linda Ronstadt in 1975; "Winterlong" received a cover by Pixies on the Neil Young tribute album from 1989, ""; and "Campaigner" is a Young song critical of Richard Nixon. The track "Long May You Run" is a different mix to that found on the album of the same name, featuring the harmonies of the full Crosby Stills & Nash before David Crosby and Graham Nash left the recording sessions.
For many years, "Decade" was the only Neil Young compilation album available. A 1993 compilation called "Lucky Thirteen" was released, but it only covered Young's 1982–1988 output. It was not until 2004 that Reprise Records released a single-disc retrospective of his best-known tracks, titled "Greatest Hits". Throughout the 1980s and '90s, Young promised fans a follow-up to the original "Decade" collection, provisionally titled "Decade II"; eventually, this idea was scrapped in favor of a much more comprehensive anthology to be titled "Archives", spanning his entire career and ranging in size from a box set to an entire series of audio and/or video releases. The first release of archival material since "Decade" and "Lucky Thirteen" would appear in 2006, "Live at the Fillmore East", a recording from a 1970 concert featuring Crazy Horse with Danny Whitten. Several other archival live releases followed, and in 2009 the first of several planned multi-disc box sets, "The Archives Vol. 1 1963–1972", was issued. In April 2017 "Decade" was reissued on vinyl as a limited-edition Record Store Day release, with remastered vinyl and CD editions planned for general release in June 2017.
Initially, "Decade" was to be released in 1976, but was pulled at the last minute by Young. It was shelved until the following year, where it appeared with two songs removed from the original track list (a live version of "Don't Cry No Tears" recorded in Japan in 1976, and a live version of "Pushed It Over the End" recorded in 1974). Also removed were the following comments on those two songs and "Time Fades Away", from Young's handwritten liner notes:
The album has been lauded in many quarters as one of the best examples of a career retrospective for a rock artist, and as a template for the box set collections that would follow in the 1980s and beyond. However, in the original article on Young from the first edition of the "Rolling Stone Illustrated History of Rock and Roll" and a subsequent article in the "1983 Rolling Stone Record Guide", critic Dave Marsh used this album to accuse Young of deliberately manufacturing a self-mythology, arguing that while his highlights could be seen to place him on a level with other artists from his generation like Bob Dylan or The Beatles, the particulars of his catalogue did not bear this out. The magazine has since excised the article from subsequent editions of the "Illustrated History" book.
All songs written by Neil Young.
Album

</doc>
<doc id="8230" url="https://en.wikipedia.org/wiki?curid=8230" title="Demeter">
Demeter

In ancient Greek religion and mythology, Demeter (; Attic: "Dēmḗtēr" ; Doric: "Dāmā́tēr") is the Olympian goddess of the harvest and agriculture, presiding over grains and the fertility of the earth. Her cult titles include Sito (), "she of the Grain", as the giver of food or grain, and Thesmophoros (, "thesmos": divine order, unwritten law; , "phoros": bringer, bearer), "Law-Bringer", as a mark of the civilized existence of agricultural society.
Though Demeter is often described simply as the goddess of the harvest, she presided also over the sacred law, and the cycle of life and death. She and her daughter Persephone were the central figures of the Eleusinian Mysteries, a religious tradition that predated the Olympian pantheon, and which may have its roots in the Mycenaean period c. 1400–1200 BC. Demeter was often considered to be the same figure as the Anatolian goddess Cybele, and she was identified with the Roman goddess Ceres.
It is possible that Demeter appears in Linear A as "da-ma-te" on three documents (AR Zf 1 and 2, and KY Za 2), all three apparently dedicated in religious situations and all three bearing just the name ("i-da-ma-te" on AR Zf 1 and 2). It is unlikely that Demeter appears as "da-ma-te" in a Linear B (Mycenean Greek) inscription (PY En 609); the word , "da-ma-te", probably refers to "households". On the other hand, , "si-to-po-ti-ni-ja", "Potnia of the Grain", is regarded as referring to her Bronze Age predecessor or to one of her epithets.
Demeter's character as mother-goddess is identified in the second element of her name "meter" () derived from Proto-Indo-European (PIE) "*méh₂tēr" (mother). In antiquity, different explanations were already proffered for the first element of her name. It is possible that "Da" (), a word which corresponds to "Gē" () in Attic, is the Doric form of "De" (), "earth", the old name of the chthonic earth-goddess, and that Demeter is "Mother-Earth". Liddell & Scott find this "improbable" and Beekes writes, "there is no indication that ["da"] means "earth", although it has also been assumed in the name of Poseidon found in the Linear B inscription "E-ne-si-da-o-ne", "earth-shaker". John Chadwick also argues that the "dā" element in the name of Demeter is not so simply equated with "earth".
M. L. West has proposed that Demeter, initially "Damater", could be a borrowing from an Illyrian deity attested in the Messapic goddess "Damatura", with a form "dā-" ("earth", from PIE "*dʰǵʰ(e)m-") attached to -"matura" ("mother"), akin to the Illyrian god Dei-paturos ("dei-", "sky", attached to -"paturos," "father"). The Lesbian form "Dō-" may simply reflect a different dialectal pronunciation of the non-Greek name. The element "De"- may also be connected with "Deo", an epithet of Demeter could derive from the Cretan word "dea" (), Ionic "zeia" ()—variously identified with emmer, spelt, rye, or other grains by modern scholars—so that she is the Mother and the giver of food generally. This view is shared by British scholar Jane Ellen Harrison, who suggests that Démeter's name means "Grain-Mother", instead of "Earth-Mother".
Wanax ("wa-na-ka") was her male companion (Greek: Πάρεδρος, "Paredros") in Mycenaean cult. The Arcadian cult links her to the god Poseidon, who probably substituted the male companion of the Great Goddess ; Demeter may therefore be related to a Minoan Great Goddess (Cybele).
An alternative Proto-Indo-European etymology comes through Potnia and Despoina, where "Des-" represents a derivative of PIE "*dem" (house, dome), and Demeter is "mother of the house" (from PIE "*dems-méh₂tēr"). R. S. P. Beekes rejects a Greek interpretation, but not necessarily an Indo-European one.
Demeter was frequently associated with images of the harvest, including flowers, fruit, and grain. She was also sometimes pictured with her daughter Persephone. Demeter is not generally portrayed with any of her consorts; the exception is Iasion, the youth of Crete who lay with her in a thrice-ploughed field, and was sacrificed afterwards by a jealous Zeus with a thunderbolt.
Demeter is assigned the zodiac constellation Virgo the Virgin by Marcus Manilius in his 1st century Roman work Astronomicon. In art, constellation Virgo holds Spica, a sheaf of wheat in her hand and sits beside constellation Leo the Lion. 
In Arcadia, she was known as "Black Demeter". She was said to have taken the form of a mare to escape the pursuit of her younger brother, Poseidon, and having been raped by him despite her disguise, dressed all in black and retreated into a cave to mourn and to purify herself. She was consequently depicted with the head of a horse in this region. A sculpture of the Black Demeter was made by Onatas.
In epic poetry and Hesiod's "Theogony", Demeter is the Corn-Mother, the goddess of cereals who provides grain for bread and blesses its harvesters. This was her main function at Eleusis, and became panhellenic. In Cyprus, "grain-harvesting" was "damatrizein". The main theme in the Eleusinian mysteries was the reunion of Persephone with her mother Demeter, when new crops were reunited with the old seed, a form of eternity.
According to the Athenian rhetorician Isocrates, Demeter's greatest gifts to humankind were agriculture, particularly of cereals, and the Mysteries which give the initiate higher hopes in this life and the afterlife. These two gifts were intimately connected in Demeter's myths and mystery cults. In Hesiod, prayers to Zeus-Chthonios (chthonic Zeus) and Demeter help the crops grow full and strong. Demeter's emblem is the poppy, a bright red flower that grows among the barley.
Demeter was also "zeidoros arοura", the Homeric "Mother Earth arοura" who gave the gift of cereals ("zeai" or "deai").
In addition to her role as an agricultural goddess, Demeter was often worshipped more generally as a goddess of the earth. In Arcadia, she was represented as snake-haired, holding a dove and dolphin, perhaps to symbolize her power over the underworld, the air, and the water. In the cult of Flya, she was worshiped as "Anesidora", one who sends up gifts from the underworld. There was a temple of Demeter under this name in Phlius in Attica. In Sparta, she was known as Demeter-Chthonia (chthonic Demeter). The Athenians called the dead "Demetrioi", and this may reflect a link between Demeter and ancient cult of the dead, linked to the agrarian-belief that a new life would sprout from the dead body, as a new plant arises from buried seed. This was probably a belief shared by initiates in Demeter's mysteries, as interpreted by Pindar: "Happy is he who has seen what exists under the earth, because he knows not only the end of life, but also his beginning that the Gods will give".
In the mysteries of Pheneos in Arcadia, Demeter was known as "Kidaria". Her priest would put on the mask of Demeter, which was kept in a secret place. The cult may have been connected with both the underworld and a form of agrarian magic.
Theocritus described one of Demeter's earlier roles as that of a goddess of poppies:
Karl Kerenyi asserted that poppies were connected with a Cretan cult which was eventually carried to the Eleusinian mysteries in Classical Greece. In a clay statuette from Gazi (Heraklion Museum, Kereny 1976 fig 15), the Minoan poppy goddess wears the seed capsules, sources of nourishment and narcosis, in her diadem. According to Kernyi, "It seems probable that the Great Mother Goddess who bore the names Rhea and Demeter, brought the poppy with her from her Cretan cult to Eleusis and it is almost certain that in the Cretan cult sphere opium was prepared from poppies." Robert Graves speculated that the meaning of the depiction and use of poppies in the Greco-Roman myths is the symbolism of the bright scarlet color as signifying the promise of resurrection after death.
Demeter's epithets show her many religious functions. She was the "Corn-Mother" who blesses the harvesters. Some cults interpreted her as "Mother-Earth". Demeter may be linked to goddess-cults of Minoan Crete, and embody aspects of a pre-Hellenic Mother Goddess. It is possible that the title "Mistress of the labyrinth", which appears in a Linear B inscription, belonged originally to "Sito" ("[she] of the grain"), the Great Mother Demeter and that in the Eleusinian mysteries this title was kept by her daughter Persephone (Kore or Despoina). However, there is no evidence that the name of Potnia in Eleusis was originally Demeter. Her other epithets include:
Demeter might also be invoked in the guises of:
The earliest recorded worship of a deity possibly equivalent to Demeter is found in Linear B Mycenean Greek tablets of c. 1400–1200 BC found at Pylos. The tablets describe worship of the "two queens and the king", which may be related to Demeter, Persephone and Poseidon. An early name which may refer to Demeter, "si-to-po-ti-ni-ja" (Sito Potnia), appears in Linear B inscriptions found at Mycenae and Pylos. In Crete, Poseidon was often given the title "wa-na-ka" ("wanax") in Linear B inscriptions, in his role as king of the underworld, and his title " E-ne-si-da-o-ne" indicates his chthonic nature. In the cave of Amnisos, "Enesidaon" is associated with the cult of Eileithyia, the goddess of childbirth, who was involved with the annual birth of the divine child. During the Bronze Age, a goddess of nature dominated both in Minoan and Mycenean cults, and "Wanax" ("wa-na-ka") was her male companion (paredros) in the Mycenean cult. Elements of this early form of worship survived in the Eleusinian cult, where the following words were uttered: "Mighty Potnia bore a strong son."
Tablets from Pylos record sacrificial goods destined for "the Two Queens and Poseidon" ("to the Two Queens and the King" :"wa-na-ssoi", "wa-na-ka-te"). The "Two Queens" may be related with Demeter and Persephone, or their precursors, goddesses who were no longer associated with Poseidon in later periods.
Major cults to Demeter are known at Eleusis in Attica, Hermion (in Crete), Megara, Celeae, Lerna, Aegila, Munychia, Corinth, Delos, Priene, Akragas, Iasos, Pergamon, Selinus, Tegea, Thoricus, Dion (in Macedonia) Lykosoura, Mesembria, Enna (Sicily), and Samothrace.
An ancient Amphictyony, probably the earliest centred on the cult of Demeter at Anthele (Ἀνθήλη), which lay on the coast of Malis south of Thessaly. This was the locality of Thermopylae.
After the "First Sacred War", the Anthelan body was known thenceforth as the Delphic Amphictyony
Demeter of Mysia had a seven-day festival at Pellené in Arcadia. Pausanias passed the shrine to Demeter at Mysia on the road from Mycenae to Argos but all he could draw out to explain the archaic name was a myth of an eponymous Mysius who venerated Demeter.
Demeter's two major festivals were sacred mysteries. Her Thesmophoria festival (11–13 October) was women-only. Her Eleusinian mysteries were open to initiates of any gender or social class. At the heart of both festivals were myths concerning Demeter as Mother and Persephone as her daughter.
In the Roman period, Demeter became conflated with the Roman agricultural goddess Ceres under the Interpretatio graeca. The worship of Demeter was formally merged with that of Ceres around 205 BC, along with the "ritus graecia cereris", a Greek-inspired form of cult, as part of Rome's general religious recruitment of deities as allies against Carthage, towards the end of the Second Punic War. The cult originated in southern Italy (part of Magna Graecia) and was probably based on the Thesmophoria, a mystery cult dedicated to Demeter and Persephone as "Mother and Maiden". It arrived along with its Greek priestesses, who were granted Roman citizenship so that they could pray to the gods "with a foreign and external knowledge, but with a domestic and civil intention". The new cult was installed in the already ancient Temple of Ceres, Liber and Libera, Rome's Aventine patrons of the plebs; from the end of the 3rd century BC, Demeter's temple at Enna, in Sicily, was acknowledged as Ceres' oldest, most authoritative cult center, and Libera was recognized as Proserpina, Roman equivalent to Persephone. Their joint cult recalls Demeter's search for Persephone, after the latter's abduction into the underworld by Hades (or Pluto). At the Aventine, the new cult took its place alongside the old. It made no reference to Liber, whose open and gender-mixed cult continued to play a central role in plebeian culture, as a patron and protector of plebeian rights, freedoms and values. The exclusively female initiates and priestesses of the new "greek style" mysteries of Ceres and Proserpina were expected to uphold Rome's traditional, patrician-dominated social hierarchy and traditional morality. Unmarried girls should emulate the chastity of Proserpina, the maiden; married women should seek to emulate Ceres, the devoted and fruitful Mother. Their rites were intended to secure a good harvest, and increase the fertility of those who partook in the mysteries.
Beginning in the 5th century BCE in Asia Minor, Demeter was also considered equivalent to the Phrygian goddess Cybele. Demeter's festival of Thesmophoria was popular throughout Asia Minor, and the myth of Persephone and Adonis in many ways mirrors the myth of Cybele and Attis.
Some late antique sources syncretized several "great goddess" figures into a single deity. The Platonist philosopher Apuleius, writing in the late 2nd century, identified Ceres (Demeter) with Isis, having her declare:
"I, mother of the universe, mistress of all the elements, first-born of the ages, highest of the gods, queen of the shades, first of those who dwell in heaven, representing in one shape all gods and goddesses. My will controls the shining heights of heaven, the health-giving sea-winds, and the mournful silences of hell; the entire world worships my single godhead in a thousand shapes, with divers rites, and under many a different name. The Phrygians, first-born of mankind, call me the Pessinuntian Mother of the gods; ... the ancient Eleusinians Actaean Ceres; ... and the Egyptians who excel in ancient learning, honour me with the worship which is truly mine and call me by my true name: Queen Isis."
Some of the earliest accounts of Demeter's relationships to other deities comes from Hesiod's "Theogeny", written c. 700 BC. In it, Demeter is described as the daughter of Cronus and Rhea.
Demeter's most well-known relationship is with her daughter, Persephone, queen of the underworld. Both Homer and Hesiod described Persephone as the daughter of Zeus and his older sister, Demeter, though no myths exist describing her conception or birth. The exception is a fragment of the lost Orphic theogony, which preserves part of a myth in which Zeus mates with his mother, Rhea, in the form of a snake, explaining the origin of the symbol on Hermes' staff. Their daughter is said to be Persephone, whom Zeus in turn mates with to conceive Dionysus. According to the Orphic fragments, "After becoming the mother of Zeus, she who was formerly Rhea became Demeter."
Before her abduction by Hades, Persephone was known as Kore ("maiden"), and there is some evidence that the figures of Persephone Queen of the Underworld and Kore daughter of Demeter were originally considered separate goddesses. However, they must have become conflated with each other by the time of Hesiod in the 7th century BC. Demeter and Persephone were often worshiped together and were often referred to by joint cultic titles. In their cult at Eleusis, they were referred to simply as "the goddesses", often distinguished as "the older" and "the younger"; in Rhodes and Sparta, they were worshiped as "the Demeters"; in the Thesmophoria, they were known as "the thesmophoroi" ("the legislators").
In Arcadia they were known as "the Great Goddesses" and "the mistresses". In Mycenaean Pylos, Demeter and Persephone were probably called the "queens" (wa-na-ssoi).
Both Homer and Hesiod, writing c. 700 BC, described the agricultural hero Iasion as a consort of Demeter. According to Hesiod, they had intercourse in a ploughed furrow. Demeter subsequently gave birth to two sons, Philomelus and Ploutos.
According to Diodorus Siculus, in his "Bibliotheca historica" written in the 1st century BC, Demeter and her husband Zeus were also the parents of Dionysus. Diodorus described the myth of Dionysus' double birth (once from the earth, i.e. Demeter, when the plant sprouts) and once from the vine (when the fruit sprouts from the plant). Diodorus also related a version of the myth of Dionysus' destruction by the Titans ("sons of Gaia"), who boiled him, and how Demeter gathered up his remains so that he could be born a third time (Diod. iii.62). Diodorus states that Dionysus' birth from Zeus and his older sister Demeter was somewhat of a minority belief, possibly via conflation of Demeter with her daughter, as most sources state that the parents of Dionysus were Zeus and Persephone, and later Zeus and Semele.
In Arcadia, a major Arcadian deity known as Despoina was said to be the daughter of Demeter and Poseidon. According to Pausanias, the myths told that during her search for Persephone, Poseidon pursued her. Demeter turned into a horse in order to avoid her younger brother's advances, but he turned into a stallion and mated with the goddess, resulting in the birth of the horse god Arion. Pausinias stated that some traditions held that the offspring of Poseidon and his older sister, Demeter, was not a horse but in fact (or, in addition,) the Despoina, "whose name they are not wont to divulge to the uninitiated".
Demeter's daughter Persephone was abducted to the underworld by Hades, who received permission from her father Zeus to take her as his bride. Demeter searched for her ceaselessly, preoccupied with her loss and her grief. The seasons halted; living things ceased their growth, then began to die. Faced with the extinction of all life on earth, Zeus sent his messenger Hermes to the underworld to bring Persephone back. Hades agreed to release her if she had eaten nothing while in his realm; but Persephone had eaten a small number of pomegranate seeds. This bound her to Hades and the underworld for certain months of every year, either the dry Mediterranean summer, when plant life is threatened by drought, or the autumn and winter. There are several variations on the basic myth; the earliest account, the "Homeric Hymn to Demeter", relates that Persephone is secretly slipped a pomegranate seed by Hades and in Ovid's version, Persephone willingly and secretly eats the pomegranate seeds, thinking to deceive Hades, but is discovered and made to stay. Contrary to popular perception, Persephone's time in the underworld does not correspond with the unfruitful seasons of the ancient Greek calendar, nor her return to the upper world with springtime. Demeter's descent to retrieve Persephone from the underworld is connected to the Eleusinian Mysteries.
The myth of the capture of Persephone seems to be pre-Greek. In the Greek version, Ploutos (πλούτος, wealth) represents the wealth of the corn that was stored in underground silos or ceramic jars ("pithoi"). Similar subterranean "pithoi" were used in ancient times for funerary practices. At the beginning of the autumn, when the corn of the old crop is laid on the fields, she ascends and is reunited with her mother Demeter, for at this time the old crop and the new meet each other.
According to the personal mythology of Robert Graves, Persephone is not only the younger self of Demeter, she is in turn also one of three guises of the Triple Goddess – Kore (the youngest, the maiden, signifying green young grain), Persephone (in the middle, the nymph, signifying the ripe grain waiting to be harvested), and Hecate (the eldest of the three, the crone, the harvested grain), which to a certain extent reduces the name and role of Demeter to that of group name. Before her abduction, she is called Kore; and once taken she becomes Persephone ('she who brings destruction').
Demeter's search for her daughter Persephone took her to the palace of Celeus, the King of Eleusis in Attica. She assumed the form of an old woman, and asked him for shelter. He took her in, to nurse Demophon and Triptolemus, his sons by Metanira. To reward his kindness, she planned to make Demophon immortal; she secretly anointed the boy with ambrosia and laid him in the flames of the hearth, to gradually burn away his mortal self. But Metanira walked in, saw her son in the fire and screamed in fright. Demeter abandoned the attempt. Instead, she taught Triptolemus the secrets of agriculture, and he in turn taught them to any who wished to learn them. Thus, humanity learned how to plant, grow and harvest grain. The myth has several versions; some are linked to figures such as Eleusis, Rarus and Trochilus. The Demophon element may be based on an earlier folk tale.
Homer's "Odyssey" (c. late 8th century BC) contains perhaps the earliest direct references to the myth of Demeter and her consort Iasion, a Samothracian hero whose name may refer to bindweed, a small white flower that frequently grows in wheat fields. In the "Odyssey", Calypso describes how Demeter, "without disguise", made love to Iasion. "So it was when Demeter of the braided tresses followed her heart and lay in love with Iasion in the triple-furrowed field; Zeus was aware of it soon enough and hurled the bright thunderbolt and killed him." In ancient Greek culture, part of the opening of each agricultural year involved the cutting of three furrows in the field to ensure its fertility.
Hesiod expanded on the basics of this myth. According to him, the liaison between Demeter and Iasion took place at the wedding of Cadmus and Harmonia in Crete. Demeter, in this version, had lured Iasion away from the other revelers. Hesiod says that Demeter subsequently gave birth to two sons, Philomelus and Ploutos.
In Arcadia, located in what is now southern Greece, the major goddess Despoina was considered the daughter of Demeter and Poseidon "Hippios", Horse-Poseidon. In the associated myths, Poseidon represents the river spirit of the underworld, and he appears as a horse as often happens in northern European folklore. The myth describes how he pursued his older sister, Demeter, who hid from him among the horses of King Onkios, but even in the form of a mare, she could not conceal her divinity. In the form of a stallion, Poseidon caught and raped his older sister. Demeter was furious at Poseidon's assault; in this furious form, she became known as "Demeter Erinys". Her anger at Poseidon drove her to dress all in black and retreat into a cave in order to purify herself, an act which was the cause of a universal famine. Demeter's absence caused the death of crops, of livestock, and eventually of the people who depended on them (later Arcadian tradition held that it was "both" her rage at Poseidon and her loss of her daughter that caused the famine, merging the two myths). Demeter washed away her anger in the River Ladon, becoming "Demeter Lousia", the "bathed Demeter".
"In her alliance with Poseidon," Karl Kerenyi noted, "she was Earth, who bears plants and beasts, and could therefore assume the shape of an ear of grain or a mare." She bore a daughter Despoina (: the "Mistress"), whose name should not be uttered outside the Arcadian Mysteries, and a horse named Arion, with a black mane and tail.
At Phigaleia, a "xoanon" (wood-carved statue) of Demeter was erected in a cave which, tradition held, was the cave into which Black Demeter retreated. The statue depicted a Medusa-like figure with a horse's head and snake-like hair, holding a dove and a dolphin, which probably represented her power over air and water:
Another myth involving Demeter's rage resulting in famine is that of Erysichthon, king of Thessaly. The myth tells of Erysichthon ordering all of the trees in one of Demeter's sacred groves to be cut down. One tree, a huge oak, was found to be covered with votive wreaths, symbols of the prayers Demeter had granted, and so Erysichthon's men refused to cut it down. The king used an axe to cut it down himself, killing a dryad nymph in the process. The nymph's dying words were a curse on Erysichthon. Demeter punished the king by calling upon Limos, the spirit of unrelenting and insatiable hunger, to enter his stomach. The more the king ate, the hungrier he became. Erysichthon sold all his possessions to buy food, but was still hungry. Finally, he sold his own daughter, Mestra, into slavery. Mestra was freed from slavery by her former lover, Poseidon, who gave her the gift of shape-shifting into any creature at will to escape her bonds. Erysichthon used her shape-shifting ability to sell her numerous times to make more money to feed himself, but no amount of food was enough. Eventually, Erysichthon ate himself.

</doc>
<doc id="8233" url="https://en.wikipedia.org/wiki?curid=8233" title="Death metal">
Death metal

Death metal is an extreme subgenre of heavy metal music. It typically employs heavily distorted and low-tuned guitars, played with techniques such as palm muting and tremolo picking, deep growling vocals, aggressive, powerful drumming featuring double kick and blast beat techniques, minor keys or atonality, abrupt tempo, key, and time signature changes, and chromatic chord progressions. The lyrical themes of death metal may include slasher film-style violence, political conflict, religion, nature, philosophy, and science fiction.
Building from the musical structure of thrash metal and early black metal, death metal emerged during the mid-1980s. Bands such as Venom, Celtic Frost, Slayer, and Kreator were important influences on the genre's creation. Possessed, Death, Necrophagia, Obituary, Autopsy, and Morbid Angel are often considered pioneers of the genre. In the late 1980s and early 1990s, death metal gained more media attention as popular genre. Niche record labels like Combat, Earache, and Roadrunner began to sign death metal bands at a rapid rate.
Since then, death metal has diversified, spawning several subgenres. Melodic death metal combines death metal elements with those of the new wave of British heavy metal. Technical death metal is a complex style, with uncommon time signatures, atypical rhythms, and unusual harmonies and melodies. Death-doom combines the deep growled vocals and double-kick drumming of death metal with the slow tempos and melancholic atmosphere of doom metal. Deathgrind, goregrind, and pornogrind mix the complexity of death metal with the intensity, speed, and brevity of grindcore. Deathcore combines death metal with metalcore traits. Death 'n' roll combines death metal's growled vocals and highly distorted, detuned guitar riffs with elements of 1970s hard rock and heavy metal.
English extreme metal band Venom, from Newcastle, crystallized the elements of what later became known as thrash metal, death metal and black metal, with their first two albums "Welcome to Hell" and "Black Metal". Their dark, blistering sound, harsh vocals, and macabre, proudly Satanic imagery proved a major inspiration for extreme metal bands. Another highly influential band, Slayer, formed in 1981. Although the band was a thrash metal act, Slayer's music was more violent than their thrash contemporaries Metallica, Megadeth, and Anthrax. Their breakneck speed and instrumental prowess combined with lyrics about death, violence, war, and Satanism won Slayer a cult following. According to Mike McPadden, "Hell Awaits", Slayer's sophomore album, "largely invent[ed] much of the sound and fury that would evolve into death metal." According to AllMusic, their third album "Reign in Blood" inspired the entire death metal genre. It had a big impact on genre leaders such as Death, Obituary, and Morbid Angel.
Possessed, a band that formed in the San Francisco Bay Area during 1983, is described by Allmusic as "connecting the dots" between thrash metal and death metal with their 1985 debut album, "Seven Churches". While attributed as having a Slayer influence, current and former members of the band had actually cited Venom and Motörhead, as well as early work by Exodus, as the main influences on their sound. Although the group had released only two studio albums and an EP in their formative years, they have been described by music journalists and musicians as either being "monumental" in developing the death metal style, or as being the first death metal band. Earache Records noted that "the likes of Trey Azagthoth and Morbid Angel based what they were doing in their formative years on the Possessed blueprint laid down on the legendary "Seven Churches" recording. Possessed arguably did more to further the cause of 'Death Metal' than any of the early acts on the scene back in the mid-late 80's."
During the same period as the dawn of Possessed, a second influential metal band was formed in Orlando, Florida: Death. Originally called Mantas, Death was formed in 1983 by Chuck Schuldiner, Kam Lee, and Rick Rozz. In 1984, they released their first demo entitled "Death by Metal", followed by several more. The tapes circulated through the tape trader world, quickly establishing the band's name. With Death guitarist Schuldiner adopting vocal duties, the band made a major impact on the scene. The fast minor-key riffs and solos were complemented with fast drumming, creating a style that would catch on in tape trading circles. Schuldiner has been credited by Allmusic's Eduardo Rivadavia for being widely recognized as the "Father of Death Metal". Death's 1987 debut release, "Scream Bloody Gore", has been described by About.com's Chad Bowar as being the "evolution from thrash metal to death metal", and "the first true death metal record" by the "San Francisco Chronicle". In an Interview Jeff Becerra talked about the discussions of being the creator of the genre, saying that Schuldiner cited Possessed as a massive influence, and Death were even called "Possessed clones" early on. Along with Possessed and Death, other pioneers of death metal in the United States include Macabre, Master, Massacre, Immolation, Cannibal Corpse, Obituary, and Post Mortem.
By 1989, many bands had been signed by eager record labels wanting to cash in on the subgenre, including Florida's Obituary, Morbid Angel and Deicide. This collective of death metal bands hailing from Florida are often labeled as "Florida death metal". Morbid Angel pushed the genre's limits both musically and lyrically, with the release of their debut album "Altars of Madness" in 1989. The album "redefined what it meant to be heavy while influencing an upcoming class of brutal death metal."
Death metal spread to Sweden in the late 1980s, flourishing with pioneers such as Carnage, God Macabre, Entombed, Dismember, Grave and Unleashed. In the early 1990s, the rise of melodic death metal was recognized, with Swedish bands such as Dark Tranquillity, At the Gates, and In Flames.
Following the original death metal innovators, new subgenres began by the end of the decade. British band Napalm Death became increasingly associated with death metal, in particular, on their 1990 album "Harmony Corruption". This album displays aggressive and fairly technical guitar riffing, complex rhythmics, a sophisticated growling vocal delivery by Mark "Barney" Greenway, and socially aware lyrical subjects, merging death metal with the "grindcore" subgenre. Other bands contributing significantly to this early movement include Britain's Bolt Thrower and Carcass and New York's Suffocation.
To close the circle, Death released their fourth album "Human" in 1991. Death's founder Schuldiner helped push the boundaries of uncompromising speed and technical virtuosity, mixing technical and intricate rhythm guitar work with complex arrangements and emotive guitar solos.
Earache Records, Relativity Records and Roadrunner Records became the genre's most important labels, with Earache releasing albums by Carcass, Napalm Death, Morbid Angel, and Entombed, and Roadrunner releasing albums by Obituary, and Pestilence. Although these labels had not been death metal labels, initially, they became the genre's flagship labels in the beginning of the 1990s. In addition to these, other labels formed as well, such as Nuclear Blast, Century Media, and Peaceville. Many of these labels would go on to achieve successes in other genres of metal throughout the 1990s.
In September 1990, Death's manager Eric Greif held one of the first North American death metal festivals, "Day of Death", in Milwaukee suburb Waukesha, Wisconsin, and featured 26 bands including Autopsy, Broken Hope, Hellwitch, Obliveon, Revenant, Viogression, Immolation, Atheist, and Cynic.
Death metal's popularity achieved its initial peak during 1992–1993, with some bands such as Morbid Angel and Cannibal Corpse enjoying mild commercial success. However, the genre as a whole never broke into the mainstream. The genre's mounting popularity may have been partly responsible for a strong rivalry between Norwegian black metal and Swedish death metal scenes. Fenriz of Darkthrone has noted that Norwegian black metal musicians were "fed up with the whole death metal scene" at the time. Death metal diversified in the 1990s, spawning a rich variety of subgenres that still have a large "underground" following at the present.
The setup most frequently used within the death metal genre is two guitarists, a bass player, a vocalist and a drummer often using "hyper double-bass blast beats". Although this is the standard setup, bands have been known to occasionally incorporate other instruments such as electronic keyboards. The genre is often identified by fast, heavily distorted and low tuned guitars, played with techniques such as palm muting and tremolo picking. The percussion is usually aggressive and powerful.
Death metal is known for its abrupt tempo, key, and time signature changes. Death metal may include chromatic chord progressions and a varied song structure. In some circumstances, the style will incorporate melodic riffs and harmonies for effect. This incorporation of melody and harmonious playing was even further used in the creation of melodic death metal. These compositions tend to emphasize an ongoing development of themes and motifs.
Death metal vocals are referred to as death growls; hoarse roars/snarls. Death growling is mistakenly thought to be a form of screaming using the lowest vocal register known as vocal fry, however vocal fry is actually a form of overtone screaming, and while growling can be performed this way by experienced vocalists who use the fry screaming technique, "true" death growling is in fact created by an altogether different technique. The three major methods of harsh vocalization used in the genre are often mistaken for each other, encompassing vocal fry screaming, false chord screaming, and "true" death growls. Growling is sometimes also referred to as Cookie Monster vocals, tongue-in-cheek, due to the vocal similarity to the voice of the popular "Sesame Street" character of the same name. Although often criticized, death growls serve the aesthetic purpose of matching death metal's aggressive lyrical content. High-pitched screaming is occasionally utilized in death metal, being heard in songs by Death, Aborted, Exhumed, Dying Fetus, Cannibal Corpse, and Deicide.
The lyrical themes of death metal may invoke slasher film-stylised violence, but may also extend to topics like religion (sometimes including Satanism), occultism, Lovecraftian horror, nature, mysticism, mythology, theology, philosophy, science fiction, and politics. Although violence may be explored in various other genres as well, death metal may elaborate on the details of extreme acts, including psychopathy, delirium, mutilation, mutation, dissection, exorcism, torture, rape, cannibalism, and necrophilia. Sociologist Keith Kahn-Harris commented this apparent glamorisation of violence may be attributed to a "fascination" with the human body that all people share to some degree, a fascination that mixes desire and disgust. Heavy metal author Gavin Baddeley also stated there does seem to be a connection between "how acquainted one is with their own mortality" and "how much they crave images of death and violence" via the media. Additionally, contributing artists to the genre often defend death metal as little more than an extreme form of art and entertainment, similar to horror films in the motion picture industry. This explanation has brought such musicians under fire from activists internationally, who claim that this is often lost on a large number of adolescents, who are left with the glamorisation of such violence without social context or awareness of why such imagery is stimulating.
According to Alex Webster, bassist of Cannibal Corpse, "The gory lyrics are probably not, as much as people say, [what's keeping us] from being mainstream. Like, 'death metal would never go into the mainstream because the lyrics are too gory?' I think it's really the music, because violent entertainment is totally mainstream."
The most popular theory of the subgenre's christening is Possessed's 1984 demo, "Death Metal"; the song from the eponymous demo would also be featured on the band's 1985 debut album, "Seven Churches". Possessed vocalist/bassist Jeff Becerra said he coined the term in early 1983 for a high school English class assignment. Another possible origin was a magazine called "Death Metal", started by Thomas Fischer and Martin Ain of Hellhammer and Celtic Frost. The name was later given to the 1984 compilation "Death Metal" released by Noise Records. The term might also have originated from other recordings, such as the demo released by Death in 1984, called "Death by Metal".
Cited examples are not necessarily exclusive to one particular style. Many bands can easily be placed in two or more of the following categories, and a band's specific categorization is often a source of contention due to personal opinion and interpretation.
Blackened death-doom is a genre that combines the slow tempos and monolithic drumming of doom metal, the complex and loud riffage of death metal and the shrieking vocals of black metal. Examples of blackened death-doom bands include Morast, Faustcoven, The Ruins of Beverast, Bölzer, Necros Christos, Harvest Gulgaltha, Dragged into Sunlight, Hands of Thieves, and Soulburn.
Blackened death metal is commonly death metal that incorporates musical, lyrical or ideological elements of black metal, such as an increased use of tremolo picking, anti-Christian or Satanic lyrical themes and chord progressions similar to those used in black metal. Blackened death metal bands are also more likely to wear corpse paint and suits of armour, than bands from other styles of death metal. Lower range guitar tunings, death growls and abrupt tempo changes are common in the genre. Examples of blackened death metal bands are Belphegor, Behemoth, Akercocke, and Sacramentum.
Melodic black-death (also known as blackened melodic death metal or melodic blackened death metal) is a genre of extreme metal that describes the style created when melodic death metal bands began being inspired by black metal and European romanticism. However, unlike most other black metal, this take on the genre would incorporate an increased sense of melody and narrative. Some bands who have played this style include Dissection, Sacramentum, Embraced, Naglfar, Satariel, Throes of Dawn, Obscurity, Dawn, Cries of the Past-era Underoath, Catamenia, Midvinter, Twin Obscenity, Nokturnal Mortum Unanimated, Epoch of Unlight, This Ending, Suidakra, Oathean, Thulcandra, Skeletonwitch, and Cardinal Sin.
War metal (also known as war black metal or bestial black metal) is an aggressive, cacophonous and chaotic subgenre of blackened death metal, described by "Rock Hard" journalist Wolf-Rüdiger Mühlmann as "rabid" and "hammering". Important influences include first wave black metal band Sodom, first wave black metal/death metal band Possessed as well as old grindcore, black and death metal bands like Repulsion, Autopsy, Sarcófago and the first two Sepultura releases. War metal bands include Blasphemy, Archgoat, Impiety, In Battle, Beherit, Crimson Thorn, Bestial Warlust, and Zyklon-B.
Brutal death metal is a subgenre of death metal that privileges heaviness, speed, and complex rhythms over other aspects, such as melody and timbres. Brutal death metal bands employ high-speed, palm-muted power chording and single-note riffage. Notable bands include Cannibal Corpse, Dying Fetus, Suffocation and Skinless.
Death-doom is a style that combines the slow tempos and pessimistic atmosphere of doom metal with the deep growling vocals and double-kick drumming of death metal.<ref name="Doom Metal Special: Doom/Death Terrorizer #142">"Doom Metal Special: Doom/Death", "Terrorizer #142".</ref> Influenced mostly by the early work of Hellhammer and Celtic Frost, the style emerged during the late 1980s and gained a certain amount of popularity during the 1990s. Death-doom was also pioneered by bands such as Winter, Disembowelment, Paradise Lost, Autopsy, Anathema, and My Dying Bride.
Funeral doom is a genre that crosses death-doom with funeral dirge music. It is played at a very slow tempo, and places an emphasis on evoking a sense of emptiness and despair. Typically, electric guitars are heavily distorted and dark ambient aspects such as keyboards or synthesizers are often used to create a "dreamlike" atmosphere. Vocals consist of mournful chants or growls and are often in the background. Funeral doom was pioneered by Mournful Congregation (Australia), Esoteric (United Kingdom), Evoken (United States), Funeral (Norway), Thergothon (Finland), and Skepticism (Finland).
Death 'n' roll is a style that combines death metal's growled vocals and highly distorted detuned guitar riffs along with elements of 1970s hard rock and heavy metal. Notable examples include Entombed, Gorefest, and Six Feet Under.
With the rise in popularity of metalcore, some of its traits have been combined with death metal. Bands such as Suicide Silence, Carnifex and Salt the Wound combine death metal with a variance of metalcore elements. Characteristics of death metal, such as fast drumming (including blast beats), down-tuned guitars, tremolo picking, growled vocals, and high-pitched shrieks are combined with the breakdowns of metalcore. "Decibel" magazine stated that "one of Suffocation's trademarks, breakdowns, has spawned an entire metal subgenre: deathcore."
Goregrind, deathgrind and pornogrind are styles that mix the intensity, speed, and brevity of grindcore with the complexity of death metal, with goregrind focused on themes like gore and forensic pathology, and pornogrind dealing with sexual and pornographic themes. Some notable examples of these genres are Brujeria, Cattle Decapitation, Cephalic Carnage, Pig Destroyer, Circle of Dead Children, Rotten Sound, Gut, and Cock and Ball Torture.
Deathrash, also known as death-thrash, is a shorthand term to describe bands who play a fusion of death metal and thrash metal. The genre gained notoriety in Bali, Indonesia, where it attracted criticism of being related to the accelerated tourism development on the island and the superseding of its local culture, particularly by Jakartan one. Notable bands include Grave, Mortification, The Crown, Incapacity, Darkane, Deathchain, and Sepultura.
Industrial death metal is a genre of death metal that adds elements of industrial music. Some notable bands include Fear Factory, Anaal Nathrakh, Autokrator, and Meathook Seed.
Swedish death metal could be considered the forerunner of "melodic death metal". Melodic death metal, sometimes referred to as "melodeath", is heavy metal mixed with some death metal elements and is heavily influenced by the new wave of British heavy metal. Unlike most other death metal, melodeath usually features screams instead of growls, slower tempos, much more melody and even clean vocals are heard at rare times. Carcass is sometimes credited with releasing the first melodic death metal album with 1993's "Heartwork", although Swedish bands In Flames, Dark Tranquillity, and At the Gates are usually mentioned as the main pioneers of the genre and of the Gothenburg metal sound.
Slam death metal (also referred to simply as "slam") is a microgenre that evolved from the 1990s New York death metal scene, incorporating elements of hardcore punk. In contrast to other death metal styles, it is not generally focused on guitar solos and blast beats; instead it employs mid-tempo rhythms, breakdowns, palm-muted riffing, as well as hip hop-inspired vocal and drum beat rhythms. Notable acts include Devourment, Cephalotripsy, and Abominable Putridity.
Symphonic death metal is a genre of death metal that add elements of classical music. Bands described as symphonic death metal include Fleshgod Apocalypse, Septicflesh, Necronomicon, and Children of Bodom. Haggard's 2000 album, "Awaking the Centuries", has been described as death metal-styled symphonic metal.
Technical death metal (also known as tech-death, progressive death metal, or prog-death) is a subgenre of death metal that employs dynamic song structures, uncommon time signatures, atypical rhythms and unusual harmonies and melodies. Bands described as technical death metal or progressive death metal usually fuse common death metal aesthetics with elements of progressive rock, jazz or classical music. While the term technical death metal is sometimes used to describe bands that focus on speed and extremity as well as complexity, the line between progressive and technical death metal is thin. "Tech death" and "prog death", for short, are terms commonly applied to such bands as Nile, Edge of Sanity, and Opeth. Necrophagist and Spawn of Possession are known for a classical music-influenced death metal style. Death metal pioneers Death also refined their style in a more progressive direction in their final years. Some albums for this subgenre are "Hallucinations" (1990) by the German band Atrocity and Death's "Human" (1991). This style has significantly influenced many bands, creating a stream that in Europe was carried out at first by bands such as Gory Blister and Electrocution. The Polish band Decapitated gained recognition as one of Europe's primary modern technical death metal acts.

</doc>
<doc id="8237" url="https://en.wikipedia.org/wiki?curid=8237" title="Don Quixote">
Don Quixote

The Ingenious Gentleman Don Quixote of La Mancha (Modern Spanish: ("in Part 2," caballero) , ), or just (, ; ), is a Spanish novel by Miguel de Cervantes. It was published in two parts, in 1605 and 1615. A founding work of Western literature, it is often labeled "the first modern novel" and many authors consider it to be the best literary work ever written.
The plot revolves around the adventures of a noble (hidalgo) from La Mancha named Alonso Quixano, who reads so many chivalric romances that he loses his mind and decides to become a knight-errant ("caballero andante") to revive chivalry and serve his nation, under the name "Don Quixote de la Mancha". He recruits a simple farmer, Sancho Panza, as his squire, who often employs a unique, earthy wit in dealing with Don Quixote's rhetorical monologues on knighthood, already considered old-fashioned at the time. Don Quixote, in the first part of the book, does not see the world for what it is and prefers to imagine that he is living out a knightly story.
The book had a major influence on the literary community, as evidenced by direct references in Alexandre Dumas' "The Three Musketeers" (1844), Mark Twain's "Adventures of Huckleberry Finn" (1884), and Edmond Rostand's "Cyrano de Bergerac" (1897), as well as the word "quixotic" and the epithet "Lothario"; the latter refers to a character in "El curioso impertinente" ("The Impertinently Curious Man"), an intercalated story that appears in Part One, chapters 33–35. The 19th-century German philosopher Arthur Schopenhauer cited "Don Quixote" as one of the four greatest novels ever written.
When first published, "Don Quixote" was usually interpreted as a comic novel. After the French Revolution, it was better known for its central ethic that individuals can be right while society is quite wrong and seen as disenchanting. In the 19th century, it was seen as a social commentary, but no one could easily tell "whose side Cervantes was on". Many critics came to view the work as a tragedy in which Don Quixote's idealism and nobility are viewed by the post-chivalric world as insane, and are defeated and rendered useless by common reality. By the 20th century, the novel had come to occupy a canonical space as one of the foundations of modern literature.
Cervantes wrote that the first chapters were taken from "the archives of La Mancha", and the rest were translated from an Arabic text by the Moorish author Cide Hamete Benengeli. This metafictional trick appears to give a greater credibility to the text, implying that Don Quixote is a real character and that the events related truly occurred several decades prior to the recording of this account. However, it was also common practice in that era for fictional works to make some pretense of being factual, such as the common opening line of fairy tales "Once upon a time in a land far away...".
In the course of their travels, the protagonists meet innkeepers, prostitutes, goat-herders, soldiers, priests, escaped convicts and scorned lovers. The aforementioned characters sometimes tell tales that incorporate events from the real world, like the conquest of the Kingdom of Maynila or battles in the Eighty Years' War. Their encounters are magnified by Don Quixote's imagination into chivalrous quests. Don Quixote's tendency to intervene violently in matters irrelevant to himself, and his habit of not paying debts, result in privations, injuries, and humiliations (with Sancho often the victim). Finally, Don Quixote is persuaded to return to his home village. The narrator hints that there was a third quest, but says that records of it have been lost.
Alonso Quixano, the protagonist of the novel (though he is not given this name until much later in the book), is a hidalgo (member of the lesser Spanish nobility), nearing 50 years of age, living in an unnamed section of La Mancha with his niece and housekeeper, as well as a boy who is never heard of again after the first chapter. Although Quixano is usually a rational man, in keeping with the humoral physiology theory of the time, not sleeping adequately—because he was reading—has caused his brain to dry. Quixano's temperament is thus choleric, the hot and dry humor. As a result, he is easily given to anger and believes every word of these fictional books of chivalry to be true.
Imitating the protagonists of these books, he decides to become a knight errant in search of adventure. To these ends, he dons an old suit of armor, renames himself "Don Quixote", names his exhausted horse "Rocinante", and designates Aldonza Lorenzo, a neighboring farm girl, as his lady love, renaming her Dulcinea del Toboso, while she knows nothing of this. Expecting to become famous quickly, he arrives at an inn, which he believes to be a castle, calls the prostitutes he meets "ladies" ("doncellas"), and demands that the innkeeper, who he takes to be the lord of the castle, dub him a knight. He spends the night holding vigil over his armor and becomes involved in a fight with muleteers who try to remove his armor from the horse trough so that they can water their mules. In a pretended ceremony, the innkeeper dubs him a knight to be rid of him and sends him on his way.
Don Quixote next "frees" a young boy named Andres who is tied to a tree and beaten by his master, and makes his master swear to treat the boy fairly, but the boy's beating is continued (and in fact redoubled) as soon as Quixote leaves. Don Quixote then encounters traders from Toledo, who "insult" the imaginary Dulcinea. He attacks them, only to be severely beaten and left on the side of the road, and is returned to his home by a neighboring peasant.
While Don Quixote is unconscious in his bed, his niece, the housekeeper, the parish curate, and the local barber burn most of his chivalric and other books. A large part of this section consists of the priest deciding which books deserve to be burned and which to be saved. It is a scene of high comedy: If the books are so bad for morality, how does the priest know them well enough to describe every naughty scene? Even so, this gives an occasion for many comments on books Cervantes himself liked and disliked. For example, Cervantes' own pastoral novel "La Galatea" is saved, while the rather unbelievable romance "Felixmarte de Hyrcania" is burned. After the books are dealt with, they seal up the room which contained the library, later telling Don Quixote that it was the action of a wizard ("encantador").
After a short period of feigning health, Don Quixote requests his neighbour, Sancho Panza, to be his squire, promising him a petty governorship ("ínsula"). Sancho is a poor and simple farmer but more practical than the head-in-the-clouds Don Quixote and agrees to the offer, sneaking away with Don Quixote in the early dawn. It is here that their famous adventures begin, starting with Don Quixote's attack on windmills that he believes to be ferocious giants.
The two next encounter two Benedictine friars travelling on the road ahead of a lady in a carriage. The friars are not travelling with the lady, but happen to be travelling on the same road. Don Quixote takes the friars to be enchanters who hold the lady captive, knocks a friar from his horse, and is challenged by an armed Basque traveling with the company. As he has no shield, the Basque uses a pillow from the carriage to protect himself, which saves him when Don Quixote strikes him. Cervantes chooses this point, in the middle of the battle, to say that his source ends here. Soon, however, he resumes Don Quixote's adventures after a story about finding Arabic notebooks containing the rest of the story by Cid Hamet Ben Engeli. The combat ends with the lady leaving her carriage and commanding those traveling with her to "surrender" to Don Quixote.
Sancho and Don Quixote fall in with a group of goat herders. Don Quixote tells Sancho and the goat herders about the "Golden Age" of man, in which property does not exist and men live in peace. The goatherders invite the Knight and Sancho to the funeral of Grisóstomo, a former student who left his studies to become a shepherd after reading pastoral novels (paralleling Don Quixote's decision to become a knight), seeking the shepherdess Marcela. At the funeral Marcela appears, vindicating herself from the bitter verses written about her by Grisóstomo, and claiming her own autonomy and freedom from expectations put on her by pastoral clichés. She disappears into the woods, and Don Quixote and Sancho follow. Ultimately giving up, the two dismount by a pond to rest. Some Galicians arrive to water their ponies, and Rocinante (Don Quixote's horse) attempts to mate with the ponies. The Galicians hit Rocinante with clubs to dissuade him, whereupon Don Quixote tries to defend Rocinante. The Galicians beat Don Quixote and Sancho, leaving them in great pain.
After escaping the musketeers, Don Quixote and Sancho ride to a nearby inn. Once again, Don Quixote imagines the inn is a castle, although Sancho is not quite convinced. Don Quixote is given a bed in a former hayloft, and Sancho sleeps on the rug next to the bed; they share the loft with a muleteer. When night comes, Don Quixote imagines the servant girl at the inn, Helen, to be a beautiful princess, and makes her sit on his bed with him, scaring her. Seeing what is happening, the muleteer attacks Don Quixote, breaking the fragile bed and leading to a large and chaotic fight in which Don Quixote and Sancho are once again badly hurt. Don Quixote's explanation for everything is that they fought with an enchanted Moor. He also believes that he can cure their wounds with a mixture he calls "the balm of Fierabras", which only makes them sick. Don Quixote and Sancho decide to leave the inn, but Quixote, following the example of the fictional knights, leaves without paying. Sancho, however, remains and ends up wrapped in a blanket and tossed up in the air (blanketed) by several mischievous guests at the inn, something that is often mentioned over the rest of the novel. After his release, he and Don Quixote continue their travels.
After Don Quixote has adventures involving a dead body, a helmet, and freeing a group of galley slaves, he and Sancho wander into the Sierra Morena and there encounter the dejected Cardenio. Cardenio relates the first part of his story, in which he falls deeply in love with his childhood friend Lucinda, and is hired as the companion to the Duke's son, leading to his friendship with the Duke's younger son, Don Fernando. Cardenio confides in Don Fernando his love for Lucinda and the delays in their engagement, caused by Cardenio's desire to keep with tradition. After reading Cardenio's poems praising Lucinda, Don Fernando falls in love with her. Don Quixote interrupts when Cardenio suggests that his beloved may have become unfaithful after the formulaic stories of spurned lovers in chivalric novels. They get into a fight, ending with Cardenio beating all of them and walking away to the mountains.
Quixote pines for Dulcinea, imitating Cardenio. Quixote sends Sancho to deliver a letter to Dulcinea, but instead Sancho finds the barber and priest from his village and brings them to Quixote. The priest and barber make plans with Sancho to trick Don Quixote to come home. They get the help of Dorotea, a woman whom they discover in the forest, that has been deceived by Don Fernando with promises of love and marriage. She pretends that she is the Princess Micomicona and desperate to get Quixote's help. Quixote runs into Andres, who insults his incompetence.
Convinced that he is on a quest to return princess Miconiconia to the throne of her kingdom, Quixote and the group return to the previous inn where the priest reads aloud the manuscript of the story of Anselmo (The Impertinentely Curious Man) while Quixote, sleepwalking, battles with wineskins that he takes to be the giant who stole the princess Micomiconia's kingdom. A stranger arrives at the inn accompanying a young woman. The stranger is revealed to be Don Fernando, and the young woman Lucinda. Dorotea is reunited with Don Fernando and Cardenio with Lucinda. A captive from Moorish lands in company of an Arabic speaking lady arrive and is asked to tell the story of his life; "If your worships will give me your attention you will hear a true story which, perhaps, fictitious one constructed with ingenious and studied art can not come up to." A judge arrives, and it is found that the captive is his long-lost brother, and the two are reunited.
An officer of the Santa Hermandad has a warrant for Quixote's arrest for freeing the galley slaves. The priest begs for the officer to have mercy on account of Quixote's insanity. The officer agrees, and Quixote is locked in a cage and made to think that it is an enchantment and that there is a prophecy of his heroic return home. While traveling, the group stops to eat and lets Quixote out of the cage; he gets into a fight with a goatherd and with a group of pilgrims, who beat him into submission, and he is finally brought home. The narrator ends the story by saying that he has found manuscripts of Quixote's further adventures.
Although the two parts are now published as a single work, "Don Quixote, Part Two" was a sequel published ten years after the original novel. While "Part One" was mostly farcical, the second half is more serious and philosophical about the theme of deception.
"Part Two" of "Don Quixote" explores the concept of a character understanding that he is written about, an idea much explored in the 20th century. As "Part Two" begins, it is assumed that the literate classes of Spain have all read the first part of the story. Cervantes' meta-fictional device was to make even the characters in the story familiar with the publication of "Part One", as well as with an actually published, fraudulent Part Two.
When strangers encounter the duo in person, they already know their famous history. A Duke and Duchess, and others, deceive Don Quixote for entertainment, setting forth a string of imagined adventures resulting in a series of practical jokes. Some of them put Don Quixote's sense of chivalry and his devotion to Dulcinea through many tests. Pressed into finding Dulcinea, Sancho brings back three ragged peasant girls and tells Don Quixote that they are Dulcinea and her ladies-in-waiting. When Don Quixote only sees the peasant girls, Sancho pretends (reversing some incidents of "Part One") that their derelict appearance results from an enchantment.
Sancho later gets his comeuppance for this when, as part of one of the Duke and Duchess's pranks, the two are led to believe that the only method to release Dulcinea from her spell is for Sancho to give himself three thousand three hundred lashes. Sancho naturally resists this course of action, leading to friction with his master. Under the Duke's patronage, Sancho eventually gets a governorship, though it is false, and he proves to be a wise and practical ruler although this ends in humiliation as well. Near the end, Don Quixote reluctantly sways towards sanity.
The lengthy untold "history" of Don Quixote's adventures in knight-errantry comes to a close after his battle with the Knight of the White Moon (a young man from Don Quixote's hometown who had previously posed as the Knight of Mirrors) on the beach in Barcelona, in which the reader finds him conquered. Bound by the rules of chivalry, Don Quixote submits to prearranged terms that the vanquished is to obey the will of the conqueror: here, it is that Don Quixote is to lay down his arms and cease his acts of chivalry for the period of one year (in which he may be cured of his madness). He and Sancho undergo one more prank by the Duke and Duchess before setting off.
Upon returning to his village, Don Quixote announces his plan to retire to the countryside as a shepherd, but his housekeeper urges him to stay at home. Soon after, he retires to his bed with a deathly illness, and later awakes from a dream, having fully recovered his sanity. Sancho tries to restore his faith, but Quixano (his proper name) only renounces his previous ambition and apologizes for the harm he has caused. He dictates his will, which includes a provision that his niece will be disinherited if she marries a man who reads books of chivalry. After Alonso Quixano dies, the author emphasizes that there are no more adventures to relate and that any further books about Don Quixote would be spurious.
Harold Bloom says "Don Quixote" is the first modern novel, and that the protagonist is at war with Freud's reality principle, which accepts the necessity of dying.
Edith Grossman, who wrote and published a highly acclaimed English translation of the novel in 2003, says that the book is mostly meant to move people into emotion using a systematic change of course, on the verge of both tragedy and comedy at the same time. Grossman has stated:The question is that Quixote has multiple interpretations [...] and how do I deal with that in my translation. I'm going to answer your question by avoiding it [...] so when I first started reading the Quixote I thought it was the most tragic book in the world, and I would read it and weep [...] As I grew older [...] my skin grew thicker [...] and so when I was working on the translation I was actually sitting at my computer and laughing out loud. This is done [...] as Cervantes did it [...] by never letting the reader rest. You are never certain that you truly got it. Because as soon as you think you understand something, Cervantes introduces something that contradicts your premise.
Jonathan Shockley has placed the novel in the context of Terror Management Theory, claiming that the figure of Don Quixote represents the hidden essence of human culture: the centrality of heroic madness and its related death anxiety in all people. The flimsy, delusional (and evil-causing) nature of the things that grant humans conviction and self-aggrandizement. And the ironic (and ultimately tragic) need to acquire this conviction and self-aggrandizement to experience the goodness, richness and reality of life.
The novel's structure is episodic in form. The full title is indicative of the tale's object, as "ingenioso" (Spanish) means "quick with inventiveness", marking the transition of modern literature from dramatic to thematic unity. The novel takes place over a long period of time, including many adventures united by common themes of the nature of reality, reading, and dialogue in general.
Although burlesque on the surface, the novel, especially in its second half, has served as an important thematic source not only in literature but also in much of art and music, inspiring works by Pablo Picasso and Richard Strauss. The contrasts between the tall, thin, fancy-struck and idealistic Quixote and the fat, squat, world-weary Panza is a motif echoed ever since the book's publication, and Don Quixote's imaginings are the butt of outrageous and cruel practical jokes in the novel.
Even faithful and simple Sancho is forced to deceive him at certain points. The novel is considered a satire of orthodoxy, veracity and even nationalism. In exploring the individualism of his characters, Cervantes helped move beyond the narrow literary conventions of the chivalric romance literature that he spoofed, which consists of straightforward retelling of a series of acts that redound to the knightly virtues of the hero. The character of Don Quixote became so well known in its time that the word "quixotic" was quickly adopted by many languages. Characters such as Sancho Panza and Don Quixote's steed, Rocinante, are emblems of Western literary culture. The phrase "tilting at windmills" to describe an act of attacking imaginary enemies (or an act of extreme idealism), derives from an iconic scene in the book.
It stands in a unique position between medieval chivalric romance and the modern novel. The former consist of disconnected stories featuring the same characters and settings with little exploration of the inner life of even the main character. The latter are usually focused on the psychological evolution of their characters. In Part I, Quixote imposes himself on his environment. By Part II, people know about him through "having read his adventures", and so, he needs to do less to maintain his image. By his deathbed, he has regained his sanity, and is once more "Alonso Quixano the Good".
Sources for "Don Quixote" include the Castilian novel "Amadis de Gaula", which had enjoyed great popularity throughout the 16th century. Another prominent source, which Cervantes evidently admires more, is "Tirant lo Blanch", which the priest describes in Chapter VI of "Quixote" as "the best book in the world." (However, the sense in which it was "best" is much debated among scholars. Since the 19th century, the passage has been called "the most difficult passage of "Don Quixote"".)
The scene of the book burning gives us an excellent list of Cervantes' likes and dislikes about literature.
Cervantes makes a number of references to the Italian poem "Orlando furioso". In chapter 10 of the first part of the novel, Don Quixote says he must take the magical helmet of Mambrino, an episode from Canto I of "Orlando", and itself a reference to Matteo Maria Boiardo's "Orlando innamorato". The interpolated story in chapter 33 of Part four of the First Part is a retelling of a tale from Canto 43 of "Orlando", regarding a man who tests the fidelity of his wife.
Another important source appears to have been Apuleius's "The Golden Ass", one of the earliest known novels, a picaresque from late classical antiquity. The wineskins episode near the end of the interpolated tale "The Curious Impertinent" in chapter 35 of the first part of "Don Quixote" is a clear reference to Apuleius, and recent scholarship suggests that the moral philosophy and the basic trajectory of Apuleius's novel are fundamental to Cervantes' program. Similarly, many of both Sancho's adventures in Part II and proverbs throughout are taken from popular Spanish and Italian folklore.
Cervantes' experiences as a galley slave in Algiers also influenced "Quixote".
Medical theories may have also influenced Cervantes’ literary process. Cervantes had familial ties to the distinguished medical community. His father, Rodrigo de Cervantes, and his great-grandfather, Juan Díaz de Torreblanca, were surgeons. Additionally, his sister, Andrea de Cervantes, was a nurse. He also befriended many individuals involved in the medical field, in that he knew medical author Francisco Díaz, an expert in urology, and royal doctor Antonio Ponce de Santa Cruz who served as a personal doctor to both Philip III and Philip IV of Spain. 
Apart from the personal relations Cervantes maintained within the medical field, Cervantes’ personal life was defined by an interest in medicine. He frequently visited patients from the Hospital de Inocentes in Sevilla. Furthermore, Cervantes explored medicine in his personal library. His library contained more than 200 volumes and included books like "Examen de Ingenios" by Juan Huarte and "Practica y teórica de cirugía" by Dionisio Daza Chacón that defined medical literature and medical theories of his time. 
It is not certain when Cervantes began writing "Part Two" of "Don Quixote", but he had probably not proceeded much further than Chapter LIX by late July 1614. About September, however, a spurious Part Two, entitled "Second Volume of the Ingenious Gentleman Don Quixote of La Mancha: by the Licenciado (doctorate) Alonso Fernández de Avellaneda, of Tordesillas", was published in Tarragona by an unidentified Aragonese who was an admirer of Lope de Vega, rival of Cervantes. It was translated into English by William Augustus Yardley, Esquire in two volumes in 1784.
Some modern scholars suggest that Don Quixote's fictional encounter with Avellaneda in Chapter 59 of Part II should not be taken as the date that "Cervantes" encountered it, which may have been much earlier.
Avellaneda's identity has been the subject of many theories, but there is no consensus as to who he was. In its prologue, the author gratuitously insulted Cervantes, who not surprisingly took offense and responded; the last half of Chapter LIX and most of the following chapters of Cervantes' "Segunda Parte" lend some insight into the effects upon him; Cervantes manages to work in some subtle digs at Avellaneda's own work, and in his preface to Part II, comes very near to criticizing Avellaneda directly.
In his introduction to "The Portable Cervantes", Samuel Putnam, a noted translator of Cervantes' novel, calls Avellaneda's version "one of the most disgraceful performances in history".
The second part of Cervantes' "Don Quixote", finished as a direct result of the Avellaneda book, has come to be regarded by some literary critics as superior to the first part, because of its greater depth of characterization, its discussions, mostly between Quixote and Sancho, on diverse subjects, and its philosophical insights. In Cervantes' "Segunda Parte", Don Quixote visits a printing-house in Barcelona and finds Avellaneda's "Second Part" being printed there, in an early example of metafiction.
"Don Quixote, Part One" contains a number of stories which do not directly involve the two main characters, but which are narrated by some of the picaresque figures encountered by the Don and Sancho during their travels. The longest and best known of these is "El Curioso Impertinente" (the impertinently curious man), found in Part One, Book Four. This story, read to a group of travelers at an inn, tells of a Florentine nobleman, Anselmo, who becomes obsessed with testing his wife's fidelity, and talks his close friend Lothario into attempting to seduce her, with disastrous results for all.
In "Part Two", the author acknowledges the criticism of his digressions in "Part One" and promises to concentrate the narrative on the central characters (although at one point he laments that his narrative muse has been constrained in this manner). Nevertheless, "Part Two" contains several back narratives related by peripheral characters.
Several abridged editions have been published which delete some or all of the extra tales in order to concentrate on the central narrative.
Cervantes wrote his work in early modern Spanish, heavily borrowing from Old Spanish, the medieval form of the language. The language of "Don Quixote", although still containing archaisms, is far more understandable to modern Spanish readers than is, for instance, the completely medieval Spanish of the "Poema de mio Cid", a kind of Spanish that is as different from Cervantes' language as Middle English is from Modern English. The Old Castilian language was also used to show the higher class that came with being a knight errant.
In "Don Quixote", there are basically two different types of Castilian: Old Castilian is spoken only by Don Quixote, while the rest of the roles speak a contemporary (late 16th century) version of Spanish. The Old Castilian of Don Quixote is a humoristic resource—he copies the language spoken in the chivalric books that made him mad; and many times, when he talks nobody is able to understand him because his language is too old. This humorous effect is more difficult to see nowadays because the reader must be able to distinguish the two old versions of the language, but when the book was published it was much celebrated. (English translations can get some sense of the effect by having Don Quixote use King James Bible or Shakespearean English, or even Middle English.)
In Old Castilian, the letter "x" represented the sound written "sh" in modern English, so the name was originally pronounced . However, as Old Castilian evolved towards modern Spanish, a sound change caused it to be pronounced with a voiceless velar fricative sound (like the Scots or German "ch"), and today the Spanish pronunciation of "Quixote" is . The original pronunciation is reflected in languages such as Asturian, Leonese, Galician, Catalan, Italian, Portuguese, and French, where it is pronounced with a "sh" or "ch" sound; the French opera "Don Quichotte" is one of the best-known modern examples of this pronunciation.
Today, English speakers generally attempt something close to the modern Spanish pronunciation of "Quixote" ("Quijote"), as , although the traditional English spelling-based pronunciation with the value of the letter x in modern English is still sometimes used, resulting in or . In Australian English, the preferred pronunciation amongst members of the educated classes was until well into the 1970s, as part of a tendency for the upper class to "anglicise its borrowing ruthlessly". The traditional English rendering is preserved in the pronunciation of the adjectival form "quixotic", i.e., , defined by "Merriam-Webster" as the foolishly impractical pursuit of ideals, typically marked by rash and lofty romanticism.
Cervantes' story takes place on the plains of La Mancha, specifically the "comarca" of Campo de Montiel.
The story also takes place in El Toboso where Don Quixote goes to seek Dulcinea's blessings.
The location of the village to which Cervantes alludes in the opening sentence of "Don Quixote" has been the subject of debate since its publication over four centuries ago. Indeed, Cervantes deliberately omits the name of the village, giving an explanation in the final chapter:
Theories
In 2004, a multidisciplinary team of academics from Complutense University, led by Francisco Parra Luna, Manuel Fernández Nieto, and Santiago Petschen Verdaguer, deduced that the village was that of Villanueva de los Infantes. Their findings were published in a paper titled "'El Quijote' como un sistema de distancias/tiempos: hacia la localización del lugar de la Mancha", which was later published as a book: "El enigma resuelto del Quijote". The result was replicated in two subsequent investigations: "La determinación del lugar de la Mancha como problema estadístico" and "The Kinematics of the Quixote and the Identity of the 'Place in La Mancha'".
Researchers Isabel Sanchez Duque and Francisco Javier Escudero have found relevant information regarding the possible sources of inspiration of Cervantes for writing Don Quixote. Cervantes was friend of the family Villaseñor, which was involved in a combat with Francisco de Acuña. Both sides combated disguised as medieval knights in the road from El Toboso to Miguel Esteban in 1581. They also found a person called Rodrigo Quijada, who bought the title of nobility of "hidalgo", and created diverse conflicts with the help of a squire.
Because of its widespread influence, "Don Quixote" also helped cement the modern Spanish language. The opening sentence of the book created a classic Spanish cliché with the phrase ("whose name I do not wish to recall"): ("In a village of La Mancha, whose name I do not wish to recall, there lived, not very long ago, one of those gentlemen with a lance in the lance-rack, an ancient shield, a skinny old horse, and a fast greyhound.")
The novel's farcical elements make use of punning and similar verbal playfulness. Character-naming in "Don Quixote" makes ample figural use of contradiction, inversion, and irony, such as the names "Rocinante" (a reversal) and "Dulcinea" (an allusion to illusion), and the word itself, possibly a pun on (jaw) but certainly (Catalan: thighs), a reference to a horse's rump.
As a military term, the word "quijote" refers to "cuisses", part of a full suit of plate armour protecting the thighs. The Spanish suffix "-ote" denotes the augmentative—for example, "grande" means large, but "grandote" means extra large. Following this example, "Quixote" would suggest 'The Great Quijano', a play on words that makes much sense in light of the character's delusions of grandeur.
"La Mancha" is a region of Spain, but "mancha" (Spanish word) means spot, mark, stain. Translators such as John Ormsby have declared La Mancha to be one of the most desertlike, unremarkable regions of Spain, the least romantic and fanciful place that one would imagine as the home of a courageous knight.
In July 1604, Cervantes sold the rights of "El ingenioso hidalgo don Quixote de la Mancha" (known as "Don Quixote, Part I") to the publisher-bookseller Francisco de Robles for an unknown sum. License to publish was granted in September, the printing was finished in December, and the book came out on 16 January 1605.
The novel was an immediate success. The majority of the 400 copies of the first edition were sent to the New World, with the publisher hoping to get a better price in the Americas. Although most of them disappeared in a shipwreck near La Havana, approximately 70 copies reached Lima, from where they were sent to Cuzco in the heart of the defunct Inca Empire.
No sooner was it in the hands of the public than preparations were made to issue derivative (pirated) editions. "Don Quixote" had been growing in favour, and its author's name was now known beyond the Pyrenees. By August 1605, there were two Madrid editions, two published in Lisbon, and one in Valencia. Publisher Francisco de Robles secured additional copyrights for Aragon and Portugal for a second edition.
Sale of these publishing rights deprived Cervantes of further financial profit on "Part One". In 1607, an edition was printed in Brussels. Robles, the Madrid publisher, found it necessary to meet demand with a third edition, a seventh publication in all, in 1608. Popularity of the book in Italy was such that a Milan bookseller issued an Italian edition in 1610. Yet another Brussels edition was called for in 1611. Since then, numerous editions have been released and in total, the novel is believed to have sold more than 500 million copies worldwide. The work has been produced in numerous editions and languages, the Cervantes Collection, at the State Library of New South Wales includes over 1,100 editions. These were collected, by Dr Ben Haneman, over a period of thirty years.
In 1613, Cervantes published the "Novelas Ejemplares", dedicated to the Maecenas of the day, the Conde de Lemos. Eight and a half years after "Part One" had appeared came the first hint of a forthcoming "Segunda Parte" (Part Two). "You shall see shortly," Cervantes says, "the further exploits of Don Quixote and humours of Sancho Panza." "Don Quixote, Part Two", published by the same press as its predecessor, appeared late in 1615, and quickly reprinted in Brussels and Valencia (1616) and Lisbon (1617). Parts One and Two were published as one edition in Barcelona in 1617. Historically, Cervantes' work has been said to have "smiled Spain's chivalry away", suggesting that Don Quixote as a chivalric satire contributed to the demise of Spanish Chivalry.
There are many translations of the book, and it has been adapted many times in shortened versions. Many derivative editions were also written at the time, as was the custom of envious or unscrupulous writers. Seven years after the "Parte Primera" appeared, "Don Quixote" had been translated into French, German, Italian, and English, with the first French translation of 'Part II' appearing in 1618, and the first English translation in 1620. One abridged adaptation, authored by Agustín Sánchez, runs slightly over 150 pages, cutting away about 750 pages.
Thomas Shelton's English translation of the "First Part" appeared in 1612 while Cervantes was still alive, although there is no evidence that Shelton had met the author. Although Shelton's version is cherished by some, according to John Ormsby and Samuel Putnam, it was far from satisfactory as a carrying over of Cervantes' text. Shelton's translation of the novel's "Second Part" appeared in 1620.
Near the end of the 17th century, John Phillips, a nephew of poet John Milton, published what Putnam considered the worst English translation. The translation, as literary critics claim, was not based on Cervantes' text but mostly upon a French work by Filleau de Saint-Martin and upon notes which Thomas Shelton had written.
Around 1700, a version by Pierre Antoine Motteux appeared. Motteux's translation enjoyed lasting popularity; it was reprinted as the Modern Library Series edition of the novel until recent times. Nonetheless, future translators would find much to fault in Motteux's version: Samuel Putnam criticized "the prevailing slapstick quality of this work, especially where Sancho Panza is involved, the obtrusion of the obscene where it is found in the original, and the slurring of difficulties through omissions or expanding upon the text". John Ormsby considered Motteux's version "worse than worthless", and denounced its "infusion of Cockney flippancy and facetiousness" into the original.
The proverb 'The proof of the pudding is in the eating' is widely attributed to Cervantes. The Spanish word for pudding, 'budín', however, doesn't appear in the original text but premieres in the Motteux translation. In Smolletts translation of 1755, he notes that the original text reads literally "you will see when the eggs are fried" meaning 'time will tell'.
A translation by Captain John Stevens, which revised Thomas Shelton's version, also appeared in 1700, but its publication was overshadowed by the simultaneous release of Motteux's translation.
In 1742, the Charles Jervas translation appeared, posthumously. Through a printer's error, it came to be known, and is still known, as "the Jarvis translation". It was the most scholarly and accurate English translation of the novel up to that time, but future translator John Ormsby points out in his own introduction to the novel that the Jarvis translation has been criticized as being too stiff. Nevertheless, it became the most frequently reprinted translation of the novel until about 1885. Another 18th-century translation into English was that of Tobias Smollett, himself a novelist, first published in 1755. Like the Jarvis translation, it continues to be reprinted today.
A translation by Alexander James Duffield appeared in 1881 and another by Henry Edward Watts in 1888. Most modern translators take as their model the 1885 translation by John Ormsby.
An expurgated children's version, under the title "The Story of Don Quixote", was published in 1922 (available on Project Gutenberg). It leaves out the risqué sections as well as chapters that young readers might consider dull, and embellishes a great deal on Cervantes' original text. The title page actually gives credit to the two editors as if they were the authors, and omits any mention of Cervantes.
The most widely read English-language translations of the mid-20th century are by Samuel Putnam (1949), J. M. Cohen (1950; Penguin Classics), and Walter Starkie (1957). The last English translation of the novel in the 20th century was by Burton Raffel, published in 1996. The 21st century has already seen five new translations of the novel into English. The first is by John D. Rutherford and the second by Edith Grossman. Reviewing the novel in the "New York Times", Carlos Fuentes called Grossman's translation a "major literary achievement" and another called it the "most transparent and least impeded among more than a dozen English translations going back to the 17th century."
In 2005, the year of the novel's 400th anniversary, Tom Lathrop published a new English translation of the novel, based on a lifetime of specialized study of the novel and its history. The fourth translation of the 21st century was released in 2006 by former university librarian James H Montgomery, 26 years after he had begun it, in an attempt to "recreate the sense of the original as closely as possible, though not at the expense of Cervantes' literary style."
In 2011, another translation by Gerald J. Davis appeared. It is the latest and the fifth translation of the 21st century.
Tilting at windmills is an English idiom that means attacking imaginary enemies. The expression is derived from "Don Quixote", and the word "tilt" in this context comes from jousting.
The phrase is sometimes used to describe either confrontations where adversaries are incorrectly perceived, or courses of action that are based on misinterpreted or misapplied heroic, romantic, or idealistic justifications. It may also connote an importune, unfounded, and vain effort against adversaries real or imagined.
Reviewing the English translations as a whole, Daniel Eisenberg stated that there is no one translation ideal for every purpose, but expressed a preference for those of Putnam and the revision of Ormsby's translation by Douglas and Jones.

</doc>
<doc id="8239" url="https://en.wikipedia.org/wiki?curid=8239" title="Dylan">
Dylan

Dylan may refer to: 

</doc>
<doc id="8240" url="https://en.wikipedia.org/wiki?curid=8240" title="Dada">
Dada

Dada () or Dadaism was an art movement of the European avant-garde in the early 20th century, with early centers in Zürich, Switzerland, at the Cabaret Voltaire (circa 1916); New York Dada began circa 1915, and after 1920 Dada flourished in Paris. Developed in reaction to World War I, the Dada movement consisted of artists who rejected the logic, reason, and aestheticism of modern capitalist society, instead expressing nonsense, irrationality, and anti-bourgeois protest in their works. The art of the movement spanned visual, literary, and sound media, including collage, sound poetry, cut-up writing, and sculpture. Dadaist artists expressed their discontent toward violence, war, and nationalism, and maintained political affinities with radical left-wing and far-left politics.
There is no consensus on the origin of the movement's name; a common story is that the German artist Richard Huelsenbeck slid a paper knife (letter-opener) at random into a dictionary, where it landed on "dada", a colloquial French term for a hobby horse. Jean Arp wrote that Tristan Tzara invented the word at 6 pm on 6 February 1916, in the Café de la Terrasse in Zurich. Others note that it suggests the first words of a child, evoking a childishness and absurdity that appealed to the group. Still others speculate that the word might have been chosen to evoke a similar meaning (or no meaning at all) in any language, reflecting the movement's internationalism.
The roots of Dada lie in pre-war avant-garde. The term anti-art, a precursor to Dada, was coined by Marcel Duchamp around 1913 to characterize works that challenge accepted definitions of art. Cubism and the development of collage and abstract art would inform the movement's detachment from the constraints of reality and convention. The work of French poets, Italian Futurists and the German Expressionists would influence Dada's rejection of the tight correlation between words and meaning. Works such as "Ubu Roi" (1896) by Alfred Jarry, and the ballet "Parade" (1916–17) by Erik Satie would also be characterized as proto-Dadaist works. The Dada movement's principles were first collected in Hugo Ball's in 1916.
The Dadaist movement included public gatherings, demonstrations, and publication of art/literary journals; passionate coverage of art, politics, and culture were topics often discussed in a variety of media. Key figures in the movement included Jean Arp, Johannes Baader, Hugo Ball, Marcel Duchamp, Max Ernst, Elsa von Freytag-Loringhoven, George Grosz, Raoul Hausmann, John Heartfield, Emmy Hennings, Hannah Höch, Richard Huelsenbeck, Francis Picabia, Man Ray, Hans Richter, Kurt Schwitters, Sophie Taeuber-Arp, Tristan Tzara, and Beatrice Wood, among others. The movement influenced later styles like the avant-garde and downtown music movements, and groups including Surrealism, "nouveau réalisme", pop art and Fluxus.
Dada was an informal international movement, with participants in Europe and North America. The beginnings of Dada correspond to the outbreak of World War I. For many participants, the movement was a protest against the bourgeois nationalist and colonialist interests, which many Dadaists believed were the root cause of the war, and against the cultural and intellectual conformity—in art and more broadly in society—that corresponded to the war.
Avant-garde circles outside France knew of pre-war Parisian developments. They had seen (or participated in) Cubist exhibitions held at Galeries Dalmau, Barcelona (1912), Galerie Der Sturm in Berlin (1912), the Armory Show in New York (1913), SVU Mánes in Prague (1914), several Jack of Diamonds exhibitions in Moscow and at De Moderne Kunstkring, Amsterdam (between 1911 and 1915). Futurism developed in response to the work of various artists. Dada subsequently combined these approaches.
Many Dadaists believed that the 'reason' and 'logic' of bourgeois capitalist society had led people into war. They expressed their rejection of that ideology in artistic expression that appeared to reject logic and embrace chaos and irrationality. For example, George Grosz later recalled that his Dadaist art was intended as a protest "against this world of mutual destruction."
According to Hans Richter Dada was not art: it was "anti-art." Dada represented the opposite of everything which art stood for. Where art was concerned with traditional aesthetics, Dada ignored aesthetics. If art was to appeal to sensibilities, Dada was intended to offend.
As Hugo Ball expressed it, "For us, art is not an end in itself ... but it is an opportunity for the true perception and criticism of the times we live in."
A reviewer from the "American Art News" stated at the time that "Dada philosophy is the sickest, most paralyzing and most destructive thing that has ever originated from the brain of man." Art historians have described Dada as being, in large part, a "reaction to what many of these artists saw as nothing more than an insane spectacle of collective homicide."
Years later, Dada artists described the movement as "a phenomenon bursting forth in the midst of the postwar economic and moral crisis, a savior, a monster, which would lay waste to everything in its path... [It was] a systematic work of destruction and demoralization... In the end it became nothing but an act of sacrilege."
To quote Dona Budd's "The Language of Art Knowledge", Dada was born out of negative reaction to the horrors of the First World War. This international movement was begun by a group of artists and poets associated with the Cabaret Voltaire in Zürich. Dada rejected reason and logic, prizing nonsense, irrationality and intuition. The origin of the name Dada is unclear; some believe that it is a nonsensical word. Others maintain that it originates from the Romanian artists Tristan Tzara's and Marcel Janco's frequent use of the words "da, da," meaning "yes, yes" in the Romanian language. Another theory says that the name "Dada" came during a meeting of the group when a paper knife stuck into a French–German dictionary happened to point to 'dada', a French word for 'hobbyhorse'. The movement primarily involved visual arts, literature, poetry, art manifestos, art theory, theatre, and graphic design, and concentrated its anti-war politics through a rejection of the prevailing standards in art through anti-art cultural works.
The creations of Duchamp, Picabia, Man Ray, and others between 1915 and 1917 eluded the term Dada at the time, and "New York Dada" came to be seen as a post facto invention of Duchamp. At the outset of the 1920s the term Dada flourished in Europe with the help of Duchamp and Picabia, who had both returned from New York. Notwithstanding, Dadaists such as Tzara and Richter claimed European precedence. Art historian David Hopkins notes:
Ironically, though, Duchamp's late activities in New York, along with the machinations of Picabia, re-cast Dada's history. Dada's European chroniclers—primarily Richter, Tzara, and Huelsenbeck—would eventually become preoccupied with establishing the pre-eminence of Zurich and Berlin at the foundations of Dada, but it proved to be Duchamp who was most strategically brilliant in manipulating the genealogy of this avant-garde formation, deftly turning New York Dada from a late-comer into an originating force.
Dada emerged from a period of artistic and literary movements like Futurism, Cubism and Expressionism; centered mainly in Italy, France and Germany respectively, in those years. However, unlike the earlier movements Dada was able to establish a broad base of support, giving rise to a movement that was international in scope. Its adherents were based in cities all over the world including New York, Zürich, Berlin, Paris and others. There were regional differences like an emphasis on literature in Zürich and political protest in Berlin.
Prominent Dadaists published manifestos, but the movement was loosely organized and there was no central hierarchy. On 14 July 1916, Ball recited the ; In 1917, Tzara wrote a second , considered one of the most important Dada writings, which was published in 1918. 
Tristan Tzara's manifesto articulated the concept of "Dadaist disgust"—the contradiction implicit in avant-garde works between the criticism and affirmation of modernist reality. In the Dadaist perspective modern art and culture are considered a type of fetishization where the objects of consumption (including organized systems of thought like philosophy and morality) are chosen, much like a preference for cake or cherries, to fill a void.
The shock and scandal the movement inflamed was deliberate; Dadist magazines were banned and their exhibits closed. Some of the artists even faced imprisonment. These provocations were part of the entertainment but, over time, audiences' expectations eventually outpaced the movement's capacity to deliver. As the artists' well-known "sarcastic laugh" started to come from the audience, the provocations of Dadaists began to lose their impact. Dada was an active movement during years of political turmoil from 1916 when European countries were actively engaged in World War I, the conclusion of which, in 1918, set the stage for a new political order.
There is some disagreement about where Dada originated. The movement is commonly identified with the Cabaret Voltaire (housed inside the "Holländische Meierei" bar in Zurich) co-founded by poet and cabaret singer Emmy Hennings and Hugo Ball. Some sources propose a Romanian origin, arguing that Dada was an offshoot of a vibrant artistic tradition that transposed to Switzerland when a group of Jewish modernist artists, including Tristan Tzara, Marcel Janco, and Arthur Segal settled in Zürich. Before World War I, similar art had already existed in Bucharest and other Eastern European cities; it is likely that Dada's catalyst was the arrival in Zürich of artists like Tzara and Janco.
The name "Cabaret Voltaire" was a reference to the French philosopher Voltaire, whose novel "Candide" mocked the religious and philosophical dogmas of the day. Opening night was attended by Ball, Tzara, Jean Arp, and Janco. These artists along with others like Sophie Taeuber, Richard Huelsenbeck and Hans Richter started putting on performances at the Cabaret Voltaire and using art to express their disgust with the war and the interests that inspired it. Having left Germany and Romania during World War I, the artists arrived in politically neutral Switzerland. They used abstraction to fight against the social, political, and cultural ideas of that time. They used techniques of shock, provocation and "vaudevilleian excess" were all tools to subvert the conventions they believed had caused the Great War. The Dadaists believed those ideas to be a byproduct of bourgeois society that was so apathetic it would wage war against itself rather than challenge the "status quo":
Ball said that Janco's mask and costume designs, inspired by Romanian folk art, made "the horror of our time, the paralyzing background of events" visible. According to Ball, performances were accompanied by a "balalaika orchestra playing delightful folk-songs." Influenced by African music, arrhythmic drumming and jazz were common at Dada gatherings.
After the cabaret closed down, Dada activities moved on to a new gallery, and Hugo Ball left for Bern. Tzara began a relentless campaign to spread Dada ideas. He bombarded French and Italian artists and writers with letters, and soon emerged as the Dada leader and master strategist. The Cabaret Voltaire re-opened, and is still in the same place at the Spiegelgasse 1 in the Niederdorf.
Zürich Dada, with Tzara at the helm, published the art and literature review "Dada" beginning in July 1917, with five editions from Zürich and the final two from Paris.
Other artists, such as André Breton and Philippe Soupault, created "literature groups to help extend the influence of Dada".
After the fighting of the First World War had ended in the armistice of November 1918, most of the Zürich Dadaists returned to their home countries, and some began Dada activities in other cities. Others, such as the Swiss native Sophie Taeuber, would remain in Zürich into the 1920s.
"Berlin was a city of tightened stomachers, of mounting, thundering hunger, where hidden rage was transformed into a boundless money lust, and men's minds were concentrating more and more on questions of naked existence... Fear was in everybody's bones" – Richard Hülsenbeck
Raoul Hausmann, who helped establish Dada in Berlin, published his manifesto "Synthethic Cino of Painting" in 1918 where he attacked Expressionism and the art critics who promoted it. Dada is envisioned in contrast to art forms, such as Expressionism, that appeal to viewers' emotional states: "the exploitation of so-called echoes of the soul". In Hausmann's conception of Dada, new techniques of creating art would open doors to explore new artistic impulses. Fragmented use of real world stimuli allowed an expression of reality that was radically different from other forms of art:
The groups in Germany were not as strongly anti-art as other groups. Their activity and art were more political and social, with corrosive manifestos and propaganda, satire, public demonstrations and overt political activities. The intensely political and war-torn environment of Berlin had a dramatic impact on the ideas of Berlin Dadaists. Conversely, New York's geographic distance from the war spawned its more theoretically-driven, less political nature. According to Hans Richter, a Dadaist who was in Berlin yet “aloof from active participation in Berlin Dada”, several distinguishing characteristics of the Dada movement there included: “its political element and its technical discoveries in painting and literature”; “inexhaustible energy”; “mental freedom which included the abolition of everything”; and “members intoxicated with their own power in a way that had no relation to the real world”, who would “turn their rebelliousness even against each other”.
In February 1918, while the Great War was approaching its climax, Huelsenbeck gave his first Dada speech in Berlin, and he produced a Dada manifesto later in the year. Following the October Revolution in Russia, by then out of the war, Hannah Höch and George Grosz used Dada to express communist sympathies. Grosz, together with John Heartfield, Höch and Hausmann developed the technique of photomontage during this period. Johannes Baader, the uninhibited Oberdada, was the “crowbar” of the Berlin movement's direct action according to Hans Richter and is credited with creating the first giant collages, according to Raoul Hausmann.
After the war, the artists published a series of short-lived political magazines and held the First International Dada Fair, 'the greatest project yet conceived by the Berlin Dadaists', in the summer of 1920. As well as work by the main members of Berlin Dada – Grosz, Raoul Hausmann, Hannah Höch, Johannes Baader, Huelsenbeck and Heartfield – the exhibition also included the work of Otto Dix, Francis Picabia, Jean Arp, Max Ernst, Rudolf Schlichter, Johannes Baargeld and others. In all, over 200 works were exhibited, surrounded by incendiary slogans, some of which also ended up written on the walls of the Nazi's "Entartete Kunst" exhibition in 1937. Despite high ticket prices, the exhibition lost money, with only one recorded sale.
The Berlin group published periodicals such as "Club Dada", "Der Dada", "Everyman His Own Football", and "Dada Almanach". They also established a political party, the Central Council of Dada for the World Revolution.
In Cologne, Ernst, Baargeld, and Arp launched a controversial Dada exhibition in 1920 which focused on nonsense and anti-bourgeois sentiments. Cologne's Early Spring Exhibition was set up in a pub, and required that participants walk past urinals while being read lewd poetry by a woman in a communion dress. The police closed the exhibition on grounds of obscenity, but it was re-opened when the charges were dropped.
Like Zürich, New York City was a refuge for writers and artists from the First World War. Soon after arriving from France in 1915, Marcel Duchamp and Francis Picabia met American artist Man Ray. By 1916 the three of them became the center of radical anti-art activities in the United States. American Beatrice Wood, who had been studying in France, soon joined them, along with Elsa von Freytag-Loringhoven. Arthur Cravan, fleeing conscription in France, was also in New York for a time. Much of their activity centered in Alfred Stieglitz's gallery, 291, and the home of Walter and Louise Arensberg.
The New Yorkers, though not particularly organized, called their activities "Dada," but they did not issue manifestos. They issued challenges to art and culture through publications such as "The Blind Man", "Rongwrong", and "New York Dada" in which they criticized the traditionalist basis for "museum" art. New York Dada lacked the disillusionment of European Dada and was instead driven by a sense of irony and humor. In his book "Adventures in the arts: informal chapters on painters, vaudeville and poets" Marsden Hartley included an essay on "".
During this time Duchamp began exhibiting "readymades" (everyday objects found or purchased and declared art) such as a bottle rack, and was active in the Society of Independent Artists. In 1917 he submitted the now famous "Fountain", a urinal signed R. Mutt, to the Society of Independent Artists exhibition but they rejected the piece. First an object of scorn within the arts community, the "Fountain" has since become almost canonized by some as one of the most recognizable modernist works of sculpture. Art world experts polled by the sponsors of the 2004 Turner Prize, Gordon's gin, voted it "the most influential work of modern art". As recent scholarship documents, the work is still controversial. Duchamp indicated in a 1917 letter to his sister that a female friend was centrally involved in the conception of this work: "One of my female friends who had adopted the pseudonym Richard Mutt sent me a porcelain urinal as a sculpture." The piece is in line with the scatological aesthetics of Duchamp's neighbour, the Baroness Elsa von Freytag-Loringhoven. In an attempt to "pay homage to the spirit of Dada" a performance artist named Pierre Pinoncelli made a crack in a replica of "The Fountain" with a hammer in January 2006; he also urinated on it in 1993.
Picabia's travels tied New York, Zürich and Paris groups together during the Dadaist period. For seven years he also published the Dada periodical "391" in Barcelona, New York City, Zürich, and Paris from 1917 through 1924.
By 1921, most of the original players moved to Paris where Dada had experienced its last major incarnation.
The French avant-garde kept abreast of Dada activities in Zürich with regular communications from Tristan Tzara (whose pseudonym means "sad in country," a name chosen to protest the treatment of Jews in his native Romania), who exchanged letters, poems, and magazines with Guillaume Apollinaire, André Breton, Max Jacob, Clément Pansaers, and other French writers, critics and artists.
Paris had arguably been the classical music capital of the world since the advent of musical Impressionism in the late 19th century. One of its practitioners, Erik Satie, collaborated with Picasso and Cocteau in a mad, scandalous ballet called "Parade". First performed by the Ballets Russes in 1917, it succeeded in creating a scandal but in a different way than Stravinsky's "Le Sacre du printemps" had done almost five years earlier. This was a ballet that was clearly parodying itself, something traditional ballet patrons would obviously have serious issues with.
Dada in Paris surged in 1920 when many of the originators converged there. Inspired by Tzara, Paris Dada soon issued manifestos, organized demonstrations, staged performances and produced a number of journals (the final two editions of "Dada", "Le Cannibale", and "Littérature" featured Dada in several editions.)
The first introduction of Dada artwork to the Parisian public was at the "Salon des Indépendants" in 1921. Jean Crotti exhibited works associated with Dada including a work entitled, "Explicatif" bearing the word "Tabu". In the same year Tzara staged his Dadaist play "The Gas Heart" to howls of derision from the audience. When it was re-staged in 1923 in a more professional production, the play provoked a theatre riot (initiated by André Breton) that heralded the split within the movement that was to produce Surrealism. Tzara's last attempt at a Dadaist drama was his "ironic tragedy" "Handkerchief of Clouds" in 1924.
In the Netherlands the Dada movement centered mainly around Theo van Doesburg, best known for establishing the De Stijl movement and magazine of the same name. Van Doesburg mainly focused on poetry, and included poems from many well-known Dada writers in "De Stijl" such as Hugo Ball, Hans Arp and Kurt Schwitters. Van Doesburg and (a cordwainer and artist in Drachten) became friends of Schwitters, and together they organized the so-called "Dutch Dada campaign" in 1923, where van Doesburg promoted a leaflet about Dada (entitled "What is Dada?"), Schwitters read his poems, Vilmos Huszár demonstrated a mechanical dancing doll and Nelly van Doesburg (Theo's wife), played avant-garde compositions on piano.
Van Doesburg wrote Dada poetry himself in "De Stijl", although under a pseudonym, I.K. Bonset, which was only revealed after his death in 1931. 'Together' with I.K. Bonset, he also published a short-lived Dutch Dada magazine called "Mécano" (1922–3). Another Dutchman identified by K. Schippers in his study of the movement in the Netherlands was the Groningen typographer H. N. Werkman, who was in touch with van Doesburg and Schwitters while editing his own magazine, "The Next Call" (1923–6). Two more artists mentioned by Schippers were German-born and eventually settled in the Netherlands. These were Otto van Rees, who had taken part in the liminal exhibitions at the Café Voltaire in Zürich, and Paul Citroen.
Though Dada itself was unknown in Georgia until at least 1920, from 1917 until 1921 a group of poets called themselves "41st Degree" (referring both to the latitude of Tbilisi, Georgia and to the temperature of a high fever) organized along Dadaist lines. The most important figure in this group was Iliazd, whose radical typographical designs visually echo the publications of the Dadaists. After his flight to Paris in 1921, he collaborated with Dadaists on publications and events.
In Yugoslavia, alongside the new art movement Zenitism, there was significant Dada activity between 1920 and 1922, run mainly by Dragan Aleksić and including work by Mihailo S. Petrov, Ljubomir Micić and Branko Ve Poljanski. Aleksić used the term "Yougo-Dada" and is known to have been in contact with Raoul Hausmann, Kurt Schwitters, and Tristan Tzara.
The Dada movement in Italy, based in Mantua, was met with distaste and failed to make a significant impact in the world of art. It published a magazine for a short time and held an exhibition in Rome, featuring paintings, quotations from Tristan Tzara, and original epigrams such as "True Dada is against Dada". The most notable member of this group was Julius Evola, who went on to become an eminent scholar of occultism, as well as a right-wing philosopher.
A prominent Dada group in Japan was Mavo, founded in July 1923 by Tomoyoshi Murayama, and later joined by Tatsuo Okada. Other prominent artists were Jun Tsuji, Eisuke Yoshiyuki, Shinkichi Takahashi and Katué Kitasono.
In the Tsuburaya Productions's "Ultra Series", an alien named Dada was inspired by Dadaism movement, with said character first appearing in episode 28 of the 1966 tokusatsu series, "Ultraman", its design by character artist Toru Narita. Dada's design is primarily monochromatic, and features numerous sharp lines and alternating black and white stripes, in reference to the movement and, in particular, to chessboard and Go patterns. On May 19, 2016, in celebration to the 100 year anniversary of Dadaism in Tokyo, the Ultra Monster was invited to meet the Swiss Ambassador Urs Bucher.
Butoh, the Japanese dance-form originating in 1959, can be considered to have direct connections to the spirit of the Dada movement, as Tatsumi Hijikata, one of Butoh's founders, "was influenced early in his career by Dadaism".
Dada in itself was relatively unknown in Russia, however, avant-garde art was widespread due to the Bolshevik's revolutionary agenda. The , a literary group sharing Dadaist ideals achieved infamy after one of its members suggested that Vladimir Mayakovsky should go to the "Pampushka" (Pameatnik Pushkina – Pushkin monument) on the "Tverbul" (Tverskoy Boulevard) to clean the shoes of anyone who desired it, after Mayakovsky declared that he was going to cleanse Russian literature. For more information on Dadaism's influence upon Russian avant-garde art, see the book "Russian Dada 1914–1924".
Dadists used shock, nihilism, negativity, paradox, randomness, subconscious forces and antinomianism to subvert established traditions in the aftermath of the Great War. Tzara's 1920 manifesto proposed cutting words from a newspaper and randomly selecting fragments to write poetry, a process in which the synchronous universe itself becomes an active agent in creating the art. A poem written using this technique would be a "fruit" of the words that were clipped from the article.
In literary arts Dadaists focused on poetry, particularly the so-called sound poetry invented by Hugo Ball. Dadaist poems attacked traditional conceptions of poetry, including structure, order, as well as the interplay of sound and the meaning of language. For Dadaists, the existing system by which information is articulated robs language of its dignity. The dismantling of language and poetic conventions are Dadaist attempts to restore language to its purest and most innocent form: "With these sound poem, we wanted to dispense with a language which journalism had made desolate and impossible."
Simultaneous poems (or "poèmes simultanés") were recited by a group of speakers who, collectively, produced a chaotic and confusing set of voices. These poems are considered manifestations of modernity including advertising, technology, and conflict. Unlike movements such as Expressionism, Dadaism did not take a negative view of modernity and the urban life. The chaotic urban and futuristic world is considered natural terrain that opens up new ideas for life and art.
Dada was not confined to the visual and literary arts; its influence reached into sound and music. Kurt Schwitters developed what he called "sound poems", while Francis Picabia and Georges Ribemont-Dessaignes composed Dada music performed at the Festival Dada in Paris on 26 May 1920. Other composers such as Erwin Schulhoff, Hans Heusser and Alberto Savinio all wrote "Dada music", while members of Les Six collaborated with members of the Dada movement and had their works performed at Dada gatherings. Erik Satie also dabbled with Dadaist ideas during his career, although he is primarily associated with musical Impressionism.
While broadly based, the movement was unstable. By 1924 in Paris, Dada was melding into Surrealism, and artists had gone on to other ideas and movements, including Surrealism, social realism and other forms of modernism. Some theorists argue that Dada was actually the beginning of postmodern art.
By the dawn of the Second World War, many of the European Dadaists had emigrated to the United States. Some (Otto Freundlich, Walter Serner) died in death camps under Adolf Hitler, who actively persecuted the kind of "degenerate art" that he considered Dada to represent. The movement became less active as post-war optimism led to the development of new movements in art and literature.
Dada is a named influence and reference of various anti-art and political and cultural movements, including the Situationist International and culture jamming groups like the Cacophony Society. Upon breaking up in July 2012, anarchist pop band Chumbawamba issued a statement which compared their own legacy with that of the Dada art movement.
At the same time that the Zürich Dadaists were making noise and spectacle at the Cabaret Voltaire, Lenin was planning his revolutionary plans for Russia in a nearby apartment. Tom Stoppard used this coincidence as a premise for his play "Travesties" (1974), which includes Tzara, Lenin, and James Joyce as characters. French writer Dominique Noguez imagined Lenin as a member of the Dada group in his tongue-in-cheek "Lénine Dada" (1989).
The former building of the Cabaret Voltaire fell into disrepair until it was occupied from January to March 2002, by a group proclaiming themselves Neo-Dadaists, led by Mark Divo. The group included Jan Thieler, Ingo Giezendanner, Aiana Calugar, Lennie Lee, and Dan Jones. After their eviction, the space was turned into a museum dedicated to the history of Dada. The work of Lee and Jones remained on the walls of the new museum.
Several notable retrospectives have examined the influence of Dada upon art and society. In 1967, a large Dada retrospective was held in Paris. In 2006, the Museum of Modern Art in New York City mounted a Dada exhibition in partnership with the National Gallery of Art in Washington D.C. and the Centre Pompidou in Paris. The LTM label has released a large number of Dada-related sound recordings, including interviews with artists such as Tzara, Picabia, Schwitters, Arp, and Huelsenbeck, and musical repertoire including Satie, Ribemont-Dessaignes, Picabia, and Nelly van Doesburg.
Musician Frank Zappa was a self-proclaimed Dadaist after learning of the movement:In the early days, I didn't even know what to call the stuff my life was made of. You can imagine my delight when I discovered that someone in a distant land had the same idea—AND a nice, short name for it.
Dadaism also blurred the line between literary and visual arts:
Dada is the groundwork to abstract art and sound poetry, a starting point for performance art, a prelude to postmodernism, an influence on pop art, a celebration of antiart to be later embraced for anarcho-political uses in the 1960s and the movement that laid the foundation for Surrealism.
The Dadaists imitated the techniques developed during the cubist movement through the pasting of cut pieces of paper items, but extended their art to encompass items such as transportation tickets, maps, plastic wrappers, etc. to portray aspects of life, rather than representing objects viewed as still life. They also invented the “chance collage" technique, involving dropping torn scraps of paper onto a larger sheet and then pasting the pieces wherever they landed.
Cut-up technique is an extension of collage to words themselves, Tristan Tzara describes this in the Dada Manifesto:
<poem style="margin-left: 2em;">
TO MAKE A DADAIST POEM
Take a newspaper.
Take some scissors.
Choose from this paper an article of the length you want to make your poem.
Cut out the article.
Next carefully cut out each of the words that makes up this article and put them all in a bag.
Shake gently.
Next take out each cutting one after the other.
Copy conscientiously in the order in which they left the bag.
The poem will resemble you.
And there you are – an infinitely original author of charming sensibility, even though unappreciated by the vulgar herd.
</poem>
The Dadaists – the "monteurs" (mechanics) – used scissors and glue rather than paintbrushes and paints to express their views of modern life through images presented by the media. A variation on the collage technique, photomontage utilized actual or reproductions of real photographs printed in the press. In Cologne, Max Ernst used images from the First World War to illustrate messages of the destruction of war.
The assemblages were three-dimensional variations of the collage – the assembly of everyday objects to produce meaningful or meaningless (relative to the war) pieces of work including war objects and trash. Objects were nailed, screwed or fastened together in different fashions. Assemblages could be seen in the round or could be hung on a wall.
Marcel Duchamp began to view the manufactured objects of his collection as objects of art, which he called "readymades". He would add signatures and titles to some, converting them into artwork that he called "readymade aided" or "rectified readymades". Duchamp wrote: "One important characteristic was the short sentence which I occasionally inscribed on the 'readymade.' That sentence, instead of describing the object like a title, was meant to carry the mind of the spectator towards other regions more verbal. Sometimes I would add a graphic detail of presentation which in order to satisfy my craving for alliterations, would be called 'readymade aided.'" One such example of Duchamp's readymade works is the urinal that was turned onto its back, signed "R. Mutt", titled "Fountain", and submitted to the Society of Independent Artists exhibition that year, though it was not displayed.
Sources
Manifestos

</doc>
<doc id="8242" url="https://en.wikipedia.org/wiki?curid=8242" title="Debian">
Debian

Debian (), also known as Debian GNU/Linux, is a Linux distribution composed of free and open-source software, developed by the community-supported Debian Project, which was established by Ian Murdock on August 16, 1993. The first version of Debian (0.01) was released on September 15, 1993, and its first stable version (1.1) was released on June 17, 1996. The Debian Stable branch is the most popular edition for personal computers and servers. Debian is also the basis for many other distributions, most notably Ubuntu.
Debian is one of the oldest operating systems based on the Linux kernel. The project is coordinated over the Internet by a team of volunteers guided by the Debian Project Leader and three foundational documents: the Debian Social Contract, the Debian Constitution, and the Debian Free Software Guidelines. New distributions are updated continually, and the next candidate is released after a time-based freeze.
Since its founding, Debian has been developed openly and distributed freely according to the principles of the GNU Project. Because of this, the Free Software Foundation sponsored the project from November 1994 to November 1995. When the sponsorship ended, the Debian Project formed the nonprofit organization Software in the Public Interest to continue financially supporting development.
Debian has access to online repositories that contain over 51,000 packages. Debian officially contains only free software, but non-free software can be downloaded and installed from the Debian repositories. Debian includes popular free programs such as LibreOffice, Firefox web browser, Evolution mail, K3b disc burner, VLC media player, GIMP image editor, and Evince document viewer. Debian is a popular choice for servers, for example as the operating system component of a LAMP stack.
Several flavors of the Linux kernel exist for each port. For example, the i386 port has flavors for IA-32 PCs supporting Physical Address Extension and real-time computing, for older PCs, and for x86-64 PCs. The Linux kernel does not officially contain firmware without sources, although such firmware is available in non-free packages and alternative installation media.
Debian offers CD and DVD images specifically built for XFCE, GNOME, KDE, MATE, Cinnamon, LXDE, and LXQT. MATE is officially supported, while Cinnamon support was added with Debian 8.0 Jessie. Less common window managers such as Enlightenment, Openbox, Fluxbox, IceWM, Window Maker and others are available.
The default desktop environment of version 7.0 Wheezy was temporarily switched to Xfce, because GNOME 3 did not fit on the first CD of the set. The default for the version 8.0 Jessie was changed again to Xfce in November 2013, and back to GNOME in September 2014.
Several parts of Debian are translated into languages other than American English, including package descriptions, configuration messages, documentation and the website. The level of software localization depends on the language, ranging from the highly supported German and French to the barely translated Creek and Samoan. The Debian 10 installer is available in 76 languages.
Debian offers DVD and CD images for installation that can be downloaded using BitTorrent or jigdo. Physical discs can also be bought from retailers. The full sets are made up of several discs (the amd64 port consists of 13 DVDs or 84 CDs), but only the first disc is required for installation, as the installer can retrieve software not contained in the first disc image from online repositories.
Debian offers different network installation methods. A minimal install of Debian is available via the "netinst" CD, whereby Debian is installed with just a base and later added software can be downloaded from the Internet. Another option is to boot the installer from the network.
Installation images are hybrid on some architectures and can be used to create a bootable USB drive (Live USB).
The default bootstrap loader is GNU GRUB version 2, though the package name is simply grub, while version 1 was renamed to grub-legacy. This conflicts with e.g. Fedora, where grub version 2 is named grub2.
The default desktop may be chosen from the DVD boot menu among GNOME, KDE Plasma, Xfce and LXDE, and from special disc 1 CDs.
Debian releases live install images for CDs, DVDs and USB thumb drives, for IA-32 and x86-64 architectures, and with a choice of desktop environments. These "Debian Live" images allow users to boot from removable media and run Debian without affecting the contents of their computer.
A full install of Debian to the computer's hard drive can be initiated from the live image environment.
Personalized images can be built with the live-build tool for discs, USB drives and for network booting purposes.
Debian distribution codenames are based on the names of characters from the "Toy Story" films. Debian's "unstable" trunk is named after Sid, a character who regularly destroyed his toys.
Debian was first announced on August 16, 1993, by Ian Murdock, who initially called the system "the Debian Linux Release". The word "Debian" was formed as a portmanteau of the first name of his then-girlfriend (later ex-wife) Debra Lynn and his own first name. Before Debian's release, the Softlanding Linux System (SLS) had been a popular Linux distribution and the basis for Slackware. The perceived poor maintenance and prevalence of bugs in SLS motivated Murdock to launch a new distribution.
Debian 0.01, released on September 15, 1993, was the first of several internal releases. Version 0.90 was the first public release, providing support through mailing lists hosted at Pixar. The release included the Debian Linux Manifesto, outlining Murdock's view for the new operating system. In it he called for the creation of a distribution to be maintained openly, in the spirit of Linux and GNU.
The Debian project released the 0.9x versions in 1994 and 1995. During this time it was sponsored by the Free Software Foundation for one year. Ian Murdock delegated the base system, the core packages of Debian, to Bruce Perens and Murdock focused on the management of the growing project. The first ports to non-IA-32 architectures began in 1995, and Debian 1.1 was released in 1996. By that time and thanks to Ian Jackson, the dpkg package manager was already an essential part of Debian.
In 1996, Bruce Perens assumed the project leadership. Perens was a controversial leader, regarded as authoritarian and strongly attached to Debian. He drafted a social contract and edited suggestions from a month-long discussion into the Debian Social Contract and the Debian Free Software Guidelines. After the FSF withdrew their sponsorship in the midst of the free software vs. open source debate, Perens initiated the creation of the legal umbrella organization Software in the Public Interest instead of seeking renewed involvement with the FSF. He led the conversion of the project from a.out to ELF. He created the BusyBox program to make it possible to run a Debian installer on a single floppy, and wrote a new installer. By the time Debian 1.2 was released, the project had grown to nearly two hundred volunteers. Perens left the project in 1998.
Ian Jackson became the leader in 1998. Debian 2.0 introduced the second official port, m68k. During this time the first port to a non-Linux kernel, Debian GNU/Hurd, was started. On December 2, the first Debian Constitution was ratified.
From 1999, the project leader was elected yearly. The Advanced Packaging Tool was deployed with Debian 2.1. The number of applicants was overwhelming and the project established the new member process. The first Debian derivatives, namely Libranet, Corel Linux and Stormix's Storm Linux, were started in 1999. The 2.2 release in 2000 was dedicated to Joel Klecker, a developer who died of Duchenne muscular dystrophy.
In late 2000, the project reorganized the archive with new package "pools" and created the "Testing" distribution, made up of packages considered stable, to reduce the freeze for the next release. In the same year, developers began holding an annual conference called DebConf with talks and workshops for developers and technical users. In May 2001, Hewlett-Packard announced plans to base its Linux development on Debian.
In July 2002, the project released version 3.0, code-named Woody, the first release to include cryptographic software, a free licensed KDE and internationalization. During these last release cycles, the Debian project drew considerable criticism from the free software community because of the long time between stable releases.
Some events disturbed the project while working on Sarge, as Debian servers were attacked by fire and hackers. One of the most memorable was the Vancouver prospectus. After a meeting held in Vancouver, release manager Steve Langasek announced a plan to reduce the number of supported ports to four in order to shorten future release cycles. There was a large reaction because the proposal looked more like a decision and because such a drop would damage Debian's aim to be "the universal operating system".
The 3.1 Sarge release was made in June 2005. This release updated 73% of the software and included over 9,000 new packages. A new installer with a modular design, Debian-Installer, allowed installations with RAID, XFS and LVM support, improved hardware detection, made installations easier for novice users, and was translated into almost forty languages. An installation manual and release notes were in ten and fifteen languages respectively. The efforts of Skolelinux, Debian-Med and Debian-Accessibility raised the number of packages that were educational, had a medical affiliation, and ones made for people with disabilities.
In 2006, as a result of a much-publicized dispute, Mozilla software was rebranded in Debian, with Firefox forked as Iceweasel and Thunderbird as Icedove. The Mozilla Corporation stated that software with unapproved modifications could not be distributed under the Firefox trademark. Two reasons that Debian modifies the Firefox software are to change the non-free artwork and to provide security patches. In February 2016, it was announced that Mozilla and Debian had reached an agreement and Iceweasel would revert to the name Firefox; similar agreement was anticipated for Icedove/Thunderbird.
A fund-raising experiment, Dunc-Tank, was created to solve the release cycle problem and release managers were paid to work full-time; in response, unpaid developers slowed down their work and the release was delayed. Debian 4.0 (Etch) was released in April 2007, featuring the x86-64 port and a graphical installer. Debian 5.0 (Lenny) was released in February 2009, supporting Marvell's Orion platform and netbooks such as the Asus Eee PC. The release was dedicated to Thiemo Seufer, a developer who died in a car crash.
In July 2009, the policy of time-based development freezes on a two-year cycle was announced. Time-based freezes are intended to blend the predictability of time based releases with Debian's policy of feature based releases, and to reduce overall freeze time. The Squeeze cycle was going to be especially short; however, this initial schedule was abandoned. In September 2010, the backports service became official, providing more recent versions of some software for the stable release.
Debian 6.0 (Squeeze) was released in February 2011, introduced Debian GNU/kFreeBSD as a technology preview, featured a dependency-based boot system, and moved problematic firmware to the non-free area. Debian 7.0 (Wheezy) was released in May 2013, featuring multiarch support and Debian 8.0 (Jessie) was released in April 2015, using systemd as the new init system. Debian 9.0 (Stretch) was released in June 2017. Debian 10.0 (Buster) was released in July 2019. Debian is still in development and new packages are uploaded to "unstable" every day.
Debian used to be released as a very large set of CDs for each architecture, but with the release of Debian 9 (Stretch) in 2017, these have been dropped.
Throughout Debian's lifetime, both the Debian distribution and its website have won various awards from different organizations, including "Server Distribution of the Year" 2011, "The best Linux distro of 2011", and a "Best of the Net" award for October 1998.
On December 2, 2015, Microsoft announced that they would offer Debian GNU/Linux as an endorsed distribution on the Azure cloud platform. Microsoft has also added a user environment to their Windows 10 desktop operating system called Windows Subsystem for Linux that offers a Debian subset.
Package management operations can be performed with different tools available on Debian, from the lowest level command dpkg to graphical front-ends like Synaptic. The recommended standard for administering packages on a Debian system is the apt toolset.
dpkg provides the low-level infrastructure for package management. The dpkg database contains the list of installed software on the current system. The dpkg command tool does not know about repositories. The command can work with local .deb package files, and information from the dpkg database.
An Advanced Packaging Tool (APT) allows administering an installed Debian system to retrieve and resolve package dependencies from repositories. APT share dependency information and cached packages.
GDebi is an APT tool which can be used in command-line and on the GUI. GDebi can install a local .deb file via the command line like the dpkg command, but with access to repositories to resolve dependencies. Other graphical front-ends for APT include Software Center, Synaptic and Apper.
GNOME Software is a graphical front-end for PackageKit, which itself can work on top of various software packaging systems.
The Debian Free Software Guidelines (DFSG) define the distinctive meaning of the word "free" as in "free and open-source software". Packages that comply with these guidelines, usually under the GNU General Public License, Modified BSD License or Artistic License, are included inside the "main" area; otherwise, they are included inside the "non-free" and "contrib" areas. These last two areas are not distributed within the official installation media, but they can be adopted manually.
Non-free includes packages that do not comply with the DFSG, such as documentation with invariant sections and proprietary software, and legally questionable packages. Contrib includes packages which do comply with the DFSG but fail other requirements. For example, they may depend on packages which are in non-free or requires such for building them.
Richard Stallman and the Free Software Foundation have criticized the Debian project for hosting the non-free repository and because the contrib and non-free areas are easily accessible, an opinion echoed by some in Debian including the former project leader Wichert Akkerman. The internal dissent in the Debian project regarding the non-free section has persisted, but the last time it came to a vote in 2004, the majority decided to keep it.
Three branches of Debian (also called "releases", "distributions" or "suites") are regularly maintained:
Other branches in Debian:
The "snapshot" archive provides older versions of the branches. They may be used to install a specific older version of some software.
"Stable" and "oldstable" get minor updates, called "point releases"; , the "stable" release is version 10.6, released on , and the "oldstable" release is version 9.13
The numbering scheme for the point releases up to Debian 4.0 was to include the letter "r" (for "revision") after the main version number and then the number of the point release; for example, the latest point release of version 4.0 is 4.0r9. This scheme was chosen because a new dotted version would make the old one look obsolete and vendors would have trouble selling their CDs.
From Debian 5.0, the numbering scheme of point releases was changed, conforming to the GNU version numbering standard; the first point release of Debian 5.0 was 5.0.1 instead of 5.0r1. The numbering scheme was once again changed for the first Debian 7 update, which was version 7.1. The "r" scheme is no longer in use, but point release announcements include a note about not throwing away old CDs.
The code names of Debian releases are names of characters from the "Toy Story" films.
Debian 8, the old old stable, was named "Jessie" after the cowgirl in "Toy Story 2", "Toy Story 3" and "Toy Story 4".
Debian 9, the current old stable, was named "Stretch" after the toy rubber octopus in "Toy Story 3".
Debian 10, the current stable, is named "Buster", after the pet dachshund in "Toy Story".
Debian 11 will be called "Bullseye", after Woody's horse.
Debian 12 will be called "Bookworm", after the intelligent worm toy with a built-in flash-light seen in "Toy Story 3".
The "unstable" suite is permanently nicknamed Sid, after the emotionally unstable boy next door who regularly destroyed toys, with many of his own toys being either destroyed, have missing pieces, or replaced with parts from other toys.
This naming tradition came about because Bruce Perens was involved in the early development of Debian while working at Pixar.
Debian is one of the most popular Linux distributions, and many other distributions have been created from the Debian codebase. , DistroWatch lists 141 active Debian derivatives. The Debian project provides its derivatives with guidelines for best practices and encourages derivatives to merge their work back into Debian.
Debian Pure Blends are subsets of a Debian release configured out-of-the-box for users with particular skills and interests. For example, Debian Jr. is made for children, while Debian Science is for researchers and scientists. The complete Debian distribution includes all available Debian Pure Blends. "Debian Blend" (without "Pure") is a term for a Debian-based distribution that strives to become part of mainstream Debian, and have its extra features included in future releases.
Debian GNU/kFreeBSD is a discontinued Debian flavor. It used the FreeBSD kernel and GNU userland. The majority of software in Debian GNU/kFreeBSD was built from the same sources as Debian, with some kernel packages from FreeBSD. The "k" in "kFreeBSD" is an abbreviation for "kernel", which refers to the FreeBSD kernel. Before discontinuing the project, Debian maintained i386 and amd64 ports. The last version of Debian kFreeBSD was Debian 8 (Jessie) RC3.
Debian GNU/kFreeBSD was created in 2002. It was included in Debian 6.0 (Squeeze) as a technology preview, and in Debian 7.0 (Wheezy) as an official port. Debian GNU/kFreeBSD was discontinued as an officially supported platform as of Debian 8.0. Debian developers cited OSS, pf, jails, NDIS, and ZFS as reasons for being interested in the FreeBSD kernel.
It has not been updated since Debian 8. As of July 2019, however, the operating system continues to be maintained unofficially.
Debian GNU/Hurd is a flavor based on the Hurd microkernel, instead of Linux. Debian GNU/Hurd has been in development since 1998, and made a formal release in May 2013, with 78% of the software packaged for Debian GNU/Linux ported to the GNU Hurd. Hurd is not yet an official Debian release, and is maintained and developed as an unofficial port.
Debian GNU/Hurd is distributed as an installer CD (running the official Debian installer) or ready-to-run virtual disk image (Live CD, Live USB). The CD uses the IA-32 architecture, making it compatible with IA-32 and x86-64 PCs. The current version of Debian GNU/Hurd is 2019, published in July 2019.
The Debian "swirl" logo was designed by Raul Silva in 1999 as part of a contest to replace the semi-official logo that had been used. The winner of the contest received an @debian.org email address, and a set of Debian 2.1 install CDs for the architecture of their choice. There has been no official statement from the Debian project on the logo's meaning, but at the time of the logo's selection, it was suggested that the logo represented the magic smoke ( or the genie ) that made computers work.
One theory about the origin of the Debian logo is that Buzz Lightyear, the chosen character for the first named Debian release, has a swirl in his chin. Stefano Zacchiroli also suggested that this swirl is the Debian one. Buzz Lightyear's swirl is a more likely candidate as the codenames for Debian are names of Toy Story characters. The developer of Debian also used to work for Pixar.
Multimedia support has been problematic in Debian regarding codecs threatened by possible patent infringements, without sources or under too restrictive licenses, and regarding technologies such as Adobe Flash. Even though packages with problems related to their distribution could go into the non-free area, software such as libdvdcss is not hosted at Debian.
A notable third party repository exists, formerly named debian-multimedia.org, providing software not present in Debian such as Windows codecs, libdvdcss and the Adobe Flash Player. Even though this repository is maintained by Christian Marillat, a Debian developer, it is not part of the project and is not hosted on a Debian server. The repository provides packages already included in Debian, interfering with the official maintenance. Eventually, project leader Stefano Zacchiroli asked Marillat to either settle an agreement about the packaging or to stop using the "Debian" name. Marillat chose the latter and renamed the repository to deb-multimedia.org. The repository was so popular that the switchover was announced by the official blog of the Debian project.
Hardware requirements are at least those of the kernel and the GNU toolsets. Debian's recommended system requirements depend on the level of installation, which corresponds to increased numbers of installed components:
The real minimum memory requirements depend on the architecture and may be much less than the numbers listed in this table. It is possible to install Debian with 170 MB of RAM for x86-64; the installer will run in low memory mode and it is recommended to create a swap partition. The installer for z/Architecture requires about 20 MB of RAM, but relies on network hardware. Similarly, disk space requirements, which depend on the packages to be installed, can be reduced by manually selecting the packages needed. , no Pure Blend exists that would lower the hardware requirements easily.
It is possible to run graphical user interfaces on older or low-end systems, but the installation of window managers instead of desktop environments is recommended, as desktop environments are more resource intensive. Requirements for individual software vary widely and must be considered, with those of the base operating environment.
, the official ports are:
Unofficial ports are available as part of the "unstable" distribution:
Debian supports a variety of ARM-based NAS devices. The NSLU2 was supported by the installer in Debian 4.0 and 5.0, and Martin Michlmayr is providing installation tarballs since version 6.0. Other supported NAS devices are the Buffalo Kurobox Pro, GLAN Tank, Thecus N2100 and QNAP Turbo Stations.
Devices based on the Kirkwood system on a chip (SoC) are supported too, such as the SheevaPlug plug computer and OpenRD products. There are efforts to run Debian on mobile devices, but this is not a project goal yet since the Debian Linux kernel maintainers would not apply the needed patches. Nevertheless, there are packages for resource-limited systems.
There are efforts to support Debian on wireless access points. Debian is known to run on set-top boxes. Work is ongoing to support the AM335x processor, which is used in electronic point of service solutions. Debian may be customized to run on cash machines.
BeagleBoard, a low-power open-source hardware single-board computer (made by Texas Instruments) has switched to Debian Linux preloaded on its Beaglebone Black board's flash.
Roqos Core, manufactured by Roqos, is a x86-64 based IPS firewall router running Debian Linux.
Debian's policies and team efforts focus on collaborative software development and testing processes. As a result, a new major release tends to occur every two years with revision releases that fix security issues and important problems. The Debian project is a volunteer organization with three foundational documents:
Debian developers are organized in a web of trust. There are about one thousand active Debian developers, but it is possible to contribute to the project without being an official developer.
The project maintains official mailing lists and conferences for communication and coordination between developers. For issues with single packages and other tasks, a public bug tracking system is used by developers and end users. Internet Relay Chat channels (primarily on the Open and Free Technology Community (OFTC) and freenode networks) are also used for communication among developers and to provide real time help.
Debian is supported by donations made to organizations authorized by the leader. The largest supporter is Software in the Public Interest, the owner of the Debian trademark, manager of the monetary donations and umbrella organization for various other community free software projects.
A Project Leader is elected once per year by the developers. The leader has special powers, but they are not absolute, and appoints delegates to perform specialized tasks. Delegates make decisions as they think is best, taking into account technical criteria and consensus. By way of a General Resolution, the developers may recall the leader, reverse a decision made by the leader or a delegate, amend foundational documents and make other binding decisions. The voting method is based on the Schulze method (Cloneproof Schwartz Sequential Dropping).
Project leadership is distributed occasionally. Branden Robinson was helped by the Project Scud, a team of developers that assisted the leader, but there were concerns that such leadership would split Debian into two developer classes. Anthony Towns created a supplemental position, Second In Charge (2IC), that shared some powers of the leader. Steve McIntyre was 2IC and had a 2IC himself.
One important role in Debian's leadership is that of a release manager. The release team sets goals for the next release, supervises the processes and decides when to release. The team is led by the next release managers and stable release managers. Release assistants were introduced in 2003.
The Debian Project has an influx of applicants wishing to become developers. These applicants must undergo a vetting process which establishes their identity, motivation, understanding of the project's principles, and technical competence. This process has become much harder throughout the years.
Debian developers join the project for many reasons. Some that have been cited include:
Debian developers may resign their positions at any time or, when deemed necessary, they can be expelled. Those who follow the retiring protocol are granted the "emeritus" status and they may regain their membership through a shortened new member process.
Each software package has a "maintainer" that may be either one person or a team of Debian developers and non-developer maintainers. The maintainer keeps track of upstream releases, and ensures that the package coheres with the rest of the distribution and meets the standards of quality of Debian. Packages may include modifications introduced by Debian to achieve compliance with Debian Policy, even to fix non-Debian specific bugs, although coordination with upstream developers is advised.
The maintainer releases a new version by uploading the package to the "incoming" system, which verifies the integrity of the packages and their digital signatures. If the package is found to be valid, it is installed in the package archive into an area called the "pool" and distributed every day to hundreds of mirrors worldwide. The upload must be signed using OpenPGP-compatible software. All Debian developers have individual cryptographic key pairs. Developers are responsible for any package they upload even if the packaging was prepared by another contributor.
Initially, an accepted package is only available in the "unstable" branch. For a package to become a candidate for the next release, it must migrate to the "Testing" branch by meeting the following:
Thus, a release-critical bug in a new version of a shared library on which many packages depend may prevent those packages from entering "Testing", because the updated library must meet the requirements too. From the branch viewpoint, the migration process happens twice per day, rendering "Testing" in perpetual beta.
Periodically, the release team publishes guidelines to the developers in order to ready the release. A new release occurs after a freeze, when all important software is reasonably up-to-date in the "Testing" branch and any other significant issues are solved. At that time, all packages in the "testing" branch become the new "stable" branch. Although freeze dates are time-based, release dates are not, which are announced by the release managers a couple of weeks beforehand.
A version of a package can belong to more than one branch, usually "testing" and "unstable". It is possible for a package to keep the same version between stable releases and be part of "oldstable", "stable", "testing" and "unstable" at the same time. Each branch can be seen as a collection of pointers into the package "pool" mentioned above.
A new "stable" branch of Debian gets released approximately every 2 years. It will receive official support for about 3 years with update for major security or usability fixes. Point releases will be available every several months as determined by Stable Release Managers (SRM).
Debian also launched its Long Term Support (LTS) project since Debian 6 (Debian Squeeze). For each Debian release, it will receive two years of extra security updates provided by LTS Team after its End Of Life (EOL). However, no point releases will be made. Now each Debian release can receive 5 years of security support in total.
The Debian project handles security through public disclosure rather than through obscurity. Debian security advisories are compatible with the Common Vulnerabilities and Exposures dictionary, are usually coordinated with other free software vendors and are published the same day a vulnerability is made public. There used to be a security audit project that focused on packages in the stable release looking for security bugs; Steve Kemp, who started the project, retired in 2011 but resumed his activities and applied to rejoin in 2014.
The "stable" branch is supported by the Debian security team; "oldstable" is supported for one year. Although Squeeze is not officially supported, Debian is coordinating an effort to provide long-term support (LTS) until February 2016, five years after the initial release, but only for the IA-32 and x86-64 platforms. "Testing" is supported by the "testing" security team, but does not receive updates in as timely a manner as "stable". "Unstable"s security is left for the package maintainers.
The Debian project offers documentation and tools to harden a Debian installation both manually and automatically. AppArmor support is available and enabled by default since Buster. Debian provides an optional hardening wrapper, and does not harden all of its software by default using gcc features such as PIE and buffer overflow protection, unlike operating systems such as OpenBSD, but tries to build as many packages as possible with hardening flags.
In May 2008, a Debian developer discovered that the OpenSSL package distributed with Debian and derivatives such as Ubuntu made a variety of security keys vulnerable to a random number generator attack, since only 32,767 different keys were generated. The security weakness was caused by changes made in 2006 by another Debian developer in response to memory debugger warnings. The complete resolution procedure was cumbersome because patching the security hole was not enough; it involved regenerating all affected keys and certificates.
The cost of developing all of the packages included in Debian 5.0 Lenny (323 million lines of code) has been estimated to be about , using one method based on the COCOMO model. , Black Duck Open Hub estimates that the current codebase (74 million lines of code) would cost about to develop, using a different method based on the same model.
A large number of forks and derivatives have been built upon Debian over the years. Among the more notable are Ubuntu, developed by Canonical LTD. and first released in 2004, which has surpassed Debian in popularity with desktop users; Knoppix, first released in the year 2000 and one of the first distributions optimized to boot from external storage; and Devuan, which gained attention in 2014 when it forked in disagreement over Debian's adoption of the systemd software suite, and has been mirroring Debian releases since 2017.

</doc>
<doc id="8243" url="https://en.wikipedia.org/wiki?curid=8243" title="Doonesbury">
Doonesbury

Doonesbury is a comic strip by American cartoonist Garry Trudeau that chronicles the adventures and lives of an array of characters of various ages, professions, and backgrounds, from the President of the United States to the title character, Michael Doonesbury, who has progressed from a college student to a youthful senior citizen over the decades.
Created in "the throes of '60s and '70s counterculture," and frequently political in nature, "Doonesbury" features characters representing a range of affiliations, but the cartoon is noted for a liberal viewpoint. The name "Doonesbury" is a combination of the word "doone" (prep school slang for someone who is clueless, inattentive, or careless) and the surname of Charles Pillsbury, Trudeau's roommate at Yale University.
"Doonesbury" is written and penciled by Garry Trudeau, then inked and lettered by an assistant: Don Carlton
then Todd Pound. Sunday strips are colored by George Corsillo. A daily strip through most of its existence, since February 2014 "Doonesbury" has run repeat strips Monday through Saturday, and new strips on Sunday.
"Doonesbury" began as a continuation of "Bull Tales", which appeared in the Yale University student newspaper, the "Yale Daily News", from 1968 to 1970. It focused on local campus events at Yale.
"Doonesbury" proper debuted as a daily strip in twenty-eight newspapers on October 26, 1970 (it being the first strip from Universal Press Syndicate). A Sunday strip began on March 21, 1971. Many of the early strips were reprints of the "Bull Tales" cartoons, with some changes to the drawings and plots. B. D.'s helmet changed from having a "Y" (for Yale) to a star (for the fictional Walden College). Mike and B. D. started "Doonesbury" as roommates; they were not roommates in "Bull Tales".
"Doonesbury" became known for its social and political commentary. As of the mid-2010s it is syndicated in approximately 1,400 newspapers worldwide.
In May 1975, "Doonesbury" became the first daily comic strip to win a Pulitzer Prize, taking the award for Editorial Cartooning. That year, US President Gerald Ford told the Radio and Television Correspondents' Association at their annual dinner, "There are only three major vehicles to keep us informed as to what is going on in Washington: the electronic media, the print media, and "Doonesbury", not necessarily in that order."
In 1977, Trudeau wrote a script for a 26-minute animated special, "A Doonesbury Special", which was produced and directed by Trudeau along with John Hubley (who died during the storyboarding stage) and Faith Hubley. The special was first broadcast by NBC on November 27, 1977. It won a Special Jury Award at the Cannes International Film Festival for best short film, and received an Oscar nomination (for best animated short film), both in 1978. Voice actors for the special included Barbara Harris, William Sloane Coffin, Jr., Jack Gilford and Will Jordan. Also included were two songs "sung" by the character Jimmy Thudpucker (actually actor/singer/songwriter/producer James Allen "Jimmy" Brewer), entitled "Stop in the Middle" and "I Do Believe", also part of the "Special". While the compositions and performances were credited to "Jimmy Thudpucker", they were in fact co-written and sung by Brewer, who also co-wrote and provided the vocals for "Ginny's Song", a 1976 single on the Warner Bros. label, and "Jimmy Thudpucker's Greatest Hits", an LP released by Windsong Records, John Denver's subsidiary of RCA Records.
Trudeau took a 22-month hiatus, from January 1983 to October 1984. Before the break in the strip, the characters were eternal college students, living in a commune together near Walden College, which was modeled after Trudeau's alma mater, Yale. During the break, Trudeau helped create a Broadway musical of the strip, showing the graduation of the main characters. The Broadway adaptation opened at the Biltmore Theatre on November 21, 1983, and played 104 performances. Elizabeth Swados composed the music for Trudeau's book and lyrics.
The strip resumed some time after the events in the musical, with further changes having taken place after the end of the musical's plot. While Mike, Mark, Zonker, B.D., and Boopsie were all now graduates, B.D. and Boopsie were living in Malibu, California, where B.D. was a third-string quarterback for the Los Angeles Rams, and Boopsie was making a living from walk-on and cameo roles. Mark was living in Washington, DC, working for National Public Radio. Michael and J.J. had gotten married, and Mike had dropped out of business school to start work in an advertising agency in New York City. Zonker, still not ready for the "real world", was living with Mike and J.J. until he was accepted as a medical student at his Uncle Duke's "Baby Doc College" in Haiti.
Prior to the hiatus, the strip's characters had aged only slightly. But when Trudeau returned to "Doonesbury," the characters began to age in something close to real time, as in "Gasoline Alley" and "For Better or for Worse." Since then, the main characters' ages and career developments have tracked that of standard media portrayals of baby boomers, with jobs in advertising, law enforcement, and the dot-com boom. Current events are mirrored through the original characters, their offspring (the "second generation"), and occasional new characters.
Garry Trudeau received the National Cartoonist Society Newspaper Comic Strip Award for 1994, and their Reuben Award for 1995 for his work on the strip.
"Doonesbury"'s syndicate, Universal Uclick, announced on May 29, 2013, that the comic strip would go on hiatus from June 10 to Labor Day of that year while Garry Trudeau worked on his streaming video comedy "Alpha House", which was picked up by Amazon Studios. "Doonesbury Flashbacks" were offered during those weeks, but due to the unusually long hiatus, some newspapers opted to run different comic strips instead. Sunday strips returned as scheduled, but the daily strip's hiatus was extended until November 2013.
After "Alpha House" was retained for a second season in February 2014, Trudeau announced that he would now produce only Sunday strips for the foreseeable future. Since March 3, 2014, the strip has offered reruns starting from the very beginning of its history as opposed to the recent ones that re-run when Trudeau is on vacation.
Though "Alpha House" was cancelled following its second season and has not been in production since 2014, Trudeau has not returned to creating new daily "Doonesbury" strips.
In a 2018 interview with "Rolling Stone", Trudeau said that while Donald Trump appears in only a limited number of strips, "for the last two years, he's been subtext in almost all of them."
With the exception of Walden College, Trudeau has frequently used real-life settings, based on real scenarios, but with fictional results. Due to deadlines, some real-world events have rendered some of Trudeau's comics unusable, such as a 1973 series featuring John Ehrlichman, a 1989 series set in Tiananmen Square in Beijing, China, a 1993 series involving Zoë Baird, and a 2005 series involving Harriet Miers. Trudeau has also displayed fluency in various forms of jargon, including those of real estate agents, flight attendants, computer scientists, journalists, presidential aides, and soldiers in Iraq.
The unnamed college attended by the main characters was later given the name "Walden College", revealed to be in Connecticut (the same state as Yale), and depicted as devolving into a third-rate institution under the weight of grade inflation, slipping academic standards, and the end of tenure, issues that Trudeau has consistently revisited since the original characters graduated. Some of the second generation of "Doonesbury" characters have attended Walden, a venue Trudeau uses to advance his concerns about academic standards in the United States.
President King, the leader of Walden College, was originally intended as a parody of Kingman Brewster, President of Yale, but all that remains of that is a certain physical resemblance.
Even though "Doonesbury" frequently features real-life U.S. politicians, they are rarely depicted with their real faces. Originally, strips featuring the President of the United States would show an external view of the White House, with dialogue emerging from inside. During the Gerald Ford administration, characters would be shown speaking to Ford at press conferences, and fictional dialogue supposedly spoken by Ford would be written as coming "off-panel". Similarly, while having several characters as students in a class taught by Henry Kissinger, the dialogue made up for Kissinger would also come from "off-panel" (although Kissinger had earlier appeared as a character with his face shown in a 1972 series of strips in which he met Mark Slackmeyer while the latter was on a trip to Washington). Sometimes hands, or in rare cases, the back of heads would also be seen.
Later, personal symbols reflecting some aspect of their character came into use. For example, during the 1980s, character Ron Headrest served as a doppelgänger for Ronald Reagan and was depicted as a computer-generated artificial-intelligence, an image based on the television character "Max Headroom". Members of the Bush family have been depicted as invisible. During his term as Vice President, George H. W. Bush was first depicted as completely invisible, his words emanating from a little "voice box" in the air. (In one strip, published March 20, 1988, the vice president almost materialized, but only made it to an outline before reverting to invisibility.)
George W. Bush was symbolized by a Stetson hat atop the same invisible point, because he was Governor of Texas prior to his presidency (Trudeau accused him of being "all hat and no cattle", reiterating the characterization of Bush by columnist Molly Ivins). The point became a giant asterisk (a la Roger Maris) following the 2000 presidential elections and the controversy over vote-counting. Later, President Bush's hat was changed to a Roman military helmet (again, atop an asterisk) representing imperialism. Towards the end of his first term, the helmet became battered, with the gilt work starting to come off and with clumps of bristles missing from the top. By late 2008, the helmet had been dented almost beyond recognition. No symbol for Barack Obama has appeared in the strip; the May 30, 2009, strip had Obama and an aide wondering what the reason for this might be (off panel).
Other symbols include a waffle for Bill Clinton (chosen by popular vote—the other possibility had been a flipping coin), an unexploded (but sometimes lit) bomb for Newt Gingrich, a feather for Dan Quayle, and a giant groping hand for Arnold Schwarzenegger (who is addressed by other characters as "Herr Gröpenfuhrer", a reference to accusations of sexual assault against Schwarzenegger). Many less well-known politicians have also been represented as icons over the years, like a swastika for David Duke, but only for the purposes of a gag strip or two. Trudeau has made his use of icons something of an in joke to readers, where the first appearance of a new one is often a punchline in itself.
The long career of the series and continual use of real-life political figures, analysts note, have led to some uncanny cases of the cartoon foreshadowing a national shift in the politicians' political fortunes. Tina Gianoulis in "St. James Encyclopedia of Popular Culture" observes that "In 1971, well before the conservative Reagan years, a forward-looking BD called Ronald Reagan his 'hero.' In 1984, almost ten years before Congressman Newt Gingrich became Speaker of the House, another character worried that he would 'wake up someday in a country run by Newt Gingrich.'" In 1999, Donald Trump was depicted as a presidential candidate. In its 2003 series "John Kerry: A Candidate in the Making" on the 2004 presidential race, the "Boston Globe" reprinted and discussed 1971 "Doonesbury" cartoons of the young Kerry's Vietnam War protest speeches.
"Doonesbury" has a large group of recurring characters, with 24 currently listed at the strip's website. There, it notes that "readers new to "Doonesbury" sometimes experience a temporary bout of character shock," as the sheer number of characters (and the historical connections among them) can be overwhelming.
The main characters are a group who attended the fictional Walden College during the strip's first 12 years, and moved into a commune together in April 1972. Most of the other characters first appeared as family members, friends, or other acquaintances. The original Walden Commune residents were Mike Doonesbury, Zonker Harris, Mark Slackmeyer, Nicole, Bernie, and DiDi. In September 1972, Joanie Caucus joined the comic, meeting Mike and Mark in Colorado and eventually moving into the commune. They were later joined by B.D. and his girlfriend (later wife) Boopsie, upon B.D.'s return from Vietnam. Nicole, DiDi, and Bernie were mostly phased out in subsequent years, and Zonker's Uncle Duke was introduced as the most prominent character outside the Walden group, and the main link to many secondary characters.
The Walden students graduated in 1983, after which the strip began to progress in something closer to real time. Their spouses and developing families became more important after this: Joanie's daughter J.J. Caucus married Mike and they had a daughter, Alex Doonesbury. They divorced, Mike married Kim Rosenthal, a Vietnamese refugee (who had appeared in the strip as a baby adopted by a Jewish family just after the fall of Saigon; see Operation Babylift), and J.J. married Zeke Brenner, her former boyfriend and Uncle Duke's former groundskeeper. Joanie married Rick Redfern, and they had a son, Jeff. Uncle Duke and Roland Hedley have also appeared often, frequently in more topical settings unconnected to the main characters. In more recent years the second generation has taken prominence as they have grown to college age: Jeff Redfern, Alex Doonesbury, Zonker's nephew Zipper Harris, and Uncle Duke's son Earl.
"Doonesbury" has delved into a number of political and social issues, causing controversies and breaking new ground on the comics pages. Among the controversies:
Charles M. Schulz of "Peanuts" called Trudeau "unprofessional" for taking a long sabbatical. (See also, similar comments by Schulz about sabbaticals taken by Bill Watterson.) Nor was the return of the strip itself greeted with universal acclaim; in 1985, "Saturday Review" listed Trudeau as one of the country's "Most Overrated People in American Arts and Letters," commenting that the "most publicized return since MacArthur's has produced a strip that is predictable, mean-spirited, and not as funny as before."
"Doonesbury" has angered, irritated, or been rebuked by many of the political figures that have appeared or been referred to in the strip over the years. A 1984 series of strips showing Vice President George H.W. Bush placing his manhood in a blind trust—in parody of Bush's use of that financial instrument to fend off concerns that his governmental decisions would be influenced by his investment holdings—brought the politician to complain, ""Doonesbury"'s carrying water for the opposition. Trudeau is coming out of deep left field."
Some conservatives have intensely criticized "Doonesbury". Several examples are cited in the Milestones section of the strip's website. The strip has also met criticism from its readers almost since it began syndicated publication. For example, when Lacey Davenport's husband Dick, in the last moments before his death, calls on God, several conservative pundits called the strip blasphemous. The sequence of Dick Davenport's final bird-watching and fatal heart attack was run in November 1986.
Liberal politicians skewered by Trudeau in the strip have also complained, including Democrats such as former U.S. House Speaker Tip O'Neill and California Governor Jerry Brown.
Strips about America's wars have also generated controversy, including Vietnam, Grenada, Panama and both Gulf Wars.
After many letter-writing campaigns demanding the removal of the strip were unsuccessful, conservatives changed their tactics, and instead of writing to newspaper editors, they began writing to one of the printers who prints the color Sunday comics. In 2005, Continental Features gave in to their demands, and refused to continue printing the Sunday "Doonesbury", causing it to disappear from the 38 Sunday papers that Continental Features printed. Of the 38, only one newspaper, "The Anniston Star" in Anniston, Alabama, continued to carry the Sunday "Doonesbury", though of necessity in black and white.
Some newspapers have dealt with the criticism by moving the strip from the comics page to the editorial page, because many people believe that a politically based comic strip like "Doonesbury" does not belong in a traditionally child-friendly comics section. The "Lincoln Journal" started the trend in 1973. In some papers (such as the "Tulsa World" and "Orlando Sentinel") "Doonesbury" appears on the opinions page alongside "Mallard Fillmore", a politically conservative comic strip.

</doc>
<doc id="8244" url="https://en.wikipedia.org/wiki?curid=8244" title="Dice">
Dice

Dice (singular die or dice) are small, throwable objects with marked sides that can rest in multiple positions. They are used for generating random numbers, commonly as part of tabletop games, including dice games, board games, role-playing games, and games of chance.
A traditional die is a cube with each of its six faces marked with a different number of dots (pips) from one to six. When thrown or rolled, the die comes to rest showing a random integer from one to six on its upper surface, with each value being equally likely. Dice may also have polyhedral or irregular shapes and may have faces marked with numerals or symbols instead of pips. Loaded dice are designed to favor some results over others for cheating or entertainment.
Dice have been used since before recorded history, and it is uncertain where they originated. It is theorized that dice developed from the practice of fortune-telling with the talus of hoofed animals, colloquially known as knucklebones. The Egyptian game of senet was played with flat two-sided throwsticks which indicated the number of squares a player could move, and thus functioned as a form of dice. Senet was played before 3000 BC and up to the 2nd century AD. Perhaps the oldest known dice were excavated as part of a backgammon-like game set at the Burnt City, an archeological site in south-eastern Iran, estimated to be from between 2800–2500 BC. Bone dice from Skara Brae have been dated to 3100–2400 BC. Excavations from graves at Mohenjo-daro, an Indus Valley civilization settlement, unearthed terracotta dice dating to 2500–1900 BC.
Games involving dice are mentioned in the ancient Indian "Rigveda", "Atharvaveda," "Mahabharata" and Buddhist games list. There are several biblical references to "casting lots" ( "yappîlū ḡōrāl"), as in Psalm 22, indicating that dicing (or a related activity) was commonplace when the psalm was composed. Knucklebones was a game of skill played in ancient Greece; a derivative form had the four sides of bones receive different values like modern dice.
Although gambling was illegal, many Romans were passionate gamblers who enjoyed dicing, which was known as "aleam ludere" ("to play at dice"). There were two sizes of Roman dice. "Tali" were large dice inscribed with one, three, four, and six on four sides. "Tesserae" were smaller dice with sides numbered from one to six. Twenty-sided dice date back to the 2nd century AD and from Ptolemaic Egypt as early as the 2nd century BC.
Dominoes and playing cards originated in China as developments from dice. The transition from dice to playing cards occurred in China around the Tang dynasty, and coincides with the technological transition from rolls of manuscripts to block printed books. In Japan, dice were used to play a popular game called sugoroku. There are two types of sugoroku. "Ban-sugoroku" is similar to backgammon and dates to the Heian period, while "e-sugoroku" is a racing game.
Dice are thrown onto a surface either from the hand or from a container designed for this (such as a cup or tray). The face of the die that is uppermost when it comes to rest provides the value of the throw. 
The result of a die roll is determined by the way it is thrown, according to the laws of classical mechanics. A die roll is made random by uncertainty in minor factors such as tiny movements in the thrower's hand; they are thus a crude form of hardware random number generator. 
One typical contemporary dice game is craps, where two dice are thrown simultaneously and wagers are made on the total value of the two dice. Dice are frequently used to introduce randomness into board games, where they are often used to decide the distance through which a piece will move along the board (as in backgammon and "Monopoly").
Common dice are small cubes, most often across, whose faces are numbered from one to six, usually by patterns of round dots called pips. (While the use of Arabic numerals is occasionally seen, such dice are less common.)
Opposite sides of a modern die traditionally add up to seven, requiring the 1, 2, and 3 faces to share a vertex. The faces of a die may be placed clockwise or counterclockwise about this vertex. If the 1, 2, and 3 faces run counterclockwise, the die is called "right-handed". If those faces run clockwise, the die is called "left-handed". Western dice are normally right-handed, and Chinese dice are normally left-handed.
The pips on standard six-sided dice are arranged in specific patterns as shown. Asian style dice bear similar patterns to Western ones, but the pips are closer to the center of the face; in addition, the pips are differently sized on Asian style dice, and the pips are colored red on the 1 and 4 sides. Red fours may be of Indian origin.
Non-precision dice are manufactured via the plastic injection molding process. The pips or numbers on the die are a part of the mold. Different pigments can be added to the dice to make them opaque or transparent, or multiple pigments may be added to make the dice speckled or marbled.
The coloring for numbering is achieved by submerging the die entirely in paint, which is allowed to dry. The die is then polished via a tumble finishing process similar to rock polishing. The abrasive agent scrapes off all of the paint except for the indents of the numbering. A finer abrasive is then used to polish the die. This process also creates the smoother, rounded edges on the dice.
Precision casino dice may have a polished or sand finish, making them transparent or translucent respectively. Casino dice have their pips drilled, then filled flush with a paint of the same density as the material used for the dice, such that the center of gravity of the dice is as close to the geometric center as possible. This mitigates concerns that the pips will cause a small bias. All such dice are stamped with a serial number to prevent potential cheaters from substituting a die. Precision backgammon dice are made the same way; they tend to be slightly smaller and have rounded corners and edges, to allow better movement inside the dice cup and stop forceful rolls from damaging the playing surface.
The word die comes from Old French "dé"; from Latin "datum" "something which is given or played".
While the terms "ace", "deuce", "trey", "cater", "cinque" and "sice" are generally obsolete, with the names of the numbers preferred, they are still used by some professional gamblers to designate different sides of the dice. "Ace" is from the Latin "as", meaning "a unit"; the others are 2 to 6 in Old French.
The term "snake eyes" is the outcome of rolling the dice and getting only one pip on each die. The "Online Etymology Dictionary" traces use of the term as far back as 1919.
The term "boxcars", also known as "midnight", is the outcome of rolling the dice and getting a six on each die. The pair of six pips resembles a pair of boxcars on a freight train.
Using Unicode characters, the faces ⚀ ⚁ ⚂ ⚃ ⚄ ⚅, can be shown in text using the range U+2680 to U+2685 or using decimal codice_1 to codice_2.
A loaded, weighted, cheat, or crooked die is one that has been tampered with so that it will land with a specific side facing upwards more or less often than a fair die would. There are several methods for creating loaded dice, including rounded faces, off-square faces, and weights. Casinos and gambling halls frequently use transparent cellulose acetate dice as tampering is easier to detect than with opaque dice.
Various shapes like two-sided or four-sided dice are documented in archaeological findings e.g. from Ancient Egypt or the Middle East. While the cubical six-sided die became the most common type in many parts of the world, other shapes were always known, like 20-sided dice in Ptolemaic and Roman times. 
The modern tradition of using "sets" of polyhedral dice started around the end of the 1960s when non-cubical dice became popular among players of wargames, and since have been employed extensively in role-playing games and trading card games. Dice using both the numerals 6 and 9, which are reciprocally symmetric through rotation, typically distinguish them with a dot or underline.
Dice are often sold in sets, matching in color, of six different shapes. Five of the dice are shaped like the Platonic solids, whose faces are regular polygons. Aside from the cube, the other four Platonic solids have 4, 8, 12, and 20 faces, allowing for those number ranges to be generated. The only other common non-cubical die is the 10-sided die, a pentagonal trapezohedron die, whose faces are ten kites, each with two different edge lengths, three different angles, and two different kinds of vertices. Such sets frequently include a second 10-sided die either of contrasting color or numbered by tens, allowing the pair of 10-sided dice to be combined to generate numbers between 1 and 100.
Using these dice in various ways, games can closely approximate a variety of probability distributions. For instance, 10-sided dice can be rolled in pairs to produce a uniform distribution of random percentages, and summing the values of multiple dice will produce approximations to normal distributions. 
Unlike other common dice, a four-sided (tetrahedral) die does not have a side that faces upward when it is at rest on a surface, so it must be read in a different way. On some four-sided dice, each face features multiple numbers, with same number printed near each vertex on all sides. In this case, the number around the vertex pointing up is used. Alternatively, the numbers on a tetrahedral die can be placed at the middles of the edges, in which case the numbers around the base are used.
Normally, the faces on a die will be placed so opposite faces will add up to one more than the number of faces. (This is not possible with 4-sided dice and dice with an odd-number of faces.) Some dice, such as those with 10 sides, are usually numbered sequentially beginning with 0, in which case the opposite faces will add to one less than the number of faces.
Some twenty-sided dice sport a different arrangement used for the purpose of keeping track of a numerical value that counts down, such as health points. These "spindown dice" are arranged so the faces are consecutively numbered, allowing the user to simply turn to the next lower number to keep track of a declining value. They are commonly used with collectible card games.
"Uniform fair dice" are dice where all faces have equal probability of outcome due to the symmetry of the die as it is face-transitive. Theoretically, these include:
Two other types of polyhedra are technically not face-transitive, but are still fair dice due to symmetry:
Long dice and teetotums can in principle be made with any number of faces, including odd numbers. Long dice are based on the infinite set of prisms. All the rectangular faces are mutually face-transitive, so they are equally probable. The two ends of the prism may be rounded or capped with a pyramid, designed so that the die cannot rest on those faces. 4-sided long dice are easier to roll than tetrahedra, and are used in the traditional board games dayakattai and daldøs.
The faces of most dice are labelled using sequences of whole numbers, usually starting at one, expressed with either pips or digits. However, there are some applications that require results other than numbers. Examples include letters for Boggle, directions for "Warhammer Fantasy Battle", Fudge dice, playing card symbols for poker dice, and instructions for sexual acts using sex dice.
Dice may have numbers that do not form a counting sequence starting at one. One variation on the standard die is known as the "average" die. These are six-sided dice with sides numbered codice_3, which have the same arithmetic mean as a standard die (3.5 for a single die, 7 for a pair of dice), but have a narrower range of possible values (2 through 5 for one, 4 through 10 for a pair). They are used in some table-top wargames, where a narrower range of numbers is required. Other numbered variations include Sicherman dice and nontransitive dice.
A die can be constructed in the shape of a sphere, with the addition of an internal cavity in the shape of the dual polyhedron of the desired die shape and an internal weight. The weight will settle in one of the points of the internal cavity, causing it to settle with one of the numbers uppermost. For instance, a sphere with an octahedral cavity and a small internal weight will settle with one of the 6 points of the cavity held downwards by the weight.
Polyhedral dice are commonly used in role-playing games. The fantasy role-playing game "Dungeons & Dragons" (D&D) is largely credited with popularizing dice in such games. Some games use only one type, like "Exalted" which uses only ten-sided dice. Others use numerous types for different game purposes, such as D&D, which makes use of all common polyhedral dice. Dice are usually used to determine the outcome of events. Games typically determine results either as a total on one or more dice above or below a fixed number, or a certain number of rolls above a certain number on one or more dice. Due to circumstances or character skill, the initial roll may have a number added to or subtracted from the final result, or have the player roll extra or fewer dice. To keep track of rolls easily, dice notation is frequently used.
Many board games use dice to randomize how far pieces move or to settle conflicts. Typically, this has meant that rolling higher numbers is better. Some games, such as "Axis & Allies", have inverted this system by making the lower values more potent. In the modern age, a few games and game designers have approached dice in a different way by making each side of the die similarly valuable. In "Castles of Burgundy", players spend their dice to take actions based on the die's value. In this game, a six is not better than a one, or vice versa. In "Quarriors" (and its descendant, Dicemasters), different sides of the dice can offer completely different abilities. Several sides often give resources while others grant the player useful actions.
Dice can be used for divination and using dice for such a purpose is called cleromancy. A pair of common dice is usual, though other forms of polyhedra can be used. Tibetan Buddhists sometimes use this method of divination. It is highly likely that the Pythagoreans used the Platonic solids as dice. They referred to such dice as "the dice of the gods" and they sought to understand the universe through an understanding of geometry in polyhedra.
Astrological dice are a specialized set of three 12-sided dice for divination; the first die represents planets, the Sun, the Moon, and the nodes of the Moon, the second die represents the 12 zodiac signs, and the third represents the 12 houses. A specialized icosahedron die provides the answers of the Magic 8-Ball, conventionally used to provide answers to yes-or-no questions.
Dice can be used to generate random numbers for use in passwords and cryptography applications. The Electronic Frontier Foundation describes a method by which dice can be used to generate passphrases. Diceware is a method recommended for generating secure but memorable passphrases, by repeatedly rolling five dice and picking the corresponding word from a pre-generated list.
In many gaming contexts, especially tabletop role-playing games, shorthand notations representing different dice rolls are used. A "d" or "D" is used to indicate a die with a specific number of sides; for example, codice_4 denotes a four-sided die. If several dice of the same type are to be rolled, this is indicated by a leading number specifying the number of dice. Hence, codice_5 means the player should roll six eight-sided dice and add the results. Modifiers to a die roll can also be indicated as desired. For example, codice_6 instructs the player to roll three six-sided dice, calculate the total, and add four to it.

</doc>
<doc id="8246" url="https://en.wikipedia.org/wiki?curid=8246" title="Dumpster diving">
Dumpster diving

Dumpster diving (also totting, skipping, skip diving or skip salvage,) is salvaging from large commercial, residential, industrial and construction containers for unused items discarded by their owners, but deemed useful to the picker. It is not confined to dumpsters and skips specifically, and may cover standard household waste containers, curb sides, landfills or small dumps.
Different terms are used to refer to different forms of this activity. For picking materials from the curbside trash collection, expressions such as curb shopping, trash picking or street scavenging are sometimes used. When seeking primarily metal to be recycled, one is scrapping. When picking the leftover food from traditional or industrial farming left in the fields one is gleaning. 
People dumpster dive for items such as clothing, furniture, food, and similar items in good working condition. Some people do this out of necessity due to poverty, while others do so professionally and systematically for profit.
The term "dumpster diving" emerged in the 1980s, combining "diving" with "dumpster", a large commercial trash bin. The term "Dumpster" itself comes from the Dempster Dumpster, a brand of bins manufactured by Dempster Brothers beginning in 1937. "Dumpster" became genericized by the 1970s. According to the "Oxford English Dictionary", the term "dumpster diving" is chiefly found in American English and first appeared in print in 1983, with the verb "dumpster-dive" appearing a few years later. In British English, the practice may be known as "skipping", from skip, another term for this type of container.
Alternative names for the practice include bin-diving, containering, D-mart, dumpstering, totting, and skipping. In Australia, garbage picking is called "skip dipping."
The term "binner" is often used to describe individuals who collect recyclable materials for their deposit value. For example, in Vancouver, British Columbia, binners, or bottle collectors, search garbage cans and dumpsters for recyclable materials that can be redeemed for their deposit value. On average, these binners earn about $40 a day for several garbage bags full of discarded containers.
The karung guni, Zabbaleen, the rag and bone man, waste picker, junk man or bin hoker are terms for people who make their living by sorting and trading trash. A similar process known as gleaning was practised in rural areas and some ancient agricultural societies, where the residue from farmers' fields was collected.
Some dumpster divers, who self-identify as freegans, aim to reduce their ecological footprint by living from dumpster-dived-goods, sometimes exclusively.
The activity is performed by people out of necessity in the developing world. Some scavengers perform in organized groups, and some organize on various internet forums and social networking websites. By reusing, or repurposing, resources destined for the landfill, dumpster diving is sometimes considered to be an environmentalist endeavor, and is thus practiced by many pro-green communities. The wastefulness of consumer society and throw-away culture compels some individuals to rescue usable items (for example, computers or smartphones, which are frequently discarded due to the extensive use of planned obsolescence in the technology industry) from destruction and divert them to those who can make use of the items.
A wide variety of things may be disposed while still repairable or in working condition, making salvage of them a source of potentially free items for personal use, or to sell for profit. Irregular, blemished or damaged items that are still otherwise functional are regularly thrown away. Discarded food that might have slight imperfections, near its expiration date, or that is simply being replaced by newer stock is often tossed out despite being still edible. Many retailers are reluctant to sell this stock at reduced prices because of the risks that people will buy it instead of the higher-priced newer stock, that extra handling time is required, and that there are liability risks. In the United Kingdom, cookery books have been written on the cooking and consumption of such foods, which has contributed to the popularity of skipping. Artists often use discarded materials retrieved from trash receptacles to create works of found objects or assemblage.
Students have been known to partake in dumpster diving to obtain high tech items for technical projects, or simply to indulge their curiosity for unusual items. Dumpster diving can additionally be used in support of academic research. Garbage picking serves as the main tool for garbologists, who study the sociology and archeology of trash in modern life. Private and government investigators may pick through garbage to obtain information for their inquiries. Illegal cigarette consumption may be deduced from discarded packages.
Dumpster diving can be hazardous, due to potential exposure to biohazardous matter, broken glass, and overall unsanitary conditions that may exist in dumpsters.
Arguments against garbage picking often focus on the health and cleanliness implications of people rummaging in trash. This exposes the dumpster divers to potential health risks, and, especially if the dumpster diver does not return the non-usable items to their previous location, may leave trash scattered around. Divers can also be seriously injured or killed by garbage collection vehicles; in January 2012, in La Jolla, Swiss-American man Alfonso de Bourbon was killed by a truck while dumpster diving. Further, there are also concerns around the legality of taking items that may still technically belong to the person who threw them away (or to the waste management operator), and whether the taking of some items like discarded documents is a violation of privacy. In general a legal concept called abandonment of property covers this question of the subject of ownership of property that is disposed of.
Discarded billing records may be used for identity theft. As a privacy violation, discarded medical records as trash led to a $140,000 penalty against Massachusetts billing company Goldthwait Associates and a group of pathology offices in 2013 and a $400,000 settlement between Midwest Women's Healthcare Specialists and 1,532 clients in Kansas City in 2014.
Since dumpsters are usually located on private premises, divers may occasionally get in trouble for trespassing while dumpster diving, though the law is enforced with varying degrees of rigor. Some businesses may lock dumpsters to prevent pickers from congregating on their property, vandalism to their property, and to limit potential liability if a dumpster diver is injured while on their property. 
Police searches of discarded waste as well as similar methods are also generally not considered violations; evidence seized in this manner has been permitted in many criminal trials. In the United States this has been affirmed by numerous courts including and up to the Supreme Court, in the decision "California v. Greenwood". The doctrine is not as well established in regards to civil litigation.
Companies run by private investigators specializing in such techniques have emerged as a result of the need for discreet, undetected retrieval of documents and evidence for civil and criminal trials. Private investigators have also written books on "P.I. technique" in which dumpster diving or its equivalent "wastebasket recovery" figures prominently.
In 2009, a Belgian dumpster diver and eco-activist nicknamed Ollie was detained for a month for removing food from a garbage can, and was accused of theft and burglary. On February 25, 2009, he was arrested for removing food from a garbage can at an AD Delhaize supermarket in Bruges. Ollie's trial evoked protests in Belgium against restrictions from taking discarded food items.
In Ontario, Canada, the "Trespass to Property Act"—legislation dating back to the "British North America Act" of 1867—grants property owners and security guards the power to ban anyone from their premises, for any reason, permanently. This is done by issuing a notice to the intruder, who will only be breaking the law upon return. Similar laws exist in Prince Edward Island and Saskatchewan. A recent case in Canada, which involved a police officer who retrieved a discarded weapon from a trash receptacle as evidence, created some controversy. The judge ruled the policeman's actions as legal although there was no warrant present, which led some to speculate the event as validation for any Canadian citizen to raid garbage disposals.
Skipping in England and Wales may qualify as theft within the Theft Act 1968 or as common-law theft in Scotland, though there is very little enforcement in practice.
In Germany, dumpster diving is referred to as "containern", and a waste container's contents are regarded as the property of the container's owner. Therefore, taking items from such a container is viewed as theft. However, the police will routinely disregard the illegality of garbage picking since the items found are generally of low value. There has only been one known instance where people were prosecuted. In 2009 individuals were arrested on assumed burglary as they had surmounted a supermarket's fence which was then followed by a theft complaint by the owner; the case was suspended.
In the United States, the 1988 "California v. Greenwood" case in the U.S. Supreme Court held that there is no common law expectation of privacy for discarded materials. There are, however, limits to what can legally be taken from a company's refuse. In a 1983 Minnesota case involving the theft of customer lists from a garbage can, "Tennant Company v. Advance Machine Company" (355 N.W.2d 720), the owner of the discarded information was awarded $500,000 in damages.
Dumpster diving is practiced differently in developed countries than in developing countries.
In the 1960s, Jerry Schneider, using recovered instruction manuals from The Pacific Telephone & Telegraph Company, used the company's own procedures to acquire hundreds of thousands of dollars' worth of telephone equipment over several years until his arrest.
The "Castle Infinity" videogame, after its shutdown in 2005, was brought back from the dead by a fan rescuing its servers from the trash.
Food Not Bombs is an anti-hunger organization that gets a significant amount of its food from dumpster diving from the dumpsters at small markets and corporate grocery stores in the US and UK.
In October 2013, in North London, three men were arrested and charged under the 1824 Vagrancy Act when they were caught taking discarded food: tomatoes, mushrooms, cheese and cakes from bins behind an Iceland supermarket. The charges were dropped on 29 January 2014 after much public criticism as well as a request by Iceland's chief executive, Malcolm Walker.

</doc>
<doc id="8247" url="https://en.wikipedia.org/wiki?curid=8247" title="Digital synthesizer">
Digital synthesizer

A digital synthesizer is a synthesizer that uses digital signal processing (DSP) techniques to make musical sounds. This in contrast to older analog synthesizers, which produce music using analog electronics, and samplers, which play back digital recordings of acoustic, electric, or electronic instruments. Some digital synthesizers emulate analog synthesizers; others include sampling capability in addition to digital synthesis.
The very earliest digital synthesis experiments were made with computers, as part of academic research into sound generation. In 1973, the Japanese company Yamaha licensed the algorithms for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971. Yamaha's engineers began adapting Chowning's algorithm for use in a commercial digital synthesizer, adding improvements such as the "key scaling" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation, though it would take several years before Yamaha were to release their FM digital synthesizers. In the 1970s, Yamaha were granted a number of patents, under the company's former name "Nippon Gakki Seizo Kabushiki Kaisha", evolving Chowning's early work on FM synthesis technology. Yamaha built the first prototype digital synthesizer in 1974.
By the end of 1977 New England Digital (NED) released the Synclavier, the first commercial synthesizer to use purely digital sound generation and also the worlds first commercial FM synthesizer.
Released in 1979, the Casio VL-1 was the first low budget digital synthesizer, selling for $69.95. Yamaha eventually commercialized their FM synthesis technology and released the companies first FM digital synthesizer in 1980, the Yamaha GS-1, but at an expensive retail price of $16,000.
Early commercial digital synthesizers used simple hard-wired digital circuitry to implement techniques such as additive synthesis and FM synthesis. Other techniques, such as wavetable synthesis and physical modeling, only became possible with the advent of high-speed microprocessor and digital signal processing technology. Two other early commercial digital synthesizers were the Fairlight CMI, introduced in 1979, and the New England Digital Synclavier II introduced 1979 as an upgrade to the original Synclavier . The Fairlight CMI was the worlds first sampling synthesizer, while the Synclavier originally used FM synthesis technology licensed from Yamaha, before adding the worlds first 16bit, real time hard-drive streaming sampler later in 1982 . The Fairlight CMI and the Synclavier were both expensive systems, retailing for more than $20,000 in the early 1980s. The cost of digital synthesizers began falling rapidly in the early 1980s. E-mu Systems introduced the Emulator sampling synthesizer in 1982 at a retail price of $7,900. Although not as flexible or powerful as either the Fairlight CMI or the Synclavier, its lower cost and portability made it popular.
Introduced in 1983, the Yamaha DX7 was the breakthrough digital synthesizer to have a major impact, both innovative and affordable, and thus spelling the decline of analog synthesizers. It used FM synthesis and, although it was incapable of the sampling synthesis of the Fairlight CMI, its price was around $2,000, putting it within range of a much larger number of musicians. The DX-7 was also known for its "key scaling" method to avoid distortion and for its recognizabley bright tonality that was partly due to its high sampling rate of 57 kHz. It became indispensable to many music artists of the 1980s, and would become one of the best-selling synthesizers of all time.
In 1987, Roland released its own influential synthesizer of the time, the D-50. This popular synth broke new ground in affordably combining short samples and digital oscillators, as well as the innovation of built-in digital effects (reverb., chorus, equalizer). Roland called this Linear Arithmetic (LA) synthesis. This instrument is responsible for some of the very recognisable preset synthesizer sounds of the late 1980s, such as the Pizzagogo sound used on Enya's "Orinoco Flow."
It gradually became feasible to include high quality samples of existing instruments as opposed to synthesizing them. In 1988, Korg introduced the last of the hugely popular trio of digital synthesizers of the 1980s after the DX7 and D50, the M1. This heralded both the increasing popularisation of digital sample-based synthesis, and the rise of 'workstation' synthesizers. After this time, many popular modern digital synthesizers have been described as not being full synthesizers in the most precise sense, as they play back samples stored in their memory. However, they still include options to shape the sounds through use of envelopes, LFOs, filters and effects such as reverb. The Yamaha Motif and Roland Fantom series of keyboards are typical examples of this type, described as 'ROMplers' ; at the same time, they are also examples of "workstation" synthesizers.
With the addition of sophisticated sequencers on board, now added to built-in effects and other features, the 'workstation' synthesizer had been born. These always include a multi-track sequencer, and can often record and play back samples, and in later years full audio tracks, to be used to record an entire song. These are usually also ROMplers, playing back samples, to give a wide variety of realistic instrument and other sounds such as drums, string instruments and wind instruments to sequence and compose songs, along with popular keyboard instrument sounds such as electric pianos and organs.
As there was still interest in analog synthesizers, and with the increase of computing power, over the 1990s another type of synthesizer arose : the analog modeling, or "virtual analog" synthesizer. These use computing power to simulate traditional analog waveforms and circuitry such as envelopes and filters, with the most popular examples of this type of instrument including the Nord Lead and Access Virus.
As the cost of processing power and memory fell, new types of synthesizers emerged, offering a variety of novel sound synthesis options. The Korg Oasys was one such example, packaging multiple digital synthesizers into a single unit.
Digital synthesizers can now be completely emulated in software ("softsynth"), and run on conventional PC hardware. Such soft implementations require careful programming and a fast CPU to get the same latency response as their dedicated equivalents. To reduce latency, some professional sound card manufacturers have developed specialized Digital Signal Processing ([DSP]) hardware. Dedicated digital synthesizers have the advantage of a performance-friendly user interface (physical controls like buttons for selecting features and enabling functionality, and knobs for setting variable parameters). On the other hand, software synthesizers have the advantages afforded by a rich graphical display.
With focus on performance-oriented keyboards and digital computer technology, manufacturers of commercial electronic instruments created some of the earliest digital synthesizers for studio and experimental use with computers being able to handle built-in sound synthesis algorithms.
The main difference is that a digital synthesizer uses digital processors and usually uses the direct digital synthesis architecture, while an analog synthesizer uses analog circuitry and a phase-locked loop. A digital synthesizer uses a numerically-controlled oscillator while an analog synthesizer may use a voltage-controlled oscillator. A digital synthesizer is in essence a computer with (often) a piano-keyboard and an LCD as an interface. An analog synthesizer is made up of sound-generating circuitry and modulators. Because computer technology is rapidly advancing, it is often possible to offer more features in a digital synthesizer than in an analog synthesizer at a given price. However, both technologies have their own merit. Some forms of synthesis, such as, for instance, sampling and additive synthesis are not feasible in analog synthesizers, while on the other hand, many musicians prefer the character of analog synthesizers over their digital equivalent.
The new wave era of the 1980s first brought the digital synthesizer to the public ear. Bands like Talking Heads and Duran Duran used the digitally made sounds on some of their most popular albums. Other more pop-inspired bands like Hall & Oates began incorporating the digital synthesizer into their sound in the 1980s. Through breakthroughs in technology in the 1990s many modern synthesizers use DSP.
Working in more or less the same way, every digital synthesizer appears similar to a computer. At a steady sample rate, digital synthesis produces a stream of numbers. Sound from speakers is then produced by a conversion to analog form. Direct digital synthesis is the typical architecture for digital synthesizers. Through signal generation, voice and instrument-level processing, a signal flow is created and controlled either by MIDI capabilities or voice and instrument-level controls.

</doc>
<doc id="8249" url="https://en.wikipedia.org/wiki?curid=8249" title="Definition of music">
Definition of music

A definition of music endeavors to give an accurate and concise explanation of music's basic attributes or essential nature and it involves a process of defining what is meant by the term "music". Many authorities have suggested definitions, but defining music turns out to be more difficult than might first be imagined, and there is ongoing debate. A number of explanations start with the notion of music as "organized sound," but they also highlight that this is perhaps too broad a definition and cite examples of organized sound that are not defined as music, such as human speech and sounds found in both natural and industrial environments . The problem of defining music is further complicated by the influence of culture in music cognition. 
The "Concise Oxford Dictionary" defines music as "the art of combining vocal or instrumental sounds (or both) to produce beauty of form, harmony, and expression of emotion" . However, some music genres, such as noise music and musique concrète, challenge these ideas by using sounds not widely considered as musical, beautiful or harmonius, like randomly produced electronic distortion, feedback, static, cacophony, and sounds produced using compositional processes which utilize indeterminacy (; ).
An oft cited example of the dilemma in defining music is the work "4'33"" (1952) by the American composer John Cage (1912–1992). The written score has three movements and directs the performer(s) to appear on stage, indicate by gesture or other means when the piece begins, then make no sound throughout the duration of the piece, marking sections and the end by gesture. The audience hears only whatever ambient sounds may occur in the room. Some argue that "4'33" is not music because, among other reasons, it contains no sounds that are conventionally considered "musical" and the composer and performer(s) exert no control over the organization of the sounds heard . Others argue it is music because the conventional definitions of musical sounds are unnecessarily and arbitrarily limited, and control over the organization of the sounds is achieved by the composer and performer(s) through their gestures that divide what is heard into specific sections and a comprehensible form .
Because of differing fundamental concepts of music, the languages of many cultures do not contain a word that can be accurately translated as "music" as that word is generally understood by Western cultures . Inuit and most North American Indian languages do not have a general term for music. Among the Aztecs, the ancient Mexican theory of rhetoric, poetry, dance, and instrumental music used the Nahuatl term "In xochitl-in kwikatl" to refer to a complex mix of music and other poetic verbal and non-verbal elements, and reserved the word "Kwikakayotl" (or cuicacayotl) only for the sung expressions . There is no term for music in Nigerian languages Tiv, Yoruba, Igbo, Efik, Birom, Hausa, Idoma, Eggon or Jarawa. Many other languages have terms which only partly cover what Western culture typically means by the term "music" (). The Mapuche of Argentina do not have a word for "music", but they do have words for instrumental versus improvised forms ("kantun"), European and non-Mapuche music ("kantun winka"), ceremonial songs ("öl"), and "tayil" .
While some languages in West Africa have no term for music, some West African languages accept the general concepts of music (). "Musiqi" is the Persian word for the science and art of music, "muzik" being the sound and performance of music (), though some things European-influenced listeners would include, such as Quran chanting, are excluded.
Ben Watson points out that Ludwig van Beethoven's "Grosse Fuge" (1825) "sounded like noise" to his audience at the time. Indeed, Beethoven's publishers persuaded him to remove it from its original setting as the last movement of a string quartet. He did so, replacing it with a sparkling "Allegro". They subsequently published it separately . Musicologist Jean-Jacques Nattiez considers the difference between noise and music nebulous, explaining that "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus ... By all accounts there is no "single" and "intercultural" universal concept defining what music might be" .
An often-cited definition of music is that it is "organized sound", a term originally coined by modernist composer Edgard Varèse in reference to his own musical aesthetic. Varèse's concept of music as "organized sound" fits into his vision of "sound as living matter" and of "musical space as open rather than bounded" . He conceived the elements of his music in terms of "sound-masses", likening their organization to the natural phenomenon of crystallization . Varèse thought that "to stubbornly conditioned ears, anything new in music has always been called noise", and he posed the question, "what is music but organized noises?" .
The fifteenth edition of the "Encyclopædia Britannica" states that "while there are no sounds that can be described as inherently unmusical, musicians in each culture have tended to restrict the range of sounds they will admit." A human organizing element is often felt to be implicit in music (sounds produced by non-human agents, such as waterfalls or birds, are often described as "musical", but perhaps less often as "music"). The composer R. Murray states that the sound of classical music "has decays; it is granular; it has attacks; it fluctuates, swollen with impurities—and all this creates a musicality that comes before any 'cultural' musicality." However, in the view of semiologist Jean-Jacques Nattiez, "just as music is whatever people choose to recognize as such, noise is whatever is recognized as disturbing, unpleasant, or both" . (See "music as social construct" below.)""'
Levi R. Bryant defines music not as a language, but as a marked-based, problem-solving method, comparable to mathematics .
Most definitions of music include a reference to sound and a list of universals of music can be generated by stating the elements (or aspects) of sound: pitch, timbre, loudness, duration, spatial location and texture (). However, in terms more specifically relating to music: following Wittgenstein, cognitive psychologist Eleanor Rosch proposes that categories are not clean cut but that something may be more or less a member of a category . As such the search for musical universals would fail and would not provide one with a valid definition . This is primarily because other cultures have different understandings in relation to the sounds that English language writers refer to as music.
Many people do, however, share a general idea of music. The Websters definition of music is a typical example: "the science or art of ordering tones or sounds in succession, in combination, and in temporal relationships to produce a composition having unity and continuity" ("Webster's Collegiate Dictionary", online edition).
This approach to the definition focuses not on the "construction" but on the "experience" of music. An extreme statement of the position has been articulated by the Italian composer Luciano Berio: “Music is everything that one listens to with the intention of listening to music” . This approach permits the boundary between music and noise to change over time as the conventions of musical interpretation evolve within a culture, to be different in different cultures at any given moment, and to vary from person to person according to their experience and proclivities. It is further consistent with the subjective reality that even what would commonly be considered music is experienced as non-music if the mind is concentrating on other matters and thus not perceiving the sound's "essence" "as music" .
In his 1983 book, "Music as Heard", which sets out from the phenomenological position of Husserl, Merleau-Ponty, and Ricœur, Thomas Clifton defines music as "an ordered arrangement of sounds and silences whose meaning is presentative rather than denotative. . . . This definition distinguishes music, as an end in itself, from compositional technique, and from sounds as purely physical objects." More precisely, "music is the actualization of the possibility of any sound whatever to present to some human being a meaning which he experiences with his body—that is to say, with his mind, his feelings, his senses, his will, and his metabolism" . It is therefore "a certain reciprocal relation established between a person, his behavior, and a sounding object" .
Clifton accordingly differentiates music from non-music on the basis of the human behavior involved, rather than on either the nature of compositional technique or of sounds as purely physical objects. Consequently, the distinction becomes a question of what is meant by musical behavior: "a musically behaving person is one whose very being is absorbed in the significance of the sounds being experienced." However, "It is not altogether accurate to say that this person is listening "to" the sounds. First, the person is doing more than listening: he is perceiving, interpreting, judging, and feeling. Second, the preposition 'to' puts too much stress on the sounds as such. Thus, the musically behaving person experiences musical significance by means of, or through, the sounds" .
In this framework, Clifton finds that there are two things that separate music from non-music: (1) musical meaning is presentative, and (2) music and non-music are distinguished in the idea of personal involvement. "It is the notion of personal involvement which lends significance to the word "ordered" in this definition of music" . This is not to be understood, however, as a sanctification of extreme relativism, since "it is precisely the 'subjective' aspect of experience which lured many writers earlier in this century down the path of sheer opinion-mongering. Later on this trend was reversed by a renewed interest in 'objective,' scientific, or otherwise non-introspective musical analysis. But we have good reason to believe that a musical experience is not a purely private thing, like seeing pink elephants, and that reporting about such an experience need not be subjective in the sense of it being a mere matter of opinion" .
Clifton's task, then, is to describe musical experience and the objects of this experience which, together, are called "phenomena," and the activity of describing phenomena is called "phenomenology" . It is important to stress that this definition of music says nothing about aesthetic standards. Music is not a fact or a thing in the world, but a meaning constituted by human beings. . . . To talk about such experience in a meaningful way demands several things. First, we have to be willing to let the composition speak to us, to let it reveal its own order and significance. . . . Second, we have to be willing to question our assumptions about the nature and role of musical materials. . . . Last, and perhaps most important, we have to be ready to admit that describing a meaningful experience is itself meaningful. 
"Music, often an art/entertainment, is a total social fact whose definitions vary according to era and culture," according to Jean . It is often contrasted with noise. According to musicologist Jean-Jacques Nattiez: "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus... By all accounts there is no "single" and "intercultural" universal concept defining what music might be" . Given the above demonstration that "there is no limit to the number or the genre of variables that might intervene in a definition of the musical," an organization of definitions and elements is necessary.
Nattiez (1990, 17) describes definitions according to a tripartite semiological scheme similar to the following:
There are three levels of description, the poietic, the neutral, and the esthesic:
Table describing types of definitions of music :
Because of this range of definitions, the study of music comes in a wide variety of forms. There is the study of sound and vibration or acoustics, the cognitive study of music, the study of music theory and performance practice or music theory and ethnomusicology and the study of the reception and history of music, generally called musicology.
Composer Iannis Xenakis in "Towards a Metamusic" (chapter 7 of "Formalized Music") defined music in the following way :

</doc>
<doc id="8253" url="https://en.wikipedia.org/wiki?curid=8253" title="Dayton, Ohio">
Dayton, Ohio

Dayton () is the sixth-largest city in the state of Ohio and the county seat of Montgomery County. A small part of the city extends into Greene County. The 2019 U.S. census estimate put the city population at 140,407, while Greater Dayton was estimated to be at 803,416 residents. This makes Dayton the fourth-largest metropolitan area in Ohio and 63rd in the United States. Dayton is within Ohio's Miami Valley region, just north of Greater Cincinnati.
Ohio's borders are within of roughly 60 percent of the country's population and manufacturing infrastructure, making the Dayton area a logistical centroid for manufacturers, suppliers, and shippers. Dayton also hosts significant research and development in fields like industrial, aeronautical, and astronautical engineering that have led to many technological innovations. Much of this innovation is due in part to Wright-Patterson Air Force Base and its place in the community. With the decline of heavy manufacturing, Dayton's businesses have diversified into a service economy that includes insurance and legal sectors as well as healthcare and government sectors.
Along with defense and aerospace, healthcare accounts for much of the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000 and a yearly economic impact of $6.8 billion. It is estimated that Premier Health Partners, a hospital network, contributes more than $2 billion a year to the region through operating, employment, and capital expenditures. In 2011, Dayton was rated the #3 city in the nation by HealthGrades for excellence in healthcare.
Dayton is also noted for its association with aviation; the city is home to the National Museum of the United States Air Force and is the birthplace of Orville Wright. Other well-known individuals born in the city include poet Paul Laurence Dunbar and entrepreneur John H. Patterson. Dayton is also known for its many patents, inventions, and inventors, most notably the Wright brothers' invention of powered flight. In 2007 Dayton was a part of the top 100 cities in America. In 2008, 2009, and 2010, "Site Selection" magazine ranked Dayton the #1 mid-sized metropolitan area in the nation for economic development. Also in 2010, Dayton was named one of the best places in the United States for college graduates to find a job.
On Memorial Day of 2019 Dayton was affected by a tornado outbreak, in which a total of 15 tornadoes touched down in the Dayton area. One was a half-mile wide EF4 that tore through the heart of the city causing damage.
Dayton was founded on April 1, 1796, by 12 settlers known as the Thompson Party. They traveled in March from Cincinnati up the Great Miami River by pirogue and landed at what is now St. Clair Street, where they found two small camps of Native Americans. Among the Thompson Party was Benjamin Van Cleve, whose memoirs provide insights into the Ohio Valley's history. Two other groups traveling overland arrived several days later.
In 1797, Daniel C. Cooper laid out Mad River Road, the first overland connection between Cincinnati and Dayton, opening the "Mad River Country" to settlement. Ohio was admitted into the Union in 1803, and the village of Dayton was incorporated in 1805 and chartered as a city in 1841. The city was named after Jonathan Dayton, a captain in the American Revolutionary War who signed the U.S. Constitution and owned a significant amount of land in the area. In 1827, construction on the Dayton-Cincinnati canal began, which would provide a better way to transport goods from Dayton to Cincinnati and contribute significantly to Dayton's economic growth during the 1800s.
Innovation led to business growth in the region. In 1884, John Henry Patterson acquired James Ritty's National Manufacturing Company along with his cash register patents and formed the National Cash Register Company (NCR). The company manufactured the first mechanical cash registers and played a crucial role in the shaping of Dayton's reputation as an epicenter for manufacturing in the early 1900s. In 1906, Charles F. Kettering, a leading engineer at the company, helped develop the first electric cash register, which propelled NCR into the national spotlight. NCR also helped develop the US Navy Bombe, a code-breaking machine that helped crack the Enigma machine cipher during World War II.
Dayton has been the home for many patents and inventions since the 1870s. According to the National Park Service, citing information from the U.S. Patent Office, Dayton had granted more patents per capita than any other U.S. city in 1890 and ranked fifth in the nation as early as 1870. The Wright brothers, inventors of the airplane, and Charles F. Kettering, world-renowned for his numerous inventions, hailed from Dayton. The city was also home to James Ritty's Incorruptible Cashier, the first mechanical cash register, and Arthur E. Morgan's hydraulic jump, a flood prevention mechanism that helped pioneer hydraulic engineering. Paul Laurence Dunbar, an African-American poet and novelist, penned his most famous works in the late 19th century and became an integral part of the city's history.
Powered aviation began in Dayton. Orville and Wilbur Wright were the first to construct and demonstrate powered flight. Although the first flight was in Kitty Hawk, North Carolina, their Wright Flyer was built in and returned to Dayton for improvements and further flights at Huffman Field, a cow pasture eight miles (13 km) northeast of Dayton, near the current Wright Patterson Air Force Base.
When the government tried to move development to Langley field in southern Virginia, six Dayton businessmen including Edward A. Deeds, formed the Dayton-Wright Airplane Company in Moraine and established a flying field. Deeds also opened a field to the north in the flood plain of the Great Miami River between the confluences of that river, the Stillwater River, and the Mad River, near downtown Dayton. Later named McCook Field for Alexander McDowell McCook, an American Civil War general, this became the Army Signal Corps' primary aviation research and training location. Wilbur Wright also purchased land near Huffman prairie to continue their research.
During World War I, the Army purchased 40 acres adjacent to Huffman Prairie for the Fairfield Aviation General Supply Depot. As airplanes developed more capability, they needed more runway space than McCook could offer, and a new location was sought. The Patterson family formed the Dayton Air Service Committee, Inc which held a campaign that raised $425,000 in two days and purchased 4,520.47 acres (18.2937 km2) northeast of Dayton, including Wilbur Wright Field and the Huffman Prairie Flying Field. Wright Field was "formally dedicated" on 12 October 1927. After World War II, Wright Field and the adjacent Patterson Field, Dayton Army Air Field, and Clinton Army Air Field were merged as the Headquarters, Air Force Technical Base. On 13 January 1948, the facility was renamed Wright-Patterson Air Force Base.
A catastrophic flood in March 1913, known as the Great Dayton Flood, led to the creation of the Miami Conservancy District, a series of dams as well as hydraulic pumps installed around Dayton, in 1914. Like other cities across the country, Dayton was heavily involved in the war effort during World War II. Several locations around the city hosted the Dayton Project, a branch of the larger Manhattan Project, to develop polonium triggers used in early atomic bombs. The war efforts led to a manufacturing boom throughout the city, including high demand for housing and other services. At one point, emergency housing was put into place due to a housing shortage in the region, much of which is still in use today.
Between the 1940s and the 1970s, the city saw significant growth in suburban areas from population migration. Veterans were returning from military service in large numbers seeking industrial and manufacturing jobs, a part of the local industry that was expanding rapidly. Advancements in architecture also contributed to the suburban boom. New, modernized shopping centers and the Interstate Highway System allowed workers to commute greater distances and families to live further from the downtown area. More than 127,000 homes were built in Montgomery County during the 1950s.
Since the 1980s, however, Dayton's population has declined, mainly due to the loss of manufacturing jobs and decentralization of metropolitan areas, as well as the national housing crisis that began in 2008. While much of the state has suffered for similar reasons, the impact on Dayton has been greater than most. Dayton had the third-greatest percentage loss of population in the state since the 1980s, behind Cleveland and Youngstown. Despite this, Dayton has begun diversifying its workforce from manufacturing into other growing sectors such as healthcare and education.
Downtown expansion that began in the 2000s has helped revitalize the city and encourage growth. Day Air Ballpark, home of the Dayton Dragons, was built in 2000. The highly successful minor league baseball team has been an integral part of Dayton's culture. In 2001, the city's public park system, Five Rivers MetroParks, built RiverScape MetroPark, an outdoor entertainment venue that attracts more than 400,000 visitors each year. A new performance arts theater, the Schuster Center, opened in 2003. A large health network in the region, Premier Health Partners, expanded its Miami Valley Hospital with a 12-story tower addition.
In 2010, the Downtown Dayton Partnership, in cooperation with the City of Dayton and community leaders, introduced the Greater Downtown Dayton Plan. It focuses on job creation and retention, infrastructure improvements, housing, recreation, and collaboration. The plan is to be implemented through the year 2020.
In 1995, the Dayton Agreement, a peace accord between the parties to the hostilities of the conflict in Bosnia-Herzegovina and the former Yugoslavia, was negotiated at Wright-Patterson Air Force Base, near Fairborn, Ohio, from November 1 to 21.
Richard Holbrooke wrote about these events in his memoirs:
Dayton is known as the "Gem City." The nickname's origin is uncertain, but several theories exist. In the early 19th century, a well-known racehorse named Gem hailed from Dayton. In 1845, an article published in the "Cincinnati Daily Chronicle" by an author known as T stated:
In the late 1840s, Major William D. Bickham of the "Dayton Journal" began a campaign to nickname Dayton the "Gem City." The name was adopted by the city's Board of Trade several years later. Paul Laurence Dunbar referred to the nickname in his poem, "Toast to Dayton," as noted in the following excerpt:
<poem>
She shall ever claim our duty,
For she shines—the brightest gem
That has ever decked with beauty
</poem>
Dayton also plays a role in a nickname given to the state of Ohio, "Birthplace of Aviation." Dayton is the hometown of the Wright brothers, aviation pioneers who are credited with inventing and building the first practical airplane in history. After their first manned flights in Kitty Hawk, North Carolina, which they had chosen due to its ideal weather and climate conditions, the Wrights returned to Dayton and continued testing at nearby Huffman Prairie.
Additionally, Dayton is colloquially referred to as "Little Detroit". This nickname comes from Dayton's prominence as a Midwestern manufacturing center.
According to the United States Census Bureau, the city has a total area of , of which is land and is water.
Dayton's climate features warm, muggy summers and cold, dry winters, and is classified as a humid continental climate (Köppen "Dfa"). Unless otherwise noted, all normal figures quoted within the text below are from the official climatology station, Dayton International Airport, at an elevation of about to the north of downtown Dayton, which lies within the valley of the Miami River; thus temperatures there are typically cooler than in downtown.
At the airport, monthly mean temperatures range from in January to in July. The highest temperature ever recorded in Dayton was on July 22, 1901, and the coldest was on February 13 during the Great Blizzard of 1899. On average, there are 14 days of + highs and 4.5 nights of sub- lows annually. Snow is moderate, with a normal seasonal accumulation of , usually occurring from November to March, occasionally April, and rarely October. Precipitation averages annually, with total rainfall peaking in May.
Dayton is subject to severe weather typical of the Midwestern United States. Tornadoes are possible from the spring to the fall. Floods, blizzards, and severe thunderstorms can also occur.
Around midnight May 27–28, 2019, 14 tornadoes cut a path through the region, causing extensive property damage, but only one death. The tornadoes closed several streets, including portions of I-75 and North Dixie Street. 64,000 residents lost power and much of the region's water supply was cut off. Although some of the tornadoes were only EF0 and remained on the ground for less than a mile, an EF4 tornado passed through the communities of Brookville, Trotwood, Dayton, Beavercreek, and Riverside.
The Dayton Audubon Society is the National Audubon Society's local chapter. The Dayton chapter manages local activities contributing to the annual, hemisphere-wide Christmas Bird Count. The Chapter began participation in the National Count in 1924. The local Count was initially coordinated by Ben Blincoe, who was succeeded by Jim Hill in 1970. In the mid-1960s, the freezing of Lake Erie and associated marshlands led species of waterfowl to appear in the Dayton-area, where surface waters remained unfrozen. Nine varieties of birds have been observed every year in the Dayton area: downy woodpecker, Carolina chickadee, tufted titmouse, brown creeper, cardinal, junco, tree sparrow, song sparrow and crow.
Dayton's population declined significantly from a peak of 262,332 residents in 1960 to only 141,759 in 2010. This was in part due to the slowdown of the region's manufacturing and the growth of Dayton's affluent suburbs including Oakwood, Englewood, Beavercreek, Springboro, Miamisburg, Kettering, and Centerville. The city's most populous ethnic group, white, declined from 78.1% in 1960 to 51.7% by 2010. However, recent census estimates show a 1.3% population increase since 2010, the first increase in five decades.
As of the 2000 census, the median income for a household in the city was $27,523, and the median income for a family was $34,978. Males had a median income of $30,816 versus $24,937 for females. The per capita income for the city was $34,724. About 18.2% of families and 23.0% of the population were below the poverty line, including 32.0% of those under age 18 and 15.3% of those age 65 or over.
As of the 2010 census, there were 141,759 people, 58,404 households, and 31,064 families residing in the city. The population density was . There were 74,065 housing units at an average density of . The racial makeup of the city was 51.7% White, 42.9% African American, 0.3% Native American, 0.9% Asian, 1.3% from other races, and 2.9% from two or more races. Hispanic or Latino of any race were 3.0% of the population.
There were 58,404 households, of which 28.3% had children under the age of 18 living with them, 25.9% were married couples living together, 21.4% had a female householder with no husband present, 5.9% had a male householder with no wife present, and 46.8% were non-families. 38.8% of all households were made up of individuals, and 11.2% had someone living alone who was 65 years of age or older. The average household size was 2.26, and the average family size was 3.03.
The median age in the city was 34.4 years. 22.9% of residents were under the age of 18; 14.2% were between the ages of 18 and 24; 25.3% were from 25 to 44; 25.8% were from 45 to 64, and 11.8% were 65 years of age or older. The gender makeup of the city was 48.7% male and 51.3% female.
The 2013 census population estimate showed an increasing city of Dayton population for the first time in five decades, attributed to revitalization efforts downtown and the increasing downtown population. However, the 2014 population estimate indicates a net decrease of 897 individuals from 2013's estimate.
Dayton's economy is relatively diversified and vital to the overall economy of the state of Ohio. In 2008 and 2009, "Site Selection" magazine ranked Dayton the #1 medium-sized metropolitan area in the U.S. for economic development. Dayton is also among the top 100 metropolitan areas in both exports and export-related jobs, ranked 16 and 14 respectively by the Brookings Institution. The 2010 report placed the value of exports at $4.7 billion and the number of export-related jobs at 44,133. The Dayton Metropolitan Statistical Area ranks 4th in Ohio's Gross Domestic Product with a 2008 industry total of $33.78 billion. Additionally, Dayton ranks third among 11 major metropolitan areas in Ohio for exports to foreign countries. The Dayton Development Coalition is attempting to leverage the region's large water capacity, estimated to be 1.5 trillion gallons of renewable water aquifers, to attract new businesses. Moody's Investment Services revised Dayton's bond rating from A1 to the stronger rating of Aa2 as part of its global recalibration process. Standard & Poor's upgraded Dayton's rating from A+ to AA- in the summer of 2009.
"Bloomberg Businessweek" ranked Dayton in 2010 as one of the best places in the U.S. for college graduates looking for a job. Companies such as Reynolds and Reynolds, CareSource, DPL, LexisNexis, Kettering Health Network, Premier Health Partners, and Standard Register have their headquarters in Dayton. It is also the former home of the Speedwell Motor Car Company, MeadWestvaco (formerly known as the Mead Paper Company), and NCR. NCR was headquartered in Dayton for over 125 years and was a major innovator in computer technology.
The Dayton region gave birth to aviation and is known for its high concentration of aerospace and aviation technology. In 2009, Governor Ted Strickland designated Dayton as Ohio's aerospace innovation hub, the state's first such technology hub. Two major United States research and development organizations have leveraged Dayton's historical leadership in aviation and maintain their headquarters in the area: The National Air and Space Intelligence Center (NASIC) and the Air Force Research Laboratory (AFRL). Both have their headquarters at Wright-Patterson Air Force Base. 
Several research organizations support NASIC, AFRL, and the Dayton community. The Advanced Technical Intelligence Center is a confederation of government, academic, and industry partners. The University of Dayton Research Institute (UDRI) is led by the University of Dayton. The Cognitive Technologies Division (CTD) of Applied Research Associates, Inc., which carries out human-centered research and design, is headquartered in the Dayton suburb of Fairborn. The city of Dayton has started Tech Town, a development project to attract technology-based firms and revitalize the downtown area. Tech Town is home to the world's first RFID business incubator. The University of Dayton-led Institute for Development & Commercialization of Sensor Technologies (IDCAST) at TechTown is a center for remote sensing and sensing technology. It is one of Dayton's technology business incubators housed in The Entrepreneurs Center building.
The Kettering Health Network and Premier Health Partners have a major role on the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000 and a yearly economic impact of $6.8 billion. In addition, several Dayton area hospitals consistently earn top national ranking and recognition including the "U.S. News & World Report"'s list of "America's Best Hospitals" as well as many of HealthGrades top ratings. The most notable hospitals are Miami Valley Hospital and Kettering Medical Center. 
The Dayton region has several key institutes and centers for health care. The Center for Tissue Regeneration and Engineering at Dayton focuses on the science and development of human tissue regeneration. The National Center for Medical Readiness (NCMR) is also in the Dayton area. The center includes Calamityville, which is a disaster training facility. Over five years, Calamityville is estimated to have a regional economic impact of $374 million. Also, the Neurological Institute at Miami Valley Hospital is an institute focused on the diagnosis, treatment, and research of neurological disorders.
According to the city's 2018 Comprehensive Annual Financial Report, the top employers in the city proper in are:
The Dayton City Commission is composed of the mayor and four city commissioners. Each city commission member is elected at-large on a non-partisan basis for four-year, overlapping terms. All policy items are decided by the City Commission, which is empowered by the City Charter to pass ordinances and resolutions, adopt regulations, and appoint the city manager. The city manager is responsible for budgeting and implementing policies and initiatives. Dayton was the first large American city to adopt the city manager form of municipal government, in 1913.
Unlike many Midwestern cities its age, Dayton has very broad and straight downtown streets (generally two or three full lanes in each direction) that improved access to the downtown even after the automobile became popular. The main reason for the broad streets was that Dayton was a marketing and shipping center from its beginning; streets were broad to enable wagons drawn by teams of three to four pairs of oxen to turn around. Also, some of today's streets were once barge canals flanked by draw-paths.
A courthouse building was built in downtown Dayton in 1888 to supplement Dayton's original Neoclassical courthouse, which still stands. This second, "new" courthouse has since been replaced with new facilities as well as a park. The Old Court House has been a favored political campaign stop. On September 17, 1859, Abraham Lincoln delivered an address on its steps. Eight other presidents have visited the courthouse, either as presidents or during presidential campaigns: Andrew Johnson, James Garfield, John F. Kennedy, Lyndon B. Johnson, Richard Nixon, Gerald Ford, Ronald Reagan, and Bill Clinton.
The Dayton Arcade, which opened on March 3, 1904, was built in the hopes of replacing open-air markets throughout the city. Throughout the decades, the Arcade has gone through many transformations but has retained its charm. Some of its main features include a Flemish facade at the Third Street entrance, a glass dome above the Arcade rotunda, and a chateau roof line above the Third Street facade. The Dayton Arcade is currently under renovations with no official completion date set.
In 2009, the CareSource Management Group finished construction of a $55 million corporate headquarters in downtown Dayton. The , 10-story building was downtown's first new office tower in more than a decade.
Dayton's two tallest buildings are the Kettering Tower at and the KeyBank Tower at . Kettering Tower was originally Winters Tower, the headquarters of Winters Bank. The building was renamed after Virginia Kettering when Winters was merged into Bank One. KeyBank Tower was known as the MeadWestvaco Tower before KeyBank gained naming rights to the building in 2008.
Ted Rall said in 2015 that over the last five decades Dayton has been demolishing some of its architecturally significant buildings to reduce the city's rental vacancy rate and thus increase the occupancy rate.
Dayton's ten historic neighborhoods — Oregon District, Wright Dunbar, Dayton View, Grafton Hill, McPherson Town, Webster Station, Huffman, Kenilworth, St. Anne's Hill, and South Park — feature mostly single-family houses and mansions in the Neoclassical, Jacobethan, Tudor Revival, English Gothic, Chateauesque, Craftsman, Queen Anne, Georgian Revival, Colonial Revival, Renaissance Revival Architecture, Shingle Style Architecture, Prairie, Mission Revival, Eastlake/Italianate, American Foursquare, and Federal styles. Downtown Dayton is also a large area that encompasses several neighborhoods itself and has seen a recent uplift and revival.
Dayton's suburbs with a population of 10,000 or more include Beavercreek, Centerville, Clayton, Englewood, Fairborn, Harrison Township, Huber Heights, Kettering, Miami Township, Miamisburg, Oakwood, Riverside, Springboro (partial), Trotwood, Vandalia, Washington Township, West Carrollton, and Xenia.
Dayton was named National Geographic's outdoor adventure capital of the Midwest in 2019 due in large part to the metropolitan area's revitalized Five Rivers MetroPark, extensive bicycle and jogging trail system, urban green spaces, lakes and camping areas.
In cooperation with the Miami Conservancy District, Five Rivers MetroParks hosts 340 miles of paved trails, the largest network of paved off-street trails in the United States. In 2010, the city of Troy was named "bike friendly" by the League of American Bicyclists, which gave the city the organization's bronze designation. The honorable mention made Dayton one of two cities in Ohio to receive the award, the other being Columbus, and one of 15 cities nationwide.
The Dayton Region ranked within the top 10% in the nation in arts and culture. In a 2012 readers' poll by "American Style" magazine, Dayton ranked #2 in the country among mid-size cities as an arts destination, ranking higher than larger cities such as Atlanta, St. Louis, and Cincinnati. Dayton is the home of the Dayton Art Institute.
The Benjamin and Marian Schuster Performing Arts Center in downtown Dayton is a world-class performing arts center and the home venue of the Dayton Philharmonic Orchestra, Dayton Opera, and the Dayton Ballet. In addition to philharmonic and opera performances, the Schuster Center hosts concerts, lectures, and traveling Broadway shows, and is a popular spot for weddings and other events. The historic Victoria Theatre in downtown Dayton hosts concerts, traveling Broadway shows, ballet, a summertime classic film series, and more. The Loft Theatre, also downtown, is the home of the Human Race Theatre Company. The Dayton Playhouse, in West Dayton, is the site of numerous plays and theatrical productions. Between 1957 and 1995, the Kenley Players presented live theater productions in Dayton. In 2013, John Kenley was inducted into the Dayton Theatre Hall of Fame.
Dayton is the home to several ballet companies including:
The city's fine dining restaurants include The Pine Club, a nationally known steakhouse. Dayton is home to a variety of pizza chains that have become woven into local culture, the most notable of which are Cassano's and Marion's Piazza. Notable Dayton-based restaurant chains include Hot Head Burritos.
In addition to restaurants, the city is also home to Esther Price Candies, a candy and chocolate company, and Mike-sells, the oldest potato chip company in the United States.
The city began developing a reputation for its number of breweries and craft beer venues by the late 2010s.
Many major religions are represented in Dayton. Christianity is represented in Dayton by dozens of denominations and their respective churches. Notable Dayton churches include the First Lutheran Church, Sacred Heart Church, and Ginghamsburg Church. Dayton's Muslim community is largely represented by the Islamic Society of Greater Dayton (ISGD), a Muslim community that includes a mosque on Josie Street. Dayton is also home to the United Theological Seminary, one of 13 seminaries affiliated with the United Methodist Church. Judaism is represented by Temple Israel. Hinduism is represented by the Hindu Temple of Dayton. Old North Dayton also has a number of Catholic churches built by immigrants from Lithuania, Poland, Hungary, and Germany.
Tourism also accounts for one out of every 14 private sector jobs in the county. Tourism in the Dayton region is led by the National Museum of the United States Air Force at Wright-Patterson Air Force Base, the largest and oldest military aviation museum in the world. The museum draws over 1.3 million visitors per year and is one of the most-visited tourist attractions in Ohio. The museum houses the National Aviation Hall of Fame.
Other museums also play significant roles in the tourism and economy of the Dayton area. The Dayton Art Institute, a museum of fine arts, owns collections containing more than 20,000 objects spanning 5,000 years of art and archaeological history. The Dayton Art Institute was rated one of the top 10 best art museums in the United States for children. The Boonshoft Museum of Discovery is a children's museum of science with numerous exhibits, one of which includes an indoor zoo with nearly 100 different animals.
There are also some notable historical museums in the region. The Dayton Aviation Heritage National Historical Park, operated by the National Park Service, commemorates the lives and achievements of Dayton natives Orville and Wilbur Wright and Paul Laurence Dunbar. The Wright brothers' famous Wright Flyer III aircraft is housed in a museum at Carillon Historical Park. Dayton is also home to America's Packard Museum, which contains many restored historical Packard vehicles. SunWatch Indian Village/Archaeological Park, a partially reconstructed 12th-century prehistoric American Indian village, is on the south end of Dayton; it is organized around a central plaza dominated by wood posts forming an astronomical calendar. The park includes a museum where visitors can learn about the Indian history of the Miami Valley.
The Vectren Dayton Air Show is an annual air show that takes place at the Dayton International Airport. The Vectren Dayton Airshow is one of the largest air shows in the United States.
The Dayton area is served by Five Rivers MetroParks, encompassing over 23 facilities for year-round recreation, education, and conservation. In cooperation with the Miami Conservancy District, the MetroParks maintains over of paved, multi-use scenic trails that connect Montgomery County with Greene, Miami, Warren, and Butler counties. 
From 1996 to 1998, Dayton hosted the National Folk Festival. Since then, the annual Cityfolk Festival has continued to bring folk, ethnic, and world music and arts to Dayton. The Five Rivers MetroParks also owns and operates the PNC Second Street Market near downtown Dayton.
The Dayton area hosts several arenas and venues. South of Dayton in Kettering is the Fraze Pavilion, whose notable performances have included the Backstreet Boys, Boston, and Steve Miller Band. South of downtown, on the banks of the Great Miami River, is the University of Dayton Arena, home venue for the University of Dayton Flyers basketball teams and the location of various other events and concerts. It also hosts the Winter Guard International championships, at which hundreds of percussion and color guard ensembles from around the world compete. In addition, the Dayton Amateur Radio Association hosts the annual Dayton Hamvention, North America's largest hamfest, at the Greene County Fairgrounds in nearby Xenia. The Nutter Center, which is just east of Dayton in the suburb of Fairborn, is the home arena for athletics of Wright State University and the former Dayton Bombers hockey team. This venue is used for many concerts, community events, and various national traveling shows and performances.
The Oregon District is a historic residential and commercial district in southeast downtown Dayton. The district is populated with art galleries, specialty shops, pubs, nightclubs, and coffee houses.
The city of Dayton is also host to yearly festivals, such as the Dayton Celtic Festival, the Dayton Blues Festival, Dayton Music Fest, Urban Nights, Women in Jazz, the African American and Cultural Festival, and the Dayton Reggae Fest.
The Dayton area is home to several minor league and semi pro teams, as well as NCAA Division I sports programs.
The Dayton Dragons professional baseball team is a Class A minor league affiliate for the Cincinnati Reds. The Dayton Dragons are the first (and only) team in minor league baseball history to sell out an entire season before it began and was voted as one of the top 10 hottest tickets to get in all of professional sports by Sports Illustrated. The Dayton Dragons 815 consecutive sellouts surpassed the NBA's Portland Trail Blazers for the longest sellout streak across all professional sports in the U.S.
The University of Dayton and Wright State University both host NCAA basketball. The University of Dayton Arena has hosted more games in the NCAA men's basketball tournament over its history than any other venue. UD Arena is also the site of the First Round games of the NCAA Tournament. In 2012, eight teams competed for the final four spots in the NCAA Basketball Tournament. Wright State University's NCAA men's basketball is the Wright State Raiders and the University of Dayton's NCAA men's basketball team is the Dayton Flyers.
The Dayton Gems were a minor league ice hockey team in the International Hockey League from 1964 to 1977, 1979 to 1980, and most recently 2009 to 2012.
The Dayton Bombers were an ECHL ice hockey team from 1991 to 2009. They most recently played the North Division of the ECHL's American Conference. In June 2009, it was announced the Bombers would turn in their membership back to the league.
Despite the folding of the Bombers, hockey remained in Dayton as the Dayton Gems of the International Hockey League were formed in the fall of 2009 at Hara Arena. The Gems folded after the 2011–12 season. Shortly after the Gems folded, it was announced a new team, the Dayton Demonz, would begin play in 2012 in the Federal Hockey League (FHL). The Demonz folded in 2015 and were immediately replaced by the Dayton Demolition, also in the FHL. However, the Demolition would cease operations after only one season when Hara Arena decided to close due to financial difficulties.
Dayton hosted the first American Professional Football Association game (precursor to the NFL). The game was played at Triangle Park between the Dayton Triangles and the Columbus Panhandles on October 3, 1920, and is considered one of the first professional football games ever played. Football teams in the Dayton area include the Dayton Flyers and the Dayton Sharks.
The Dayton region is also known for the many golf courses and clubs that it hosts. The Miami Valley Golf Club, Moraine Country Club, NCR Country Club, and the Pipestone Golf Course are some of the more notable courses. Also, several PGA Championships have been held at area golf courses. The Miami Valley Golf Club hosted the 1957 PGA Championship, the Moraine Country Club hosted the 1945 PGA Championship, and the NCR Country club hosted the 1969 PGA Championship. Additionally, NCR CC hosted the 1986 U.S. Women's Open and the 2005 U.S. Senior Open. Other notable courses include the Yankee Trace Golf Club, the Beavercreek Golf Club, Dayton Meadowbrook Country Club, Sycamore Creek Country Club, Heatherwoode Golf Club, Community Golf Course, and Kitty Hawk Golf Course.
The city of Dayton is the home to the Dayton Area Rugby Club which hosts their home games at the Dayton Rugby Grounds. As of 2018, the club fields two men's and one women's side for Rugby Union and several Rugby Sevens sides. The club also hosts the annual Gem City 7's tournament.
Dayton is served in print by "The Dayton Daily News", the city's sole remaining daily newspaper. The "Dayton Daily News" is owned by Cox Enterprises. The Dayton region's main business newspaper is the "Dayton Business Journal". The "Dayton City Paper," a community paper focused on music, art, and independent thought ceased operation in 2018. "The Dayton Weekly News" has been published since 1993, providing news and information to Dayton’s African-American community.
There are numerous magazines produced in and for the Dayton region. "The Dayton Magazine" provides insight into arts, food, and events. "Focus on Business" is published by the Chamber of Commerce to provide awareness of companies and initiatives affecting the regional economy
Nielsen Media Research ranked the 11-county Dayton television market as the No. 62 market in the United States. The market is served by stations affiliated with major American networks including: WKEF, Channel 22 – ABC, operated by Sinclair Broadcasting, WHIO-TV, Channel 7 – CBS, operated by Cox Media Group, WPTD, Channel 16 – PBS, operated by ThinkTV, which also operates WPTO, assigned to Oxford, WDTN, Channel 2 – NBC, operated by Nexstar Media Group, WBDT, Channel 26 – The CW, operated by Vaughan Media (a shell corporation of Nexstar), and WRGT-TV, Channel 45 – Fox/My Network TV, operated under a local marketing agreement by Sinclair Broadcasting. The nationally syndicated morning talk show "The Daily Buzz" originated from WBDT-TV, the Acme property in Miamisburg, before moving to its current home in Florida.
Dayton is also served by 42 AM and FM radio stations directly, and numerous other stations are heard from elsewhere in southwest Ohio, which serve outlying suburbs and adjoining counties.
The Greater Dayton Regional Transit Authority (RTA) operates public bus routes in the Dayton metro area. In addition to routes covered by traditional diesel-powered buses, RTA has several electric trolley bus routes. The Dayton trolleybus system is the second longest-running of the five remaining trolleybus systems in the U.S., having entered service in 1933. It is the present manifestation of an electric transit service that has operated continuously in Dayton since 1888.
Dayton operates a Greyhound Station which provides inter-city bus transportation to and from Dayton. The hub is in the Greater Dayton Regional Transit Authority North-West hub in Trotwood.
Air transportation is available north of Dayton proper, via Dayton International Airport in Vandalia, Ohio. The airport offers service to 21 markets through 10 airlines. In 2008, it served 2.9 million passengers. The Dayton International Airport is also a significant regional air freight hub hosting FedEx Express, UPS Airlines, United States Postal Service, and major commercial freight carriers.
The Dayton area also has several regional airports. The Dayton–Wright Brothers Airport is a general aviation airport owned by the City of Dayton south of the central business district of Dayton on Springboro Pike in Miami Township. It serves as the reliever airport for Dayton International Airport. The airport primarily serves corporate and personal aircraft users. The Dahio Trotwood Airport, also known as Dayton-New Lebanon Airport, is a privately owned, public-use airport west of the central business district of Dayton. The Moraine Airpark is a privately owned, public-use airport southwest of the city of Dayton.
The Dayton region is primarily served by three interstates:
Other major routes for the region include:
From 2010 through 2017, the Ohio Department of Transportation (ODOT) performed a $533 million construction project to modify, reconstruct and widen I-75 through downtown Dayton, from Edwin C Moses Blvd. to Stanley Avenue.
Dayton hosts several inter-modal freight railroad terminals. Two Class I railroads, CSX and Norfolk Southern Railway, operate switching yards in the city.
Formerly the Baltimore & Ohio Railroad, New York Central Railroad and the Pennsylvania Railroad, and afterward, Amtrak made long-distance passenger train stops at Dayton Union Station on S. Sixth Street. The last train leaving there was the "National Limited" in October 1979.
The Dayton Public Schools operates 34 schools that serve 16,855 students, including:
The city of Dayton has more than 35 private schools within the city, including:
Dayton has 33 charter schools. Three of the top five charter schools named in 2011 are K-8 schools managed by National Heritage Academies. Notable charter schools include:
The Dayton area was ranked tenth for higher education among metropolitan areas in the United States by "Forbes" in 2009. The city is home to two major universities. The University of Dayton is a private, Catholic institution founded in 1850 by the Marianist order. It has the only American Bar Association (ABA)-approved law school in the Dayton area. The University of Dayton is Ohio's largest private university and is also home to the University of Dayton Research Institute, which ranks third in the nation for sponsored materials research, and the Center for Tissue Regeneration and Engineering at Dayton, which focuses on human tissue regeneration.
The public Wright State University became a state university in 1967. Wright State University established the National Center for Medical Readiness, a national training program for disaster preparedness and relief. Wright State's Boonshoft School of Medicine is the Dayton area's only medical school and is a leader in biomedical research.
Dayton is also home to Sinclair Community College, the largest community college at a single location in Ohio and one of the nation's largest community colleges. Sinclair is acclaimed as one of the country's best community colleges. Sinclair was founded as the YMCA college in 1887.
Other schools just outside Dayton that shape the educational landscape are Antioch College and Antioch University, both in Yellow Springs, Central State University in Wilberforce, Kettering College of Medical Arts and School of Advertising Art in Kettering, DeVry University in Beavercreek, and Clark State Community College and Wittenberg University in Springfield. The Air Force Institute of Technology, which was founded in 1919 and serves as a graduate school for the United States Air Force, is at the nearby Wright-Patterson Air Force Base.
Dayton's crime declined between 2003 and 2008 in key categories according to FBI Uniform Crime Reports and Dayton Police Department data. In 2009, crime continued to fall in the city of Dayton. Crime in the categories of forcible rape, aggravated assault, property crime, motor vehicle theft, robbery, burglary, theft and arson all showed declines for 2009. Overall, crime in Dayton dropped 40% over the previous year. The Dayton Police Department reported a total of 39 murders in 2016, which marked a 39.3% increase in homicides from 2015.
John Dillinger, a bank robber during the early 1930s, was captured and arrested by Dayton city police while visiting his girlfriend at a high-class boarding house in downtown Dayton.
On August 4, 2019, a mass shooting took place in Dayton. Ten people were killed, including the perpetrator; and twenty-seven were injured.

</doc>
<doc id="8254" url="https://en.wikipedia.org/wiki?curid=8254" title="Diode">
Diode

A diode is a two-terminal electronic component that conducts current primarily in one direction (asymmetric conductance); it has low (ideally zero) resistance in one direction, and high (ideally infinite) resistance in the other. A diode vacuum tube or thermionic diode is a vacuum tube with two electrodes, a heated cathode and a plate, in which electrons can flow in only one direction, from cathode to plate. A semiconductor diode, the most commonly used type today, is a crystalline piece of semiconductor material with a p–n junction connected to two electrical terminals. Semiconductor diodes were the first semiconductor electronic devices. The discovery of asymmetric electrical conduction across the contact between a crystalline mineral and a metal was made by German physicist Ferdinand Braun in 1874. Today, most diodes are made of silicon, but other semiconducting materials such as gallium arsenide and germanium are also used.
The most common function of a diode is to allow an electric current to pass in one direction (called the diode's "forward" direction), while blocking it in the opposite direction (the "reverse" direction). As such, the diode can be viewed as an electronic version of a check valve. This unidirectional behavior is called rectification, and is used to convert alternating current (ac) to direct current (dc). Forms of rectifiers, diodes can be used for such tasks as extracting modulation from radio signals in radio receivers.
However, diodes can have more complicated behavior than this simple on–off action, because of their nonlinear current-voltage characteristics. Semiconductor diodes begin conducting electricity only if a certain threshold voltage or cut-in voltage is present in the forward direction (a state in which the diode is said to be "forward-biased"). The voltage drop across a forward-biased diode varies only a little with the current, and is a function of temperature; this effect can be used as a temperature sensor or as a voltage reference. Also, diodes' high resistance to current flowing in the reverse direction suddenly drops to a low resistance when the reverse voltage across the diode reaches a value called the breakdown voltage.
A semiconductor diode's current–voltage characteristic can be tailored by selecting the semiconductor materials and the doping impurities introduced into the materials during manufacture. These techniques are used to create special-purpose diodes that perform many different functions. For example, diodes are used to regulate voltage (Zener diodes), to protect circuits from high voltage surges (avalanche diodes), to electronically tune radio and TV receivers (varactor diodes), to generate radio-frequency oscillations (tunnel diodes, Gunn diodes, IMPATT diodes), and to produce light (light-emitting diodes). Tunnel, Gunn and IMPATT diodes exhibit negative resistance, which is useful in microwave and switching circuits.
Diodes, both vacuum, and semiconductor can be used as shot-noise generators.
Thermionic (vacuum-tube) diodes and solid-state (semiconductor) diodes were developed separately, at approximately the same time, in the early 1900s, as radio receiver detectors. Until the 1950s, vacuum diodes were used more frequently in radios because the early point-contact semiconductor diodes were less stable. In addition, most receiving sets had vacuum tubes for amplification that could easily have the thermionic diodes included in the tube (for example the 12SQ7 double diode triode), and vacuum-tube rectifiers and gas-filled rectifiers were capable of handling some high-voltage/high-current rectification tasks better than the semiconductor diodes (such as selenium rectifiers) that were available at that time.
In 1873, Frederick Guthrie observed that a grounded, white-hot metal ball brought in close proximity to an electroscope would discharge a positively charged electroscope, but not a negatively charged electroscope.
In 1880, Thomas Edison observed unidirectional current between heated and unheated elements in a bulb, later called Edison effect, and was granted a patent on application of the phenomenon for use in a DC voltmeter.
About 20 years later, John Ambrose Fleming (scientific adviser to the Marconi Company
and former Edison employee) realized that the Edison effect could be used as a radio detector. Fleming patented the first true thermionic diode, the Fleming valve, in Britain on November 16, 1904 (followed by in November 1905).
Throughout the vacuum tube era, valve diodes were used in almost all electronics such as radios, televisions, sound systems, and instrumentation. They slowly lost market share beginning in the late 1940s due to selenium rectifier technology and then to semiconductor diodes during the 1960s. Today they are still used in a few high power applications where their ability to withstand transient voltages and their robustness gives them an advantage over semiconductor devices, and in musical instrument and audiophile applications.
In 1874, German scientist Karl Ferdinand Braun discovered the "unilateral conduction" across a contact between a metal and a mineral. Jagadish Chandra Bose was the first to use a crystal for detecting radio waves in 1894. The crystal detector was developed into a practical device for wireless telegraphy by Greenleaf Whittier Pickard, who invented a silicon crystal detector in 1903 and received a patent for it on November 20, 1906. Other experimenters tried a variety of other minerals as detectors. Semiconductor principles were unknown to the developers of these early rectifiers. During the 1930s understanding of physics advanced and in the mid 1930s researchers at Bell Telephone Laboratories recognized the potential of the crystal detector for application in microwave technology. Researchers at Bell Labs, Western Electric, MIT, Purdue and in the UK intensively developed point-contact diodes ("crystal rectifiers" or "crystal diodes") during World War II for application in radar. After World War II, AT&T used these in their microwave towers that criss-crossed the United States, and many radar sets use them even in the 21st century. In 1946, Sylvania began offering the 1N34 crystal diode. During the early 1950s, junction diodes were developed.
At the time of their invention, asymmetrical conduction devices were known as rectifiers. In 1919, the year tetrodes were invented, William Henry Eccles coined the term "diode" from the Greek roots "di" (from "δί"), meaning 'two', and "ode" (from "οδός"), meaning 'path'. The word "diode", however, as well as "triode, tetrode, pentode, hexode", were already in use as terms of multiplex telegraphy.
Although all diodes "rectify", the term "rectifier" is usually applied to diodes intended for power supply application in order to differentiate them from diodes intended for small signal circuits.
A thermionic diode is a thermionic-valve device consisting of a sealed, evacuated glass or metal envelope containing two electrodes: a cathode and a plate. The cathode is either "indirectly heated" or "directly heated". If indirect heating is employed, a heater is included in the envelope.
In operation, the cathode is heated to red heat (800–1000 °C, 1500-1800°F). A directly heated cathode is made of tungsten wire and is heated by a current passed through it from an external voltage source. An indirectly heated cathode is heated by infrared radiation from a nearby heater that is formed of Nichrome wire and supplied with current provided by an external voltage source.
The operating temperature of the cathode causes it to release electrons into the vacuum, a process called thermionic emission. The cathode is coated with oxides of alkaline earth metals, such as barium and strontium oxides. These have a low work function, meaning that they more readily emit electrons than would the uncoated cathode.
The plate, not being heated, does not emit electrons; but is able to absorb them.
The alternating voltage to be rectified is applied between the cathode and the plate. When the plate voltage is positive with respect to the cathode, the plate electrostatically attracts the electrons from the cathode, so a current of electrons flows through the tube from cathode to plate. When the plate voltage is negative with respect to the cathode, no electrons are emitted by the plate, so no current can pass from the plate to the cathode.
Point-contact diodes were developed starting in the 1930s, out of the early crystal detector technology, and are now generally used in the 3 to 30 gigahertz range. Point-contact diodes use a small diameter metal wire in contact with a semiconductor crystal, and are of either "non-welded" contact type or "welded contact" type. Non-welded contact construction utilizes the Schottky barrier principle. The metal side is the pointed end of a small diameter wire that is in contact with the semiconductor crystal. In the welded contact type, a small P region is formed in the otherwise N-type crystal around the metal point during manufacture by momentarily passing a relatively large current through the device. Point contact diodes generally exhibit lower capacitance, higher forward resistance and greater reverse leakage than junction diodes.
A p–n junction diode is made of a crystal of semiconductor, usually silicon, but germanium and gallium arsenide are also used. Impurities are added to it to create a region on one side that contains negative charge carriers (electrons), called an n-type semiconductor, and a region on the other side that contains positive charge carriers (holes), called a p-type semiconductor. When the n-type and p-type materials are attached together, a momentary flow of electrons occurs from the n to the p side resulting in a third region between the two where no charge carriers are present. This region is called the depletion region because there are no charge carriers (neither electrons nor holes) in it. The diode's terminals are attached to the n-type and p-type regions. The boundary between these two regions called a p–n junction, is where the action of the diode takes place. When a sufficiently higher electrical potential is applied to the P side (the anode) than to the N side (the cathode), it allows electrons to flow through the depletion region from the N-type side to the P-type side. The junction does not allow the flow of electrons in the opposite direction when the potential is applied in reverse, creating, in a sense, an electrical check valve.
Another type of junction diode, the Schottky diode, is formed from a metal–semiconductor junction rather than a p–n junction, which reduces capacitance and increases switching speed.
A semiconductor diode's behavior in a circuit is given by its current–voltage characteristic, or I–V graph (see graph below). The shape of the curve is determined by the transport of charge carriers through the so-called "depletion layer" or "depletion region" that exists at the p–n junction between differing semiconductors. When a p–n junction is first created, conduction-band (mobile) electrons from the N-doped region diffuse into the P-doped region where there is a large population of holes (vacant places for electrons) with which the electrons "recombine". When a mobile electron recombines with a hole, both hole and electron vanish, leaving behind an immobile positively charged donor (dopant) on the N side and negatively charged acceptor (dopant) on the P side. The region around the p–n junction becomes depleted of charge carriers and thus behaves as an insulator.
However, the width of the depletion region (called the depletion width) cannot grow without limit. For each electron–hole pair recombination made, a positively charged dopant ion is left behind in the N-doped region, and a negatively charged dopant ion is created in the P-doped region. As recombination proceeds and more ions are created, an increasing electric field develops through the depletion zone that acts to slow and then finally stop recombination. At this point, there is a "built-in" potential across the depletion zone.
If an external voltage is placed across the diode with the same polarity as the built-in potential, the depletion zone continues to act as an insulator, preventing any significant electric current flow (unless electron–hole pairs are actively being created in the junction by, for instance, light; see photodiode). This is called the "reverse bias" phenomenon.
However, if the polarity of the external voltage opposes the built-in potential, recombination can once again proceed, resulting in a substantial electric current through the p–n junction (i.e. substantial numbers of electrons and holes recombine at the junction). For silicon diodes, the built-in potential is approximately 0.7 V (0.3 V for germanium and 0.2 V for Schottky). Thus, if an external voltage greater than and opposite to the built-in voltage is applied, a current will flow and the diode is said to be "turned on" as it has been given an external "forward bias". The diode is commonly said to have a forward "threshold" voltage, above which it conducts and below which conduction stops. However, this is only an approximation as the forward characteristic is smooth (see I-V graph above).
A diode's I–V characteristic can be approximated by four regions of operation:
In a small silicon diode operating at its rated currents, the voltage drop is about 0.6 to 0.7 volts. The value is different for other diode types—Schottky diodes can be rated as low as 0.2 V, germanium diodes 0.25 to 0.3 V, and red or blue light-emitting diodes (LEDs) can have values of 1.4 V and 4.0 V respectively.
At higher currents, the forward voltage drop of the diode increases. A drop of 1 V to 1.5 V is typical at full rated current for power diodes.
The "Shockley ideal diode equation" or the "diode law" (named after the bipolar junction transistor co-inventor William Bradford Shockley) gives the I–V characteristic of an ideal diode in either forward or reverse bias (or no bias). The following equation is called the "Shockley ideal diode equation" when "n", the ideality factor, is set equal to 1 :
where
The thermal voltage "V" is approximately 25.85 mV at 300 K, a temperature close to "room temperature" commonly used in device simulation software. At any temperature it is a known constant defined by:
where "k" is the Boltzmann constant, "T" is the absolute temperature of the p–n junction, and "q" is the magnitude of charge of an electron (the elementary charge).
The reverse saturation current, "I", is not constant for a given device, but varies with temperature; usually more significantly than "V", so that "V" typically decreases as "T" increases.
The "Shockley ideal diode equation" or the "diode law" is derived with the assumption that the only processes giving rise to the current in the diode are drift (due to electrical field), diffusion, and thermal recombination–generation (R–G) (this equation is derived by setting n = 1 above). It also assumes that the R–G current in the depletion region is insignificant. This means that the "Shockley ideal diode equation" does not account for the processes involved in the reverse breakdown and photon-assisted R–G. Additionally, it does not describe the "leveling off" of the I–V curve at high forward bias due to internal resistance. Introducing the ideality factor, n, accounts for recombination and generation of carriers.
Under "reverse bias" voltages the exponential in the diode equation is negligible, and the current is a constant (negative) reverse current value of −"I". The reverse "breakdown region" is not modeled by the Shockley diode equation.
For even rather small "forward bias" voltages the exponential is very large since the thermal voltage is very small in comparison. The subtracted '1' in the diode equation is then negligible and the forward diode current can be approximated by
The use of the diode equation in circuit problems is illustrated in the article on diode modeling.
At forward voltages less than the saturation voltage, the voltage versus current characteristic curve of most diodes is not a straight line. The current can be approximated by formula_3 as mentioned in the previous section.
In detector and mixer applications, the current can be estimated by a Taylor's series. The odd terms can be omitted because they produce frequency components that are outside the pass band of the mixer or detector. Even terms beyond the second derivative usually need not be included because they are small compared to the second order term. The desired current component is approximately proportional to the square of the input voltage, so the response is called "square law" in this region.
Following the end of forwarding conduction in a p–n type diode, a reverse current can flow for a short time. The device does not attain its blocking capability until the mobile charge in the junction is depleted.
The effect can be significant when switching large currents very quickly. A certain amount of "reverse recovery time" (on the order of tens of nanoseconds to a few microseconds) may be required to remove the reverse recovery charge from the diode. During this recovery time, the diode can actually conduct in the reverse direction. This might give rise to a large constant current in the reverse direction for a short time while the diode is reverse biased. The magnitude of such a reverse current is determined by the operating circuit (i.e., the series resistance) and the diode is said to be in the storage-phase. In certain real-world cases it is important to consider the losses that are incurred by this non-ideal diode effect. However, when the slew rate of the current is not so severe (e.g. Line frequency) the effect can be safely ignored. For most applications, the effect is also negligible for Schottky diodes.
The reverse current ceases abruptly when the stored charge is depleted; this abrupt stop is exploited in step recovery diodes for the generation of extremely short pulses.
Normal (p–n) diodes, which operate as described above, are usually made of doped silicon or germanium. Before the development of silicon power rectifier diodes, cuprous oxide and later selenium was used. Their low efficiency required a much higher forward voltage to be applied (typically 1.4 to 1.7 V per "cell", with multiple cells stacked so as to increase the peak inverse voltage rating for application in high voltage rectifiers), and required a large heat sink (often an extension of the diode's metal substrate), much larger than the later silicon diode of the same current ratings would require. The vast majority of all diodes are the p–n diodes found in CMOS integrated circuits, which include two diodes per pin and many other internal diodes.
Other uses for semiconductor diodes include the sensing of temperature, and computing analog logarithms (see Operational amplifier applications#Logarithmic output).
The symbol used to represent a particular type of diode in a circuit diagram conveys the general electrical function to the reader. There are alternative symbols for some types of diodes, though the differences are minor. The triangle in the symbols points to the forward direction, i.e. in the direction of conventional current flow.
There are a number of common, standard and manufacturer-driven numbering and coding schemes for diodes; the two most common being the EIA/JEDEC standard and the European Pro Electron standard:
The standardized 1N-series numbering "EIA370" system was introduced in the US by EIA/JEDEC (Joint Electron Device Engineering Council) about 1960. Most diodes have a 1-prefix designation (e.g., 1N4003). Among the most popular in this series were: 1N34A/1N270 (germanium signal), 1N914/1N4148 (silicon signal), 1N400x (silicon 1A power rectifier), and 1N580x (silicon 3A power rectifier).
The JIS semiconductor designation system has all semiconductor diode designations starting with "1S".
The European Pro Electron coding system for active components was introduced in 1966 and comprises two letters followed by the part code. The first letter represents the semiconductor material used for the component (A = germanium and B = silicon) and the second letter represents the general function of the part (for diodes, A = low-power/signal, B = variable capacitance, X = multiplier, Y = rectifier and Z = voltage reference); for example:
Other common numbering/coding systems (generally manufacturer-driven) include:
In optics, an equivalent device for the diode but with laser light would be the Optical isolator, also known as an Optical Diode, that allows light to only pass in one direction. It uses a Faraday rotator as the main component.
The first use for the diode was the demodulation of amplitude modulated (AM) radio broadcasts. The history of this discovery is treated in depth in the crystal detector article. In summary, an AM signal consists of alternating positive and negative peaks of a radio carrier wave, whose amplitude or envelope is proportional to the original audio signal. The diode rectifies the AM radio frequency signal, leaving only the positive peaks of the carrier wave. The audio is then extracted from the rectified carrier wave using a simple filter and fed into an audio amplifier or transducer, which generates sound waves.
In microwave and millimeter wave technology, beginning in the 1930s, researchers improved and miniaturized the crystal detector. Point contact diodes ("crystal diodes") and Schottky diodes are used in radar, microwave and millimeter wave detectors.
Rectifiers are constructed from diodes, where they are used to convert alternating current (AC) electricity into direct current (DC). Automotive alternators are a common example, where the diode, which rectifies the AC into DC, provides better performance than the commutator or earlier, dynamo. Similarly, diodes are also used in "Cockcroft–Walton voltage multipliers" to convert AC into higher DC voltages.
Since most electronic circuits can be damaged when the polarity of their power supply inputs are reversed, a series diode is sometimes used to protect against such situations. This concept is known by multiple naming variations that mean the same thing: reverse voltage protection, reverse polarity protection, and reverse battery protection.
Diodes are frequently used to conduct damaging high voltages away from sensitive electronic devices. They are usually reverse-biased (non-conducting) under normal circumstances. When the voltage rises above the normal range, the diodes become forward-biased (conducting). For example, diodes are used in (stepper motor and H-bridge) motor controller and relay circuits to de-energize coils rapidly without the damaging voltage spikes that would otherwise occur. (A diode used in such an application is called a flyback diode). Many integrated circuits also incorporate diodes on the connection pins to prevent external voltages from damaging their sensitive transistors. Specialized diodes are used to protect from over-voltages at higher power (see Diode types above).
Diodes can be combined with other components to construct AND and OR logic gates. This is referred to as diode logic.
In addition to light, mentioned above, semiconductor diodes are sensitive to more energetic radiation. In electronics, cosmic rays and other sources of ionizing radiation cause noise pulses and single and multiple bit errors.
This effect is sometimes exploited by particle detectors to detect radiation. A single particle of radiation, with thousands or millions of electron volt, s of energy, generates many charge carrier pairs, as its energy is deposited in the semiconductor material. If the depletion layer is large enough to catch the whole shower or to stop a heavy particle, a fairly accurate measurement of the particle's energy can be made, simply by measuring the charge conducted and without the complexity of a magnetic spectrometer, etc.
These semiconductor radiation detectors need efficient and uniform charge collection and low leakage current. They are often cooled by liquid nitrogen. For longer-range (about a centimeter) particles, they need a very large depletion depth and large area. For short-range particles, they need any contact or un-depleted semiconductor on at least one surface to be very thin. The back-bias voltages are near breakdown (around a thousand volts per centimeter). Germanium and silicon are common materials. Some of these detectors sense position as well as energy.
They have a finite life, especially when detecting heavy particles, because of radiation damage. Silicon and germanium are quite different in their ability to convert gamma rays to electron showers.
Semiconductor detectors for high-energy particles are used in large numbers. Because of energy loss fluctuations, accurate measurement of the energy deposited is of less use.
A diode can be used as a temperature measuring device, since the forward voltage drop across the diode depends on temperature, as in a silicon bandgap temperature sensor. From the Shockley ideal diode equation given above, it might "appear" that the voltage has a "positive" temperature coefficient (at a constant current), but usually the variation of the reverse saturation current term is more significant than the variation in the thermal voltage term. Most diodes therefore have a "negative" temperature coefficient, typically −2 mV/°C for silicon diodes. The temperature coefficient is approximately constant for temperatures above about 20 kelvin. Some graphs are given for 1N400x series, and CY7 cryogenic temperature sensor.
Diodes will prevent currents in unintended directions. To supply power to an electrical circuit during a power failure, the circuit can draw current from a battery. An uninterruptible power supply may use diodes in this way to ensure that the current is only drawn from the battery when necessary. Likewise, small boats typically have two circuits each with their own battery/batteries: one used for engine starting; one used for domestics. Normally, both are charged from a single alternator, and a heavy-duty split-charge diode is used to prevent the higher-charge battery (typically the engine battery) from discharging through the lower-charge battery when the alternator is not running.
Diodes are also used in electronic musical keyboards. To reduce the amount of wiring needed in electronic musical keyboards, these instruments often use keyboard matrix circuits. The keyboard controller scans the rows and columns to determine which note the player has pressed. The problem with matrix circuits is that, when several notes are pressed at once, the current can flow backward through the circuit and trigger "phantom keys" that cause "ghost" notes to play. To avoid triggering unwanted notes, most keyboard matrix circuits have diodes soldered with the switch under each key of the musical keyboard. The same principle is also used for the switch matrix in solid-state pinball machines.
Diodes can be used to limit the positive or negative excursion of a signal to a prescribed voltage.
A diode clamp circuit can take a periodic alternating current signal that oscillates between positive and negative values, and vertically displace it such that either the positive or the negative peaks occur at a prescribed level. The clamper does not restrict the peak-to-peak excursion of the signal, it moves the whole signal up or down so as to place the peaks at the reference level.
Diodes are usually referred to as "D" for diode on PCBs. Sometimes the abbreviation "CR" for "crystal rectifier" is used.

</doc>
<doc id="8256" url="https://en.wikipedia.org/wiki?curid=8256" title="Drexel University">
Drexel University

Drexel University is a private research university with its main campus in Philadelphia, Pennsylvania. It was founded in 1891 by Anthony J. Drexel, a financier and philanthropist. Founded as Drexel Institute of Art, Science, and Industry, it was renamed Drexel Institute of Technology in 1936, before assuming its current name in 1970.
, more than 24,000 students were enrolled in over 70 undergraduate programs and more than 100 master's, doctoral, and professional programs at the university. Drexel's cooperative education program (co-op) is a prominent aspect of the school's degree programs, offering students the opportunity to gain up to 18 months of paid, full-time work experience in a field relevant to their undergraduate major or graduate degree program prior to graduation.
Drexel University was founded in 1891 as the Drexel Institute of Art, Science and Industry, by Philadelphia financier and philanthropist Anthony J. Drexel. The original mission of the institution was to provide educational opportunities in the "practical arts and sciences" for women and men of all backgrounds. The institution became known as the Drexel Institute of Technology in 1936, and in 1970 the Drexel Institute of Technology gained university status, becoming Drexel University.
Although there were many changes during its first century, the university's identity has been held constant as a privately controlled, non-sectarian, coeducational center of higher learning, distinguished by a commitment to practical education and hands-on experience in an occupational setting. The central aspect of Drexel University's focus on career preparation, in the form of its cooperative education program, was introduced in 1919. The program became integral to the university's unique educational experience. Participating students alternate periods of classroom-based study with periods of full-time, practical work experience related to their academic major and career interests.
Between 1995 and 2009, Drexel University underwent a period of significant change to its programs, enrollment, and facilities under the leadership of Dr. Constantine Papadakis, the university's president during that time. Papadakis oversaw Drexel's largest expansion in its history, with a 471 percent increase in its endowment and a 102 percent increase in student enrollment. His leadership also guided the university toward improved performance in collegiate rankings, a more selective approach to admissions, and a more rigorous academic program at all levels. It was during this period of expansion that Drexel acquired and assumed management of the former MCP Hahnemann University, creating the Drexel University College of Medicine in 2002. In 2006, the university established the Thomas R. Kline School of Law, and in 2011 the School of Law achieved full accreditation by the American Bar Association.
Dr. Constantine Papadakis died of pneumonia in April 2009 while still employed as the university's president. His successor, John Anderson Fry, was formerly the president of Franklin & Marshall College and served as the Executive Vice President of the University of Pennsylvania. Under Fry's leadership, Drexel has continued its expansion, including the July 2011 acquisition of The Academy of Natural Sciences.
The College of Arts and Sciences was formed in 1990 when Drexel merged the two existing College of Sciences and College of Humanities together.
The College of Media Arts and Design "fosters the study, exploration and management of the arts: media, design, the performing and visual". The college offers sixteen undergraduate programs, and 6 graduate programs, in modern art and design fields that range from graphic design and dance to fashion design and television management. Its wide range of programs has helped the college earn full accreditation from the National Association of Schools of Art and Design, the National Architectural Accrediting Board, and the Council for Interior Design Accreditation.
The Bennett S. LeBow College of Business history dates to the founding in 1891 of the Drexel Institute, that later became Drexel University, and of its Business Department in 1896. Today LeBow offers thirteen undergraduate majors, eight graduate programs, and two doctoral programs; 22 percent of Drexel University's undergraduate students are enrolled in a LeBow College of Business program. 
The LeBow College of Business has been ranked as the 38th best private business school in the nation. Its online MBA program is ranked 14th in the world by the "Financial Times"; the publication also ranks the undergraduate business program at LeBow as 19th in the United States. The part-time MBA program ranks 1st in academic quality in the 2015 edition of "Business Insider's" rankings. Undergraduate and graduate entrepreneurship programs are ranked 19th in the country by the "Princeton Review".
Economics programs at the LeBow College of Business are housed within the School of Economics. In addition to the undergraduate program in economics, the school is home to an M.S. in Economics program as well as a PhD program in economics. Faculty members in the School of Economics have been published in the "American Economic Review", "Rand Journal of Economics", and "Review of Economics and Statistics." The school has been ranked among the best in the world for its extensive research into matters of international trade.
Drexel's College of Engineering is one of its oldest and largest academic colleges, and served as the original focus of the career-oriented school upon its founding in 1891. The College of Engineering is home to several notable alumni, including two astronauts; financier Bennett S. LeBow, for whom the university's College of Business is named; and Paul Baran, inventor of the packet-switched network. Today, Drexel University's College of Engineering, which is home to 19 percent of the undergraduate student body, is known for creating the world's first engineering degree in appropriate technology. The college is also one of only 17 U.S. universities to offer a bachelor's degree in architectural engineering, and only one of five private institutions to do so.
The 2006 edition of U.S. News ranks the undergraduate engineering program #57 in the country and the 2007 edition of graduate schools ranks the graduate program #61. The 2008 edition ranks the University Engineering Program at #55 and in the 2009 US News Ranking, the university has moved up to the #52 position.
The engineering curriculum used by the school was originally called E4 (Enhanced Educational Experience for Engineers) which was established in 1986 and funded in part by the Engineering Directorate of the National Science Foundation. In 1988 the program evolved into tDEC (the Drexel Engineering Curriculum) which is composed of two full years of rigorous core engineering courses which encompass the freshman and sophomore years of the engineering student. The College of Engineering hasn't used the tDEC curriculum since approximately 2005.
The College of Computing and Informatics is a recent addition to Drexel University, though its programs have been offered to students for many years. The college was formed by the consolidation of the former College of Information Science & Technology (often called the "iSchool"), the Department of Computer Science, and the Computing and Security Technology program. Undergraduate and graduate programs in computer science, software engineering, data science, information systems, and computer security are offered by the college.
The Drexel University College of Medicine was added to the colleges and schools of the university in 2002, having been formed upon the acquisition of MCP Hahnemann University. In addition to its M.D. program, the College of Medicine offers more than 40 graduate programs in its Graduate School of Biomedical Sciences and Professional Studies.
The Graduate School of Biomedical Sciences and Professional studies offers both Master of Science and Doctor of Philosophy degree programs in fields like biochemistry, biotechnology, clinical research, and forensic science. The school also serves as the center for biomedical research at Drexel University.
Founded in 1961 as the United States’ first Biomedical Engineering and Science Institute, the School of Biomedical Engineering, Science and Health Systems focuses on the emerging field of biomedical science at the undergraduate, graduate, and doctoral levels. Primary research areas within the school include bioinformatics, biomechanics, biomaterials, neuroengineering, and cardiovascular engineering.
Formed in 2002 along with the College of Medicine, Drexel's College of Nursing and Health Professions offers more than 25 programs to undergraduate and graduate students in the fields of nursing, nutrition, health sciences, health services, and radiologic technology. The college's research into matters of nutrition and rehabilitation have garnered approximately $2.9 million in external research funding on an annual basis. The physician assistant program at Drexel's College of Nursing and Health Professions is ranked in the top 15 such programs in the United States; its anesthesia programs and physical therapy programs are, respectively, ranked as top-50 programs nationwide.
Established in 1892, the department now known as the College of Professional Studies has focused exclusively on educational programs and pursuits for nontraditional adult learners. Today, the Goodwin College of Professional Studies offers several options designed for adult learners at all stages of career and educational development. Bachelor of Science degree completion programs are offered in part-time evening or weekend formats; graduate programs and doctoral programs are offered at the graduate level, as are self-paced "continuing education" courses and nearly a dozen self-paced certification programs.
The Pennoni Honors College, named for Drexel alumnus and trustee Dr. C.R. "Chuck" Pennoni '63, '66, Hon. '92, and his wife Annette, recognizes and promotes excellence among Drexel students. Students admitted to the Honors College live together and take many of the same classes; the college provides these students with access to unique cultural and social activities and a unique guest speaker series. Students are also involved in the university's Honors Student Advisory Committee and have the opportunity to take part in Drexel's "Alternative Spring Break", an international study tour held each spring.
Upon its founding in 2006, the Thomas R. Kline School of Law, originally known as the Earle Mack School of Law, was the first law school founded in Philadelphia in more than three decades. The School of Law offers L.L.M. and Master of Legal Studies degrees, in addition to the flagship Juris Doctorate program, and uniquely offers cooperative education as part of its curriculum across all programs. In 2015, "Bloomberg Business" ranked the Kline School of Law as the second most underrated law school in the United States.
One of the oldest schools within Drexel University, the modern School of Education dates back to the 1891 founding of the school. Originally, the Department of Education offered teacher training to women as one of its original, career-focused degree programs. Today, the School of Education offers a coeducational approach to teacher training at the elementary and secondary levels for undergraduates. Other undergraduate programs include those focused on the intersection between learning and technology, teacher certification for non-education majors, and a minor in education for students with an interest in instruction. Graduate degrees offered by the School of Education include those in administration and leadership, special education, higher education, mathematics education, international education, and educational creativity and innovation. Doctoral degrees are offered in educational leadership and learning technologies. 
The School of Public Health states that its mission is to "provide education, conduct research, and partner with communities and organizations to improve the health of populations". To that end, the school offers both a B.S. and a minor in public health for undergraduate students as well as several options for students pursuing graduate and doctoral degrees in the field. At the graduate level, the Dornsife School offers both a Master of Public Health and an Executive Master of Public Health, as well as an M.S. in biostatistics and an M.S. in epidemiology. Two Doctor of Public Health degrees are also offered, as isa Doctor of Philosophy in epidemiology. The school's graduate and doctoral students are heavily invested in the research activities of the Dornsife School of Public Health, which has helped the school attract annual funding for its four research centers.
The Center for Hospitality and Sport Management was formed in 2013, in an effort to house and consolidate academic programs in hospitality, tourism management, the culinary arts, and sport management. Academic programs combine the unique skills required of the sports and hospitality industries with the principles and curriculum espoused by the management programs within Drexel's LeBow College of Business.
Focusing specifically on the skills required to successfully start and launch a business, the Charles D. Close School of Entrepreneurship is the first and only freestanding school of entrepreneurship in the United States. Undergraduate students take part in a B.A. program in entrepreneurship and innovation, while graduate students a combined Master of Science degree in biomedicine and entrepreneurship. Minors in entrepreneurship are also offered to undergraduate students.
Housed within the Close School is the Baiada Institute for Entrepreneurship. The institute serves as an incubator for Drexel student startups, providing resources and mentorships to students and some post-graduates who are starting their own business while enrolled in one of the Close School's degree programs or academic minors.
Drexel University launched its first Internet-based education program, a master's degree in Library & Information Science, in 1996. In 2001, Drexel created its wholly owned, for-profit online education subsidiary, Drexel e-Learning, Inc., better known as Drexel University Online. It was announced in October 2013 that Drexel University Online would no longer be a for-profit venture, but rather become an internal division within the university to better serve its online student population. Although headquartered in Philadelphia, Drexel announced a new Washington, D.C., location in December 2012 to serve as both an academic and outreach center, catering to the online student population.
In an effort to create greater awareness of distance learning and to recognize exceptional leaders and best practices in the field, Drexel University Online founded National Distance Learning Week, in conjunction with the United States Distance Learning Association, in 2007. In September 2010, Drexel University Online received the Sloan-C award for institution-wide excellence in online education indicating that it had exceptional programs of "demonstrably high quality" at the regional and national levels and across disciplines. Drexel University Online won the 2008 United States Distance Learning Association's Best Practices Awards for Distance Learning Programming. In 2007, the online education subsidiary had a revenue of $40 million. In March 2013, Drexel Online had more than 7,000 unique students from all 50 states and more than 20 countries pursuing a bachelor's, master's, or certificate. , Drexel University Online offers more than 100 fully accredited master's degrees, bachelor's degrees and certificate programs.
Drexel's longstanding cooperative education, or "co-op" program is one of the largest and oldest in the United States. Drexel has a fully internet-based job database, where students can submit résumés and request interviews with any of the thousands of companies that offer positions. Students also have the option of obtaining a co-op via independent search. A student graduating from Drexel's 5-year degree program typically has a total of 18 months of co-op with up to three different companies. The majority of co-ops are paid, averaging $15,912 per 6-month period, however this figure changes with major. About one third of Drexel graduates are offered full-time positions by their co-op employers right after graduation.
Drexel is classified among ". The university was ranked 51st in the 2018 edition of the "Top 100 Worldwide Universities Granted U.S. Utility Patents" list released by the National Academy of Inventors and the Intellectual Property Owners Association.
Research Centers and Institutes at Drexel include:
In its 2020 rankings, "U.S. News & World Report" ranked Drexel tied for 97th among national universities in the United States, 23rd in the "Most Innovative Schools" category, and 74th in "Best Value Schools".
In its 2018 rankings, "Times Higher Education World University Rankings" and the "Wall Street Journal" ranked Drexel 74th among national universities and 351st-400th among international universities.
In its 2018 rankings, "Forbes" ranked Drexel 24th among STEM universities. In 2019, it also ranked Drexel 226th among 650 national universities, liberal arts colleges and service academies, 120th among research universities, 154th among private universities, and 96th among universities in the Northeast.
In 2016, "Bloomberg Businessweek" ranked the undergraduate business program 78th in the country. In 2014, Business Insider ranked Drexel's graduate business school 19th in the country for networking.
In 2014, "The Princeton Review" ranked Drexel 20th in its list of worst college libraries.
Drexel University's programs are divided across three Philadelphia-area campuses: the University City Campus, the Center City Campus and the Queen Lane College of Medicine Campus.
The University City Main Campus of Drexel University is located just west of the Schuylkill River in the University City district of Philadelphia. It is Drexel's largest and oldest campus; the campus contains the university's administrative offices and serves as the main academic center for students. The northern, residential portion of the main campus is located in the Powelton Village section of West Philadelphia. The two prominent performing stages at Drexel University are the Mandell Theater and the Main Auditorium. The Main Auditorium dates back to the founding of Drexel and construction of its main hall. It features over 1000 seats, and a pipe organ installed in 1928. The organ was purchased by Saturday Evening Post publisher Cyrus H. K. Curtis after he had donated a similar organ, the Curtis Organ, to nearby University of Pennsylvania and it was suggested that he do the same for Drexel. The 424-seat Mandell Theater was built in 1973 and features a more performance-oriented stage, including a full fly system, modern stage lighting facilities, stadium seating, and accommodations for wheelchairs. It is used for the semiannual spring musical, as well as various plays and many events.
The Queen Lane Campus was purchased by Drexel University as part of its acquisition of MCP Hahnemann University. It is located in the East Falls neighborhood of northwest Philadelphia and is primarily utilized by first- and second-year medical students, and researchers. A free shuttle is available, connecting the Queen Lane Campus to the Center City Hahnemann and University City Main campuses.
The Center City Campus is in the middle of Philadelphia, straddling the Vine Street Expressway between Broad and 15th Streets. Shuttle service is offered between the Center City Campus and both the University City and Queen Lane campuses of the university.
In 2011, The Academy of Natural Sciences entered into an agreement to become a subsidiary of Drexel University. Founded in 1812, the Academy of Natural Sciences is America's oldest natural history museum and is a world leader in biodiversity and environmental research.
On January 5, 2009, Drexel University opened the Center for Graduate Studies in Sacramento, California. Eventually renamed Drexel University Sacramento upon the addition of an undergraduate program in business administration, the campus also offered an Ed.D. program in Educational Leadership and Management and master's degree programs in Business Administration, Finance, Higher Education, Human Resource Development, Public Health, and Interdepartmental Medical Science. On March 5, 2015, Drexel University announced the closure of the Sacramento campus, with an 18-month "phase out" period designed to allow current students to complete their degrees.
The Undergraduate Student Government Association of Drexel University works with administrators to solve student problems and tries to promote communication between the students and the administration.
The Graduate Student Association "advocates the interests and addresses concerns of graduate students at Drexel; strives to enhance graduate student life at the University in all aspects, from academic to campus security; and provides a formal means of communication between graduate students and the University community".
The Campus Activities Board (CAB) is an undergraduate, student-run event planning organization. CAB creates events for the undergraduate population. To assist with planning and organization, the Campus Activities Board is broken down into 5 committees: Special Events, Traditions, Marketing, Culture and Discovery, and Performing and Fine Arts.
Drexel has an approximate Jewish population of 5% and has both a Chabad House and a Hillel. Both provide services to Jewish and non-Jewish students at Drexel. Due to the recent influx of Orthodox Jewish students the Chabad now has its own daily kosher meal plan. The Hillel also has hot kosher food but only on select nights. There is also an eruv which is jointly managed by Jewish students from Drexel and the University of Pennsylvania.
WKDU is Drexel's student-run FM radio station, with membership open to all undergraduate students. Its status as an 800-watt, non-commercial station in a major market city has given it a wider audience and a higher profile than many other college radio stations.
DUTV is Drexel's Philadelphia cable television station. The student operated station is part of the Paul F. Harron Studios at Drexel University. The purpose of DUTV is to provide "the people of Philadelphia with quality educational television, and providing Drexel students the opportunity to gain experience in television management and production". The Programing includes an eclectic variety of shows from a bi-monthly news show, DNews, to old films, talk shows dealing with important current issues and music appreciation shows. Over 75 percent of DUTV’s programming is student produced.
"The Triangle" has been the university's newspaper since 1926 and currently publishes on a weekly basis every Friday. 
"The Triangle" has won several Mark of Excellence Awards which honor the best in Student Journalism from the Society of Professional Journalists. First place in Editorial Writing (2000), General Column Writing (2000), Second place in Editorial Writing (2001), and third place in Sports Column Writing (2001). In 2004, it won two National Pacemaker Awards for excellence in college newspapers. In December 2019 "The Triangle" announced the creation of their podcasting division, "Tri-Pod,", which debuted on January 10, 2020. Tri-Pod currently has two active podcasts, "Last Call". and "Mark and Jair Explain Sports".
The school yearbook was first published in 1911 and named the Lexerd in 1913. Prior to the publishing of a campus wide yearbook in 1911 "The Hanseatic" and "The Eccentric" were both published in 1896 as class books. Other publications include "MAYA", the undergraduate student literary and artistic magazine; "D&M Magazine", Design & Merchandising students crafted magazine; "The Smart Set from Drexel University", an online magazine founded in 2005; and "The Drexelist" a blog-style news source founded in 2010.
The Drexel Publishing Group serves as a medium for literary publishing on campus. The Drexel Publishing Group oversees "ASK" (The Journal of the College of Arts and Sciences at Drexel University), "Painted Bride Quarterly", a 36-year-old national literary magazine housed at Drexel; "The 33rd", an annual anthology of student and faculty writing at Drexel; "DPG Online Magazine", and "Maya", the undergraduate literary and artistic magazine. The Drexel Publishing Group also serves as a pedagogical organization by allowing students to intern and work on its publications.
Drexel requires all non-commuting first- and second-year students to live in one of its ten residence halls or in "university approved housing". First year students must live in one of the residence halls designated specifically for first-years. These residence halls include Millennium, Bentley, Kelly, Myers, Towers, Van Rensselaer, North, and Race Halls. Kelly, Myers, Towers, and Bentley Halls are traditional residence halls (a bedroom shared with one or more roommate(s) and one bathroom per floor), while Race, North, Caneris, and Van Rensselaer Halls are suite-style residence halls (shared bedrooms, private bathrooms, kitchens, and common area within the suite). Millennium Hall, Drexel's newest residence hall, is a modified suite (a bedroom shared with one roommate, and bathrooms and showers that look like closets with open sinks in the hallway).
Each residence hall is designed to facilitate the Freshman Experience in a slightly different way. Millennium, Kelly, and Towers Halls are all typical residence halls. Myers Hall offers "Living Learning Communities" where a group of students who share common interests such as language or major live together. Most of Bentley Hall is reserved for students of the Pennoni Honors College, although some floors are occupied by other students.
Second-year students have the option of living in a residence hall designated for upperclassmen, or "university approved housing". The residence halls for upperclassmen are North and Caneris Halls. North Hall operates under the For Students By Students Residential Experience Engagement Model, developed by the Residential Living Office. There are many apartments that are university approved that second-year students can choose to live in. Three of the largest apartment buildings that fit this description are Chestnut Square, University Crossings, and The Summit, all owned by American Campus Communities. Many other students live in smaller apartment buildings or individual townhouse-style apartments in Powelton Village. A second-year student can choose one of the already listed university approved housing options or petition the university to add a new property to the approved list. While living in a university approved apartment offers the freedom of living outside a residence hall, due to the Drexel co-op system, many students end up in the residence halls because they operate on a quarter to quarter basis, and don't require students to be locked into leases.
Graduate students can live in Stiles Hall.
All residence halls except Caneris Hall and Stiles Memorial Hall are located north of Arch Street between 34th Street and 32nd Street in the Powelton Village area.
Drexel University recognizes over 250 student organizations in the following categories:
The following groups are recognized as honors or professional organizations under the Office of Campus Activities and are not considered part of social Greek life at Drexel University.
Approximately 12 percent of Drexel's undergraduate population are members of a social Greek-letter organization. There are currently fourteen Interfraternity Council (IFC) chapters, seven Panhellenic Council (PHC) chapters and thirteen Multi-cultural Greek Council (MGC) chapters.
Two IFC chapters have been awarded Top Chapters in 2008 by their respective national organizations; Pi Kappa Alpha, and Alpha Chi Rho. In 2013, Sigma Phi Epsilon and Alpha Epsilon Pi were awarded the Top Chapter award by their respective national headquarters.
Drexel's school mascot is a dragon known, as "Mario the Magnificent", named in honor of alumnus and Board of Trustees member Mario V. Mascioli. The Dragon has been the mascot of the school since around the mid-1920s; the first written reference to the Dragons occurred in 1928, when the football team was called "The Dragons in The Triangle". Before becoming known as the Dragons, the athletic teams had been known by such names as the Blue & Gold, the Engineers, and the Drexelites. The school's sports teams, now known as the Drexel Dragons, participate in the NCAA's Division I as a member of the Colonial Athletic Association. They do not currently field a varsity football team.
In addition to its NCAA Division I teams, Drexel University is home to 33 active club teams including men's ice hockey, lacrosse, water polo, squash, triathlon, and cycling. Other club teams include soccer, baseball, rugby, field hockey, and roller hockey. The club teams operate under the direction of the Club Sports Council and the Recreational Sports Office.
Tradition suggests that rubbing the toe of the bronze "Waterboy" statue, located in the Main Building atrium, can result in receiving good grades on exams. Although the rest of the bronze statue has developed a dark brown patina over the years, the toe has remained highly polished and shines like new.
Drexel has appeared in news and television media several times. In 2006 Drexel served as the location for ABC Family's reality show "Back on Campus". Also in 2006, the Epsilon Zeta chapter of Delta Zeta won ABC Daytime's Summer of Fun contest. As a result, the sorority was featured in national television spots for a week and hosted an ABC party on campus, which was attended by cast members from "General Hospital" and "All My Children".
John Langdon, who taught typography in the Antoinette Westphal College of Media Arts & Design from 1988 to 2015, created the ambigram featured on the cover of Dan Brown's Angels & Demons; a number of other ambigrams served as the central focus of the book and its corresponding film. It is believed Prof. Langdon was the inspiration for the name of the lead character, played by Tom Hanks in the film adaptation.
In 2007, Drexel was the host of the 2008 Democratic Presidential candidate debate in Philadelphia, televised by MSNBC. The university hosted the US Table Tennis Olympic Trials between January 10 and 13, 2008. Drexel University also hosted the 2011 U.S. Open Squash Championships from October 1–6, 2011, as well as the 2012 U.S. Open Squash Championships from October 4–12, 2012.
Since its founding the university has graduated over 100,000 alumni. Certificate-earning alumni such as artist Violet Oakley and illustrator Frank Schoonover reflect the early emphasis on art as part of the university's curriculum. With World War II, the university's technical programs swelled, and as a result Drexel graduated alumni such as Paul Baran, one of the founding fathers of the Internet and one of the inventors of the packet switching network, and Norman Joseph Woodland the inventor of barcode technology. In addition to its emphasis on technology Drexel has graduated several notable athletes such as National Basketball Association (NBA) basketball players Michael Anderson and Malik Rose, and several notable business people such as Raj Gupta, former President and Chief executive officer (CEO) of Rohm and Haas, and Kenneth C. Dahlberg, former CEO of Science Applications International Corporation (SAIC). Alassane Dramane Ouattara President of the Republic of Ivory Coast. In 2018, Tirthak Saha -a 2016 graduate of the ECE school - was named to the Forbes 30 Under 30 list for achievements in the Energy field.
In 1991, the university's centennial anniversary, Drexel created an association called the Drexel 100, for alumni who have demonstrated excellence work, philanthropy, or public service. After the creation of the association 100 alumni were inducted in 1992 and since then the induction process has been on a biennial basis. In 2006 164 total alumni had been inducted into the association.
Drexel University created the annual $100,000 Anthony J. Drexel Exceptional Achievement Award to recognize a faculty member from a U.S. institution whose work transforms both research and the society it serves. The first recipient was bioengineer James J. Collins of Boston University (now at MIT) and the Howard Hughes Medical Institute.
In 2004, in conjunction with BAYADA Home Health Care, Drexel University's College of Nursing and Health Professions created the BAYADA Award for Technological Innovation in Nursing Education and Practice. The award honors nursing educators and practicing nurses whose innovation leads to improved patient care or improved nursing education.

</doc>
<doc id="8258" url="https://en.wikipedia.org/wiki?curid=8258" title="Daedalus">
Daedalus

In Greek mythology, Daedalus (; ; ; Etruscan: "Taitale") was a skillful architect, craftsman and artist, and was seen as a symbol of wisdom, knowledge, and power. He is the father of Icarus, the uncle of Perdix, and possibly also the father of Iapyx, although this is unclear. He invented and built the Labyrinth for King Minos of Crete, but shortly after finishing it King Minos had Daedalus imprisoned within the labyrinth. He and his son Icarus devised a plan to escape by using wings made of wax that Daedalus had invented. They escaped, but Icarus did not heed his father's warnings and flew too close to the sun. The wax melted and Icarus fell to his death. This left Daedalus heartbroken, but instead of giving up he flew to the island of Sicily.
Daedalus's parentage was supplied as a later addition, providing him with a father in Metion, Eupalamus, or Palamaon, and a mother, Alcippe, Iphinoe, or Phrasmede. Daedalus had two sons: Icarus and Iapyx, along with a nephew either Talos or Perdix.
Athenians transferred Cretan Daedalus to make him Athenian-born, the grandson of the ancient king Erechtheus, claiming that Daedalus fled to Crete after killing his nephew Talos. Over time, other stories were told of Daedalus.
Daedalus is first mentioned by Homer as the creator of a wide dancing-ground for Ariadne. He also created the Labyrinth on Crete, in which the Minotaur (part man, part bull) was kept. In the story of the labyrinth as told by the Hellenes, the Athenian hero Theseus is challenged to kill the Minotaur, finding his way with the help of Ariadne's thread. Daedalus' appearance in Homer is in an extended metaphor, "plainly not Homer's invention", Robin Lane Fox observes: "He is a point of comparison and so he belongs in stories which Homer's audience already recognized." In Bronze Age Crete, an inscription (//) has been read as referring to a place at Knossos, and a place of worship.
In Homer's language, "daidala" refers to finely crafted objects. They are mostly objects of armor, but fine bowls and furnishings are also "daidala", and on one occasion so are the "bronze-working" of "clasps, twisted brooches, earrings and necklaces" made by Hephaestus while cared for in secret by the goddesses of the sea.
Ignoring Homer, later writers envisaged the Labyrinth as an edifice rather than a single dancing path to the center and out again, and gave it numberless winding passages and turns that opened into one another, seeming to have neither beginning nor end. Ovid, in his "Metamorphoses", suggests that Daedalus constructed the Labyrinth so cunningly that he himself could barely escape it after he built it. Daedalus built the labyrinth for King Minos, who needed it to imprison his wife's son the Minotaur. The story is told that Poseidon had given a white bull to Minos so that he might use it as a sacrifice. Instead, Minos kept it for himself; and in revenge, Poseidon, with the help of Aphrodite, made Pasiphaë, King Minos's wife, lust for the bull. For Pasiphaë, as Greek mythologers interpreted it, Daedalus also built a wooden cow so she could mate with the bull, for the Greeks imagined the Minoan bull of the sun to be an actual, earthly bull, the slaying of which later required a heroic effort by Theseus.
The most familiar literary telling explaining Daedalus' wings is a late one, that of Ovid: in his "Metamorphoses" (VIII:183–235) Daedalus was shut up in a tower to prevent the knowledge of his Labyrinth from spreading to the public. He could not leave Crete by sea, as the king kept a strict watch on all vessels, permitting none to sail without being carefully searched. Since Minos controlled the land and sea routes, Daedalus set to work to fabricate wings for himself and his young son Icarus. He tied feathers together, from smallest to largest so as to form an increasing surface. He secured the feathers at their midpoints with string and at their bases with wax, and gave the whole a gentle curvature like the wings of a bird. When the work was done, the artist, waving his wings, found himself buoyed upward and hung suspended, poising himself on the beaten air. He next equipped his son in the same manner, and taught him how to fly. When both were prepared for flight, Daedalus warned Icarus not to fly too high, because the heat of the sun would melt the wax, nor too low, because the sea foam would soak the feathers.
They had passed Samos, Delos and Lebynthos by the time the boy, forgetting himself, began to soar upward toward the sun. The blazing sun melted and softened the wax that held the feathers together and they came off. Icarus's feathers fell one by one, and they were falling like snowflakes. Icarus quickly fell in the sea and drowned. Daedalus wept (lamenting his own arts), took Icarus's body and buried it, and called the island near the place (where Icarus fell into the ocean) Icaria in the memory of his child. Some time later, the goddess Athena visited Daedalus and gave him wings, telling him to fly like a god.
An early image of winged Daedalus appears on an Etruscan jug of ca 630 BC found at Cerveteri, where a winged figure captioned "Taitale" appears on one side of the vessel, paired on the other side, uniquely, with "Metaia", Medea: "its linking of these two mythical figures is unparalleled," Robin Lane Fox observes: "The link was probably based on their wondrous, miraculous art. Magically, Daedalus could fly, and magically Medea was able to rejuvenate the old (the scene on the jug seems to show her doing just this)". The image of Daedalus demonstrates that he was already well known in the West.
Further to the west Daedalus arrived safely in Sicily, in the care of King Cocalus of Kamikos on the island's south coast; there Daedalus built a temple to Apollo, and hung up his wings, an offering to the god. In an invention of Virgil ("Aeneid" VI), Daedalus flies to Cumae and founds his temple there, rather than in Sicily; long afterward Aeneas confronts the sculpted golden doors of the temple.
Minos, meanwhile, searched for Daedalus by traveling from city to city asking a riddle. He presented a spiral seashell and asked for a string to be run through it. When he reached Kamikos, King Cocalus, knowing Daedalus would be able to solve the riddle, privately fetched the old man to him. He tied the string to an ant which, lured by a drop of honey at one end, walked through the seashell stringing it all the way through. Minos then knew Daedalus was in the court of King Cocalus and demanded he be handed over. Cocalus managed to convince Minos to take a bath first, where Cocalus' daughters killed Minos. In some versions, Daedalus himself poured boiling water on Minos and killed him.
The anecdotes are literary and late; however, in the founding tales of the Greek colony of Gela, founded in the 680s on the southwest coast of Sicily, a tradition was preserved that the Greeks had seized cult images wrought by Daedalus from their local predecessors, the Sicani.
Daedalus was so proud of his achievements that he could not bear the idea of a rival. His sister had placed her son, named variously as Perdix, Talos, or Calos, under his charge to be taught the mechanical arts. The nephew was an art scholar and showed striking evidence of ingenuity. Walking on the seashore, he picked up the spine of a fish. According to Ovid, imitating it, he took a piece of iron and notched it on the edge, and thus invented the saw. He put two pieces of iron together, connecting them at one end with a rivet, and sharpening the other ends, and made a pair of compasses. Daedalus was so envious of his nephew's accomplishments that he took an opportunity and caused him to fall from the Acropolis. Athena turned Perdix into a partridge and left a scar that looked like a partridge on Daedalus' right shoulder and Daedalus left Athens due to this.
Such anecdotal details as these were embroideries upon the reputation of Daedalus as an innovator in many arts. In Pliny's Natural History (7.198) he is credited with inventing carpentry "and with it the saw, axe, plumb-line, drill, glue, and isinglass". Pausanias, in travelling around Greece, attributed to Daedalus numerous archaic wooden cult figures (see "xoana") that impressed him: "All the works of this artist, though somewhat uncouth to look at, nevertheless have a touch of the divine in them."
It is said he first conceived masts and sails for ships for the navy of Minos. He is said to have carved statues so well they looked as if alive; even possessing self-motion. They would have escaped if not for the chain that bound them to the wall.
Daedalus gave his name, eponymously, to any Greek artificer and to many Greek contraptions that represented dextrous skill. At Plataea there was a festival, the Daedala, in which a temporary wooden altar was fashioned, and an effigy was made from an oak-tree and dressed in bridal attire. It was carried in a cart with a woman who acted as bridesmaid. The image was called "Daedale" and the archaic ritual given an explanation through a myth to the purpose
In the period of Romanticism, Daedalus came to denote the classic artist, a skilled mature craftsman, while Icarus symbolized the romantic artist, whose impetuous, passionate and rebellious nature, as well as his defiance of formal aesthetic and social conventions, may ultimately prove to be self-destructive. Stephen Dedalus, in Joyce's "Portrait of the Artist as a Young Man" envisages his future artist-self "a winged form flying above the waves ... a hawk-like man flying sunward above the sea, a prophecy of the end he had been born to serve”.
Daedalus is said to have created statues that were so realistic that they had to be tied down to stop them from wandering off. In "Meno", Socrates and Meno are debating the nature of knowledge and true belief when Socrates refers to Daedalus' statues: "... if they are not fastened up they play truant and run away; but, if fastened, they stay where they are."

</doc>
<doc id="8259" url="https://en.wikipedia.org/wiki?curid=8259" title="Deception Pass">
Deception Pass

Deception Pass is a strait separating Whidbey Island from Fidalgo Island, in the northwest part of the U.S. state of Washington. It connects Skagit Bay, part of Puget Sound, with the Strait of Juan de Fuca. A pair of bridges known collectively as Deception Pass Bridge cross Deception Pass. The bridges were added to the National Register of Historic Places in 1982.
The Deception Pass area has been home to various Coast Salish tribes for thousands of years. The first Europeans to see Deception Pass were members of the 1790 expedition of Manuel Quimper on the "Princesa Real". The Spanish gave it the name "Boca de Flon". A group of sailors led by Joseph Whidbey, part of the Vancouver Expedition, found and mapped Deception Pass on June 7, 1792. George Vancouver gave it the name "Deception" because it had misled him into thinking Whidbey Island was a peninsula. The "deception" was heightened due to Whidbey's failure to find the strait at first. In May 1792, Vancouver was anchored near the southern end of Whidbey Island. He sent Joseph Whidbey to explore the waters east of Whidbey Island, now known as Saratoga Passage, using small boats. Whidbey reached the northern end of Saratoga Passage and explored eastward into Skagit Bay, which is shallow and difficult to navigate. He returned south to rejoin Vancouver without having found Deception Pass. It appeared that Skagit Bay was a dead-end and that Whidbey Island and Fidalgo Island were a long peninsula attached to the mainland. In June the expedition sailed north along the west coast of Whidbey Island. Vancouver sent Joseph Whidbey to explore inlets leading to the east. The first inlet turned out to be a "very narrow and intricate channel, which...abounded with rocks above and beneath the surface of the water". This channel led to Skagit Bay, thus separating Whidbey Island from the mainland. Vancouver apparently felt he and Joseph Whidbey had been deceived by the tricky strait. Vancouver wrote of Whidbey's efforts: "This determined [the shore they had been exploring] to be an island, which, in consequence of Mr. Whidbey’s circumnavigation, I distinguished by the name of Whidbey’s Island: and this northern pass, leading into [Skagit Bay], Deception Passage".
In the waters of Deception Pass, just east of the present-day Deception Pass Bridge, is a small island known as Ben Ure Island. The island became infamous for its activity of human smuggling of migrant Chinese people for local labor. Ben Ure and his partner Lawrence "Pirate" Kelly were quite profitable at their human smuggling business and played hide-and-seek with the United States Customs Department for years. Ure's own operation at Deception Pass in the late 1880s consisted of Ure and his Native-American wife. Local tradition has it that his wife would camp on the nearby Strawberry Island (which was visible from the open sea) and signal him with a fire on the island's summit to alert him to whether or not it was safe to attempt to bring the human cargo he illegally transported ashore. For transport, Ure would tie the people up in burlap bags so that if customs agents approached he could toss the bagged people overboard. The tidal currents carried the entrapped drowned migrants' bodies to San Juan Island to the north and west of the pass; many ended up in Dead Man's Bay.
Between 1910 and 1914, a prison rock quarry was operated on the Fidalgo Island side of the pass. Nearby barracks housed some 40 prisoners, members of an honors program out of Walla Walla State Penitentiary and the prison population was made up of several types of prisoners, including those convicted of murder. Guards stood watch at the quarry as prisoners cut the rock into gravel and loaded it onto barges at the base of the cliff atop the pass's waters. The quarried rock was then barged to the Seattle waterfront. The camp was dismantled in 1924 and although abandoned as a quarry, the remains of the camp can still be found. The location is hazardous; over the years there have been several fatal accidents when visitors have ventured onto the steep cliffs.
Upon completion on July 31, 1935, the span Deception Pass Bridge connected Whidbey Island to the tiny Pass Island, and Pass Island to Fidalgo Island. Prior to the bridge, travelers used an inter-island ferry to commute between Fidalgo and Whidbey islands.
Deception Pass is a dramatic seascape where the tidal flow and whirlpools beneath the twin bridges connecting Fidalgo Island to Whidbey Island move quickly. During ebb and flood tide current speed reaches about , flowing in opposite directions between ebb and flood. This swift current can lead to standing waves, large whirlpools, and roiling eddies. This swift current phenomenon can be viewed from the twin bridges' pedestrian walkways or from the trail leading below the larger south bridge from the parking lot on the Whidbey Island side. Boats can be seen waiting on either side of the pass for the current to stop or change direction before going through. Thrill-seeking kayakers go there during large tide changes to surf the standing waves and brave the class 2 and 3 rapid conditions.
Diving Deception Pass is dangerous and only for the most competent and prepared divers. There are a few times each year that the tides are right for a drift dive from the cove, under the bridge, and back to the cove as the tide changes. These must be planned well in advance by divers who know how to read currents and are aware of the dangerous conditions. However, because of the large tidal exchange, Deception Pass hosts some of the most spectacular colors and life in the Pacific Northwest. The walls and bottom are covered in colorful invertebrates, lingcod, greenlings, and barnacles everywhere.
Deception Pass is surrounded by Deception Pass State Park, one of the most visited Washington state parks with over two million annual visitors. 
The park was officially established in 1923, when the original of a military reserve was transferred to Washington State Parks. The park's facilities were greatly enhanced in the 1930s when the Civilian Conservation Corps (CCC) built roads, trails, and buildings in order to develop the park. The road to West Beach was created in 1950, opening up a stretch of beach to hordes of vehicles. The former fish hatchery at Bowman Bay became a part of the park in the early 1970s. The old entrance to the park was closed in 1997 when a new entrance was created at the intersection of Highway 20 and Cornet Bay road, improving access into and out of the park.
The park's recreational facilities include campgrounds, hiking trails, beaches, and tidepools. Several miles of the Pacific Northwest Trail are within the park, most notably including the section that crosses Deception Pass on the Highway 20 bridge. In addition, the Cornet Bay Retreat Center provides cabins and dining and recreation facilities. Cornet Bay offers boat launches and fishing opportunities, while Bowman Bay has an interpretive center that explains the story of the Civilian Conservation Corps throughout Washington state. Near the center is a CCC honor statue, which can be found in 30 different states in the country. Fishing is popular in Pass Lake, on the north side of the bridge. Boat rentals and guided tours of the park are also offered.
Included in the park are ten islands: Northwest Island, Deception Island, Pass Island, Strawberry, Ben Ure, Kiket, Skagit, Hope, and Big and Little Deadman Islands. Ben Ure Island is partially privately owned. The island is not open to the public except for a small rentable cabin available via the state park, which is only accessible by rowboat.
The 2002 horror movie "The Ring" was in part filmed near the pass. The bridge is fictionalized as a toll bridge named "Desolation Bridge" in season one of The Killing. Seattle shoegaze act The Sight Below filmed the 2008 video for their track "Further Away" at Deception Pass, with Deception Island's scenic imagery prominently featured. Seattle grunge band Mudhoney named a song on their 1993 EP Five Dollar Bob's Mock Cooter Stew "Deception Pass." Seattle progressive rock band Queensrÿche filmed scenes of their video "Anybody Listening" near Deception Pass and Deception Island.

</doc>
<doc id="8262" url="https://en.wikipedia.org/wiki?curid=8262" title="Dominoes">
Dominoes

Dominoes is a family of tile-based games played with rectangular "domino" tiles. Each domino is a rectangular tile with a line dividing its face into two square "ends". Each end is marked with a number of spots (also called "pips", "nips", or "dobs") or is blank. The backs of the dominoes in a set are indistinguishable, either blank or having some common design. The domino gaming pieces make up a domino set, sometimes called a "deck" or "pack". The traditional Sino-European domino set consists of 28 dominoes, featuring all combinations of spot counts between zero and six. A domino set is a generic gaming device, similar to playing cards or dice, in that a variety of games can be played with a set.
The earliest mention of dominoes is from Song dynasty China found in the text "Former Events in Wulin" by Zhou Mi (1232–1298). Modern dominoes first appeared in Italy during the 18th century, but how Chinese dominoes developed into the modern game is unknown. Italian missionaries in China may have brought the game to Europe.
The name "domino" is most likely from the resemblance to a kind of carnival costume worn during the Venetian Carnival, often consisting of a black-hooded robe and a white mask. Despite the coinage of the word polyomino as a generalization, there is no connection between the word "domino" and the number 2 in any language.
European-style dominoes are traditionally made of bone or ivory, or a dark hardwood such as ebony, with contrasting black or white pips (inlaid or painted). Alternatively, domino sets have been made from many different natural materials: stone (e.g., marble, granite or soapstone); other woods (e.g., ash, oak, redwood, and cedar); metals (e.g., brass or pewter); ceramic clay, or even frosted glass or crystal. These sets have a more novel look, and the often heavier weight makes them feel more substantial; also, such materials and the resulting products are usually much more expensive than polymer materials. 
Modern commercial domino sets are usually made of synthetic materials, such as ABS or polystyrene plastics, or Bakelite and other phenolic resins; many sets approximate the look and feel of ivory while others use colored or even translucent plastics to achieve a more contemporary look. Modern sets also commonly use a different color for the dots of each different end value (one-spots might have black pips while two-spots might be green, three red, etc.) to facilitate finding matching ends. Occasionally, one may find a domino set made of card stock like that for playing cards. Such sets are lightweight, compact, and inexpensive, and like cards are more susceptible to minor disturbances such as a sudden breeze. Sometimes, dominoes have a metal pin (called a spinner or pivot) in the middle.
The traditional set of dominoes contains one unique piece for each possible combination of two ends with zero to six spots, and is known as a double-six set because the highest-value piece has six pips on each end (the "double six"). The spots from one to six are generally arranged as they are on six-sided dice, but because blank ends having no spots are used, seven faces are possible, allowing 28 unique pieces in a double-six set.
However, this is a relatively small number especially when playing with more than four people, so many domino sets are "extended" by introducing ends with greater numbers of spots, which increases the number of unique combinations of ends and thus of pieces. Each progressively larger set increases the maximum number of pips on an end by three, so the common extended sets are double-nine (55 tiles), double-12 (91 tiles), double-15 (136 tiles), and double-18 (190 tiles), which is the maximum in practice. Larger sets such as double-21 (253 tiles) could theoretically exist, but they seem to be extremely rare if nonexistent, as that would be far more than is normally necessary for most domino games even with eight players. As the set becomes larger, identifying the number of pips on each domino becomes more difficult, so some large domino sets use more readable Arabic numerals instead of pips..
The oldest confirmed written mention of dominoes in China comes from the "Former Events in Wulin" (i.e., the capital Hangzhou) written by the Yuan Dynasty (1271–1368) author Zhou Mi (1232–1298), who listed "pupai" (gambling plaques or dominoes), as well as dice as items sold by peddlers during the reign of Emperor Xiaozong of Song (r. 1162–1189). Andrew Lo asserts that Zhou Mi meant dominoes when referring to "pupai", since the Ming author Lu Rong (1436–1494) explicitly defined "pupai" as dominoes (in regard to a story of a suitor who won a maiden's hand by drawing out four winning "pupai" from a set).
The earliest known manual written about dominoes is the "(Manual of the Xuanhe Period)" written by Qu You (1341–1437), but some Chinese scholars believe this manual is a forgery from a later time.
In the "Encyclopedia of a Myriad of Treasures", Zhang Pu (1602–1641) described the game of laying out dominoes as "pupai", although the character for "pu" had changed, yet retained the same pronunciation. Traditional Chinese domino games include "Tien Gow, Pai Gow, Che Deng", and others. The 32-piece Chinese domino set, made to represent each possible face of two thrown dice and thus have no blank faces, differs from the 28-piece domino set found in the West during the mid 18th century. Chinese dominoes with blank faces were known during the 17th century.
Many different domino sets have been used for centuries in various parts of the world to play a variety of domino games. Each domino originally represented one of the 21 results of throwing two six-sided dice (2d6). One half of each domino is set with the pips from one die and the other half contains the pips from the second die. Chinese sets also introduce duplicates of some throws and divide the dominoes into two suits: military and civil. Chinese dominoes are also longer than typical European dominoes.
The early 18th century had dominoes making their way to Europe, making their first appearance in Italy. The game changed somewhat in the translation from Chinese to the European culture. European domino sets contain neither suit distinctions nor the duplicates that went with them. Instead, European sets contain seven additional dominoes, with six of these representing the values that result from throwing a single die with the other half of the tile left blank, and the seventh domino representing the blank-blank (0–0) combination.
Domino tiles (also known as "bones"), are normally twice as long as they are wide, which makes it easier to re-stack pieces after use. Tiles usually feature a line in the middle to divide them visually into two squares. The value of either side is the number of spots or pips. In the most common variant (double-six), the values range from six pips down to none or blank. The sum of the two values, i.e. the total number of pips, may be referred to as the rank or weight of a tile; a tile may be described as "heavier" than a "lighter" one that has fewer (or no) pips.
Tiles are generally named after their two values. For instance, the following are descriptions of a tile bearing the values two and five:
A tile that has the same pips-value on each end is called a double, and is typically referred to as double-zero, double-one, and so on. Conversely, a tile bearing different values is called a single.
Every tile which features a given number is a member of the suit of that number. A single tile is a member of two suits: for example, 0-3 belongs both to the suit of threes and the suit of blanks, or 0 suit.
In some versions the doubles can be treated as an additional suit of doubles. In these versions, the double-six belongs both to the suit of sixes and the suit of doubles. However, the dominant approach is that each double belongs to only one suit. 
The most common domino sets commercially available are double six (with 28 tiles) and double nine (with 55 tiles). Larger sets exist and are popular for games involving several players or for players looking for long domino games.
The number of tiles in a double-n set obeys the following formula:
The total number of pips in a double-n set is found by:
formula_2 i.e. the number of tiles multiplied by the maximum pip-count (n)
e.g. a 6-6 set has (7 x 8) / 2 = 56/2 = 28 tiles, the average number of pips per tile is 6 (range is from 0 to 12), giving a total pip count of 6 x 28 = 168
This formula can be simplified a little bit when formula_3 is made equal to the "total number of doubles in the domino set":
formula_4
The most popular type of play are layout games, which fall into two main categories, blocking games and scoring games.
The most basic domino variant is for two players and requires a double-six set. The 28 tiles are shuffled face down and form the "stock" or "boneyard". Each player draws seven tiles from the stock. Once the players begin drawing tiles, they are typically placed on-edge in front of the players, so each player can see their own tiles, but none can see the value of other players' tiles. Every player can thus see how many tiles remain in the opponent's hands at all times during gameplay.
One player begins by downing (playing the first tile) one of their tiles. This tile starts the line of play, in which values of adjacent pairs of tile ends must match. The players alternately extend the line of play with one tile at one of its two ends; if a player is unable to place a valid tile, they must continue drawing tiles from the stock until they are able to place a tile. The game ends when one player wins by playing their last tile, or when the game is blocked because neither player can play. If that occurs, whoever caused the block receives all of the remaining player points not counting their own.
Players accrue points during game play for certain configurations, moves, or emptying one's hand. Most scoring games use variations of the draw game. If a player does not call "domino" before the tile is laid on the table, and another player says domino after the tile is laid, the first player must pick up an extra domino.
In a draw game (blocking or scoring), players are additionally allowed to draw as many tiles as desired from the stock before playing a tile, and they are not allowed to pass before the stock is (nearly) empty. The score of a game is the number of pips in the losing player's hand plus the number of pips in the stock. Most rules prescribe that two tiles need to remain in the stock. The draw game is often referred to as simply "dominoes".
Adaptations of both games can accommodate more than two players, who may play individually or in teams.
The line of play is the configuration of played tiles on the table. It starts with a single tile and typically grows in two opposite directions when players add matching tiles. In practice, players often play tiles at right angles when the line of play gets too close to the edge of the table.
The rules for the line of play often differ from one variant to another. In many rules, the doubles serve as spinners, i.e., they can be played on all four sides, causing the line of play to branch. Sometimes, the first tile is required to be a double, which serves as the only spinner. In some games such as Chicken Foot, all sides of a spinner must be occupied before anybody is allowed to play elsewhere. Matador has unusual rules for matching. Bendomino uses curved tiles, so one side of the line of play (or both) may be blocked for geometrical reasons.
In Mexican Train and other train games, the game starts with a spinner from which various trains branch off. Most trains are owned by a player and in most situations players are allowed to extend only their own train.
In blocking games, scoring happens at the end of the game. After a player has emptied their hand, thereby winning the game for the team, the score consists of the total pip count of the losing team's hands. In some rules, the pip count of the remaining stock is added. If a game is blocked because no player can move, the winner is often determined by adding the pips in players' hands.
In scoring games, each individual can potentially add to the score. For example, in Bergen, players score two points whenever they cause a configuration in which both open ends have the same value and three points if additionally one open end is formed by a double. In Muggins, players score by ensuring the total pip count of the open ends is a multiple of a certain number. In variants of Muggins, the line of play may branch due to spinners.
In British public houses and social clubs, a scoring version of "5s-and-3s" is used. The game is normally played in pairs (two against two) and is played as a series of "ends". In each "end", the objective is for players to attach a domino from their hand to one end of those already played so that the sum of the end dominoes is divisible by five or three. One point is scored for each time five or three can be divided into the sum of the two dominoes, i.e. four at one end and five at the other makes nine, which is divisible by three three times, resulting in three points. Double five at one end and five at the other makes 15, which is divisible by three five times (five points) and divisible by five three times (three points) for a total of eight points.
An "end" stops when one of the players is out, i.e., has played all of their dominoes. In the event no player is able to empty their hand, then the player with the lowest domino left in hand is deemed to be out and scores one point. A game consists of any number of ends with points scored in the ends accumulating towards a total. The game ends when one of the pair's total score exceeds a set number of points. A running total score is often kept on a cribbage board. 5s-and-3s is played in a number of competitive leagues in the British Isles.
For 40 years the game has been played by four people, with the winner being the first player to score 150 points, in multiples of five, by using 28 bones, using mathematical strategic defenses and explosive offense. At times, it has been played with pairs of partners. The double-six set is the preferred deck with the lowest denomination of game pieces, with 28 dominoes.
In many versions of the game, the player with the highest double leads with that double, for example "double-six". If no one has it, the next-highest double is called: "double-five?", then "double-four?", etc. until the highest double in any of the players' hands is played. If no player has an "opening" double, the next heaviest domino in the highest suit is called - "six-five?", "six-four?". In some variants, players take turns picking dominoes from the stock until an opening double is picked and played. In other variants, the hand is reshuffled and each player picks seven dominoes. After the first hand, the winner (or winning team) of the previous hand is allowed to pick first and begins by playing any domino in his or her hand.
Playing the first bone of a hand is sometimes called setting, leading, downing, or posing the first bone. Dominoes aficionados often call this procedure smacking down the bone. After each hand, bones are shuffled and each player draws the number of bones required, normally seven. Play proceeds clockwise. Players, in turn, must play a bone with an end that matches one of the open ends of the layouts.
In some versions of the games, the pips or points on the end, and the section to be played next to it must add up to a given number. For example, in a double-six set, the "sum" would be six, requiring a blank to be played next to a six, an ace (one) next to a five, a deuce (two) next to a four, etc.
The stock of bones left behind, if any, is called the bone yard, and the bones therein are said to be sleeping. In draw games, players take part in the bone selection, typically drawing from the bone yard when they do not have a "match" in their hands.
If a player inadvertently picks up and sees one or more extra dominoes, those dominoes become part of his or her hand.
A player who can play a tile may be allowed to pass anyway. Passing can be signalled by tapping twice on the table or by saying "go" or "pass".
Play continues until one of the players has played all the dominoes in his or her hand, calls "Out!", "I win", or "Domino!" and wins the hand, or until all players are blocked and no legal plays remain. This is sometimes referred to as locked down or sewed up. In a common version of the game, the next player after the block picks up all the dominoes in the bone yard as if trying to find a (nonexistent) match. If all the players are blocked, or locked out, the player with the lowest hand (pip count) wins. In team play, the team with the lowest individual hand wins. In the case of a tie, the first of tied players or the first "team" in the play rotation wins.
In games where points accrue, the winning player scores a point for each pip on each bone still held by each opponent or the opposing team. If no player went out, the win is determined by the lightest hand, sometimes only the excess points held by opponents.
A game is generally played to 100 points, the tally being kept on paper. In more common games, mainly urban rules, games are played to 150, 200, or 250 points.
In some games, the tally is kept by creating , where the beginning of the house (the first 10 points) is a large +, the next 10 points are O, and scoring with a five is a /, and are placed in the four corners of the house. One house is equal to 50 points.
In some versions, if a lock down occurs, the first person to call a lock-down gains the other players bones and adds the amount of the pips to his or her house. If a person who calls rocks after a call of lock-down or domino finds the number of pips a player called is incorrect, those points become his.
When a player plays out of turn or knocks when he could have played and someone calls bogus play, the other person is awarded 50 points.
In some places this is known as a compulsory pass.
Apart from the usual blocking and scoring games, also domino games of a very different character are played, such as solitaire or trick-taking games. Most of these are adaptations of card games and were once popular in certain areas to circumvent religious proscriptions against playing cards.
A very simple example is a Concentration variant played with a double-six set; two tiles are considered to match if their total pip count is 12.
A popular domino game in Texas is 42. The game is similar to the card game spades. It is played with four players paired into teams. Each player draws seven dominoes, and the dominoes are played into tricks. Each trick counts as one point, and any domino with a multiple of five dots counts toward the total of the hand. These 35 points of "five count" and seven tricks equals 42 points, hence the name.
Dominoes is played at a professional level, similar to poker. Numerous organisations and clubs of amateur domino players exist around the world. Some organizations organize international competitions.
Besides playing games, another use of dominoes is the domino show, which involves standing them on end in long lines so that when the first tile is toppled, it topples the second, which topples the third, etc., resulting in all of the tiles falling. By analogy, the phenomenon of small events causing similar events leading to eventual catastrophe is called the domino effect.
Arrangements of millions of tiles have been made that have taken many minutes, even hours to fall. For large and elaborate arrangements, special blockages (also known as firebreaks) are employed at regular distances to prevent a premature toppling from undoing more than a section of the dominoes while still being able to be removed without damage.
The phenomenon also has some theoretical relevance (amplifier, digital signal, information processing), and this amounts to the theoretical possibility of building domino computers. Dominoes are also commonly used as components in Rube Goldberg machines.
The Netherlands has hosted an annual domino-toppling exhibition called Domino Day since 1986. The event held on 18 November 2005 knocked over 4 million dominoes by a team from Weijers Domino Productions. On Domino Day 2008 (14 November 2008), the Weijers Domino Productions team attempted to set 10 records:
This record attempt was held in the WTC Expo hall in Leeuwarden. The artist who toppled the first stone was the Finnish acrobat Salima Peippo.
At one time, Pressman Toys manufactured a product called Domino Rally that contained tiles and mechanical devices for setting up toppling exhibits.
In Berlin on 9 November 2009, giant dominoes were toppled in a 20th-anniversary commemoration of the fall of the Berlin Wall. Former Polish president and Solidarity leader Lech Wałęsa set the toppling in motion.
A 2-1 tile is used in the logo of pizza retailer Domino's Pizza.
Since April 2008, the character encoding standard Unicode includes characters that represent the double-six domino tiles in various orientations. All combinations of blank through six pips on the left or right provides 49 glyphs, the same combinations vertically for another 49, and also a horizontal and a vertical "back" for a total of 100 glyphs. In this arrangement, both orientations are present: horizontally both tiles [1|6] and [6|1] exist, while a regular game set only has one such tile. The Unicode range for dominoes is U+1F030–U+1F09F. The naming pattern in Unicode is, by example, . Few fonts are known to support these glyphs. While the complete domino set has only 28 tiles, for printing layout reasons, the Unicode set needs both horizontal and vertical forms for each tile, plus the 01-03 (plain) 03-01 (reversed) pairs, and generic backsides.

</doc>
<doc id="8263" url="https://en.wikipedia.org/wiki?curid=8263" title="Dissociation constant">
Dissociation constant

In chemistry, biochemistry, and pharmacology, a dissociation constant (formula_1) is a specific type of equilibrium constant that measures the propensity of a larger object to separate (dissociate) reversibly into smaller components, as when a complex falls apart into its component molecules, or when a salt splits up into its component ions. The dissociation constant is the inverse of the association constant. In the special case of salts, the dissociation constant can also be called an ionization constant.
For a general reaction:
</chem>
in which a complex formula_2 breaks down into "x" A subunits and "y" B subunits, the dissociation constant is defined
where [A], [B], and [AB] are the equilibrium concentrations of A, B, and the complex AB, respectively.
One reason for the popularity of the dissociation constant in biochemistry and pharmacology is that in the frequently encountered case where x=y=1, K has a simple physical interpretation: when formula_4, formula_5 or equivalently formula_6
The dissociation constant of water is denoted "K":
The concentration of water <chem>H2O</chem> is omitted by convention, which means that the value of "K" differs from the value of "K" that would be computed using that concentration.
The value of "K" varies with temperature, as shown in the table below. This variation must be taken into account when making precise measurements of quantities such as pH.

</doc>
<doc id="8267" url="https://en.wikipedia.org/wiki?curid=8267" title="Dimensional analysis">
Dimensional analysis

In engineering and science, "dimensional analysis" is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.
The concept of physical dimension was introduced by Joseph Fourier in 1822. Physical quantities that are of the same kind (also called "commensurable") (e.g., length or time or mass) have the same dimension and can be directly compared to other physical quantities of the same kind, even if they are originally expressed in differing units of measure (such as yards and metres). If physical quantities have different dimensions (such as length vs. mass), they cannot be expressed in terms of similar units and cannot be compared in quantity (also called "incommensurable"). For example, asking whether a kilogram is larger than an hour is meaningless.
Any physically meaningful equation (and any inequality) will have the same dimensions on its left and right sides, a property known as "dimensional homogeneity". Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. Compound relations with "per" are expressed with division, e.g. 60 km/1 h. Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m for square metres), or combinations thereof.
A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m), thus they are considered derived or compound units.
Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s). The newton is defined as .
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as "hundredths", since .
Taking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:
In economics, one distinguishes between stocks and flows: a stock has units of "units" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of "units/time" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency)—but one may argue that, in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance) and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.
In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and . The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to . Since any quantity can be multiplied by 1 without changing it, the expression "" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, because , and bar/bar cancels out, so .
The most basic rule of dimensional analysis is that of dimensional homogeneity. 
However, the dimensions form an abelian group under multiplication, so:
For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometre, as these have different dimensions, nor to add 1 hour to 1 kilometre. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometre being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful "expression" only quantities of the same dimension can be added, subtracted, or compared. For example, if "m", "m" and "L" denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression is meaningful, but the heterogeneous expression is meaningless. However, "m"/"L" is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
This has the implication that most mathematical functions, particularly the transcendental functions, must have a dimensionless quantity, a pure number, as the argument and must return a dimensionless number as a result. This is clear because many transcendental functions can be expressed as an infinite power series with dimensionless coefficients.
All powers of "x" must have the same dimension for the terms to be commensurable. But if "x" is not dimensionless, then the different powers of "x" will have different, incommensurable dimensions. However, power functions including root functions may have a dimensional argument and will return a result having dimension that is the same power applied to the argument dimension. This is because power functions and root functions are, loosely, just an expression of multiplication of quantities.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension , they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometres. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in metres.
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:
Each conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being re-arranged to create a factor that cancels out the original unit. For example, as "mile" is the numerator in the original fraction and formula_3, "mile" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields formula_4, which when simplified results in the dimensionless formula_5. Multiplying any quantity (physical quantity or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units "mile" and "hour", 10 miles per hour converts to 4.4704 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., formula_6) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of formula_7 by using the following information as shown below:
After canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NO concentration of 10 ppm converts to mass flow rate of 24.63 grams per hour.
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.
For example, check the Universal Gas Law equation of , when:
As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal hitherto unknown or overlooked properties of matter, in the form of left-over dimensions — dimensional adjusters — that can then be assigned physical significance. It is important to point out that such ‘mathematical manipulation’ is neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck's constant, a fundamental constant of the universe, was ‘discovered’ as a purely mathematical abstraction or representation that built on the Rayleigh-Jeans Equation for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment – not earlier.
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. (Ratio scale in Stevens's typology) Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (formula_10, rather than a linear transform formula_11) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature "T"[F] in degrees Fahrenheit to a numerical quantity value "T"[C] in degrees Celsius, this formula may be used:
To convert "T"[C] in degrees Celsius to "T"[F] in degrees Fahrenheit, this formula may be used:
Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an "n"-ball (the solid ball in "n" dimensions), or the area of its surface, the "n"-sphere: being an "n"-dimensional figure, the volume scales as formula_12 while the surface area, being formula_13-dimensional, scales as formula_14 Thus the volume of the "n"-ball in terms of the radius is formula_15 for some constant formula_16 Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.
In fluid mechanics, dimensional analysis is performed in order to obtain dimensionless pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable pi terms or groups, it is possible to develop a similar set of pi terms for a model that has the same dimensional relationships. In other words, pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:
The origins of dimensional analysis have been disputed by historians. 
The first written application of dimensional analysis has been credited to an article of François Daviet at the Turin Academy of Science. Daviet had the master Lagrange as teacher. 
His fundamental works are contained in acta of the Academy dated 1799.
This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually later formalized in the Buckingham π theorem.
Simeon Poisson also treated the same problem of the parallelogram law by Daviet, in his treatise of 1811 and 1833 (vol I, p.39). In the second edition of 1833, Poisson explicitly introduces the term "dimension" instead of the Daviet "homogeneity".
In 1822, the important Napoleonic scientist Joseph Fourier made the first credited important contributions based on the idea that physical laws like should be independent of the units employed to measure the physical variables.
Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be "the three fundamental units", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant "G" is taken as unity, thereby defining . By assuming a form of Coulomb's law in which Coulomb's constant "k" is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were , which, after substituting his equation for mass, results in charge having the same dimensions as mass, viz. .
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his 1877 book "The Theory of Sound".
The original meaning of the word "dimension", in Fourier's "Theorie de la Chaleur", was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT, instead of just the exponents.
The Buckingham π theorem describes how every physically meaningful equation involving "n" variables can be equivalently rewritten as an equation of dimensionless parameters, where "m" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The "dimension" of a physical quantity is more fundamental than some "scale" unit used to express the amount of that physical quantity. For example, "mass" is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.
There are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity "Q" is given by 
where "a", "b", "c", "d", "e", "f", "g" are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electric current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.
As examples, the dimension of the physical quantity speed "v" is
and the dimension of the physical quantity force "F" is
The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.
There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.
The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; , and the inverse to L is 1/L or L. L raised to any rational power "p" is a member of the group, having an inverse of L or 1/L. The operation of the group is multiplication, having the usual rules for handling exponents ().
This group can be described as a vector space over the rational numbers, with for example dimensional symbol MLT corresponding to the vector . When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., "m") of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π, ..., π}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity "X" can be expressed in the general form
Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form
Knowing this restriction can be a powerful tool for obtaining new insight into the system.
The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not entirely arbitrary, because they must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T], L, M, while the latter can be expressed as M, L, [T = (ML/F)].
On the other hand, length, velocity and time do not form a set of base dimensions for mechanics, for two reasons:
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ ) is defined as a base dimension, N, as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does "not" hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials ("x") of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for "x", the expression (3 m) = 9 m makes sense (as an area), while for "x" + "x", the expression (3 m) + 3 m = 9 m + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,
This is the height to which an object rises in time "t" if the acceleration of gravity is 9.8 meter per second per second and the initial upward speed is 500 meter per second. It is not necessary for "t" to be in "seconds". For example, suppose "t" = 0.01 minutes. Then the first term would be
The value of a dimensional physical quantity "Z" is written as the product of a unit ["Z"] within the dimension and a dimensionless numerical factor, "n".
When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:
The factor formula_31 is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.
Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:
This illustrates the subtle distinction between "affine" quantities (ones modeled by an affine space, such as position) and "vector" quantities (ones modeled by a vector space, such as displacement).
Properly then, positions have dimension of "affine" length, while displacements have dimension of "vector" length. To assign a number to an "affine" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a "vector" unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,
where the symbol ≘ means "corresponds to", since although these values on the respective temperature scales correspond, they represent distinct quantities in the same way that the distances from distinct starting points to the same end point are distinct quantities, and cannot in general be equated.
For temperature differences,
(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a "direction". (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.
What is the period of oscillation of a mass attached to an ideal linear spring with spring constant suspended in gravity of strength ? That period is the solution for of some dimensionless equation in the variables , , , and .
The four quantities have the following dimensions: [T]; [M]; [M/T]; and [L/T]. From these we can form only one dimensionless product of powers of our chosen variables, formula_32 = formula_33 , and putting formula_34 for some dimensionless constant gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term "group" means "collection" rather than mathematical group. They are often called dimensionless numbers as well.
Note that the variable does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines with , , and , because is the only quantity that involves the dimension L. This implies that in this problem the is irrelevant. Dimensional analysis can sometimes yield strong statements about the "irrelevance" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of : it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: formula_35, for some dimensionless constant κ (equal to formula_36 from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be "complete" – although it still may involve unknown dimensionless constants, such as .
Consider the case of a vibrating wire of length "ℓ" (L) vibrating with an amplitude "A" (L). The wire has a linear density "ρ" (M/L) and is under tension "s" (ML/T), and we want to know the energy "E" (ML/T) in the wire. Let "π" and "π" be two dimensionless products of powers of the variables chosen, given by
The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation
where "F" is some unknown function, or, equivalently as
where "f" is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function "f". But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to "ℓ", and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.
Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness "t" (L) and radius "R" (L). The disc has a density "ρ" (M/L), rotates at an angular velocity "ω" (T) and this leads to a stress "S" (MLT) in the material. There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid. As the disc becomes thicker relative to the radius then the plane stress solution breaks down. If the disc is restrained axially on its free faces then a state of plane strain will occur. However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case. An engineer might, therefore, be interested in establishing a relationship between the five variables. Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:
Through the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure. As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs
Huntley has pointed out that a dimensional analysis can become more powerful by discovering new independent dimensions in the quantities under consideration, thus increasing the rank formula_40 of the dimensional matrix. He introduced two approaches to doing so:
As an example of the usefulness of the first approach, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_41 and a horizontal velocity component formula_42, assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then formula_42, formula_41, both dimensioned as LT, , the distance travelled, having dimension L, and the downward acceleration of gravity, with dimension LT.
With these four quantities, we may conclude that the equation for the range may be written:
Or dimensionally
from which we may deduce that formula_47 and formula_48, which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then formula_42 will be dimensioned as LT, formula_41 as LT, as L and as LT. The dimensional equation becomes:
and we may solve completely as formula_52, formula_53 and formula_54. The increase in deductive power gained by the use of directed length dimensions is apparent.
In his second approach, Huntley holds that it is sometimes useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of the quantity of matter. Quantity of matter is defined by Huntley as a quantity (a) proportional to inertial mass, but (b) not implicating inertial properties. No further restrictions are added to its definition.
For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables
There are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be formula_57 and formula_58 and we may express the dimensional equation as
where and are undetermined constants. If we draw a distinction between inertial mass with dimension formula_60 and quantity of matter with dimension formula_61, then mass flow rate and density will use quantity of matter as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:
where now only is an undetermined constant (found to be equal to formula_63 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Huntley's recognition of quantity of matter as an independent quantity dimension is evidently successful in the problems where it is applicable, but his definition of quantity of matter is open to interpretation, as it lacks specificity beyond the two requirements (a) and (b) he postulated for it. For a given substance, the SI dimension amount of substance, with unit mole, does satisfy Huntley's two requirements as a measure of quantity of matter, and could be used as a quantity of matter in any problem of dimensional analysis where Huntley's concept is applicable.
Huntley's concept of directed length dimensions however has some serious limitations:
It also is often quite difficult to assign the L, L, L, L, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the "symmetry" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of "symmetry" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?
Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's directed length dimensions to real problems.
Angles are, by convention, considered to be dimensionless quantities. As an example, consider again the projectile problem in which a point mass is launched from the origin at a speed and angle above the "x"-axis, with the force of gravity directed along the negative "y"-axis. It is desired to find the range , at which point the mass returns to the "x"-axis. Conventional analysis will yield the dimensionless variable , but offers no insight into the relationship between and .
Note that the orientational symbols form a group (the Klein four-group or "Viergruppe"). In this system, scalars always have the same orientation as the identity element, independent of the "symmetry of the problem". Physical quantities that are vectors have the orientation expected: a force or a velocity in the z-direction has the orientation of . For angles, consider an angle that lies in the z-plane. Form a right triangle in the z-plane with being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation and the side opposite has an orientation . Since (using to indicate orientational equivalence) we conclude that an angle in the xy-plane must have an orientation , which is not unreasonable. Analogous reasoning forces the conclusion that has orientation while has orientation 1. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form , where and are real scalars. Note that an expression such as formula_65 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:
which for formula_67 and formula_68 yields formula_69. Siano distinguishes between geometric angles, which have an orientation in 3-dimensional space, and phase angles associated with time-based oscillations, which have no spatial orientation, i.e. the orientation of a phase angle is formula_70.
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems. In this approach one sets up the dimensional equation and solves it as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral. This puts it into "normal form". The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, , being in the xy-plane will thus have dimension and the range of the projectile will be of the form:
Dimensional homogeneity will now correctly yield and , and orientational homogeneity requires that formula_72. In other words, that must be an odd integer. In fact the required function of theta will be which is a series consisting of odd powers of .
It is seen that the Taylor series of and are orientationally homogeneous using the above multiplication table, while expressions like and are not, and are (correctly) deemed unphysical.
Siano's orientational analysis is compatible with the conventional conception of angular quantities as being dimensionless, and within orientational analysis, the radian may still be considered a dimensionless unit. The orientational analysis of a quantity equation is carried out separately from the ordinary dimensional analysis, yielding information that supplements the dimensional analysis.
The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the formula_73 in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make "back of the envelope" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, formula_74 ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on "dimensional grounds" that the non-analytical part of the free energy per lattice site should be formula_75 where formula_76 is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: "c", "ħ", and "G", in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants "ħ", "c", and "G" (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit formula_77, formula_78 and formula_79. In problems involving a gravitational field the latter limit should be taken such that the field stays finite.
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.
If , where "c" is the speed of light and "ħ" is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length "L", mass "M" and time "T" can be expressed (dimensionally) as a power of energy "E", because length, mass and time can be expressed using speed "v", action "S", and energy "E":
though speed and action are dimensionless ( and ) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:
This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge "e" though other choices are possible.
Dimensional correctness as part of type checking has been studied since 1977.
Implementations for Ada and C++ were described in 1985 and 1988.
Kennedy's 1996 thesis describes an implementation in Standard ML, and later in F#.
Griffioen's 2019 thesis extended Kennedy's Hindley–Milner type system to support Hart's matrices.

</doc>
<doc id="8270" url="https://en.wikipedia.org/wiki?curid=8270" title="December 25">
December 25


</doc>
<doc id="8271" url="https://en.wikipedia.org/wiki?curid=8271" title="Digital television">
Digital television

Digital television (DTV) is the transmission of television audiovisual signals using digital encoding, in contrast to the earlier analog television technology which used analog signals. At the time of its development it was considered an innovative advancement and represented the first significant evolution in television technology since color television in the 1950s. Modern digital television is transmitted in high definition (HDTV) with greater resolution than analog TV. It typically uses a widescreen aspect ratio (commonly 16:9) in contrast to the narrower format of analog TV. It makes more economical use of scarce radio spectrum space; it can transmit up to seven channels in the same bandwidth as a single analog channel, and provides many new features that analog television cannot. A transition from analog to digital broadcasting began around 2000. Different digital television broadcasting standards have been adopted in different parts of the world; below are the more widely used standards:
Digital television's roots have been tied very closely to the availability of inexpensive, high performance computers. It was not until the 1990s that digital TV became a real possibility. Digital television was previously not practically feasible due to the impractically high bandwidth requirements of uncompressed digital video, requiring around 200Mbit/s (25MB/s) bit-rate for a standard-definition television (SDTV) signal, and over 1Gbit/s for high-definition television (HDTV).
Digital TV became practically feasible in the early 1990s due to a major technological development, discrete cosine transform (DCT) video compression. DCT coding is a lossy compression technique that was first proposed for image compression by Nasir Ahmed in 1972, and was later adapted into a motion-compensated DCT video coding algorithm, for video coding standards such as the H.26x formats from 1988 onwards and the MPEG formats from 1991 onwards. Motion-compensated DCT video compression significantly reduced the amount of bandwidth required for a digital TV signal. DCT coding compressed down the bandwidth requirements of digital television signals to about 34Mpps bit-rate for SDTV and around 70140 Mbit/s for HDTV while maintaining near-studio-quality transmission, making digital television a practical reality in the 1990s.
A digital TV service was proposed in 1986 by Nippon Telegraph and Telephone (NTT) and the Ministry of Posts and Telecommunication (MPT) in Japan, where there were plans to develop an "Integrated Network System" service. However, it was not possible to practically implement such a digital TV service until the adoption of discrete cosine transform (DCT) video compression technology made it possible in the early 1990s.
In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, and as the MUSE analog format was proposed by Japan's public broadcaster NHK as a worldwide standard, Japanese advancements were seen as pacesetters that threatened to eclipse U.S. electronics companies. Until June 1990, the Japanese MUSE standard—based on an analog system—was the front-runner among the more than 23 different technical concepts under consideration.
Between 1988 and 1991, several European organizations were working on DCT-based digital video coding standards for both SDTV and HDTV. The EU 256 project by the CMTT and ETSI, along with research by Italian broadcaster RAI, developed a DCT video codec that broadcast SDTV at 34Mbit/s bit-rate and near-studio-quality HDTV at about 70140 Mbit/s bit-rate. RAI demonstrated this with a 1990 FIFA World Cup broadcast in March 1990. An American company, General Instrument, also demonstrated the feasibility of a digital television signal in 1990. This led to the FCC being persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.
In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new TV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images. Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being "simulcast" on different channels. The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.
The final standard adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This outcome resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—is superior. Interlaced scanning, which is used in televisions worldwide, scans even-numbered lines first, then odd-numbered ones. Progressive scanning, which is the format used in computers, scans lines in sequences, from top to bottom. The computer industry argued that progressive scanning is superior because it does not "flicker" in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offers a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format.
DirecTV in the U.S. launched the first commercial digital satellite platform in May 1994, using the Digital Satellite System (DSS) standard. Digital cable broadcasts were tested and launched in the U.S. in 1996 by TCI and Time Warner. The first digital terrestrial platform was launched in November 1998 as ONdigital in the United Kingdom, using the DVB-T standard.
Digital television supports many different picture formats defined by the broadcast television systems which are a combination of size and aspect ratio (width to height ratio).
With digital terrestrial television (DTT) broadcasting, the range of formats can be broadly divided into two categories: high definition television (HDTV) for the transmission of high-definition video and standard-definition television (SDTV). These terms by themselves are not very precise, and many subtle intermediate cases exist.
One of several different HDTV formats that can be transmitted over DTV is: 1280 × 720 pixels in progressive scan mode (abbreviated "720p") or 1920 × 1080 pixels in interlaced video mode ("1080i"). Each of these uses a aspect ratio. HDTV cannot be transmitted over analog television channels because of channel capacity issues.
SDTV, by comparison, may use one of several different formats taking the form of various aspect ratios depending on the technology used in the country of broadcast. In terms of rectangular pixels, NTSC countries can deliver a 640 × 480 resolution in 4:3 and 854 × 480 in , while PAL can give 768 × 576 in and 1024 × 576 in . However, broadcasters may choose to reduce these resolutions to reduce bit rate (e.g., many DVB-T channels in the United Kingdom use a horizontal resolution of 544 or 704 pixels per line).
Each commercial broadcasting terrestrial television DTV channel in North America is permitted to be broadcast at a bit rate up to 19 megabits per second. However, the broadcaster does not need to use this entire bandwidth for just one broadcast channel. Instead the broadcast can use the channel to include PSIP and can also subdivide across several video subchannels (a.k.a. feeds) of varying quality and compression rates, including non-video datacasting services that allow one-way high-bit-rate streaming of data to computers like National Datacast.
A broadcaster may opt to use a standard-definition (SDTV) digital signal instead of an HDTV signal, because current convention allows the bandwidth of a DTV channel (or "multiplex") to be subdivided into multiple digital subchannels, (similar to what most FM radio stations offer with HD Radio), providing multiple feeds of entirely different television programming on the same channel. This ability to provide either a single HDTV feed or multiple lower-resolution feeds is often referred to as distributing one's "bit budget" or multicasting. This can sometimes be arranged automatically, using a statistical multiplexer (or "stat-mux"). With some implementations, image resolution may be less directly limited by bandwidth; for example in DVB-T, broadcasters can choose from several different modulation schemes, giving them the option to reduce the transmission bit rate and make reception easier for more distant or mobile viewers.
There are several different ways to receive digital television. One of the oldest means of receiving DTV (and TV in general) is from terrestrial transmitters using an antenna (known as an "aerial" in some countries). This way is known as Digital terrestrial television (DTT). With DTT, viewers are limited to channels that have a terrestrial transmitter in range of their antenna.
Other ways have been devised to receive digital television. Among the most familiar to people are digital cable and digital satellite. In some countries where transmissions of TV signals are normally achieved by microwaves, digital MMDS is used. Other standards, such as Digital multimedia broadcasting (DMB) and DVB-H, have been devised to allow handheld devices such as mobile phones to receive TV signals. Another way is IPTV, that is receiving TV via Internet Protocol, relying on digital subscriber line (DSL) or optical cable line. Finally, an alternative way is to receive digital TV signals via the open Internet (Internet television), whether from a central streaming service or a P2P (peer-to-peer) system.
Some signals carry encryption and specify use conditions (such as "may not be recorded" or "may not be viewed on displays larger than 1 m in diagonal measure") backed up with the force of law under the World Intellectual Property Organization Copyright Treaty (WIPO Copyright Treaty) and national legislation implementing it, such as the U.S. Digital Millennium Copyright Act. Access to encrypted channels can be controlled by a removable smart card, for example via the Common Interface (DVB-CI) standard for Europe and via Point Of Deployment (POD) for IS or named differently CableCard.
Digital television signals must not interfere with each other, and they must also coexist with analog television until it is phased out.
The following table gives allowable signal-to-noise and signal-to-interference ratios for various interference scenarios. This table is a crucial regulatory tool for controlling the placement and power levels of stations. Digital TV is more tolerant of interference than analog TV, and this is the reason a smaller range of channels can carry an all-digital set of television stations.
People can interact with a DTV system in various ways. One can, for example, browse the electronic program guide. Modern DTV systems sometimes use a return path providing feedback from the end user to the broadcaster. This is possible with a coaxial or fiber optic cable, a dialup modem, or Internet connection but is not possible with a standard antenna.
Some of these systems support video on demand using a communication channel localized to a neighborhood rather than a city (terrestrial) or an even larger area (satellite).
1seg (1-segment) is a special form of ISDB. Each channel is further divided into 13 segments. The 12 segments of them are allocated for HDTV and remaining segment, the 13th, is used for narrow-band receivers such as mobile television or cell phone.
DTV has several advantages over analog TV, the most significant being that digital channels take up less bandwidth, and the bandwidth needs are continuously variable, at a corresponding reduction in image quality depending on the level of compression as well as the resolution of the transmitted image. This means that digital broadcasters can provide more digital channels in the same space, provide high-definition television service, or provide other non-television services such as multimedia or interactivity. DTV also permits special services such as multiplexing (more than one program on the same channel), electronic program guides and additional languages (spoken or subtitled). The sale of non-television services may provide an additional revenue source.
Digital and analog signals react to interference differently. For example, common problems with analog television include ghosting of images, noise from weak signals, and many other potential problems which degrade the quality of the image and sound, although the program material may still be watchable. With digital television, the audio and video must be synchronized digitally, so reception of the digital signal must be very nearly complete; otherwise, neither audio nor video will be usable. Short of this complete failure, "blocky" video is seen when the digital signal experiences interference.
Analog TV began with monophonic sound, and later developed multichannel television sound with two independent audio signal channels. DTV allows up to 5 audio signal channels plus a subwoofer bass channel, with broadcasts similar in quality to movie theaters and DVDs.
DTV images have some picture defects that are not present on analog television or motion picture cinema, because of present-day limitations of bit rate and compression algorithms such as MPEG-2. This defect is sometimes referred to as "mosquito noise".
Because of the way the human visual system works, defects in an image that are localized to particular features of the image or that come and go are more perceptible than defects that are uniform and constant. However, the DTV system is designed to take advantage of other limitations of the human visual system to help mask these flaws, e.g. by allowing more compression artifacts during fast motion where the eye cannot track and resolve them as easily and, conversely, minimizing artifacts in still backgrounds that may be closely examined in a scene (since time allows).
Broadcast, cable, satellite, and Internet DTV operators control the picture quality of television signal encodes using sophisticated, neuroscience-based algorithms, such as the structural similarity (SSIM) video quality measurement tool, which was accorded each of its inventors a Primetime Emmy because of its global use. Another tool, called Visual Information Fidelity (VIF), is a top-performing algorithm at the core of the Netflix VMAF video quality monitoring system, which accounts for about 35% of all U.S. bandwidth consumption.
Changes in signal reception from factors such as degrading antenna connections or changing weather conditions may gradually reduce the quality of analog TV. The nature of digital TV results in a perfectly decodable video initially, until the receiving equipment starts picking up interference that overpowers the desired signal or if the signal is too weak to decode. Some equipment will show a garbled picture with significant damage, while other devices may go directly from perfectly decodable video to no video at all or lock up. This phenomenon is known as the digital cliff effect.
Block error may occur when transmission is done with compressed images. A block error in a single frame often results in black boxes in several subsequent frames, making viewing difficult.
For remote locations, distant channels that, as analog signals, were previously usable in a snowy and degraded state may, as digital signals, be perfectly decodable or may become completely unavailable. The use of higher frequencies will add to these problems, especially in cases where a clear line-of-sight from the receiving antenna to the transmitter is not available.
Television sets with only analog tuners cannot decode digital transmissions. When analog broadcasting over the air ceases, users of sets with analog-only tuners may use other sources of programming (e.g. cable, recorded media) or may purchase set-top converter boxes to tune in the digital signals. In the United States, a government-sponsored coupon was available to offset the cost of an external converter box. Analog switch-off (of full-power stations) took place on December 11, 2006 in The Netherlands, June 12, 2009 in the United States for full-power stations, and later for Class-A Stations on September 1, 2016, July 24, 2011 in Japan, August 31, 2011 in Canada, February 13, 2012 in Arab states, May 1, 2012 in Germany, October 24, 2012 in the United Kingdom and Ireland, October 31, 2012 in selected Indian cities, and December 10, 2013 in Australia. Completion of analog switch-off is scheduled for December 31, 2017 in the whole of India, December 2018 in Costa Rica and around 2020 for the Philippines.
Prior to the conversion to digital TV, analog television broadcast audio for TV channels on a separate FM carrier signal from the video signal. This FM audio signal could be heard using standard radios equipped with the appropriate tuning circuits.
However, after the transition of many countries to digital TV, no portable radio manufacturer has yet developed an alternative method for portable radios to play just the audio signal of digital TV channels; DTV radio is not the same thing.
The adoption of a broadcast standard incompatible with existing analog receivers has created the problem of large numbers of analog receivers being discarded during digital television transition. One superintendent of public works was quoted in 2009 saying; "some of the studies I’ve read in the trade magazines say up to a quarter of American households could be throwing a TV out in the next two years following the regulation change". In 2009, an estimated 99 million analog TV receivers were sitting unused in homes in the US alone and, while some obsolete receivers are being retrofitted with converters, many more are simply dumped in landfills where they represent a source of toxic metals such as lead as well as lesser amounts of materials such as barium, cadmium and chromium.
According to one campaign group, a CRT computer monitor or TV contains an average of of lead. According to another source, the lead in glass of a CRT varies from 1.08 lb to 11.28 lb, depending on screen size and type, but the lead is in the form of "stable and immobile" lead oxide mixed into the glass. It is claimed that the lead can have long-term negative effects on the environment if dumped as landfill. However, the glass envelope can be recycled at suitably equipped facilities. Other portions of the receiver may be subject to disposal as hazardous material.
Local restrictions on disposal of these materials vary widely; in some cases second-hand stores have refused to accept working color television receivers for resale due to the increasing costs of disposing of unsold TVs. Those thrift stores which are still accepting donated TVs have reported significant increases in good-condition working used television receivers abandoned by viewers who often expect them not to work after digital transition.
In Michigan in 2009, one recycler estimated that as many as one household in four would dispose of or recycle a TV set in the following year. The digital television transition, migration to high-definition television receivers and the replacement of CRTs with flatscreens are all factors in the increasing number of discarded analog CRT-based television receivers.

</doc>
<doc id="8274" url="https://en.wikipedia.org/wiki?curid=8274" title="Declaration of Arbroath">
Declaration of Arbroath

The Declaration of Arbroath (; ) is the name usually given to a letter, dated 6 April 1320 at Arbroath, written by Scottish barons and addressed to Pope John XXII. It constituted King Robert I's response to his excommunication for disobeying the pope's demand in 1317 for a truce in the First War of Scottish Independence. The letter asserted the antiquity of the independence of the Kingdom of Scotland, denouncing English attempts to subjugate it.
Generally believed to have been written in Arbroath Abbey by Bernard of Kilwinning (or of Linton), then Chancellor of Scotland and Abbot of Arbroath, and sealed by fifty-one magnates and nobles, the letter is the sole survivor of three created at the time. The others were a letter from the King of Scots, Robert I, and a letter from four Scottish bishops which all made similar points. The "Declaration" was intended to assert Scotland's status as an independent, sovereign state and defend Scotland's right to use military action when unjustly attacked.
Submitted in Latin, the "Declaration" was little known until the late 17th century and is unmentioned by any of Scotland's major 16th century historians. In the 1680s the Latin text was printed for the first time and translated into English in the wake of the Glorious Revolution, after which time it was sometimes described as a declaration of independence.
The "Declaration" was part of a broader diplomatic campaign, which sought to assert Scotland's position as an independent kingdom, rather than its being a feudal land controlled by England's Norman kings, as well as lift the excommunication of Robert the Bruce. The pope had recognised Edward I of England's claim to overlordship of Scotland in 1305 and Bruce was excommunicated by the Pope for murdering John Comyn before the altar at Greyfriars Church in Dumfries in 1306. This excommunication was lifted in 1308; subsequently the pope threatened Robert with excommunication again if Avignon's demands in 1317 for peace with England were ignored. Warfare continued, and in 1320 John XXII again excommunicated Robert I. In reply, the "Declaration" was composed and signed and, in response, the papacy rescinded King Robert Bruce's excommunication and thereafter addressed him using his royal title.
The wars of Scottish independence began as a result of the deaths of King Alexander III of Scotland in 1286 and his heir the "Maid of Norway" in 1290, which left the throne of Scotland vacant and the subsequent succession crisis of 1290-1296 ignited a struggle among the Competitors for the Crown of Scotland, chiefly between the House of Comyn, the House of Balliol, and the House of Bruce who all claimed the crown. After July 1296's deposition of King John Balliol by Edward of England and then February 1306's killing of John Comyn III, Robert Bruce's rivals to the throne of Scotland were gone, and Robert was crowned king at Scone that year. Edward I, the "Hammer of Scots", died in 1307; his son and successor Edward II did not renew his father's campaigns in Scotland. In 1309 a parliament held at St Andrews acknowledged Robert's right to rule, received emissaries from the Kingdom of France recognising the Bruce's title, and proclaimed the independence of the kingdom from England.
By 1314 only Edinburgh, Berwick-upon-Tweed, Roxburgh, and Stirling remained in English hands. In June 1314 the Battle of Bannockburn had secured Robert Bruce's position as King of Scots; Stirling, the Central Belt, and much of Lothian came under Robert's control while the defeated Edward II's power on escaping to England via Berwick weakened under the sway of his cousin Henry, Earl of Lancaster. King Robert was thus able to consolidate his power, and sent his brother Edward Bruce to claim the Kingdom of Ireland in 1315 with an army landed in Ulster the previous year with the help of Gaelic lords from the Isles. Edward Bruce died in 1318 without achieving success, but the Scots campaigns in Ireland and in northern England were intended to press for the recognition of Robert's crown by King Edward. At the same time, it undermined the House of Plantagenet's claims to overlordship of the British Isles and halted the Plantagenets' effort to absorb Scotland as had been done in Ireland and Wales. Thus were the Scots nobles confident in their letters to Pope John of the distinct and independent nature of Scotland's kingdom; the "Declaration of Arbroath" was one such. According to historian David Crouch, "The two nations were mutually hostile kingdoms and peoples, and the ancient idea of Britain as an informal empire of peoples under the English king's presidency was entirely dead."
The text makes claims about the ancient history of Scotland and especially the "Scoti", forbears of the Scots, who the "Declaration" claims originated in "Scythia Major" and migrated via Spain to Britain, dating their migration to "1,200 years from the Israelite people's crossing of the Red Sea". The "Declaration" describes how the Scots had "thrown out the Britons and completely destroyed the Picts", resisted the invasions of "the Norse, the Danes and the English", and "held itself ever since, free from all slavery". It then claims that in the Kingdom of Scotland, "one hundred and thirteen kings have reigned of their own Blood Royal, without interruption by foreigners". The text compares Robert Bruce with the Biblical warriors Judas Maccabeus and Joshua.
The "Declaration" made a number of points: that Edward I of England had unjustly attacked Scotland and perpetrated atrocities; that Robert the Bruce had delivered the Scottish nation from this peril; and, most controversially, that the independence of Scotland was the prerogative of the Scottish people, rather than the King of Scots. (However this should be taken in the context of the time - ‘Scottish People’ refers to the Scottish nobility, rather than commoners.) In fact it stated that the nobility would choose someone else to be king if Bruce proved to be unfit in maintaining Scotland's independence.
Some have interpreted this last point as an early expression of 'popular sovereignty' – that government is contractual and that kings can be chosen by the community rather than by God alone. It has been considered to be the first statement of the contractual theory of monarchy underlying modern constitutionalism. 
Some point to the “Declaration" as evidence of the long-term persistence of the Scots as a distinct national community, giving a very early date for the emergence of nationalism. 
It has also been argued that the "Declaration" was not a statement of popular sovereignty (and that its signatories would have had no such concept) but a statement of royal propaganda supporting Bruce's faction. A justification had to be given for the rejection of King John Balliol in whose name William Wallace and Andrew de Moray had rebelled in 1297. The reason given in the "Declaration" is that Bruce was able to defend Scotland from English aggression whereas, by implication, King John could not.
Whatever the true motive, the idea of a contract between King and people was advanced to the Pope as a justification for Bruce's coronation whilst John de Balliol still lived in Papal custody.
For the full text in Latin and a translation in English, See on WikiSource.
There are 39 names—eight earls and thirty-one barons—at the start of the document, all of whom may have had their seals appended, probably over the space of some weeks and months, with nobles sending in their seals to be used. On the extant copy of the "Declaration" there are only 19 seals, and of those 19 people only 12 are named within the document. It is thought likely that at least 11 more seals than the original 39 might have been appended. The "Declaration" was then taken to the papal court at Avignon by Bishop Kininmund, Sir Adam Gordon and Sir Odard de Maubuisson.
The Pope heeded the arguments contained in the "Declaration", influenced by the offer of support from the Scots for his long-desired crusade if they no longer had to fear English invasion. He exhorted Edward II in a letter to make peace with the Scots, but the following year was again persuaded by the English to take their side and issued six bulls to that effect.
Eight years later, on 1 March 1328, the new English king, Edward III, signed a peace treaty between Scotland and England, the Treaty of Edinburgh–Northampton. In this treaty, which was in effect for until 1333, Edward renounced all English claims to Scotland. In October 1328, the interdict on Scotland, and the excommunication of its king, were removed by the Pope.
The original copy of the "Declaration" that was sent to Avignon is lost. The only existing manuscript copy of the "Declaration" survives among Scotland's state papers, measuring 540mm wide by 675mm long (including the seals), it is held by the National Archives of Scotland in Edinburgh, a part of the National Records of Scotland. 
The most widely known English language translation was made by Sir James Fergusson, formerly Keeper of the Records of Scotland, from text that he reconstructed using this extant copy and early copies of the original draft. 
G. W. S. Barrow has shown that one passage in particular, often quoted from the Fergusson translation, was carefully written using different parts of "The Conspiracy of Catiline" by the Roman author, Sallust (86–35 BC) as the direct source:
Listed below are the signatories of the Declaration of Arbroath in 1320.
The letter itself is written in Latin. It uses the Latin versions of the signatories' titles, and in some cases, the spelling of names has changed over the years. This list generally uses the titles of the signatories' Wikipedia biographies.
In addition, the names of the following do not appear in the document's text, but their names are written on seal tags and their seals are present:
In 1998 former majority leader Trent Lott succeeded in instituting an annual "National Tartan Day" on 6 April by resolution of the United States Senate. US Senate Resolution 155 of 10 November 1997 states that "the Declaration of Arbroath, the Scottish Declaration of Independence, was signed on April 6, 1320 and the American Declaration of Independence was modeled [sic] on that inspirational document". However, although this influence is accepted by some historians, it is disputed by others. 
In 2016 the Declaration of Arbroath was placed on the UK Memory of the World Register, part of UNESCO's Memory of the World Programme.
2020 is the 700th anniversary of the Declaration of Arbroath's composition; an "Arbroath 2020" festival was arranged but postponed due to the COVID-19 pandemic. The National Museum of Scotland in Edinburgh planned to display the document to the public for the first time in fifteen years.

</doc>
<doc id="8276" url="https://en.wikipedia.org/wiki?curid=8276" title="Digital data">
Digital data

Digital data, in information theory and information systems, is the discrete, discontinuous representation of information or works. Numbers and letters are commonly used representations.
Digital data can be contrasted with analog signals which behave in a continuous manner, and with continuous functions such as sounds, images, and other measurements.
The word "digital" comes from the same source as the words digit and "digitus" (the Latin word for "finger"), as fingers are often used for discrete counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word "digital" in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942. The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.
Since symbols (for example, alphanumeric characters) are not continuous, representing symbols digitally is rather simpler than conversion of continuous or analog information to digital. Instead of sampling and quantization as in analog-to-digital conversion, such techniques as polling and encoding are used.
A symbol input device usually consists of a group of switches that are polled at regular intervals to see which switches are switched. Data will be lost if, within a single polling interval, two switches are pressed, or a switch is pressed, released, and pressed again. This polling can be done by a specialized processor in the device to prevent burdening the main CPU. When a new symbol has been entered, the device typically sends an interrupt, in a specialized format, so that the CPU can read it.
For devices with only a few switches (such as the buttons on a joystick), the status of each can be encoded as bits (usually 0 for released and 1 for pressed) in a single word. This is useful when combinations of key presses are meaningful, and is sometimes used for passing the status of modifier keys on a keyboard (such as shift and control). But it does not scale to support more keys than the number of bits in a single byte or word.
Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded, or converted into a number, based on the status of modifier keys and the desired character encoding.
A custom encoding can be used for a specific application with no loss of data. However, using a standard encoding such as ASCII is problematic if a symbol such as 'ß' needs to be converted but is not in the standard.
It is estimated that in the year 1986 less than 1% of the world's technological capacity to store information was digital and in 2007 it was already 94%. The year 2002 is assumed to be the year when humankind was able to store more information in digital than in analog format (the "beginning of the digital age").
Digital data come in these three states: data at rest, data in transit and data in use. The confidentiality, integrity and availability have to be managed during the entire lifecycle from 'birth' to the destruction of the data.
All digital information possesses common properties that distinguish it from analog data with respect to communications:
Even though digital signals are generally associated with the binary electronic digital systems used in modern electronics and computing, digital systems are actually ancient, and need not be binary or electronic.

</doc>
<doc id="8278" url="https://en.wikipedia.org/wiki?curid=8278" title="Deduction">
Deduction

Deduction may refer to:

</doc>
<doc id="8280" url="https://en.wikipedia.org/wiki?curid=8280" title="Demon">
Demon

A demon is a supernatural being, typically associated with evil, prevalent historically in religion, occultism, literature, fiction, mythology, and folklore; as well as in media such as comics, video games, movies and television series.
The original Greek word "daimon" does not carry negative connotations. The Ancient Greek word "daimōn" denotes a spirit or divine power, much like the Latin "genius" or "numen". The Greek conception of a "daimōn" notably appears in the works of Plato, where it describes the divine inspiration of Socrates.
In Ancient Near Eastern religions and in the Abrahamic traditions, including ancient and medieval Christian demonology, a demon is considered a harmful spiritual entity which may cause demonic possession, calling for an exorcism. In Western occultism and Renaissance magic, which grew out of an amalgamation of Greco-Roman magic, Jewish Aggadah and Christian demonology, a demon is believed to be a spiritual entity that may be conjured and controlled.
The Ancient Greek word "daemon" denotes a spirit or divine power, much like the Latin "genius" or "numen". "Daimōn" most likely came from the Greek verb "daiesthai" (to divide, distribute). The Greek conception of a "daimōn" notably appears in the works of Plato, where it describes the divine inspiration of Socrates. The original Greek word "daimon" does not carry the negative connotation initially understood by implementation of the Koine ("daimonion"), and later ascribed to any cognate words sharing the root.
The Greek terms do not have any connotations of evil or malevolence. In fact, "eudaimonia", (literally good-spiritedness) means happiness. By the early Roman Empire, cult statues were seen, by pagans and their Christian neighbors alike, as inhabited by the numinous presence of the gods: "Like pagans, Christians still sensed and saw the gods and their power, and as something, they had to assume, lay behind it, by an easy traditional shift of opinion they turned these pagan "daimones" into malevolent 'demons', the troupe of Satan... Far into the Byzantine period Christians eyed their cities' old pagan statuary as a seat of the demons' presence. It was no longer beautiful, it was infested." The term had first acquired its negative connotations in the Septuagint translation of the Hebrew Bible into Greek, which drew on the mythology of ancient Semitic religions. This was then inherited by the Koine text of the New Testament. The Western medieval and neo-medieval conception of a "demon" derives seamlessly from the ambient popular culture of Late Antiquity. The Hellenistic "daemon" eventually came to include many Semitic and Near Eastern gods as evaluated by Christianity.
The supposed existence of demons remains an important concept in many modern religions and occultist traditions. Demons are still feared largely due to their alleged power to possess living creatures. In the contemporary Western occultist tradition (perhaps epitomized by the work of Aleister Crowley), a demon (such as Choronzon, which is Crowley's interpretation of the so-called 'Demon of the Abyss') is a useful metaphor for certain inner psychological processes (inner demons), though some may also regard it as an objectively real phenomenon. Some scholars believe that large portions of the demonology (see Asmodai) of Judaism, a key influence on Christianity and Islam, originated from a later form of Zoroastrianism, and were transferred to Judaism during the Persian era.
Both deities and demons can act as intermediaries to deliver messages to humans. Thus they share some resemblance to the Greek daimonion. The exact definition of "demon" in Egyptology posed a major problem for modern scholarship, since the borders between a deity and a demon are sometimes blurred and the ancient Egyptian language lacks a term for the modern English "demon". However, magical writings indicate that ancient Egyptians acknowledged the existence of malevolent demons by highlighting the demon names with red ink. Demons in this culture appeared to be subordinative and related to a specific deity, yet they may have occasionally acted independently of the divine will. The existence of demons can be related to the realm of chaos, beyond the created world. But even this negative connotation cannot be denied in light of the magical texts. The role of demons in relation to the human world remains ambivalent and largely depends on context.
Ancient Egyptian demons can be divided into two classes: "guardians" and "wanderers." "Guardians" are tied to a specific place; their demonic activity is topographically defined and their function can be benevolent towards those who have the secret knowledge to face them. Demons protecting the underworld may prevent human souls from entering paradise. Only by knowing right charms is the deceased able to enter the "Halls of Osiris". Here, the aggressive nature of the guardian demons is motivated by the need to protect their abodes and not by their evil essence. Accordingly, demons guarded sacred places or the gates to the netherworld. During the Ptolemaic and Roman period, the guardians shifted towards the role of Genius loci and they were the focus of local and private cults.
The "wanderers" are associated with possession, mental illness, death and plagues. Many of them serve as executioners for the major deities, such as Ra or Osiris, when ordered to punish humans on earth or in the netherworld. Wanderers can also be agents of chaos, arising from the world beyond creation to bring about misfortune and suffering without any divine instructions, led only by evil motivations. The influences of the wanderers can be warded off and kept at the borders on the human world by the use of magic, but they can never be destroyed. A sub-category of "wanderers" are nightmare demons, which were believed to cause nightmares by entering a human body.
The ancient Mesopotamians believed that the underworld was home to many demons, which are sometimes referred to as "offspring of "arali"". These demons could sometimes leave the underworld and terrorize mortals on earth. One class of demons that were believed to reside in the underworld were known as "galla"; their primary purpose appears to have been to drag unfortunate mortals back to Kur. They are frequently referenced in magical texts, and some texts describe them as being seven in number. Several extant poems describe the "galla" dragging the god Dumuzid into the underworld. Like other demons, however, "galla" could also be benevolent and, in a hymn from King Gudea of Lagash ( 2144 – 2124 BCE), a minor god named Ig-alima is described as "the great "galla" of Girsu".
Lamashtu was a demonic goddess with the "head of a lion, the teeth of a donkey, naked breasts, a hairy body, hands stained (with blood?), long fingers and fingernails, and the feet of Anzû." She was believed to feed on the blood of human infants and was widely blamed as the cause of miscarriages and cot deaths. Although Lamashtu has traditionally been identified as a demoness, the fact that she could cause evil on her own without the permission of other deities strongly indicates that she was seen as a goddess in her own right. Mesopotamian peoples protected against her using amulets and talismans. She was believed to ride in her boat on the river of the underworld and she was associated with donkeys. She was believed to be the daughter of An.
Pazuzu is a demonic god who was well-known to the Babylonians and Assyrians throughout the first millennium BCE. He is shown with "a rather canine face with abnormally bulging eyes, a scaly body, a snake-headed penis, the talons of a bird and usually wings." He was believed to be the son of the god Hanbi. He was usually regarded as evil, but he could also sometimes be a beneficent entity who protected against winds bearing pestilence and he was thought to be able to force Lamashtu back to the underworld. Amulets bearing his image were positioned in dwellings to protect infants from Lamashtu and pregnant women frequently wore amulets with his head on them as protection from her.
Šul-pa-e's name means "youthful brilliance", but he was not envisioned as youthful god. According to one tradition, he was the consort of Ninhursag, a tradition which contradicts the usual portrayal of Enki as Ninhursag's consort. In one Sumerian poem, offerings made to Šhul-pa-e in the underworld and, in later mythology, he was one of the demons of the underworld.
According to the Jewish Encyclopedia, "In Chaldean mythology the seven evil deities were known as "shedu", storm-demons, represented in ox-like form." They were represented as winged bulls, derived from the colossal bulls used as protective jinn of royal palaces.
As referring to the existence or non-existence of demons ("shedim" or "Se'irim") there are converse opinions in Judaism. There are "practically nil" roles assigned to demons in the Hebrew Bible. In Judaism today, beliefs in "demons" or "evil spirits" are either "midot hasidut" (Hebr. for "customs of the pious"), and therefore not halachah, or notions based on a superstition that are non-essential, non-binding parts of Judaism, and therefore not normative Jewish practice. That is to say, Jews are not obligated to believe in the existence of "shedim", as posek rabbi David Bar-Hayim points out.
The Tanakh mentions two classes of demonic spirits, the "se'irim" and the "shedim". The word "shedim" appears in two places in the Tanakh (, ). The "se'irim" are mentioned once in , probably a re-calling of Assyrian demons in shape of goats. The "shedim" in return are not pagan demigods, but the foreign gods themselves. Both entities appear in a scriptural context of animal or child sacrifice to "non-existent" false gods.
From Chaldea, the term "shedu" traveled to the Israelites. The writers of the Tanach applied the word as a dialogism to Canaanite deities.
There are indications that demons in popular Hebrew mythology were believed to come from the nether world. Various diseases and ailments were ascribed to them, particularly those affecting the brain and those of internal nature. Examples include catalepsy, headache, epilepsy and nightmares. There also existed a demon of blindness, "Shabriri" (lit. "dazzling glare") who rested on uncovered water at night and blinded those who drank from it.
Demons supposedly entered the body and caused the disease while overwhelming or "seizing" the victim. To cure such diseases, it was necessary to draw out the evil demons by certain incantations and talismanic performances, at which the Essenes excelled. Josephus, who spoke of demons as "spirits of the wicked which enter into men that are alive and kill them", but which could be driven out by a certain root, witnessed such a performance in the presence of the Emperor Vespasian and ascribed its origin to King Solomon. In mythology, there were few defences against Babylonian demons. The mythical mace Sharur had the power to slay demons such as Asag, a legendary gallu or edimmu of hideous strength.
In the Jerusalem Talmud notions of "shedim" ("demons" or "spirits") are almost unknown or occur only very rarely, whereas in the Babylon Talmud there are many references to "shedim" and magical incantations. The existence of "shedim" in general was not questioned by most of the Babylonian Talmudists. As a consequence of the rise of influence of the Babylonian Talmud over that of the Jerusalem Talmud, late rabbis in general took as fact the existence of "shedim", nor did most of the medieval thinkers question their reality. However, rationalists like Maimonides, Saadia Gaon and Abraham ibn Ezra and others explicitly denied their existence, and completely rejected concepts of demons, evil spirits, negative spiritual influences, attaching and possessing spirits. Their point of view eventually became mainstream Jewish understanding.
In Kabbalah demons are regarded a necessary part of the divine emanation in the material world and a byproduct of human sin (Qliphoth). However spirits such as the "shedim" may also be benevolent and were used in kabbalistic ceremonies (as with the "golem" of Rabbi Yehuda Loevy) and malevolent "shedim" ("Mazikin", from the root meaning "to damage") were often credited with possession.
Aggadic tales from the Persian tradition describe the "shedim", the " mazziḳim" ("harmers"), and the " ruḥin" ("spirits"). There were also "lilin" ("night spirits"), " ṭelane" ("shade", or "evening spirits"), " ṭiharire" ("midday spirits"), and " ẓafrire" ("morning spirits"), as well as the "demons that bring famine" and "such as cause storm and earthquake". According to some aggadic stories, demons were under the dominion of a king or chief, either Asmodai or, in the older Aggadah, Samael ("the angel of death"), who killed via poison. Stories in the fashion of this kind of folklore never became an essential feature of Jewish theology. Although occasionally an angel is called "satan" in the Babylon Talmud, this does not refer to a demon: "Stand not in the way of an ox when coming from the pasture, for Satan dances between his horns".
To the Qumran community during the Second Temple period this apotropaic prayer was assigned, stating: "And, I the Sage, declare the grandeur of his radiance in order to frighten and terri[fy] all the spirits of the ravaging angels and the bastard spirits, demons, Liliths, owls" ("Dead Sea Scrolls", "Songs of the Sage," Lines 4–5).
In the Dead Sea Scrolls, there exists a fragment entitled "Curses of Belial" ("Curses of Belial (Dead Sea Scrolls, 394, 4Q286(4Q287, fr. 6)=4QBerakhot)"). This fragment holds much rich language that reflects the sentiment shared between the Qumran towards Belial. In many ways this text shows how these people thought Belial influenced sin through the way they address him and speak of him. By addressing "Belial and all his guilty lot," (4Q286:2) they make it clear that he is not only impious, but also guilty of sins. Informing this state of uncleanliness are both his "hostile" and "wicked design" (4Q286:3,4). Through this design, Belial poisons the thoughts of those who are not necessarily sinners. Thus a dualism is born from those inclined to be wicked and those who aren't. It is clear that Belial directly influences sin by the mention of "abominable plots" and "guilty inclination" (4Q286:8,9). These are both mechanisms by which Belial advances his evil agenda that the Qumran have exposed and are calling upon God to protect them from. There is a deep sense of fear that Belial will "establish in their heart their evil devices" (4Q286:11,12). This sense of fear is the stimulus for this prayer in the first place. Without the worry and potential of falling victim to Belial's demonic sway, the Qumran people would never feel impelled to craft a curse. This very fact illuminates the power Belial was believed to hold over mortals, and the fact that sin proved to be a temptation that must stem from an impure origin.
In Jubilees 1:20, Belial's appearance continues to support the notion that sin is a direct product of his influence. Moreover, Belial's presence acts as a placeholder for all negative influences or those that would potentially interfere with God's will and a pious existence. Similarly to the "gentiles ... [who] cause them to sin against you" (Jubilees 1:19), Belial is associated with a force that drives one away from God. Coupled in this plea for protection against foreign rule, in this case the Egyptians, is a plea for protection from "the spirit of Belial" (Jubilees 1:19). Belial's tendency is to "ensnare [you] from every path of righteousness" (Jubilees 1:19). This phrase is intentionally vague, allowing room for interpretation. Everyone, in one way or another, finds themselves straying from the path of righteousness and by pawning this transgression off on Belial, he becomes a scapegoat for all misguidance, no matter what the cause. By associating Belial with all sorts of misfortune and negative external influence, the Qumran people are henceforth allowed to be let off for the sins they commit. 
Belial's presence is found throughout the War Scrolls, located in the Dead Sea Scrolls, and is established as the force occupying the opposite end of the spectrum of God. In Col. I, verse 1, the very first line of the document, it is stated that "the first attack of the Sons of Light shall be undertaken against the forces of the Sons of Darkness, the army of Belial" (1Q33;1:1). This dichotomy sheds light on the negative connotations that Belial held at the time. Where God and his Sons of Light are forces that protect and promote piety, Belial and his Sons of Darkness cater to the opposite, instilling the desire to sin and encouraging destruction. This opposition is only reinforced later in the document; it continues to read that the "holy ones" will "strike a blow at wickedness", ultimately resulting in the "annihilation of the Sons of Darkness" (1Q33:1:13). This epic battle between good and evil described in such abstract terms, however it is also applicable to everyday life and serves as a lens through which the Qumran see the world. Every day is the Sons of Light battle evil and call upon God to help them overcome evil in ways small and large.
Belial's influence is not taken lightly. In Col. XI, verse 8, the text depicts God conquering the "hordes of Belial" (1Q33;11:8). This defeat is indicative of God's power over Belial and his forces of temptation. However the fact that Belial is the leader of hordes is a testament to how persuasive he can be. If Belial was obviously an arbiter of wrongdoing and was blatantly in the wrong, he wouldn't be able to amass an army. This fact serves as a warning message, reasserting God's strength, while also making it extremely clear the breadth of Belial's prowess. Belial's "council is to condemn and convict", so the Qumran feel strongly that their people are not only aware of his purpose, but also equipped to combat his influence (1Q33;13:11).
In the Damascus Document, Belial also makes a prominent appearance, being established as a source of evil and an origin of several types of sin. In Column 4, the first mention of Belial reads: "Belial shall be unleashed against Israel" (4Q266). This phrase is able to be interpreted myriad different ways. Belial is characterized in a wild and uncontrollable fashion, making him seem more dangerous and unpredictable. The notion of being unleashed is such that once he is free to roam; he is unstoppable and able to carry out his agenda uninhibited. The passage then goes to enumerate the "three nets" (4Q266;4:16) by which Belial captures his prey and forces them to sin. "Fornication ..., riches ..., [and] the profanation of the temple" (4Q266;4:17,18) make up the three nets. These three temptations were three agents by which people were driven to sin, so subsequently, the Qumran people crafted the nets of Belial to rationalize why these specific temptations were so toxic. Later in Column 5, Belial is mentioned again as one of "the removers of bound who led Israel astray" (4Q266;5:20). This statement is a clear display of Belial's influence over man regarding sin. The passage goes on to state: "they preached rebellion against ... God" (4Q266;5:21,22). Belial's purpose is to undermine the teachings of God, and he achieves this by imparting his nets on humans, or the incentive to sin.
In the "War of the Sons of Light Against the Sons of Darkness", Belial controls scores of demons, which are specifically allotted to him by God for the purpose of performing evil. Belial, despite his malevolent disposition, is considered an angel.
Demonic entities in the Old Testament of the Christian Bible are of two classes: the "satyrs" or "shaggy goats" (from Hebr. "se'irim" "hairy beings", "he-goats" or "fauns"; , ) and the "demons" (from Hebr. "shedim" first translated as "daimonion", "daemon"; , ).
The term "demon" (from the Koine Greek δαιμόνιον "daimonion") appears 63 times in the New Testament of the Christian Bible, mostly if not all relating to occurrences of possession of individuals and exorcism by Jesus.
The King James Version kept it mistranslated as "devil" except one place in Acts 17:18 as 'gods' in the phrase "strange gods". The word "devil" by itself is the translation word for the Greek "diabolos" which occurs 38 times in the New Testament. The Tyndale Bible (1526 CE), a precursor of KJV, translated it all as "devyl", including Act 17:18 as "newe devyls".
Demons are sometimes included into biblical interpretation. In the story of Passover, the Bible tells the story as "the Lord struck down all the firstborn in Egypt" (Exodus 12:21–29). In the Book of Jubilees, which is considered canonical only by the Ethiopian Orthodox Church, this same event is told slightly differently: "All the powers of [the demon] Mastema had been let loose to slay all the first-born in the land of Egypt...And the powers of the Lord did everything according as the Lord commanded them" (Jubilees 49:2–4).
In the Genesis flood narrative the author explains how God was noticing "how corrupt the earth had become, for all the people on earth had corrupted their ways" (Genesis 6:12). In Jubilees the sins of man are attributed to "the unclean demons [who] began to lead astray the children of the sons of Noah, and to make to err and destroy them" (Jubilees 10:1). In Jubilees Mastema questions the loyalty of Abraham and tells God to "bid him offer him as a burnt offering on the altar, and Thou wilt see if he will do this command" (Jubilees 17:16). The discrepancy between the story in Jubilees and the story in Genesis 22 exists with the presence of Mastema. In Genesis, God tests the will of Abraham merely to determine whether he is a true follower, however; in Jubilees Mastema has an agenda behind promoting the sacrifice of Abraham's son, "an even more demonic act than that of the Satan in Job." In Jubilees, where Mastema, an angel tasked with the tempting of mortals into sin and iniquity, requests that God give him a tenth of the spirits of the children of the watchers, demons, in order to aid the process. These demons are passed into Mastema’s authority, where once again, an angel is in charge of demonic spirits.
The sources of demonic influence were thought to originate from the Watchers or Nephilim, who are first mentioned in Genesis 6 and are the focus of 1 Enoch Chapters 1–16, and also in Jubilees 10. The Nephilim were seen as the source of the sin and evil on earth because they are referenced in Genesis 6:4 before the story of the Flood. In Genesis 6:5, God sees evil in the hearts of men. The passage states, "the wickedness of humankind on earth was great", and that "Every inclination of the thoughts of their hearts was only continually evil" (Genesis 5). The mention of the Nephilim in the preceding sentence connects the spread of evil to the Nephilim. Enoch is a very similar story to Genesis 6:4–5, and provides further description of the story connecting the Nephilim to the corruption of humans. In Enoch, sin originates when angels descend from heaven and fornicate with women, birthing giants as tall as 300 cubits. The giants and the angels' departure of Heaven and mating with human women are also seen as the source of sorrow and sadness on Earth. The book of Enoch shows that these fallen angels can lead humans to sin through direct interaction or through providing forbidden knowledge. In Enoch, Semyaz leads the angels to mate with women. Angels mating with humans is against God's commands and is a cursed action, resulting in the wrath of God coming upon Earth. Azazel indirectly influences humans to sin by teaching them divine knowledge not meant for humans. Asael brings down the "stolen mysteries" (Enoch 16:3). Asael gives the humans weapons, which they use to kill each other. Humans are also taught other sinful actions such as beautification techniques, alchemy, astrology and how to make medicine (considered forbidden knowledge at the time). Demons originate from the evil spirits of the giants that are cursed by God to wander the earth. These spirits are stated in Enoch to "corrupt, fall, be excited, and fall upon the earth, and cause sorrow" (Enoch 15:11).
The Book of Jubilees conveys that sin occurs when Cainan accidentally transcribes astrological knowledge used by the Watchers (Jubilees 8). This differs from Enoch in that it does not place blame on the Angels. However, in Jubilees 10:4 the evil spirits of the Watchers are discussed as evil and still remain on earth to corrupt the humans. God binds only 90 percent of the Watchers and destroys them, leaving 10 percent to be ruled by Mastema. Because the evil in humans is great, only 10 percent would be needed to corrupt and lead humans astray. These spirits of the giants also referred to as "the bastards" in the Apotropaic prayer Songs of the Sage, which lists the names of demons the narrator hopes to expel.
In Christianity, demons are corrupted spirits carrying the execution of Satan's desires. They are generally regarded as three different types of spirits:
Often deities of other religions are interpreted or identified as such "demons" (from the Greek Old Testament δαιμόνιον "daimonion"). The evolution of the Christian Devil and pentagram are examples of early rituals and images that showcase evil qualities, as seen by the Christian churches.
Since Early Christianity, demonology has developed from a simple acceptance of demons to a complex study that has grown from the original ideas taken from Jewish demonology and Christian scriptures. Christian demonology is studied in depth within the Roman Catholic Church, although many other Christian churches affirm and discuss the existence of demons.
Building upon the few references to "daemons" in the New Testament, especially the poetry of the Book of Revelation, Christian writers of apocrypha from the 2nd century onwards created a more complicated tapestry of beliefs about "demons" that was largely independent of Christian scripture.
The contemporary Roman Catholic Church unequivocally teaches that angels and demons are real beings rather than just symbolic devices. The Catholic Church has a cadre of officially sanctioned exorcists which perform many exorcisms each year. The exorcists of the Catholic Church teach that demons attack humans continually but that afflicted persons can be effectively healed and protected either by the formal rite of exorcism, authorized to be performed only by bishops and those they designate, or by prayers of deliverance, which any Christian can offer for themselves or others.
At various times in Christian history, attempts have been made to classify demons according to various proposed demonic hierarchies.
In the Gospels, particularly the Gospel of Mark, Jesus cast out many demons from those afflicted with various ailments. He also lent this power to some of his disciples ().
Apuleius, by Augustine of Hippo, is ambiguous as to whether "daemons" had become "demonized" by the early 5th century:
He [Apulieus] also states that the blessed are called in Greek "eudaimones", because they are good souls, that is to say, good demons, confirming his opinion that the souls of men are demons.
Islam and Islam-related beliefs acknowledges the concept of evil spirits known as malevolent jinn, afarit and shayatin. Unlike the belief in angels, belief in demons is not obligated by the six articles of Islamic faith. However, the existence of several demonic spirits is generally assumed by Islamic theology and further elaborated beliefs persist in Islamic folklore. The Div, probably adapted under Zorastrian influences, became another prominent demonic creature in Islamic culture. Just like jinn, they are able to possess humans, but differ from jinn and shayatin in their physical strength thus also equated with Ogres or giants. They are in constant war with peri a benevolent spirit. Among Turks, the term "In" relating to demonic spirits, with characteristics comparable to jinn, are found and usually mentioned together. Nar as-samum ("fires of samum" or "poisonious fire") described in the Quran with hell, becomes associated with the minions of the Devil in tafsir.Furthermore the Quran mentions the "Zabaniyya", who torture the damned in hell, who may have originated from a class of Arabian demons. However, their execution of punishment is in accordance with God’s order, therefore they are not equalized with shayatin, that means they are not devils, who in turn are rebellious against the divine will.
Rather than demonic, jinn are depicted as similar to humans, as they live in societies and need dwelling places, food and water. Although their lifespan of multiple centuries exceeds those of humans, they still die and must procreate. As they are created from "smokeless fire," in contrast to humans made from "solid earth," the latter cannot see them. Similar to humans, jinn are subject to temptations of the shayatin and Satan. Therefore, they may either be good or evil. Evil jinn are comparable to demons, scaring or possessing humans. In folklore some Ghoul may also prey on lonely travelers to dissuade them from their paths and eat their corpses. Although not evil, a jinni may haunt a person, because it feels offended by him. Islam has no binding origin story of jinn, but Islamic beliefs commonly assume that the jinn were created on a Thursday thousands of years before mankind. Therefore, Islamic medieval narratives often called them "pre-Adamites". However, just like shayatin, jinn are held responsible for various diseases and possession. Both can be summoned and subjugated by magicians. Both are thought to lurk in dirty and desolated places.
Otherwise, the shayatin are the Islamic equivalent of "demons" in western usage.
Islam differs regarding the origin of demons. They may either be a class of heavenly creatures cast out of heaven or the descendants of Iblis. Unlike jinn and humans, shayatin are immortal and will meet their end when the world ceases to exist; however, prayers could dissolve or banish them. Unlike jinn and humans, shayatin can not attain salvation. If they attempt to reach heaven, they are chased away by angels and shooting stars. The shayatin usually do not possess people, but seduce them into committing falsehood and sin instead. This is done by whispering directly into humans' minds. These are called "waswās" and may enter the hearts of humans to amplify strong, negative emotions like depression or anger.
Another demonic spirit is called "ifrit" and although there are no descriptions regarding an iftrit's behavior found in Islamic canonical texts, Folk Islam often depicts them with traits of malevolent ghosts, returning after death or a subcategory of shayatin drawn the life-force of those who were murdered. Moreover, they are not exactly shayatin since they differ in their origin.
In the Bahá'í Faith, demons are not regarded as independent evil spirits as they are in some faiths. Rather, evil spirits described in various faiths' traditions, such as Satan, fallen angels, demons and jinn, are metaphors for the base character traits a human being may acquire and manifest when he turns away from God and follows his lower nature. Belief in the existence of ghosts and earthbound spirits is rejected and considered to be the product of superstition.
While some people fear demons, or attempt to exorcise them, others willfully attempt to summon them for knowledge, assistance, or power. The ceremonial magician usually consults a grimoire, which gives the names and abilities of demons as well as detailed instructions for conjuring and controlling them. Grimoires are not limited to demons – some give the names of angels or spirits which can be called, a process called theurgy. The use of ceremonial magic to call demons is also known as goetia, the name taken from a section in the famous grimoire known as the "Lesser Key of Solomon".
Hindu beliefs include numerous varieties of spirits such as Vetalas, Bhutas and Pishachas. Rakshasas and Asuras are often misunderstood to be demons.
"Asura", in the earliest hymns of the Rigveda, originally meant any supernatural spirit, either good or bad. Since the /s/ of the Indic linguistic branch is cognate with the /h/ of the Early Iranian languages, the word "Asura", representing a category of celestial beings. Ancient Hinduism tells that Devas (also called "suras") and Asuras are half-brothers, sons of the same father Kashyapa; although some of the Devas, such as Varuna, are also called Asuras. Later, during Puranic age, Asura and Rakshasa came to exclusively mean any of a race of anthropomorphic, powerful, possibly evil beings. Daitya (lit. sons of the mother "Diti"), Maya Danava, Rakshasa (lit. from "harm to be guarded against"), and Asura are incorrectly translated into English as "demon".
In post-Vedic Hindu scriptures, pious, highly enlightened Asuras, such as Prahlada and Vibhishana, are not uncommon. The Asura are not fundamentally against the gods, nor do they tempt humans to fall. Many people metaphorically interpret the Asura as manifestations of the ignoble passions in the human mind and as symbolic devices. There were also cases of power-hungry Asuras challenging various aspects of the gods, but only to be defeated eventually and seek forgiveness.
Hinduism advocates the reincarnation and transmigration of souls according to one's karma. Souls (Atman) of the dead are adjudged by the Yama and are accorded various purging punishments before being reborn. Humans that have committed extraordinary wrongs are condemned to roam as lonely, often mischief mongers, spirits for a length of time before being reborn. Many kinds of such spirits (Vetalas, Pishachas, Bhūta) are recognized in the later Hindu texts.
Evil spirits are the creation of the evil principle Ahriman in Zoroastrian cosmology, commonly referred to as Daeva. The first six archdemons are produced by Ahriman in direct opposition to the holy immortals created by Ahura Mazda the principle of good. This six archdemons (or seven if Ahriman is included) give existence to uncountable malevolent daeva; the Zorastrian demons. They are the embodiment of evil, causing moral imperfection, destroy, kill and torment the wicked souls in the afterlife. Some demons are related to specific vices. Humans in the state of such sin might be possessed by a corresponding demon:
In Manichaean mythology demons had a real existence, as they derived from the Kingdom of Darkness, they were not metaphors expressing the absence of good nor are they fallen angels, that means they are not originally good, but entities purely evil. The demons came into the world after the Prince of Darkness assaulted the Realm of Light. The demons ultimately failed their attack and ended up imprisoned in the structures and matter of the contemporary world. Lacking virtues and being in constant conflict with both the divine creatures and themselves, they are inferior to the divine entities and overcome by the divine beings at the end of time. They are not sophisticated or inventive creatures, but only driven by their urges.
Simultaneously, the Manichaean concept of demons remains abstract and is closely linked to ethical aspects of evil that many of them appear as personified evil qualities such as:
The Watcher, another group of demonic entities, known from the Enochian writings, appear in the canonical Book of Giants. The Watchers came into existence after the demons were chained up in the sky by Living Spirit. Later, outwitted by Third Messenger, they fall to earth, there they had intercourse with human women and beget the monstrous Nephilim. Thereupon they establish a tyrannical rule on earth, suppressing mankind, until they are defeated by the angels of punishment, setting an end to their rule.
The Algonquian people traditionally believe in a spirit called a wendigo. The spirit is believed to possess people who then become cannibals. In Athabaskan folklore, there is a belief in wechuge, a similar cannibal sprit. 
According to Rosemary Ellen Guiley, "Demons are not courted or worshipped in contemporary Wicca and Paganism. The existence of negative energies is acknowledged."
Psychologist Wilhelm Wundt remarked that "among the activities attributed by myths all over the world to demons, the harmful predominate, so that in popular belief bad demons are clearly older than good ones." Sigmund Freud developed this idea and claimed that the concept of demons was derived from the important relation of the living to the dead: "The fact that demons are always regarded as the spirits of those who have died "recently" shows better than anything the influence of mourning on the origin of the belief in demons."
M. Scott Peck, an American psychiatrist, wrote two books on the subject, "People of the Lie: The Hope For Healing Human Evil" and "Glimpses of the Devil: A Psychiatrist's Personal Accounts of Possession, Exorcism, and Redemption". Peck describes in some detail several cases involving his patients. In "People of the Lie" he provides identifying characteristics of an evil person, whom he classified as having a character disorder. In "Glimpses of the Devil" Peck goes into significant detail describing how he became interested in exorcism in order to debunk the "myth" of possession by evil spirits – only to be convinced otherwise after encountering two cases which did not fit into any category known to psychology or psychiatry. Peck came to the conclusion that possession was a rare phenomenon related to evil and that possessed people are not actually evil; rather, they are doing battle with the forces of evil.
Although Peck's earlier work was met with widespread popular acceptance, his work on the topics of evil and possession has generated significant debate and derision. Much was made of his association with (and admiration for) the controversial Malachi Martin, a Roman Catholic priest and a former Jesuit, despite the fact that Peck consistently called Martin a liar and a manipulator. Richard Woods, a Roman Catholic priest and theologian, has claimed that Dr. Peck misdiagnosed patients based upon a lack of knowledge regarding dissociative identity disorder (formerly known as multiple personality disorder) and had apparently transgressed the boundaries of professional ethics by attempting to persuade his patients into accepting Christianity. Father Woods admitted that he has never witnessed a genuine case of demonic possession in all his years.
According to S. N. Chiu, God is shown sending a demon against Saul in 1 Samuel 16 and 18 in order to punish him for the failure to follow God's instructions, showing God as having the power to use demons for his own purposes, putting the demon under his divine authority. According to the "Britannica Concise Encyclopedia", demons, despite being typically associated with evil, are often shown to be under divine control, and not acting of their own devices.

</doc>
<doc id="8286" url="https://en.wikipedia.org/wiki?curid=8286" title="Domino effect">
Domino effect

A domino effect or chain reaction is the cumulative effect produced when one event sets off a chain of similar events. The term is best known as a mechanical effect and is used as an analogy to a falling row of dominoes. It typically refers to a linked sequence of events where the time between successive events is relatively small. It can be used literally (an observed series of actual collisions) or metaphorically (causal linkages within systems such as global finance or politics). The term "domino effect" is used both to imply that an event is inevitable or highly likely (as it has already started to happen), and conversely to imply that an event is impossible or highly unlikely (the one domino left standing).
The domino effect can easily be visualized by placing a row of dominoes upright, each separated by a small distance. Upon pushing the first domino, the next domino in line will be knocked over, and so on, thus firing a linear chain in which each domino's fall is triggered by the domino immediately preceding it. The effect is the same regardless of the length of the chain. The energy used in this chain reaction is the potential energy of the dominoes due to them being in a meta-stable state; when the first domino is toppled, the energy transferred by the fall is greater than the energy needed to knock over the following domino, and so on.
The domino effect is exploited in Rube Goldberg machines.
Relevant physical theory:
Mathematical theory
Political theory
Social

</doc>
<doc id="8293" url="https://en.wikipedia.org/wiki?curid=8293" title="Diffusion pump">
Diffusion pump

Diffusion pumps use a high speed jet of vapor to direct gas molecules in the pump throat down into the bottom of the pump and out the exhaust. They were the first type of high vacuum pumps operating in the regime of free molecular flow, where the movement of the gas molecules can be better understood as diffusion than by conventional fluid dynamics. Invented in 1915 by Wolfgang Gaede, he named it a "diffusion pump" since his design was based on the finding that gas cannot diffuse against the vapor stream, but will be carried with it to the exhaust. However, the principle of operation might be more precisely described as gas-jet pump, since diffusion plays a role also in other high vacuum pumps. In modern textbooks, the diffusion pump is categorized as a momentum transfer pump.
The diffusion pump is widely used in both industrial and research applications. Most modern diffusion pumps use silicone oil or polyphenyl ethers as the working fluid.
In the late 19th century, most vacuums were creating using a Sprengel pump, which had the advantage of being very simple to operate, and capable of achieving quite good vacuum given enough time. Compared to later pumps, however, the pumping speed was very slow and the vapor pressure of the mercury limited the ultimate vacuum.
Following his invention of the molecular pump, the diffusion pump was invented in 1915 by Wolfgang Gaede, and originally used elemental mercury as the working fluid. After its invention, the design was quickly commercialized by Leybold.
It was then improved by Irving Langmuir and W. Crawford. Cecil Reginald Burch discovered the possibility of using silicone oil in 1928.
An oil diffusion pump is used to achieve higher vacuum (lower pressure) than is possible by use of positive displacement pumps alone. Although its use has been mainly associated within the high-vacuum range (down to 10 mbar), diffusion pumps today can produce pressures approaching 10 mbar when properly used with modern fluids and accessories. The features that make the diffusion pump attractive for high and ultra-high vacuum use are its high pumping speed for all gases and low cost per unit pumping speed when compared with other types of pump used in the same vacuum range. Diffusion pumps cannot discharge directly into the atmosphere, so a mechanical forepump is typically used to maintain an outlet pressure around 0.1 mbar.
The oil diffusion pump is operated with an oil of low vapor pressure. The high speed jet is generated by boiling the fluid and directing the vapor through a jet assembly. Note that the oil is gaseous when entering the nozzles. Within the nozzles, the flow changes from laminar to supersonic and molecular. Often, several jets are used in series to enhance the pumping action. The outside of the diffusion pump is cooled using either air flow, water lines or a water-filled jacket. As the vapor jet hits the outer cooled shell of the diffusion pump, the working fluid condenses and is recovered and directed back to the boiler. The pumped gases continue flowing to the base of the pump at increased pressure, flowing out through the diffusion pump outlet, where they are compressed to ambient pressure by the secondary mechanical forepump and exhausted.
Unlike turbomolecular pumps and cryopumps, diffusion pumps have no moving parts and as a result are quite durable and reliable. They can function over pressure ranges of 10 to 10 mbar. They are driven only by convection and thus have a very low energy efficiency.
One major disadvantage of diffusion pumps is the tendency to backstream oil into the vacuum chamber. This oil can contaminate surfaces inside the chamber or upon contact with hot filaments or electrical discharges may result in carbonaceous or siliceous deposits. Due to backstreaming, oil diffusion pumps are not suitable for use with highly sensitive analytical equipment or other applications which require an extremely clean vacuum environment, but mercury diffusion pumps may be in the case of ultra high vacuum chambers used for metal deposition. Often cold traps and baffles are used to minimize backstreaming, although this results in some loss of pumping speed.
The oil of a diffusion pump cannot be exposed to the atmosphere when hot. If this occurs, the oil will oxidise and has to be replaced, if a fire occurs the smoke and residue may contaminate other parts of the system.
The least expensive diffusion pump oils are based on hydrocarbons which have been purified by double-distillation. Compared with the other fluids, they have higher vapor pressure, so are usually limited to a pressure of 1 x 10 Torr. They are also the most likely to burn or explode if exposed to oxidizers.
The most common silicone oils used in diffusion pumps are trisiloxanes, which contain the chemical group Si-O-Si-O-Si, to which various phenyl groups or methyl groups are attached. These are available as the so called 702 and 703 blends, which were formerly manufactured by Dow Corning. These can be further separated into 704 and 705 oils, which are made up of the isomers of tetraphenyl tetramethyl trisiloxane and pentaphenyl trimethyl trisiloxane respectively.
For pumping reactive species, usually a polyphenyl ether based oil is used. These oils are the most chemical and heat resistant type of diffusion pump oil.
 
The steam ejector is a popular form of pump for vacuum distillation and freeze-drying. A jet of steam entrains the vapour that must be removed from the vacuum chamber. Steam ejectors can have single or multiple stages, with and without condensers in between the stages. While both steam ejectors and diffusion pumps use jets of vapor to entrain gas, they work on fundamentally different principles - steam ejectors rely on viscous flow and mixing to pump gas, whereas diffusion pumps use molecular diffusion. This has several consequences. In diffusion pumps, the inlet pressure can be much lower than the static pressure of jet, whereas in steam ejectors the two pressures are about the same. Also, diffusion pumps are capable of much higher compression ratios, and cannot discharge directly to atmosphere.

</doc>
<doc id="8299" url="https://en.wikipedia.org/wiki?curid=8299" title="Domenico Alberti">
Domenico Alberti

Domenico Alberti (c. 1710 – 14 October 1740 or 1746) was an Italian singer, harpsichordist, and composer.
Alberti was born in Venice and studied music with Antonio Lotti. He wrote operas, songs, and sonatas for keyboard instruments, for which he is best known today. These sonatas frequently employ arpeggiated accompaniment in the left hand in one of several patterns that are now collectively known as "Alberti bass". Alberti was one of the earliest composers to use these patterns, but was not the first or only. The most well-known of these patterns consists of regular broken chords, with the lowest note sounding first, then the highest, then the middle and then the highest again. This pattern is repeated. Today, Alberti is regarded as a minor composer, and his works are played or recorded only irregularly. The Alberti bass was used by many later composers, and it became an important element in much keyboard music of the Classical music era.
An example of Alberti bass (Mozart's "Piano Sonata, K 545"):
In his own lifetime, Alberti was known as a singer. He often used to accompany himself on the harpsichord. In 1736, he served as a page for Pietro Andrea Cappello, the Venetian ambassador to Spain. While at the Spanish court, the famous castrato singer Farinelli heard him sing. Farinelli was said to have been impressed, although Alberti was an amateur.
Alberti's best known pieces are his keyboard sonatas, although even they are very rarely performed. It is thought he wrote around 36 sonatas, of which 14 have survived. They all have two movements, each in binary form.
It is probable that Mozart's first violin sonatas, written at the age of seven, were modeled on Alberti's work.
Alberti died in 1740 or 1746 in Rome.

</doc>
<doc id="8300" url="https://en.wikipedia.org/wiki?curid=8300" title="Doris Day">
Doris Day

Doris Day (born Doris Mary Anne Kappelhoff; April 3, 1922 – May 13, 2019) was an American actress, singer, and animal welfare activist. She began her career as a big band singer in 1939, achieving commercial success in 1945 with two No. 1 recordings, "Sentimental Journey" and "My Dreams Are Getting Better All the Time" with Les Brown & His Band of Renown. She left Brown to embark on a solo career and recorded more than 650 songs from 1947 to 1967.
Day was one of the biggest film stars in the 1950s-1960s era. Day's film career began during the Golden Age of Hollywood with the film "Romance on the High Seas" (1948). She starred in films of many genres, including musicals, comedies, dramas, and thrillers. She played the title role in "Calamity Jane" (1953) and starred in Alfred Hitchcock's "The Man Who Knew Too Much" (1956) with James Stewart. Her best-known films are those in which she co-starred with Rock Hudson, chief among them 1959's "Pillow Talk", for which she was nominated for the Academy Award for Best Actress. She also worked with James Garner on both "Move Over, Darling" (1963) and "The Thrill of It All" (1963), and starred alongside Clark Gable, Cary Grant, James Cagney, David Niven, Ginger Rogers, Jack Lemmon, Frank Sinatra, Kirk Douglas, Lauren Bacall, and Rod Taylor in various movies. After ending her film career in 1968, only briefly removed from the height of her popularity, she starred in her own sitcom "The Doris Day Show" (1968–1973).
In 1989, she was awarded the Golden Globe Cecil B. DeMille Award for lifetime achievement in motion pictures. In 2004, she was awarded the Presidential Medal of Freedom. In 2008, she received the Grammy Lifetime Achievement Award as well as a Legend Award from the Society of Singers. In 2011 she was awarded the Los Angeles Film Critics Association's Career Achievement Award. Also in 2011, she released her 29th studio album "My Heart" which contained new material and became a UK Top 10 album. As of 2020, she was one of eight record performers to have been the top box-office earner in the United States four times.
Day was born Doris Mary Anne Kappelhoff on April 3, 1922 in Cincinnati, Ohio, the daughter of Alma Sophia ("née" Welz; 1895–1976) and William Joseph Kappelhoff (1892–1967). Her mother was a homemaker, and her father was a music teacher and choirmaster. Her maternal and paternal grandparents were German; her paternal grandfather Franz Joseph Wilhelm Kappelhoff immigrated to the United States in 1875 and settled in Cincinnati which had a large German community with its own churches, clubs, and German-language newspapers. For most of her life, Day stated she was born in 1924; it was not until her 95th birthday – when the Associated Press found her birth certificate, showing a 1922 date of birth – that she stated otherwise. It was common among actresses in Hollywood to state an age younger than they actually were in reality because youth was everything when it came to casting.
The youngest of three siblings, she had two older brothers: Richard (who died before her birth) and Paul, two to three years older. Due to her father's alleged infidelity, her parents separated. She developed an early interest in dance, and in the mid-1930s formed a dance duo with Jerry Doherty that performed locally in Cincinnati. A car accident on October 13, 1937 injured her right leg and curtailed her prospects as a professional dancer.
While recovering from her car accident, Kappelhoff started to sing along with the radio and discovered a talent she did not know she had. "During this long, boring period, I used to while away a lot of time listening to the radio, sometimes singing along with the likes of Benny Goodman, Duke Ellington, Tommy Dorsey, and Glenn Miller", she told A. E. Hotchner, one of Day's biographers. "But the one radio voice I listened to above others belonged to Ella Fitzgerald. There was a quality to her voice that fascinated me, and I'd sing along with her, trying to catch the subtle ways she shaded her voice, the casual yet clean way she sang the words."
Observing her daughter sing rekindled Alma's interest in show business, and she decided Doris must have singing lessons. She engaged a teacher, Grace Raine. After three lessons, Raine told Alma that young Doris had "tremendous potential"; Raine was so impressed that she gave Doris three lessons a week for the price of one. Years later, Day said that Raine had the biggest effect on her singing style and career.
During the eight months she was taking singing lessons, Kappelhoff had her first professional jobs as a vocalist, on the WLW radio program "Carlin's Carnival", and in a local restaurant, Charlie Yee's Shanghai Inn. During her radio performances, she first caught the attention of Barney Rapp, who was looking for a female vocalist and asked if she would like to audition for the job. According to Rapp, he had auditioned about 200 singers when Kappelhoff got the job.
While working for Rapp in 1939, she adopted the stage surname "Day", at Rapp's suggestion. Rapp felt that "Kappelhoff" was too long for marquees, and he admired her rendition of the song "Day After Day". After working with Rapp, Day worked with bandleaders Jimmy James, Bob Crosby, and Les Brown. In 1941, Day appeared as a singer in three Soundies with the Les Brown band.
While working with Brown, Day recorded her first hit recording, "Sentimental Journey", released in early 1945. It soon became an anthem of the desire of World War II demobilizing troops to return home. The song continues to be associated with Day, and she re-recorded it on several occasions, including a version in her 1971 television special. During 1945–46, Day (as vocalist with the Les Brown Band) had six other top ten hits on the "Billboard" chart: "My Dreams Are Getting Better All the Time", Tain't Me", "Till The End of Time", "You Won't Be Satisfied (Until You Break My Heart)", "The Whole World is Singing My Song", and "I Got the Sun in the Mornin. Les Brown said, "As a singer Doris belongs in the company of Bing Crosby and Frank Sinatra." (Aljean Harmetz (2019). "Wholesome Box-Office Star and Golden Voice of 'Que Sera, Sera' ". "The New York Times." p. 1 )
While singing with the Les Brown band and for nearly two years on Bob Hope's weekly radio program, she toured extensively across the United States.
Her performance of the song "Embraceable You" impressed songwriter Jule Styne and his partner, Sammy Cahn, and they recommended her for a role in "Romance on the High Seas" (1948). Day was cast for the role after auditioning for director Michael Curtiz. She was shocked at being offered the role in the film, and admitted to Curtiz that she was a singer without acting experience. But he said he liked that "she was honest", not afraid to admit it, and he wanted someone who "looked like the All-American Girl". Day was the discovery of which Curtiz was proudest during his career.
The film provided her with a hit recording as a soloist, "It's Magic", which followed by two months her first hit ("Love Somebody" in 1948) recorded as a duet with Buddy Clark. Day recorded "Someone Like You", before the film "My Dream Is Yours" (1949), which featured the song. In 1950, U.S. servicemen in Korea voted her their favorite star.
She continued to make minor and frequently nostalgic period musicals such as "On Moonlight Bay" (1951), "By the Light of the Silvery Moon" (1953), and "Tea For Two" (1950) for Warner Brothers.
Her most commercially successful film for Warner was "I'll See You in My Dreams" (1951), which broke box-office records of 20 years. The film is a musical biography of lyricist Gus Kahn. It was Day's fourth film directed by Curtiz. Day appeared as the title character in the comedic western-themed musical, "Calamity Jane" (1953). A song from the film, "Secret Love", won the Academy Award for Best Original Song and became Day's fourth No. 1 hit single in the United States.
Between 1950 and 1953, the albums from six of her movie musicals charted in the Top 10, three of them at No. 1. After filming "Lucky Me" (1954) with Bob Cummings and "Young at Heart" (1955) with Frank Sinatra, Day chose not to renew her contract with Warner Brothers.
During this period, Day also had her own radio program, "The Doris Day Show". It was broadcast on CBS in 1952–1953.
Having become primarily recognized as a musical-comedy actress, Day gradually took on more dramatic roles to broaden her range. Her dramatic star turn as singer Ruth Etting in "Love Me or Leave Me" (1955), with top billing above James Cagney, received critical and commercial success, becoming Day's biggest hit thus far. Cagney said she had "the ability to project the simple, direct statement of a simple, direct idea without cluttering it", comparing her to Laurette Taylor's Broadway performance in "The Glass Menagerie" (1945), one of the greatest performances by an American actor. Day said it was her best film performance. Producer Joe Pasternak said, "I was stunned that Doris did not get an Oscar nomination." The soundtrack album from that movie was a No. 1 hit.
Day starred in Alfred Hitchcock's suspense film "The Man Who Knew Too Much" (1956) with James Stewart. She sang two songs in the film, "Que Sera, Sera (Whatever Will Be, Will Be)" which won an Academy Award for Best Original Song, and "We'll Love Again". The film was Day's 10th movie to be in the Top 10 at the box office. Day played the title role in the thriller/noir "Julie" (also 1956) with Louis Jourdan.
After three successive dramatic films, Day returned to her musical/comedic roots in "The Pajama Game" (1957) with John Raitt. The film was based on the Broadway play of the same name. She worked with Paramount Pictures for the comedy "Teacher's Pet" (1958), alongside Clark Gable and Gig Young. She co-starred with Richard Widmark and Gig Young in the romantic comedy film "The Tunnel of Love" (also 1958), but found scant success opposite Jack Lemmon in "It Happened to Jane" (1959).
"Billboard" annual nationwide poll of disc jockeys had ranked Day as the No. 1 female vocalist nine times in ten years (1949 through 1958), but her success and popularity as a singer was now being overshadowed by her box-office appeal.
In 1959, Day entered her most successful phase as a film actress with a series of romantic comedies. This success began with "Pillow Talk" (1959), co-starring Rock Hudson who became a lifelong friend, and Tony Randall. Day received a nomination for an Academy Award for Best Actress. It was the only Oscar nomination she received in her career. Day, Hudson, and Randall made two more films together, "Lover Come Back" (1961) and "Send Me No Flowers" (1964).
Along with David Niven and Janis Paige, Day starred in "Please Don't Eat the Daisies" (1960) and with Cary Grant in the comedy "That Touch of Mink" (1962). During 1960 and the 1962 to 1964 period, she ranked number one at the box office, the second woman to be number one four times, an accomplishment equalled by no other actress except Shirley Temple. She set a record that has yet to be equaled, receiving seven consecutive Laurel Awards as the top female box office star.
Day teamed up with James Garner starting with "The Thrill of It All", followed by "Move Over, Darling" (both 1963). The film's theme song, "Move Over Darling", co-written by her son, reached in the UK. In between these comedic roles, Day co-starred with Rex Harrison in the movie thriller "Midnight Lace" (1960), an updating of the stage thriller "Gaslight".
By the late 1960s, the sexual revolution of the baby boomer generation had refocused public attitudes about sex. Times changed, but Day's films did not. Day's next film "Do Not Disturb" (1965) was popular with audiences, but her popularity soon waned. Critics and comics dubbed Day "The World's Oldest Virgin", and audiences began to shy away from her films. As a result, she slipped from the list of top box-office stars, last appearing in the top ten with the hit film "The Glass Bottom Boat" (1966). One of the roles she turned down was that of Mrs. Robinson in "The Graduate", a role that eventually went to Anne Bancroft. In her published memoirs, Day said she had rejected the part on moral grounds: she found the script "vulgar and offensive".
She starred in the western film "The Ballad of Josie" (1967). That same year, Day recorded "The Love Album", although it was not released until 1994. The following year (1968), she starred in the comedy film "Where Were You When the Lights Went Out?" which centers on the Northeast blackout of November 9, 1965. Her final feature, the comedy "With Six You Get Eggroll", was released in 1968.
From 1959 to 1970, Day received nine Laurel Award nominations (and won four times) for best female performance in eight comedies and one drama. From 1959 through 1969, she received six Golden Globe nominations for best female performance in three comedies, one drama ("Midnight Lace"), one musical ("Jumbo"), and her television series.
After her third husband Martin Melcher died on April 20, 1968, a shocked Day discovered that Melcher and his business partner and "adviser" Jerome Bernard Rosenthal had squandered her earnings, leaving her deeply in debt. Rosenthal had been her attorney since 1949, when he represented her in her uncontested divorce action against her second husband, saxophonist George W. Weidler. Day filed suit against Rosenthal in February 1969, won a successful decision in 1974, but did not receive compensation until a settlement in 1979.
Day also learned to her displeasure that Melcher had committed her to a television series, which became "The Doris Day Show".
Day hated the idea of performing on television, but felt obligated to do it. The first episode of "The Doris Day Show" aired on September 24, 1968, and, from 1968 to 1973, employed "Que Sera, Sera" as its theme song. Day persevered (she needed the work to help pay off her debts), but only after CBS ceded creative control to her and her son. The successful show enjoyed a five-year run, and functioned as a curtain raiser for the "Carol Burnett Show". It is remembered today for its abrupt season-to-season changes in casting and premise.
By the end of its run in 1973, public tastes had changed, as had those of the television industry, and her firmly established persona was regarded as passé. She largely retired from acting after "The Doris Day Show", but did complete two television specials, "The Doris Mary Anne Kappelhoff Special" (1971) and "Doris Day Today" (1975), and was a guest on various shows in the 1970s.
In the 1985–86 season, Day hosted her own television talk show, "Doris Day's Best Friends", on the Christian Broadcasting Network (CBN). The network canceled the show after 26 episodes, despite the worldwide publicity it received. Much of that attention came from the episode featuring Rock Hudson, in which Hudson was showing the first public symptoms of AIDS including severe weight loss and admitted fatigue; Hudson would die from the disease a year later. Day later said, "He was very sick. But I just brushed that off and I came out and put my arms around him and said, 'Am I glad to see you'."
Day's husband and agent, Martin Melcher, had Beverly Hills lawyer Jerome Rosenthal handle his wife's money since the 1940s. "During that period, Rosenthal committed breaches of professional ethics that are difficult to exaggerate", as one court put it.
In October 1985, the California Supreme Court rejected Rosenthal's appeal of the multimillion-dollar judgment against him for legal malpractice, and upheld conclusions of a trial court and a Court of Appeal that Rosenthal acted improperly. In April 1986, the U.S. Supreme Court refused to review the lower court's judgment. In June 1987, Rosenthal filed a $30 million lawsuit against lawyers he claimed cheated him out of millions of dollars in real estate investments. He named Day as a co-defendant, describing her as an "unwilling, involuntary plaintiff whose consent cannot be obtained". Rosenthal claimed that millions of dollars Day lost were in real estate sold after Melcher died in 1968, in which Rosenthal asserted that the attorneys gave Day bad advice, telling her to sell, at a loss, three hotels, in Palo Alto, California, Dallas, Texas, and Atlanta, Georgia, plus some oil leases in Kentucky and Ohio. He claimed he had made the investments under a long-term plan, and did not intend to sell them until they appreciated in value. Two of the hotels sold in 1970 for about $7 million, and their estimated worth in 1986 was $50 million.
Terry Melcher stated that his adoptive father's premature death saved Day from financial ruin. It remains unresolved whether Martin Melcher had himself also been duped. Day stated publicly that she believed her husband innocent of any deliberate wrongdoing, stating that he "simply trusted the wrong person". According to Day's autobiography, as told to A. E. Hotchner, the usually athletic and healthy Martin Melcher had an enlarged heart. Most of the interviews on the subject given to Hotchner (and included in Day's autobiography) paint an unflattering portrait of Melcher. Author David Kaufman asserts that one of Day's costars, actor Louis Jourdan, maintained that Day herself disliked her husband, but Day's public statements regarding Melcher appear to contradict that assertion.
Day was scheduled to present, along with Patrick Swayze and Marvin Hamlisch, the Best Original Score Oscar at the 61st Academy Awards in March 1989 but she suffered a deep leg cut and was unable to attend. She had been walking through the gardens of her hotel when she cut her leg on a sprinkler. The cut required stitches.
Day was inducted into the Ohio Women's Hall of Fame in 1981 and received the Cecil B. DeMille Award for career achievement in 1989. In 1994, Day's "Greatest Hits" album became another entry into the British charts. Her cover of "Perhaps, Perhaps, Perhaps" was included in the soundtrack of the Australian film "Strictly Ballroom."
Day participated in interviews and celebrations of her birthday with an annual Doris Day music marathon. In July 2008, she appeared on the Southern California radio show of longtime friend and newscaster George Putnam.
Day turned down a tribute offer from the American Film Institute and from the Kennedy Center Honors because they require attendance in person. In 2004, she was awarded the Presidential Medal of Freedom by President George W. Bush for her achievements in the entertainment industry and for her work on behalf of animals. President Bush stated:
Columnist Liz Smith and film critic Rex Reed mounted vigorous campaigns to gather support for an Honorary Academy Award for Day to herald her film career and her status as the top female box-office star of all time. According to "The Hollywood Reporter" in 2015, the Academy offered her the Honorary Oscar multiple times, but she declined as she saw the film industry as a part of her past life. Day received a Grammy for Lifetime Achievement in Music in 2008, albeit again in absentia.
She received three Grammy Hall of Fame Awards, in 1998, 1999 and 2012, for her recordings of "Sentimental Journey", "Secret Love", and "Que Sera, Sera", respectively. Day was inducted into the Hit Parade Hall of Fame in 2007, and in 2010 received the first Legend Award ever presented by the Society of Singers.
Day, aged 89, released "My Heart" in the United Kingdom on September 5, 2011, her first new album in nearly two decades since the release of "The Love Album", which, although recorded in 1967, was not released until 1994. The album is a compilation of previously unreleased recordings produced by Day's son, Terry Melcher, before his death in 2004. Tracks include the 1970s Joe Cocker hit "You Are So Beautiful", the Beach Boys' "Disney Girls" and jazz standards such as "My Buddy", which Day originally sang in the film "I'll See You in My Dreams" (1951).
After the disc was released in the United States it soon climbed to No. 12 on Amazon's bestseller list, and helped raise funds for the Doris Day Animal League. Day became the oldest artist to score a UK Top 10 with an album featuring new material.
In January 2012, the Los Angeles Film Critics Association presented Day with a Lifetime Achievement Award.
In April 2014, Day made an unexpected public appearance to attend the annual Doris Day Animal Foundation benefit. The benefit raises money for her Animal Foundation.
Clint Eastwood offered Day a role in a film he was planning to direct in 2015. Although she reportedly was in talks with Eastwood, her neighbor in Carmel, about a role in the film, she eventually declined.
Day granted ABC a telephone interview on her birthday in 2016, which was accompanied by photos of her life and career.
In a rare interview with "The Hollywood Reporter" on April 4, 2019, the day after her 97th birthday, Day talked about her work on the Doris Day Animal Foundation, founded in 1978. On the question of what her favorite film was, she answered "Calamity Jane": "I was such a tomboy growing up, and she was such a fun character to play. Of course, the music was wonderful, too—'Secret Love,' especially, is such a beautiful song."
To commemorate her birthday, her fans gathered each year to take part in a three-day party in her hometown of Carmel, California, in late March. The event was also a fundraiser for her Animal Foundation. During the 2019 event, there was a special screening of her film "Pillow Talk" (1959) to celebrate its 60th anniversary. About the film, Day stated in the same interview that she "had such fun working with my pal, Rock. We laughed our way through three films we made together and remained great friends. I miss him."
Day's interest in animal welfare and related issues apparently dated to her teen years. While recovering from an automobile accident, she took her dog Tiny for a walk without a leash. Tiny ran into the street and was killed by a passing car. Day later expressed guilt and loneliness about Tiny's untimely death. In 1971, she co-founded Actors and Others for Animals, and appeared in a series of newspaper advertisements denouncing the wearing of fur, alongside Mary Tyler Moore, Angie Dickinson, and Jayne Meadows.
In 1978, Day founded the Doris Day Pet Foundation, now the Doris Day Animal Foundation (DDAF). A non-profit 501(c)(3) grant-giving public charity, DDAF funds other non-profit causes throughout the US that share DDAF's mission of helping animals and the people who love them. The DDAF continues to operate independently.
To complement the Doris Day Animal Foundation, Day formed the Doris Day Animal League (DDAL) in 1987, a national non-profit citizens' lobbying organization whose mission is to reduce pain and suffering, and protect animals through legislative initiatives. Day actively lobbied the United States Congress in support of legislation designed to safeguard animal welfare on a number of occasions, and in 1995 she originated the annual Spay Day USA. The DDAL merged into The Humane Society of the United States (HSUS) in 2006. The HSUS now manages World Spay Day, the annual one-day spay/neuter event that Day originated.
A facility bearing her name, the Doris Day Horse Rescue and Adoption Center, which helps abused and neglected horses, opened in 2011 in Murchison, Texas, on the grounds of an animal sanctuary started by her late friend, author Cleveland Amory. Day contributed $250,000 toward the founding of the center.
A posthumous auction of 1,100 of Day's possessions in April 2020 generated $3 million for the Doris Day Animal Foundation.
After her retirement from films, Day lived in Carmel-by-the-Sea, California. She had many pets and adopted stray animals. She was a lifelong Republican. Her only child was music producer and songwriter Terry Melcher, who had a hit in the 1960s with "Hey Little Cobra" under the name The Rip Chords. before becoming a successful producer whose acts included The Byrds, Paul Revere & the Raiders, and---in the late 1980s---The Beach Boys; he died of melanoma in November 2004. Since the 1980s Day owned a hotel in Carmel-by-the-Sea called the Cypress Inn which she originally co-owned with her son. It was an early pet–friendly hotel and was featured in "Architectural Digest" in 1999.
Day was married four times. From March 1941 to February 1943, she was married to trombonist Al Jorden (1917-1967), a violent schizophrenic who later took his own life, whom she met in Barney Rapp's Band. Day became pregnant, and when she refused to have an abortion, he beat her in an attempt to force a miscarriage. Their son, Terrence "Terry" Paul Jorden, was born in 1942, and changed his name to Terrence Paul Melcher when he was adopted by Day's third husband. 
Her second marriage was to George William Weidler (1926-1989) from March 30, 1946, to May 31, 1949, a saxophonist and the brother of actress Virginia Weidler. Weidler and Day met again several years later during a brief reconciliation, and he introduced her to Christian Science.
Day married American film producer Martin Melcher (1915-1968) on April 3, 1951, her 29th birthday, and this marriage lasted until he died in April 1968. Melcher adopted Day's son Terry, who became a successful musician and record producer under the name Terry Melcher. Martin Melcher produced many of Day's movies. They were both Christian Scientists, resulting in her not seeing a doctor for some time for symptoms which suggested cancer.
Day's fourth marriage was to Barry Comden (1935–2009) from April 14, 1976, until April 2, 1982. He was the "maître d'hôtel" at one of Day's favorite restaurants. He knew of her great love of dogs and endeared himself to her by giving her a bag of meat scraps and bones on her way out of the restaurant. He later complained that she cared more for her "animal friends" than she did for him.
Day died on May 13, 2019, at the age of 97, after having contracted pneumonia. Her death was announced by her charity, the Doris Day Animal Foundation. Per Day's requests, the Foundation announced that there would be no funeral services, grave marker, or other public memorials.
"Source"

</doc>
<doc id="8301" url="https://en.wikipedia.org/wiki?curid=8301" title="Distillation">
Distillation

Distillation is the process of separating the components or substances from a liquid mixture by using selective boiling and condensation. Distillation may result in essentially complete separation (nearly pure components), or it may be a partial separation that increases the concentration of selected components in the mixture. In either case, the process exploits differences in the relative volatility of the mixture's components. In industrial chemistry, distillation is a unit operation of practically universal importance, but it is a physical separation process, not a chemical reaction.
Distillation has many applications. For example:
An installation used for distillation, especially of distilled beverages, is a distillery. The distillation equipment itself is a still.
Early evidence of distillation was found on Akkadian tablets dated c. 1200 BCE describing perfumery operations. The tablets provided textual evidence that an early primitive form of distillation was known to the Babylonians of ancient Mesopotamia. Early evidence of distillation was also found related to alchemists working in Alexandria in Roman Egypt in the 1st century CE. 
Distilled water has been in use since at least c. 200 CE, when Alexander of Aphrodisias described the process. Work on distilling other liquids continued in early Byzantine Egypt under Zosimus of Panopolis in the 3rd century. Distillation was practiced in the ancient Indian subcontinent, which is evident from baked clay retorts and receivers found at Taxila, Shaikhan Dheri, and Charsadda in modern Pakistan, dating to the early centuries of the Common Era. These "Gandhara stills" were only capable of producing very weak liquor, as there was no efficient means of collecting the vapors at low heat. 
Distillation in China may have begun during the Eastern Han dynasty (1st–2nd centuries CE), but the distillation of beverages began in the Jin (12th–13th centuries) and Southern Song (10th–13th centuries) dynasties, according to archaeological evidence.
Clear evidence of the distillation of alcohol comes from the Arab chemist Al-Kindi in 9th-century Iraq, where it was described by the School of Salerno in the 12th century. Fractional distillation was developed by Tadeo Alderotti in the 13th century. A still was found in an archaeological site in Qinglong, Hebei province, in China, dating back to the 12th century. Distilled beverages were common during the Yuan dynasty (13th–14th centuries).
In 1500, German alchemist Hieronymus Braunschweig published "Liber de arte destillandi" ("The Book of the Art of Distillation"), the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. In 1651, John French published "The Art of Distillation", the first major English compendium on the practice, but it has been claimed that much of it derives from Braunschweig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.
As alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle to act as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour. These alembics often featured a cooling system around the beak, using cold water, for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols, such as cognac, Scotch whisky, Irish whiskey, tequila, and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for use in the domestic production of flower water or essential oils.
Early forms of distillation involved batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists reportedly carried out as many as 500 to 600 distillations in order to obtain a pure compound.
In the early 19th century, the basics of modern techniques, including pre-heating and reflux, were developed. In 1822, Anthony Perrier developed one of the first continuous stills, and then, in 1826, Robert Stein improved that design to make his patent still. In 1830, Aeneas Coffey got a patent for improving the design even further. Coffey's continuous still may be regarded as the archetype of modern petrochemical units. The French engineer Armand Savalle developed his steam regulator around 1846. In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation, and the same and subsequent years saw developments in this theme for oils and spirits.
With the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods, such as the McCabe–Thiele method by Ernest Thiele and the Fenske equation. The first industrial plant in the United States to use distillation as a means of ocean desalination opened in Freeport, Texas in 1961 with the hope of bringing water security to the region.
The availability of powerful computers has allowed direct computer simulations of distillation columns.
The application of distillation can roughly be divided into four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate in the processing of beverages and herbs.
The main difference between laboratory scale distillation and industrial distillation are that laboratory scale distillation is often performed on a batch basis, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds, and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions, which are collected sequentially from most volatile to less volatile, with the bottoms – remaining least or non-volatile fraction – removed at the end. The still can then be recharged and the process repeated.
In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a more detailed control of the separation process.
The boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.
It is a misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure, allowing the vapors of each component to collect separately and purely. However, this does not occur, even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law and assume that vapor–liquid equilibria are attained.
Raoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up, a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.
Dalton's law states that the total pressure is the sum of the partial pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. A mixture with a given composition has one boiling point at a given pressure when the components are mutually soluble. A mixture of constant composition does not have multiple boiling points.
An implication of one boiling point is that lighter components never cleanly "boil first". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and, thus, are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily vaporize also, albeit at a lower concentration in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch vaporizes, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures. Therefore, starting from a given mixture, it appears to have a boiling range instead of a boiling point, although this is because its composition changes: each intermediate mixture has its own, singular boiling point.
The idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is when the vapor phase and liquid phase contain the same composition. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.
It is not possible to completely purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is vaporized and the other component, e.g., a salt, has zero partial pressure for practical purposes, the process is simpler.
Heating an ideal mixture of two volatile substances, A and B, with A having the higher volatility, or lower boiling point, in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid that contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid. The ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This, in turn, means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than in the starting liquid).
The result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio of A : B in the distillate.
If the difference in vapour pressure between the two components A and B is large – generally expressed as the difference in boiling points – the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.
Continuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.
Continuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.
Both batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.
There are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include
Laboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a "still", consists at a minimum of a reboiler or "pot" in which the source material is heated, a condenser in which the heated vapor is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also ).
A completely sealed distillation apparatus could experience extreme and rapidly varying internal pressure, which could cause it to burst open at the joints. Therefore, some path is usually left open (for instance, at the receiving flask) to allow the internal pressure to equalize with atmospheric pressure. Alternatively, a vacuum pump may be used to keep the apparatus at a lower than atmospheric pressure. If the substances involved are air- or moisture-sensitive, the connection to the atmosphere can be made through one or more drying tubes packed with materials that scavenge the undesired air components, or through bubblers that provide a movable liquid barrier. Finally, the entry of undesired air components can be prevented by pumping a low but steady flow of suitable inert gas, like nitrogen, into the apparatus.
In simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.
As a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C) or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.
A cutaway schematic of a simple distillation operation is shown at left. The starting liquid 15 in the boiling flask 2 is heated by a combined hotplate and magnetic stirrer 13 via a silicone oil bath (orange, 14). The vapor flows through a short Vigreux column 3, then through a Liebig condenser 5, is cooled by water (blue) that circulates through ports 6 and 7. The condensed liquid drips into the receiving flask 8, sitting in a cooling bath (blue, 16). The adapter 10 has a connection 9 that may be fitted to a vacuum pump. The components are connected by ground glass joints (gray).
For many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used in order to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.
As the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors are determined once again by Raoult's law. Each vaporization-condensation cycle (called a "theoretical plate") will yield a purer solution of the more volatile component. In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; "theoretical plate" is thus a concept rather than an accurate description.
More theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of Teflon or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.
Like vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive. The temperature of the steam is easier to control than the surface of a heating element, and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.
Steam distillation of various aromatic herbs and flowers can result in two products; an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.
Some compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.
This technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.
Molecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e. the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.
Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a "cow" or "pig" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.
The Perkin triangle, has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.
Short path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure. A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr is a kind of a short path distillation apparatus which often contain multiple chambers to collect distillate fractions.
Zone distillation is a distillation process in long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with the number of iterations.
Zone distillation is the distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization – with the replacement of the distribution co-efficient k of crystallization - for the separation factor α of distillation.
The unit process of evaporation may also be called "distillation":
Other uses:
Interactions between the components of the solution create properties unique to the solution, as most processes entail nonideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not effect separation. For example, ethyl alcohol and water form an azeotrope of 95.6% at 78.1 °C.
If the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a pure distillate. This set of techniques are known as azeotropic distillation. Some techniques achieve this by "jumping" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent (or desiccant, such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.
Immiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.
High boiling azeotropes, such as a 20 weight percent mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.
To break azeotropic distillations and cross distillation boundaries, such as in the DeRosier Problem, it is necessary to increase the composition of the light key in the distillate.
The boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it's possible to bias the boiling point of one component away from the other by exploiting the differing vapor pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to be remain identical further along the pressure axis either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.
This method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.
Under negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapors being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.
Alternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.
A unidirectional distillation will rely on a pressure change in one direction, either positive or negative.
Pressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.
This improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.
One example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.
Large scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.
To control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.
Industrial distillation is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about 65 centimeters to 16 meters and heights ranging from about 6 meters to 90 meters or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different "fractions" or products having different boiling points or boiling ranges. The "lightest" products (those with the lowest boiling point) exit from the top of the columns and the "heaviest" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.
Industrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.
Such industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.
Design and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method or the Fenske equation can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as "plates" or "trays") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.
In modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.
This packing material can either be random dumped packing (1–3" wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of "theoretical stages" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both of these factors affect packing performance.
Another factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references. Considerable work has been done on this topic by Fractionation Research, Inc. (commonly known as FRI).
The goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m of water recovered figure, and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m.
There are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.
Carbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.

</doc>
<doc id="8302" url="https://en.wikipedia.org/wiki?curid=8302" title="David Hilbert">
David Hilbert

David Hilbert (; ; 23 January 1862 – 14 February 1943) was a German mathematician and one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory, the calculus of variations, commutative algebra, algebraic number theory, the foundations of geometry, spectral theory of operators and its application to integral equations, mathematical physics, and foundations of mathematics (particularly proof theory).
Hilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.
Hilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic.
Hilbert, the first of two children and only son of Otto and Maria Therese (Erdtmann) Hilbert, was born in the Province of Prussia, Kingdom of Prussia, either in Königsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near Königsberg where his father worked at the time of his birth.
In late 1872, Hilbert entered the Friedrichskolleg Gymnasium ("Collegium fridericianum", the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (late 1879) and graduated from (early 1880) the more science-oriented Wilhelm Gymnasium. Upon graduation, in autumn 1880, Hilbert enrolled at the University of Königsberg, the "Albertina". In early 1882, Hermann Minkowski (two years younger than Hilbert and also a native of Königsberg but had gone to Berlin for three semesters), returned to Königsberg and entered the university. Hilbert developed a lifelong friendship with the shy, gifted Minkowski.
In 1884, Adolf Hurwitz arrived from Göttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann, titled "Über invariante Eigenschaften spezieller binärer Formen, insbesondere der Kugelfunktionen" ("On the invariant properties of special binary forms, in particular the spherical harmonic functions").
Hilbert remained at the University of Königsberg as a "Privatdozent" (senior lecturer) from 1886 to 1895. In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of Göttingen. During the Klein and Hilbert years, Göttingen became the preeminent institution in the mathematical world. He remained there for the rest of his life.
Among Hilbert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of Göttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.
Among his 69 Ph.D. students in Göttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925). Between 1902 and 1939 Hilbert was editor of the "Mathematische Annalen", the leading mathematical journal of the time.
Around 1925, Hilbert developed pernicious anemia, a then-untreatable vitamin deficiency whose primary symptom is exhaustion; his assistant Eugene Wigner described him as subject to "enormous fatigue" and how he "seemed quite old", and that even after eventually being diagnosed and treated, he "was hardly a scientist after 1925, and certainly not a Hilbert."
Hilbert lived to see the Nazis purge many of the prominent faculty members at University of Göttingen in 1933. Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book "Grundlagen der Mathematik" (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert–Ackermann book "Principles of Mathematical Logic" from 1928. Hermann Weyl's successor was Helmut Hasse.
About a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked whether "the "Mathematical Institute" really suffered so much because of the departure of the Jews". Hilbert replied,
"Suffered? It doesn't exist any longer, does it!"
By the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of Königsberg. News of his death only became known to the wider world six months after he died.
The epitaph on his tombstone in Göttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: "Ignoramus et ignorabimus" or "We do not know, we shall not know":
In English:
The day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt Gödel—in a round table discussion during the Conference on Epistemology held jointly with the Society meetings—tentatively announced the first expression of his incompleteness theorem. Gödel's incompleteness theorems show that even elementary axiomatic systems such as Peano arithmetic are either self-contradicting or contain logical propositions that are impossible to prove or disprove.
In 1892, Hilbert married Käthe Jerosch (1864–1945) from German Jewish family, "the daughter of a Königsberg merchant, an outspoken young lady with an independence of mind that matched his own". While at Königsberg they had their one child, Franz Hilbert (1893–1969).
Hilbert's son Franz suffered throughout his life from an undiagnosed mental illness. His inferior intellect was a terrible disappointment to his father and this misfortune was a matter of distress to the mathematicians and students at Göttingen.
Hilbert considered the mathematician Hermann Minkowski to be his "best and truest friend".
Hilbert was baptized and raised a Calvinist in the Prussian Evangelical Church. He later left the Church and became an agnostic. He also argued that mathematical truth was independent of the existence of God or other "a priori" assumptions. When Galileo Galilei was criticized for failing to stand up for his convictions on the Heliocentric theory, Hilbert objected: "But [Galileo] was not an idiot. Only an idiot could believe that scientific truth needs martyrdom; that may be necessary in religion, but scientific results prove themselves in due time."
Hilbert's first work on invariant functions led him to the demonstration in 1888 of his famous "finiteness theorem". Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. To solve what had become known in some circles as "Gordan's Problem", Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated "Hilbert's basis theorem", showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof — it did not display "an object" — but rather, it was an existence proof and relied on use of the law of excluded middle in an infinite extension.
Hilbert sent his results to the "Mathematische Annalen". Gordan, the house expert on the theory of invariants for the "Mathematische Annalen", could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:
Klein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the "Annalen". After having read the manuscript, Klein wrote to him, saying:
Later, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:
For all his successes, the nature of his proof created more trouble than Hilbert could have imagined. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that "many different constructions are subsumed under one fundamental idea" — in other words (to quote Reid): "Through a proof of existence, Hilbert had been able to obtain a construction"; "the proof" (i.e. the symbols on the page) "was" "the object". Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist "school", much to Hilbert's torment in his later years. Indeed, Hilbert would lose his "gifted pupil" Weyl to intuitionism — "Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker". Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert responded:
The text "Grundlagen der Geometrie" (tr.: "Foundations of Geometry") published by Hilbert in 1899 proposes a formal set, called Hilbert's axioms, substituting for the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the "Grundlagen" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.
Hilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat "things", about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and Kötter, by tables, chairs, glasses of beer and other such objects. It is their defined relationships that are discussed.
Hilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.
Hilbert put forth a most influential list of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned as the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.
After re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later 'foundationalist' Russell–Whitehead or 'encyclopedist' Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could enlist in problems, which he had identified as crucial aspects of the areas of mathematics he took to be key.
The problem set was launched as a talk "The Problems of Mathematics" presented during the course of the Second International Congress of Mathematicians held in Paris. The introduction of the speech that Hilbert gave said:
He presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. See also Hilbert's twenty-fourth problem. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.
Some of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some even continue to this day to remain a challenge for mathematicians.
In an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto, that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.
In 1920 he proposed explicitly a research project (in "metamathematics", as it was then termed) that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done, by showing that:
He seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the "ignorabimus", still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.
This program is still recognizable in the most popular philosophy of mathematics, where it is usually called "formalism". For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.
Hilbert wrote in 1919:
Hilbert published his views on the foundations of mathematics in the 2-volume work Grundlagen der Mathematik.
Hilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, ended in failure.
Gödel demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.
Nevertheless, the subsequent achievements of proof theory at the very least "clarified" consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand Gödel's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in the work of Alonzo Church and Alan Turing, also grew directly out of this 'debate'.
Around 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction.
Later on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.
Until 1912, Hilbert was almost exclusively a "pure" mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar in the subject in 1905.
In 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a "physics tutor" for himself. He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.
By 1907, Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years with a confounding problem of putting the theory into final form. By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to Göttingen to deliver a week of lectures on the subject. Einstein received an enthusiastic reception at Göttingen. Over the summer, Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915, Einstein published several papers culminating in "The Field Equations of Gravitation" (see Einstein field equations). Nearly simultaneously, David Hilbert published "The Foundations of Physics", an axiomatic derivation of the field equations (see Einstein–Hilbert action). Hilbert fully credited Einstein as the originator of the theory, and no public priority dispute concerning the field equations ever arose between the two men during their lives. See more at priority.
Additionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schrödinger's wave equation, and his namesake Hilbert space plays an important part in quantum theory. In 1926, von Neumann showed that, if quantum states were understood as vectors in Hilbert space, they would correspond with both Schrödinger's wave function theory and Heisenberg's matrices.
Throughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher mathematics, physicists tended to be "sloppy" with it. To a "pure" mathematician like Hilbert, this was both "ugly" and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found, most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic "Methoden der mathematischen Physik" (Methods of Mathematical Physics) including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said "Physics is too hard for physicists", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.
Hilbert unified the field of algebraic number theory with his 1897 treatise "Zahlbericht" (literally "report on numbers"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers. He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.
He made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.
Hilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert–Pólya conjecture, for reasons that are anecdotal.
His collected works ("Gesammelte Abhandlungen") have been published several times. The original versions of his papers contained "many technical errors of varying degree"; when the collection was first published, the errors were corrected and it was found that this could be done without major changes in the statements of the theorems, with one exception—a claimed proof of the continuum hypothesis. The errors were nonetheless so numerous and significant that it took Olga Taussky-Todd three years to make the corrections.

</doc>
<doc id="8303" url="https://en.wikipedia.org/wiki?curid=8303" title="Down syndrome">
Down syndrome

Down syndrome or Down's syndrome, also known as trisomy 21, is a genetic disorder caused by the presence of all or part of a third copy of chromosome 21. It is usually associated with physical growth delays, mild to moderate intellectual disability, and characteristic facial features. The average IQ of a young adult with Down syndrome is 50, equivalent to the mental ability of an 8- or 9-year-old child, but this can vary widely.
The parents of the affected individual are usually genetically normal. The probability increases from less than 0.1% in 20-year-old mothers to 3% in those of age 45. The extra chromosome is believed to occur by chance, with no known behavioral activity or environmental factor that changes the probability. Down syndrome can be identified during pregnancy by prenatal screening followed by diagnostic testing or after birth by direct observation and genetic testing. Since the introduction of screening, pregnancies with the diagnosis are often terminated via abortion. Regular screening for health problems common in Down syndrome is recommended throughout the person's life.
There is no cure for Down syndrome. Education and proper care have been shown to improve quality of life. Some children with Down syndrome are educated in typical school classes, while others require more specialized education. Some individuals with Down syndrome graduate from high school, and a few attend post-secondary education. In adulthood, about 20% in the United States do paid work in some capacity, with many requiring a sheltered work environment. Support in financial and legal matters is often needed. Life expectancy is around 50 to 60 years in the developed world with proper health care.
Down syndrome is one of the most common chromosome abnormalities in humans. It occurs in about 1 in 1,000 babies born each year. In 2015, Down syndrome was present in 5.4 million individuals globally and resulted in 27,000 deaths, down from 43,000 deaths in 1990. It is named after British doctor John Langdon Down, who fully described the syndrome in 1866. Some aspects of the condition were described earlier by French psychiatrist Jean-Étienne Dominique Esquirol in 1838 and French physician Édouard Séguin in 1844. The genetic cause of Down syndrome was discovered in 1959.
Those with Down syndrome nearly always have physical and intellectual disabilities. As adults, their mental abilities are typically similar to those of an 8- or 9-year-old. They also typically have poor immune function and generally reach developmental milestones at a later age. They have an increased risk of a number of other health problems, including congenital heart defect, epilepsy, leukemia, thyroid diseases, and mental disorders.
People with Down syndrome may have some or all of these physical characteristics: a small chin, slanted eyes, poor muscle tone, a flat nasal bridge, a single crease of the palm, and a protruding tongue due to a small mouth and relatively large tongue. These airway changes lead to obstructive sleep apnea in around half of those with Down syndrome. Other common features include: a flat and wide face, a short neck, excessive joint flexibility, extra space between big toe and second toe, abnormal patterns on the fingertips and short fingers. Instability of the atlantoaxial joint occurs in about 20% and may lead to spinal cord injury in 1–2%. Hip dislocations may occur without trauma in up to a third of people with Down syndrome.
Growth in height is slower, resulting in adults who tend to have short stature—the average height for men is 154 cm (5 ft 1 in) and for women is 142 cm (4 ft 8 in). Individuals with Down syndrome are at increased risk for obesity as they age. Growth charts have been developed specifically for children with Down syndrome.
This syndrome causes about a third of cases of intellectual disability. Many developmental milestones are delayed with the ability to crawl typically occurring around 8 months rather than 5 months and the ability to walk independently typically occurring around 21 months rather than 14 months.
Most individuals with Down syndrome have mild (IQ: 50–69) or moderate (IQ: 35–50) intellectual disability with some cases having severe (IQ: 20–35) difficulties. Those with mosaic Down syndrome typically have IQ scores 10–30 points higher. As they age, people with Down syndrome typically perform worse than their same-age peers.
Commonly, individuals with Down syndrome have better language understanding than ability to speak. Between 10 and 45% have either a stutter or rapid and irregular speech, making it difficult to understand them. After reaching 30 years of age, some may lose their ability to speak.
They typically do fairly well with social skills. Behavior problems are not generally as great an issue as in other syndromes associated with intellectual disability. In children with Down syndrome, mental illness occurs in nearly 30% with autism occurring in 5–10%. People with Down syndrome experience a wide range of emotions. While people with Down syndrome are generally happy, symptoms of depression and anxiety may develop in early adulthood.
Children and adults with Down syndrome are at increased risk of epileptic seizures, which occur in 5–10% of children and up to 50% of adults. This includes an increased risk of a specific type of seizure called infantile spasms. Many (15%) who live 40 years or longer develop Alzheimer’s disease. In those who reach 60 years of age, 50–70% have the disease.
Hearing and vision disorders occur in more than half of people with Down syndrome. 
Vision problems occur in 38 to 80%. Between 20 and 50% have strabismus, in which the two eyes do not move together. Cataracts (cloudiness of the lens of the eye) occur in 15%, and may be present at birth. Keratoconus (a thin, cone-shaped cornea) and glaucoma (increased eye pressure) are also more common, as are refractive errors requiring glasses or contacts. Brushfield spots (small white or grayish/brown spots on the outer part of the iris) are present in 38 to 85% of individuals.
Hearing problems are found in 50–90% of children with Down syndrome. This is often the result of otitis media with effusion which occurs in 50–70% and chronic ear infections which occur in 40 to 60%. Ear infections often begin in the first year of life and are partly due to poor eustachian tube function. Excessive ear wax can also cause hearing loss due to obstruction of the outer ear canal. Even a mild degree of hearing loss can have negative consequences for speech, language understanding, and academics. Additionally, it is important to rule out hearing loss as a factor in social and cognitive deterioration. Age-related hearing loss of the sensorineural type occurs at a much earlier age and affects 10–70% of people with Down syndrome.
The rate of congenital heart disease in newborns with Down syndrome is around 40%. Of those with heart disease, about 80% have an atrioventricular septal defect or ventricular septal defect with the former being more common. Mitral valve problems become common as people age, even in those without heart problems at birth. Other problems that may occur include tetralogy of Fallot and patent ductus arteriosus. People with Down syndrome have a lower risk of hardening of the arteries.
Although the overall risk of cancer in Down syndrome is not changed, the risk of testicular cancer and certain blood cancers, including acute lymphoblastic leukemia (ALL) and acute megakaryoblastic leukemia (AMKL) is increased while the risk of other non-blood cancers is decreased. People with Down syndrome are believed to have an increased risk of developing cancers derived from germ cells whether these cancers are blood or non-blood related.
Leukemia is 10 to 15 times more common in children with Down syndrome. In particular, acute lymphoblastic leukemia is 20 times more common and the megakaryoblastic form of acute myeloid leukemia (acute megakaryoblastic leukemia), is 500 times more common. Acute megakaryoblastic leukemia (AMKL) is a leukemia of megakaryoblasts, the precursors cells to megakaryocytes which form blood platelets. Acute lymphoblastic leukemia in Down syndrome accounts for 1–3% of all childhood cases of ALL. It occurs most often in those older than nine years or having a white blood cell count greater than 50,000 per microliter and is rare in those younger than one year old. ALL in Down syndrome tends to have poorer outcomes than other cases of ALL in people without Down syndrome.
In Down syndrome, AMKL is typically preceded by transient myeloproliferative disease (TMD), a disorder of blood cell production in which non-cancerous megakaryoblasts with a mutation in the "GATA1" gene rapidly divide during the later period of pregnancy. The condition affects 3–10% of babies with Down. While it often spontaneously resolves within three months of birth, it can cause serious blood, liver, or other complications. In about 10% of cases, TMD progresses to AMKL during the three months to five years following its resolution.
People with Down syndrome have a lower risk of all major solid cancers including those of lung, breast, cervix, with the lowest relative rates occurring in those aged 50 years or older. This low risk is thought due to an increase in the expression of tumor suppressor genes present on chromosome 21. One exception is testicular germ cell cancer which occurs at a higher rate in Down syndrome.
Problems of the thyroid gland occur in 20–50% of individuals with Down syndrome. Low thyroid is the most common form, occurring in almost half of all individuals. Thyroid problems can be due to a poorly or nonfunctioning thyroid at birth (known as congenital hypothyroidism) which occurs in 1% or can develop later due to an attack on the thyroid by the immune system resulting in Graves' disease or autoimmune hypothyroidism. Type 1 diabetes mellitus is also more common.
Constipation occurs in nearly half of people with Down syndrome and may result in changes in behavior. One potential cause is Hirschsprung's disease, occurring in 2–15%, which is due to a lack of nerve cells controlling the colon. Other frequent congenital problems include duodenal atresia, pyloric stenosis, Meckel diverticulum, and imperforate anus. Celiac disease affects about 7–20% and gastroesophageal reflux disease is also more common.
Individuals with Down syndrome tend to be more susceptible to gingivitis as well as early, severe periodontal disease, necrotising ulcerative gingivitis, and early tooth loss, especially in the lower front teeth. While plaque and poor oral hygiene are contributing factors, the severity of these periodontal diseases cannot be explained solely by external factors. Research suggests that the severity is likely a result of a weakened immune system. The weakened immune system also contributes to increased incidence of yeast infections in the mouth (from Candida albicans).
Individuals with Down syndrome also tend to have a more alkaline saliva resulting in a greater resistance to tooth decay, despite decreased quantities of saliva, less effective oral hygiene habits, and higher plaque indexes.
Higher rates of tooth wear and bruxism are also common. Other common oral manifestations of Down syndrome include enlarged hypotonic tongue, crusted and hypotonic lips, mouth breathing, narrow palate with crowded teeth, class III malocclusion with an underdeveloped maxilla and posterior crossbite, delayed exfoliation of baby teeth and delayed eruption of adult teeth, shorter roots on teeth, and often missing and malformed (usually smaller) teeth. Less common manifestations include cleft lip and palate and enamel hypocalcification (20% prevalence).
Males with Down syndrome usually do not father children, while females have lower rates of fertility relative to those who are unaffected. Fertility is estimated to be present in 30–50% of females. Menopause usually occurs at an earlier age. The poor fertility in males is thought to be due to problems with sperm development; however, it may also be related to not being sexually active. As of 2006, three instances of males with Down syndrome fathering children and 26 cases of females having children have been reported. Without assisted reproductive technologies, around half of the children of someone with Down syndrome will also have the syndrome.
Down syndrome is caused by having three copies of the genes on chromosome 21, rather than the usual two. The parents of the affected individual are typically genetically normal. Those who have one child with Down syndrome have about a 1% risk of having a second child with the syndrome, if both parents are found to have normal karyotypes.
The extra chromosome content can arise through several different ways. The most common cause (about 92–95% of cases) is a complete extra copy of chromosome 21, resulting in trisomy 21. In 1.0 to 2.5% of cases, some of the cells in the body are normal and others have trisomy 21, known as mosaic Down syndrome. The other common mechanisms that can give rise to Down syndrome include: a Robertsonian translocation, isochromosome, or ring chromosome. These contain additional material from chromosome 21 and occur in about 2.5% of cases. An isochromosome results when the two long arms of a chromosome separate together rather than the long and short arm separating together during egg or sperm development.
Trisomy 21 (also known by the karyotype 47,XX,+21 for females and 47,XY,+21 for males) is caused by a failure of the 21st chromosome to separate during egg or sperm development (nondisjunction). As a result, a sperm or egg cell is produced with an extra copy of chromosome 21; this cell thus has 24 chromosomes. When combined with a normal cell from the other parent, the baby has 47 chromosomes, with three copies of chromosome 21. About 88% of cases of trisomy 21 result from nonseparation of the chromosomes in the mother, 8% from nonseparation in the father, and 3% after the egg and sperm have merged.
The extra chromosome 21 material may also occur due to a Robertsonian translocation in 2–4% of cases. In this situation, the long arm of chromosome 21 is attached to another chromosome, often chromosome 14. In a male affected with Down syndrome, it results in a karyotype of 46XY,t(14q21q). This may be a new mutation or previously present in one of the parents. The parent with such a translocation is usually normal physically and mentally; however, during production of egg or sperm cells, a higher chance of creating reproductive cells with extra chromosome 21 material exists. This results in a 15% chance of having a child with Down syndrome when the mother is affected and a less than 5% probability if the father is affected. The probability of this type of Down syndrome is not related to the mother's age. Some children without Down syndrome may inherit the translocation and have a higher probability of having children of their own with Down syndrome. In this case it is sometimes known as familial Down syndrome.
The extra genetic material present in Down syndrome results in overexpression of a portion of the 310 genes located on chromosome 21. This overexpression has been estimated at around 50%, due to the third copy of the chromosome present. Some research has suggested the Down syndrome critical region is located at bands 21q22.1–q22.3, with this area including genes for amyloid, superoxide dismutase, and likely the ETS2 proto oncogene. Other research, however, has not confirmed these findings. microRNAs are also proposed to be involved.
The dementia that occurs in Down syndrome is due to an excess of amyloid beta peptide produced in the brain and is similar to Alzheimer's disease, which also involves amyloid beta build-up. Amyloid beta is processed from amyloid precursor protein, the gene for which is located on chromosome 21. Senile plaques and neurofibrillary tangles are present in nearly all by 35 years of age, though dementia may not be present. Those with Down syndrome also lack a normal number of lymphocytes and produce less antibodies which contributes to their increased risk of infection.
Down syndrome is associated with an increased risk of many chronic diseases that are typically associated with older age such as Alzheimer's disease. The accelerated aging suggest that trisomy 21 increases the biological age of tissues, but molecular evidence for this hypothesis is sparse. According to a biomarker of tissue age known as epigenetic clock, trisomy 21 increases the age of blood and brain tissue (on average by 6.6 years).
When screening tests predict a high risk of Down syndrome, a more invasive diagnostic test (amniocentesis or chorionic villus sampling) is needed to confirm the diagnosis. The false-positive rate with screening is about 2–5% (see section Screening below). Amniocentesis and chorionic villus sampling are more reliable tests, but they increase the risk of miscarriage between 0.5 and 1%. The risk of limb problems may be increased in the offspring if chorionic villus sampling is performed before 10 weeks. The risk from the procedure is greater the earlier it is performed, thus amniocentesis is not recommended before 15 weeks gestational age and chorionic villus sampling before 10 weeks gestational age.
About 92% of pregnancies in Europe with a diagnosis of Down syndrome are terminated. As a result, there is almost no one with Down's in Iceland and Denmark, where screening is commonplace. In the United States, the termination rate after diagnosis is around 75%, but varies from 61% to 93% depending on the population surveyed. Rates are lower among women who are younger and have decreased over time. When asked if they would have a termination if their fetus tested positive, 23–33% said yes, when high-risk pregnant women were asked, 46–86% said yes, and when women who screened positive are asked, 89–97% say yes.
The diagnosis can often be suspected based on the child's physical appearance at birth. An analysis of the child's chromosomes is needed to confirm the diagnosis, and to determine if a translocation is present, as this may help determine the risk of the child's parents having further children with Down syndrome. Parents generally wish to know the possible diagnosis once it is suspected and do not wish pity.
Guidelines recommend screening for Down syndrome to be offered to all pregnant women, regardless of age. A number of tests are used, with varying levels of accuracy. They are typically used in combination to increase the detection rate. None can be definitive, thus if screening is positive, either amniocentesis or chorionic villus sampling is required to confirm the diagnosis. Screening in both the first and second trimesters is better than just screening in the first trimester. The different screening techniques in use are able to pick up 90–95% of cases, with a false-positive rate of 2–5%. If Down syndrome occurs in one in 500 pregnancies and the test used has a 5% false-positive rate, this means, of 26 women who test positive on screening, only one will have Down syndrome confirmed. If the screening test has a 2% false-positive rate, this means one of eleven who test positive on screening have a fetus with Down syndrome.
Ultrasound imaging can be used to screen for Down syndrome. Findings that indicate increased risk when seen at 14 to 24 weeks of gestation include a small or no nasal bone, large ventricles, nuchal fold thickness, and an abnormal right subclavian artery, among others. The presence or absence of many markers is more accurate. Increased fetal nuchal translucency (NT) indicates an increased risk of Down syndrome picking up 75–80% of cases and being falsely positive in 6%.
Several blood markers can be measured to predict the risk of Down syndrome during the first or second trimester. Testing in both trimesters is sometimes recommended and test results are often combined with ultrasound results. In the second trimester, often two or three tests are used in combination with two or three of: α-fetoprotein, unconjugated estriol, total hCG, and free βhCG detecting about 60–70% of cases.
Testing of the mother's blood for fetal DNA is being studied and appears promising in the first trimester. The International Society for Prenatal Diagnosis considers it a reasonable screening option for those women whose pregnancies are at a high risk for trisomy 21. Accuracy has been reported at 98.6% in the first trimester of pregnancy. Confirmatory testing by invasive techniques (amniocentesis, CVS) is still required to confirm the screening result.
Efforts such as early childhood intervention, screening for common problems, medical treatment where indicated, a good family environment, and work-related training can improve the development of children with Down syndrome. Education and proper care can improve quality of life. Raising a child with Down syndrome is more work for parents than raising an unaffected child. Typical childhood vaccinations are recommended.
A number of health organizations have issued recommendations for screening those with Down syndrome for particular diseases. This is recommended to be done systematically.
At birth, all children should get an electrocardiogram and ultrasound of the heart. Surgical repair of heart problems may be required as early as three months of age. Heart valve problems may occur in young adults, and further ultrasound evaluation may be needed in adolescents and in early adulthood. Due to the elevated risk of testicular cancer, some recommend checking the person's testicles yearly.
Hearing aids or other amplification devices can be useful for language learning in those with hearing loss. Speech therapy may be useful and is recommended to be started around nine months of age. As those with Down syndrome typically have good hand-eye coordination, learning sign language may be possible. Augmentative and alternative communication methods, such as pointing, body language, objects, or pictures, are often used to help with communication. Behavioral issues and mental illness are typically managed with counseling or medications.
Education programs before reaching school age may be useful. School-age children with Down syndrome may benefit from inclusive education (whereby students of differing abilities are placed in classes with their peers of the same age), provided some adjustments are made to the curriculum. Evidence to support this, however, is not very strong. In the United States, the Individuals with Disabilities Education Act of 1975 requires public schools generally to allow attendance by students with Down syndrome.
Individuals with Down syndrome may learn better visually. Drawing may help with language, speech, and reading skills. Children with Down syndrome still often have difficulty with sentence structure and grammar, as well as developing the ability to speak clearly. Several types of early intervention can help with cognitive development. Efforts to develop motor skills include physical therapy, speech and language therapy, and occupational therapy. Physical therapy focuses specifically on motor development and teaching children to interact with their environment. Speech and language therapy can help prepare for later language. Lastly, occupational therapy can help with skills needed for later independence.
Tympanostomy tubes are often needed and often more than one set during the person's childhood. Tonsillectomy is also often done to help with sleep apnea and throat infections. Surgery, however, does not always address the sleep apnea and a continuous positive airway pressure (CPAP) machine may be useful. Physical therapy and participation in physical education may improve motor skills. Evidence to support this in adults, however, is not very good.
Efforts to prevent respiratory syncytial virus (RSV) infection with human monoclonal antibodies should be considered, especially in those with heart problems. In those who develop dementia there is no evidence for memantine, donepezil, rivastigmine, or galantamine.
Plastic surgery has been suggested as a method of improving the appearance and thus the acceptance of people with Down syndrome. It has also been proposed as a way to improve speech. Evidence, however, does not support a meaningful difference in either of these outcomes. Plastic surgery on children with Down syndrome is uncommon, and continues to be controversial. The U.S. National Down Syndrome Society views the goal as one of mutual respect and acceptance, not appearance.
Many alternative medical techniques are used in Down syndrome; however, they are poorly supported by evidence. These include: dietary changes, massage, animal therapy, chiropractic and naturopathy, among others. Some proposed treatments may also be harmful.
Between 5 and 15% of children with Down syndrome in Sweden attend regular school. Some graduate from high school; however, most do not. Of those with intellectual disability in the United States who attended high school about 40% graduated. Many learn to read and write and some are able to do paid work. In adulthood about 20% in the United States do paid work in some capacity. In Sweden, however, less than 1% have regular jobs. Many are able to live semi-independently, but they often require help with financial, medical, and legal matters. Those with mosaic Down syndrome usually have better outcomes.
Individuals with Down syndrome have a higher risk of early death than the general population. This is most often from heart problems or infections. Following improved medical care, particularly for heart and gastrointestinal problems, the life expectancy has increased. This increase has been from 12 years in 1912, to 25 years in the 1980s, to 50 to 60 years in the developed world in the 2000s. Currently between 4 and 12% die in the first year of life. The probability of long-term survival is partly determined by the presence of heart problems. In those with congenital heart problems, 60% survive to 10 years and 50% survive to 30 years of age. In those without heart problems, 85% survive to 10 years and 80% survive to 30 years of age. About 10% live to 70 years of age. The National Down Syndrome Society provide information regarding raising a child with Down syndrome.
Down syndrome is the most common chromosomal abnormality in humans. Globally, , Down syndrome occurs in about 1 per 1,000 births and results in about 17,000 deaths. More children are born with Down syndrome in countries where abortion is not allowed and in countries where pregnancy more commonly occurs at a later age. About 1.4 per 1,000 live births in the United States and 1.1 per 1,000 live births in Norway are affected. In the 1950s, in the United States, it occurred in 2 per 1000 live births with the decrease since then due to prenatal screening and abortions. The number of pregnancies with Down syndrome is more than two times greater with many spontaneously aborting. It is the cause of 8% of all congenital disorders.
Maternal age affects the chances of having a pregnancy with Down syndrome. At age 20, the chance is 1 in 1,441; at age 30, it is 1 in 959; at age 40, it is 1 in 84; and at age 50 it is 1 in 44. Although the probability increases with maternal age, 70% of children with Down syndrome are born to women 35 years of age and younger, because younger people have more children. The father's older age is also a risk factor in women older than 35, but not in women younger than 35, and may partly explain the increase in risk as women age.
English physician John Langdon Down first described Down syndrome in 1862, recognizing it as a distinct type of mental disability, and again in a more widely published report in 1866. Édouard Séguin described it as separate from cretinism in 1844. By the 20th century, Down syndrome had become the most recognizable form of mental disability.
In antiquity, many infants with disabilities were either killed or abandoned. 
In June 2020, the earliest incidence of Down syndrome was found in genomic evidence from an infant that was buried before 3200 BC at Poulnabrone dolmen in Ireland.
Researchers believe that a number of historical pieces of art portray Down syndrome, including pottery from the pre-Columbian Tumaco-La Tolita culture in present-day Colombia and Ecuador, and the 16th-century painting "The Adoration of the Christ Child".
In the 20th century, many individuals with Down syndrome were institutionalized, few of the associated medical problems were treated, and most people died in infancy or early adulthood. With the rise of the eugenics movement, 33 of the then 48 U.S. states and several countries began programs of forced sterilization of individuals with Down syndrome and comparable degrees of disability. Action T4 in Nazi Germany made public policy of a program of systematic involuntary euthanization.
With the discovery of karyotype techniques in the 1950s it became possible to identify abnormalities of chromosomal number or shape. In 1959 Jérôme Lejeune reported the discovery that Down syndrome resulted from an extra chromosome. However, Lejeune's claim to the discovery has been disputed, and in 2014 the Scientific Council of the French Federation of Human Genetics unanimously awarded its Grand Prize to his colleague Marthe Gautier for her role in this discovery. The discovery took place in the laboratory of Raymond Turpin at the Hôpital Trousseau in Paris, France. Jérôme Lejeune and Marthe Gautier were both his students.
As a result of this discovery, the condition became known as trisomy 21. Even before the discovery of its cause, the presence of the syndrome in all races, its association with older maternal age, and its rarity of recurrence had been noticed. Medical texts had assumed it was caused by a combination of inheritable factors that had not been identified. Other theories had focused on injuries sustained during birth.
Due to his perception that children with Down syndrome shared facial similarities with those of Blumenbach's Mongolian race, John Langdon Down used the term "mongoloid". He felt that the existence of Down syndrome confirmed that all peoples were genetically related. In the 1950s with discovery of the underlying cause as being related to chromosomes, concerns about the race-based nature of the name increased.
In 1961, 19 scientists suggested that "mongolism" had "misleading connotations" and had become "an embarrassing term". The World Health Organization (WHO) dropped the term in 1965 after a request by the delegation from the Mongolian People's Republic. While the term mongoloid (also mongolism, Mongolian imbecility or idiocy) continued to be used until the early 1980s, it is now considered unacceptable and is no longer in common use.
In 1975, the United States National Institutes of Health (NIH) convened a conference to standardize the naming and recommended replacing the possessive form, "Down's syndrome" with "Down syndrome". However, both the possessive and nonpossessive forms remain in use by the general population. The term "trisomy 21" is also commonly used.
Most obstetricians argue that not offering screening for Down syndrome is unethical. As it is a medically reasonable procedure, per informed consent, people should at least be given information about it. It will then be the woman's choice, based on her personal beliefs, how much or how little screening she wishes. When results from testing become available, it is also considered unethical not to give the results to the person in question.
Some bioethicists deem it reasonable for parents to select a child who would have the highest well-being. One criticism of this reasoning is that it often values those with disabilities less. Some parents argue that Down syndrome shouldn't be prevented or cured and that eliminating Down syndrome amounts to genocide. The disability rights movement does not have a position on screening, although some members consider testing and abortion discriminatory. Some in the United States who are anti-abortion support abortion if the fetus is disabled, while others do not. Of a group of 40 mothers in the United States who have had one child with Down syndrome, half agreed to screening in the next pregnancy.
Within the US, some Protestant denominations see abortion as acceptable when a fetus has Down syndrome while Orthodox Christianity and Roman Catholicism do not. Some of those against screening refer to it as a form of "eugenics". Disagreement exists within Islam regarding the acceptability of abortion in those carrying a fetus with Down syndrome. Some Islamic countries allow abortion, while others do not. Women may face stigmatization whichever decision they make.
Advocacy groups for individuals with Down syndrome began to be formed after the Second World War. These were organizations advocating for the inclusion of people with Down syndrome into the general school system and for a greater understanding of the condition among the general population, as well as groups providing support for families with children living with Down syndrome. Before this individuals with Down syndrome were often placed in mental hospitals or asylums. Organizations included the Royal Society for Handicapped Children and Adults founded in the UK in 1946 by Judy Fryd, Kobato Kai founded in Japan in 1964, the National Down Syndrome Congress founded in the United States in 1973 by Kathryn McGee and others, and the National Down Syndrome Society founded in 1979 in the United States. The first Roman Catholic order of nuns for women with Down Syndrome, Little Sisters Disciples of the Lamb, was founded in 1985 in France.
The first World Down Syndrome Day was held on 21 March 2006. The day and month were chosen to correspond with 21 and trisomy, respectively. It was recognized by the United Nations General Assembly in 2011.
Efforts are underway to determine how the extra chromosome 21 material causes Down syndrome, as currently this is unknown, and to develop treatments to improve intelligence in those with the syndrome. Two efforts being studied are the use stem cells and gene therapy. Other methods being studied include the use of antioxidants, gamma secretase inhibition, adrenergic agonists, and memantine. Research is often carried out on an animal model, the Ts65Dn mouse.
Down syndrome may also occur in animals other than humans. In great apes chromosome 22 corresponds to the human chromosome 21 and thus trisomy 22 causes Down syndrome in apes. The condition was observed in a common chimpanzee in 1969 and a Bornean orangutan in 1979, but neither lived very long. The common chimpanzee Kanako (born around 1993, in Japan) has become the longest-lived known example of this condition. Kanako has some of the same symptoms that are common in human Down syndrome. It is unknown how common this condition is in chimps but it is plausible it could be roughly as common as Down syndrome is in humans.

</doc>
<doc id="8305" url="https://en.wikipedia.org/wiki?curid=8305" title="Dyslexia">
Dyslexia

Dyslexia, also known as reading disorder, is characterized by trouble with reading despite normal intelligence. Different people are affected to different degrees. Problems may include difficulties in spelling words, reading quickly, writing words, "sounding out" words in the head, pronouncing words when reading aloud and understanding what one reads. Often these difficulties are first noticed at school. When someone who previously could read loses their ability, it is known as "alexia". The difficulties are involuntary and people with this disorder have a normal desire to learn. People with dyslexia have higher rates of attention deficit hyperactivity disorder (ADHD), developmental language disorders, and difficulties with numbers.
Dyslexia is believed to be caused by the interaction of genetic and environmental factors. Some cases run in families. Dyslexia that develops due to a traumatic brain injury, stroke, or dementia is called "acquired dyslexia". The underlying mechanisms of dyslexia are problems within the brain's language processing. Dyslexia is diagnosed through a series of tests of memory, vision, spelling, and reading skills. Dyslexia is separate from reading difficulties caused by hearing or vision problems or by insufficient teaching or opportunity to learn.
Treatment involves adjusting teaching methods to meet the person's needs. While not curing the underlying problem, it may decrease the degree or impact of symptoms. Treatments targeting vision are not effective. Dyslexia is the most common learning disability and occurs in all areas of the world. It affects 3–7% of the population, however, up to 20% of the general population may have some degree of symptoms. While dyslexia is more often diagnosed in men, it has been suggested that it affects men and women equally. Some believe that dyslexia should be best considered as a different way of learning, with both benefits and downsides.
Dyslexia is divided into developmental and acquired forms. This article is primarily about "developmental dyslexia", i.e., dyslexia that begins in early childhood. Acquired dyslexia occurs subsequent to neurological insult, such as traumatic brain injury or stroke. People with acquired dyslexia exhibit some of the signs or symptoms of the developmental disorder, but requiring different assessment strategies and treatment approaches.
In early childhood, symptoms that correlate with a later diagnosis of dyslexia include delayed onset of speech and a lack of phonological awareness. A common myth closely associates dyslexia with mirror writing and reading letters or words backwards. These behaviors are seen in many children as they learn to read and write, and are not considered to be defining characteristics of dyslexia.
School-age children with dyslexia may exhibit signs of difficulty in identifying or generating rhyming words, or counting the number of syllables in words–both of which depend on phonological awareness. They may also show difficulty in segmenting words into individual sounds or may blend sounds when producing words, indicating reduced phonemic awareness. Difficulties with word retrieval or naming things is also associated with dyslexia. People with dyslexia are commonly poor spellers, a feature sometimes called dysorthographia or dysgraphia, which depends on orthographic coding.
Problems persist into adolescence and adulthood and may include difficulties with summarizing stories, memorization, reading aloud, or learning foreign languages. Adults with dyslexia can often read with good comprehension, though they tend to read more slowly than others without a learning difficulty and perform worse in spelling tests or when reading nonsense words–a measure of phonological awareness.
Dyslexia often co-occurs with other learning disorders, but the reasons for this comorbidity have not been clearly identified. These associated disabilities include:
Researchers have been trying to find the neurobiological basis of dyslexia since the condition was first identified in 1881. For example, some have tried to associate the common problem among people with dyslexia of not being able to see letters clearly to abnormal development of their visual nerve cells.
Neuroimaging techniques, such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), have shown a correlation between both functional and structural differences in the brains of children with reading difficulties. Some people with dyslexia show less electrical activation in parts of the left hemisphere of the brain involved with reading, such as the inferior frontal gyrus, inferior parietal lobule, and the middle and ventral temporal cortex. Over the past decade, brain activation studies using PET to study language have produced a breakthrough in the understanding of the neural basis of language. Neural bases for the visual lexicon and for auditory verbal short-term memory components have been proposed, with some implication that the observed neural manifestation of developmental dyslexia is task-specific (i.e., functional rather than structural). fMRIs of people with dyslexia indicate an interactive role of the cerebellum and cerebral cortex as well as other brain structures in reading.
The cerebellar theory of dyslexia proposes that impairment of cerebellum-controlled muscle movement affects the formation of words by the tongue and facial muscles, resulting in the fluency problems that some people with dyslexia experience. The cerebellum is also involved in the automatization of some tasks, such as reading. The fact that some children with dyslexia have motor task and balance impairments could be consistent with a cerebellar role in their reading difficulties. However, the cerebellar theory has not been supported by controlled research studies.
Research into potential genetic causes of dyslexia has its roots in post-autopsy examination of the brains of people with dyslexia. Observed anatomical differences in the language centers of such brains include microscopic cortical malformations known as ectopias, and more rarely, vascular micro-malformations, and microgyrus—a smaller than usual size for the gyrus. The previously cited studies and others suggest that abnormal cortical development, presumed to occur before or during the sixth month of fetal brain development, may have caused the abnormalities. Abnormal cell formations in people with dyslexia have also been reported in non-language cerebral and subcortical brain structures. Several genes have been associated with dyslexia, including DCDC2 and KIAA0319 on chromosome 6, and DYX1C1 on chromosome 15.
The contribution of gene–environment interaction to reading disability has been intensely studied using twin studies, which estimate the proportion of variance associated with a person's environment and the proportion associated with their genes. Both environmental and genetic factors appear to contribute to reading development. Studies examining the influence of environmental factors such as parental education and teaching quality have determined that genetics have greater influence in supportive, rather than less optimal, environments. However, more optimal conditions may just allow those genetic risk factors to account for more of the variance in outcome because the environmental risk factors have been minimized.
As environment plays a large role in learning and memory, it is likely that epigenetic modifications play an important role in reading ability. Measures of gene expression, histone modifications, and methylation in the human periphery are used to study epigenetic processes; however, all of these have limitations in the extrapolation of results for application to the human brain.
The orthographic complexity of a language directly affects how difficult it is to learn to read it. English and French have comparatively "deep" phonemic orthographies within the Latin alphabet writing system, with complex structures employing spelling patterns on several levels: letter-sound correspondence, syllables, and morphemes. Languages such as Spanish, Italian and Finnish have mostly alphabetic orthographies, which primarily employ letter-sound correspondence—so-called "shallow" orthographies—which makes them easier to learn for people with dyslexia. Logographic writing systems, such as Chinese characters, have extensive symbol use; and these also pose problems for dyslexic learners.
Most people who are right-hand dominant have the left hemisphere of their brain specialize more in language processing. In terms of the mechanism of dyslexia, fMRI studies suggest that this specialization may be less pronounced or even absent in cases with dyslexia. Additionally, anatomical differences in the corpus callosum, the bundle of nerve fibers that connects the left and right hemispheres, have been linked to dyslexia via different studies.
Data via diffusion tensor MRI indicate changes in connectivity or in gray matter density in areas related to reading/language. Finally, the left inferior frontal gyrus has shown differences in phonological processing in people with dyslexia. Neurophysiological and imaging procedures are being used to ascertain phenotypic characteristics in people with dyslexia thus identifying the effects of certain genes.
The dual-route theory of reading aloud was first described in the early 1970s. This theory suggests that two separate mental mechanisms, or cognitive routes, are involved in reading aloud. One mechanism is the lexical route, which is the process whereby skilled readers can recognize known words by sight alone, through a "dictionary" lookup procedure. The other mechanism is the nonlexical or sublexical route, which is the process whereby the reader can "sound out" a written word. This is done by identifying the word's constituent parts (letters, phonemes, graphemes) and applying knowledge of how these parts are associated with each other, for example, how a string of neighboring letters sound together. The dual-route system could explain the different rates of dyslexia occurrence between different languages (e.g., the consistency of phonological rules in the Spanish language could account for the fact that Spanish-speaking children show a higher level of performance in non-word reading, when compared to English-speakers).
Dyslexia is a heterogeneous, dimensional learning disorder that impairs accurate and fluent word reading and spelling. Typical—but not universal—features include difficulties with phonological awareness; inefficient and often inaccurate processing of sounds in oral language ("phonological processing"); and verbal working memory deficits.
Dyslexia is a neurodevelopmental disorder, subcategorized in diagnostic guides as a "learning disorder with impairment in reading" (ICD-11 prefixes "developmental" to "learning disorder"; DSM-5 uses "specific"). Dyslexia is not a problem with intelligence. Emotional problems often arise secondary to learning difficulties. The National Institute of Neurological Disorders and Stroke describes dyslexia as "difficulty with phonological processing (the manipulation of sounds), spelling, and/or rapid visual-verbal responding".
The British Dyslexia Association defines dyslexia as "a learning difficulty that primarily affects the skills involved in accurate and fluent word reading and spelling" and is characterized by "difficulties in phonological awareness, verbal memory and verbal processing speed". "Phonological awareness" enables one to identify, discriminate, remember (working memory), and mentally manipulate the sound structures of language—phonemes, onsite-rime segments, syllables, and words.
There is a wide range of tests that are used in clinical and educational settings to evaluate the possibility that a person might have dyslexia. If initial testing suggests that a person might have dyslexia, such tests are often followed up with a full diagnostic assessment to determine the extent and nature of the disorder. Some tests can be administered by a teacher or computer; others require specialized training and are given by psychologists. Some test results indicate how to carry out teaching strategies. Because a variety of different cognitive, behavioral, emotional, and environmental factors all could contribute to difficultly learning to read, a comprehensive evaluation should consider these different possibilities. These tests and observations can include:
Screening procedures seek to identify children who show signs of possible dyslexia. In the preschool years, a family history of dyslexia, particularly in biological parents and siblings, predicts an eventual dyslexia diagnosis better than any test. In primary school (ages 5–7), the ideal screening procedure consist of training primary school teachers to carefully observe and record their pupils' progress through the phonics curriculum, and thereby identify children progressing slowly. When teachers identify such students they can supplement their observations with screening tests such as the "Phonics screening check" used by United Kingdom schools during Year One.
In the medical setting, child and adolescent psychiatrist M. S. Thambirajah emphasizes that "[g]iven the high prevalence of developmental disorders in school-aged children, all children seen in clinics should be systematically screened for developmental disorders irrespective of the presenting problem/s." Thambirajah recommends screening for developmental disorders, including dyslexia, by conducting a brief developmental history, a preliminary psychosocial developmental examination, and obtaining a school report regarding academic and social functioning.
Through the use of compensation strategies, therapy and educational support, individuals with dyslexia can learn to read and write. There are techniques and technical aids that help to manage or conceal symptoms of the disorder. Reducing stress and anxiety can sometimes improve written comprehension. For dyslexia intervention with alphabet-writing systems, the fundamental aim is to increase a child's awareness of correspondences between graphemes (letters) and phonemes (sounds), and to relate these to reading and spelling by teaching how sounds blend into words. Reinforced collateral training focused on reading and spelling may yield longer-lasting gains than oral phonological training alone. Early intervention can be successful in reducing reading failure.
Research does not suggest that specially-tailored fonts (such as Dyslexie and OpenDyslexic) help with reading. Children with dyslexia read text set in a regular font such as Times New Roman and Arial just as quickly, and they show a preference for regular fonts over specially-tailored fonts. Some research has pointed to increased letter-spacing being beneficial.
There is currently no evidence showing that music education significantly improves the reading skills of adolescents with dyslexia.
Dyslexic children require special instruction for word analysis and spelling from an early age. The prognosis, generally speaking, is positive for individuals who are identified in childhood and receive support from friends and family. The New York educational system (NYED) indicates "a daily uninterrupted 90 minute block of instruction in reading", furthermore "instruction in phonemic awareness, phonics, vocabulary development, reading fluency" so as to improve the individual's reading ability.
The percentage of people with dyslexia is unknown, but it has been estimated to be as low as 5% and as high as 17% of the population. While it is diagnosed more often in males, some believe that it affects males and females equally.
There are different definitions of dyslexia used throughout the world, but despite significant differences in writing systems, dyslexia occurs in different populations. Dyslexia is not limited to difficulty in converting letters to sounds, and Chinese people with dyslexia may have difficulty converting Chinese characters into their meanings. The Chinese vocabulary uses logographic, monographic, non-alphabet writing where one character can represent an individual phoneme.
The phonological-processing hypothesis attempts to explain why dyslexia occurs in a wide variety of languages. Furthermore, the relationship between phonological capacity and reading appears to be influenced by orthography.
Dyslexia was clinically described by Oswald Berkhan in 1881, but the term "dyslexia" was coined in 1883 by Rudolf Berlin, an ophthalmologist in Stuttgart. He used the term to refer to the case of a young boy who had severe difficulty learning to read and write, despite showing typical intelligence and physical abilities in all other respects. In 1896, W. Pringle Morgan, a British physician from Seaford, East Sussex, published a description of a reading-specific learning disorder in a report to the "British Medical Journal" titled "Congenital Word Blindness". The distinction between phonological versus surface types of dyslexia is only descriptive, and without any etiological assumption as to the underlying brain mechanisms. However, studies have alluded to potential differences due to variation in performance.
As is the case with any disorder, society often makes an assessment based on incomplete information. Before the 1980s, dyslexia was thought to be a consequence of education, rather than a neurological disability. As a result, society often misjudges those with the disorder. There is also sometimes a workplace stigma and negative attitude towards those with dyslexia. If the instructors of a person with dyslexia lack the necessary training to support a child with the condition, there is often a negative effect on the student's learning participation.
Since at least the 1960s in the UK, the children diagnosed with developmental dyslexia have consistently been from privileged families. Although half of prisoners in the UK have significant reading difficulties, very few have ever been evaluated for dyslexia. Access to some special educational resources and funding is contingent upon having a diagnosis of dyslexia. As a result, when Staffordshire and Warwickshire proposed in 2018 to teach reading to all children with reading difficulties, using techniques proven to be successful for most children with a diagnosis of dyslexia, without first requiring the families to obtain an official diagnosis, dyslexia advocates and parents of children with dyslexia were fearful that they were losing a privileged status.
Most dyslexia research relates to alphabetic writing systems, and especially to European languages. However, substantial research is also available regarding people with dyslexia who speak Arabic, Chinese, Hebrew, or other languages. The outward expression of individuals with reading disability and regular poor readers is the same in some respects.

</doc>
<doc id="8308" url="https://en.wikipedia.org/wiki?curid=8308" title="Delft">
Delft

Delft () is a city and municipality in the province of South Holland, Netherlands. It is located between Rotterdam, to the southeast, and The Hague, to the northwest. Together with them, it is part of both Rotterdam–The Hague metropolitan area and the Randstad.
Delft is a popular tourist destination in the Netherlands, famous for its historical connections with the reigning House of Orange-Nassau, for its blue pottery, for being home to the painter Jan Vermeer, and for hosting Delft University of Technology (TU Delft). Historically, Delft played a highly influential role in the Dutch Golden Age. In terms of science and technology, thanks to the pioneering contributions of Antonie van Leeuwenhoek and Martinus Beijerinck, Delft can be considered to be the birthplace of microbiology.
The city of Delft came into being beside a canal, the 'Delf', which comes from the word "delven", meaning to delve or dig, and this led to the name Delft. At the elevated place where this 'Delf' crossed the creek wall of the silted up river Gantel, a Count established his manor, probably around 1075. Partly because of this, Delft became an important market town, the evidence for which can be seen in the size of its central market square.
Having been a rural village in the early Middle Ages, Delft developed into a city, and on 15 April 1246, Count Willem II granted Delft its city charter. Trade and industry flourished. In 1389 the Delfshavensche Schie canal was dug through to the river Maas, where the port of Delfshaven was built, connecting Delft to the sea.
Until the 17th century, Delft was one of the major cities of the then county (and later province) of Holland. In 1400, for example, the city had 6,500 inhabitants, making it the third largest city after Dordrecht (8,000) and Haarlem (7,000). In 1560, Amsterdam, with 28,000 inhabitants, had become the largest city, followed by Delft, Leiden and Haarlem, which each had around 14,000 inhabitants.
In 1536, a large part of the city was destroyed by the great fire of Delft.
The town's association with the House of Orange started when William of Orange (Willem van Oranje), nicknamed William the Silent (Willem de Zwijger), took up residence in 1572 in the former Saint-Agatha convent (subsequently called the Prinsenhof). At the time he was the leader of growing national Dutch resistance against Spanish occupation, known as the Eighty Years' War. By then Delft was one of the leading cities of Holland and it was equipped with the necessary city walls to serve as a headquarters. In October 1573, an attack by Spanish forces was repelled in the Battle of Delft.
After the Act of Abjuration was proclaimed in 1581, Delft became the "de facto" capital of the newly independent Netherlands, as the seat of the Prince of Orange.
When William was shot dead on 10 July 1584 by Balthazar Gerards in the hall of the Prinsenhof (now the Prinsenhof Museum), the family's traditional burial place in Breda was still in the hands of the Spanish. Therefore, he was buried in the Delft Nieuwe Kerk (New Church), starting a tradition for the House of Orange that has continued to the present day.
Around this time, Delft also occupied a prominent position in the field of printing.
A number of Italian glazed earthenware makers settled in the city and introduced a new style. The tapestry industry also flourished when famous manufacturer François Spierincx moved to the city. In the 17th century, Delft experienced a new heyday, thanks to the presence of an office of the Dutch East India Company (VOC) (opened in 1602) and the manufacture of Delft Blue china.
A number of notable artists based themselves in the city, including Leonard Bramer, Carel Fabritius, Pieter de Hoogh, Gerard Houckgeest, Emanuel de Witte, Jan Steen, and Johannes Vermeer.
Reinier de Graaf and Antonie van Leeuwenhoek received international attention for their scientific research.
The Delft Explosion, also known in history as the Delft Thunderclap, occurred on 12 October 1654 when a gunpowder store exploded, destroying much of the city. Over a hundred people were killed and thousands were injured.
About of gunpowder were stored in barrels in a magazine in a former Clarist convent in the Doelenkwartier district, where the Paardenmarkt is now located. Cornelis Soetens, the keeper of the magazine, opened the store to check a sample of the powder and a huge explosion followed. Luckily, many citizens were away, visiting a market in Schiedam or a fair in The Hague.
Today, the explosion is primarily remembered for killing Rembrandt's most promising pupil, Carel Fabritius, and destroying almost all of his works.
Delft artist Egbert van der Poel painted several pictures of Delft showing the devastation.
The gunpowder store was subsequently re-housed, a 'cannonball's distance away', outside the city, in a new building designed by architect Pieter Post.
The city centre retains a large number of monumental buildings, while in many streets there are canals of which the banks are connected by typical bridges, altogether making this city a notable tourist destination.
Historical buildings and other sights of interest include:
Delft is well known for the Delft pottery ceramic products which were styled on the imported Chinese porcelain of the 17th century. The city had an early start in this area since it was a home port of the Dutch East India Company. It can still be seen at the pottery factories De Koninklijke Porceleyne Fles (or Royal Delft) and De Delftse Pauw, while new ceramics and ceramic art can be found at the Gallery Terra Delft.
The painter Johannes Vermeer (1632–1675) was born in Delft. Vermeer used Delft streets and home interiors as the subject or background in his paintings.
Several other famous painters lived and worked in Delft at that time, such as Pieter de Hoogh, Carel Fabritius, Nicolaes Maes, Gerard Houckgeest and Hendrick Cornelisz. van Vliet. They were all members of the Delft School. The Delft School is known for its images of domestic life and views of households, church interiors, courtyards, squares and the streets of Delft. The painters also produced pictures showing historic events, flowers, portraits for patrons and the court as well as decorative pieces of art.
Delft supports creative arts companies. From 2001 the , a building that had been disused since 1951, began to house small companies in the creative arts sector. However, demolition of the building started in December 2009, making way for the construction of the new railway tunnel in Delft. The occupants of the building, as well as the name 'Bacinol', moved to another building in the city. The name Bacinol relates to Dutch penicillin research during WWII.
Delft University of Technology (TU Delft) is one of four universities of technology in the Netherlands. It was founded as an academy for civil engineering in 1842 by King William II. Today well over 21,000 students are enrolled.
The UNESCO-IHE Institute for Water Education, providing postgraduate education for people from developing countries, draws on the strong tradition in water management and hydraulic engineering of the Delft university.
In the local economic field essential elements are:
East of Delft lies a relatively large nature and recreation area called the "Delftse Hout" ("Delft Wood"). Through the forest lie bike, horse-riding and footpaths. It also includes a vast lake (suitable for swimming and windsurfing), narrow beaches, a restaurant, and community gardens, plus camping ground and other recreational and sports facilities. (There is also a facility for renting bikes from the station.)
Inside the city, apart from a central park, there are several smaller town parks, including "Nieuwe Plantage", "Agnetapark", "Kalverbos".
There is also the Botanical Garden of the TU and an arboretum in Delftse Hout.
Delft was the birthplace of:
Delft is twinned with:
Trains stopping at these stations connect Delft with, among others, the nearby cities of Rotterdam and The Hague, as often as every five minutes, for most of the day.
There are several bus routes from Delft to similar destinations. Trams frequently travel between Delft and The Hague via special double tracks crossing the city.

</doc>
<doc id="8309" url="https://en.wikipedia.org/wiki?curid=8309" title="Duesberg hypothesis">
Duesberg hypothesis

The Duesberg hypothesis is the claim, associated with University of California, Berkeley professor Peter Duesberg, that various noninfectious factors such as but not limited to, recreational and pharmaceutical drug use are the cause of AIDS, and that HIV (human immunodeficiency virus) is merely a harmless passenger virus. The scientific consensus is that the Duesberg hypothesis is incorrect and that HIV is the cause of AIDS. The most prominent supporters of this hypothesis are Duesberg himself, biochemist vitamin proponent David Rasnick, and journalist Celia Farber. The scientific community contends that Duesberg's arguments are the result of cherry-picking predominantly outdated scientific data and selectively ignoring evidence in favor of HIV's role in AIDS.
Duesberg argues that there is a statistical correlation between trends in recreational drug use and trends in AIDS cases. He argues that the epidemic of AIDS cases in the 1980s corresponds to a supposed epidemic of recreational drug use in the United States and Europe during the same time frame.
These claims are not supported by epidemiologic data. The average yearly increase in opioid-related deaths from 1990 to 2002 was nearly three times the yearly increase from 1979–90, with the greatest increase in 2000–02, yet AIDS cases and deaths fell dramatically during the mid-to-late-1990s. Duesberg's claim that recreational drug use, rather than HIV, was the cause of AIDS has been specifically examined and found to be false. Cohort studies have found that only HIV-positive drug users develop opportunistic infections; HIV-negative drug users do not develop such infections, indicating that HIV rather than drug use is the cause of AIDS.
Duesberg has also argued that nitrite inhalants were the cause of the epidemic of Kaposi sarcoma (KS) in gay men. However, this argument has been described as an example of the fallacy of a statistical confounding effect; it is now known that a herpesvirus, potentiated by HIV, is responsible for AIDS-associated KS.
Moreover, in addition to recreational drugs, Duesberg argues that anti-HIV drugs such as zidovudine (AZT) can cause AIDS. Duesberg's claim that antiviral medication causes AIDS is regarded as disproven by the scientific community. Placebo-controlled studies have found that AZT as a single agent produces modest and short-lived improvements in survival and delays the development of opportunistic infections; it certainly did not cause AIDS, which develops in both treated and untreated study patients. With the subsequent development of protease inhibitors and highly active antiretroviral therapy, numerous studies have documented the fact that anti-HIV drugs prevent the development of AIDS and substantially prolong survival, further disproving the claim that these drugs "cause" AIDS.
Several studies have specifically addressed Duesberg's claim that recreational drug abuse or sexual promiscuity were responsible for the manifestations of AIDS. An early study of his claims, published in "Nature" in 1993, found Duesberg's drug abuse-AIDS hypothesis to have "no basis in fact."
A large prospective study followed a group of 715 homosexual men in the Vancouver, Canada, area; approximately half were HIV-seropositive or became so during the follow-up period, and the remainder were HIV-seronegative. After more than 8 years of follow-up, despite similar rates of drug use, sexual contact, and other supposed risk factors in both groups, only the HIV-positive group suffered from opportunistic infections. Similarly, CD4 counts dropped in the patients who were HIV-infected, but remained stable in the HIV-negative patients, despite similar rates of risk behavior. The authors concluded that "the risk-AIDS hypothesis ... is clearly rejected by our data," and that "the evidence supports the hypothesis that HIV-1 has an integral role in the CD4 depletion and progressive immune dysfunction that characterise AIDS."
Similarly, the Multicenter AIDS Cohort Study (MACS) and the Women's Interagency HIV Study (WIHS)—which between them observed more than 8,000 Americans—demonstrated that "the presence of HIV infection is the only factor that is strongly and consistently associated with the conditions that define AIDS." A 2008 study found that recreational drug use (including cannabis, cocaine, poppers, and amphetamines) had no effect on CD4 or CD8 T-cell counts, providing further evidence against a role of recreational drugs as a cause of AIDS.
Duesberg argued in 1989 that a significant number of AIDS victims had died without proof of HIV infection. However, with the use of modern culture techniques and polymerase chain reaction testing, HIV can be demonstrated in virtually all patients with AIDS. Since AIDS is now defined partially by the presence of HIV, Duesberg claims it is impossible by definition to offer evidence that AIDS doesn't require HIV. However, the first definitions of AIDS mentioned no cause and the first AIDS diagnoses were made before HIV was discovered. The addition of HIV positivity to surveillance criteria as an absolutely necessary condition for case reporting occurred only in 1993, after a scientific consensus was established that HIV caused AIDS.
According to the Duesberg hypothesis, AIDS is not found in Africa. What Duesberg calls "the myth of an African AIDS epidemic," among people" exists for several reasons, including:
Duesberg states that African AIDS cases are "a collection of long-established, indigenous diseases, such as chronic fevers, weight loss, alias "slim disease," diarrhea, and tuberculosis" that result from malnutrition and poor sanitation. African AIDS cases, though, have increased in the last three decades as HIV's prevalence has increased but as malnutrition percentages and poor sanitation have declined in many African regions. In addition, while HIV and AIDS are more prevalent in urban than in rural settings in Africa, malnutrition and poor sanitation are found more commonly in rural than in urban settings.
According to Duesberg, common diseases are easily misdiagnosed as AIDS in Africa because "the diagnosis of African AIDS is arbitrary" and does not include HIV testing. A definition of AIDS agreed upon in 1985 by the World Health Organization in Bangui did not require a positive HIV test, but since 1985, many African countries have added positive HIV tests to the Bangui criteria for AIDS or changed their definitions to match those of the U.S. Centers for Disease Control. One of the reasons for using more HIV tests despite their expense is that, rather than overestimating AIDS as Duesberg suggests, the Bangui definition alone excluded nearly half of African AIDS patients."
Duesberg notes that diseases associated with AIDS differ between African and Western populations, concluding that the causes of immunodeficiency must be different. Tuberculosis is much more commonly diagnosed among AIDS patients in Africa than in Western countries, while PCP conforms to the opposite pattern. Tuberculosis, though, had higher prevalence in Africa than in the West before the spread of HIV. In Africa and the United States, HIV has spurred a similar percentage increase in tuberculosis cases. PCP may be underestimated in Africa: since machinery "required for accurate testing is relatively rare in many resource-poor areas, including large parts of Africa, PCP is likely to be underdiagnosed in Africa. Consistent with this hypothesis, studies that report the highest rates of PCP in Africa are those that use the most advanced diagnostic methods" Duesberg also claims that Kaposi's Sarcoma is "exclusively diagnosed in male homosexual risk groups using nitrite inhalants and other psychoactive drugs as aphrodisiacs", but the cancer is fairly common among heterosexuals in some parts of Africa, and is found in heterosexuals in the United States as well.
Because reported AIDS cases in Africa and other parts of the developing world include a larger proportion of people who do not belong to Duesberg's preferred risk groups of drug addicts and male homosexuals, Duesberg writes on his website that "There are no risk groups in Africa, like drug addicts and homosexuals." However, many studies have addressed the issue of risk groups in Africa and concluded that the risk of AIDS is not equally distributed. In addition, AIDS in Africa largely kills sexually active working-age adults.
South African president Thabo Mbeki accepted Duesberg's hypothesis and, through the mid-2000s, rejected offers of medical assistance to fight HIV infection, a policy of inaction that cost over 300,000 lives.
Duesberg argues that retroviruses like HIV must be harmless to survive: they do not kill cells and they do not cause cancer, he maintains. Duesberg writes, "retroviruses do not kill cells because they depend on viable cells for the replication of their RNA from viral DNA integrated into cellular DNA." Duesberg elsewhere states that "the typical virus reproduces by entering a living cell and commandeering the cell's resources in order to make new virus particles, a process that ends with the disintegration of the dead cell."
Duesberg also rejects the involvement of retroviruses and other viruses in cancer. To him, virus-associated cancers are "freak accidents of nature" that do not warrant research programs such as the war on cancer. Duesberg rejects a role in cancer for numerous viruses, including leukemia viruses, Epstein–Barr virus, human papilloma virus, hepatitis B, feline leukemia virus, and human T-lymphotropic virus.
Duesberg claims that the supposedly innocuous nature of all retroviruses is supported by what he considers to be their normal mode of proliferation: infection from mother to child "in utero". Duesberg does not suggest that HIV is an endogenous retrovirus, a virus integrated into the germline and genetically heritable:
The consensus in the scientific community is that the Duesberg hypothesis has been refuted by a large and growing mass of evidence showing that HIV causes AIDS, that the amount of virus in the blood correlates with disease progression, that a plausible mechanism for HIV's action has been proposed, and that anti-HIV medication decreases mortality and opportunistic infection in people with AIDS.
In the 9 December 1994 issue of "Science" (Vol. 266, No. 5191), Duesberg's methods and claims were evaluated in a group of articles. The authors concluded that
The vast majority of people with AIDS have never received antiretroviral drugs, including those in developed countries prior to the licensure of AZT (zidovudine) in 1987, and people in developing countries today where very few individuals have access to these medications.
The NIAID reports that "in the mid-1980s, clinical trials enrolling patients with AIDS found that AZT given as single-drug therapy conferred a modest survival advantage compared [with] placebo. Among HIV-infected patients who had not yet developed AIDS, placebo-controlled trials found that AZT given as single-drug therapy delayed, for a year or two, the onset of AIDS-related illnesses. Significantly, long-term follow-up of these trials did not show a prolonged benefit of AZT, but also did not indicate that the drug increased disease progression or mortality. The lack of excess AIDS cases and death in the AZT arms of these placebo-controlled trials in effect counters the argument that AZT causes AIDS. Subsequent clinical trials found that patients receiving two-drug combinations had up to 50 percent improvements in time to progression to AIDS and in survival when compared with people receiving single-drug therapy. In more recent years, three-drug combination therapies have produced another 50 to 80 percent improvement in progression to AIDS and in survival when compared with two-drug regimens in clinical trials." "Use of potent anti-HIV combination therapies has contributed to dramatic reductions in the incidence of AIDS and AIDS-related deaths in populations where these drugs are widely available, an effect which clearly would not be seen if antiretroviral drugs caused AIDS."
Duesberg claims as support for his idea that many drug-free HIV-positive people have not yet developed AIDS; HIV/AIDS scientists note that many drug-free HIV-positive people have developed AIDS, and that, in the absence of medical treatment or rare genetic factors postulated to delay disease progression, it is very likely that nearly all HIV-positive people will eventually develop AIDS. Scientists also note that HIV-negative drug users do not suffer from immune system collapse.

</doc>
<doc id="8310" url="https://en.wikipedia.org/wiki?curid=8310" title="DSL (disambiguation)">
DSL (disambiguation)

DSL or digital subscriber line is a family of technologies that provide digital data transmission over the wires of a local telephone network.
DSL may also refer to:

</doc>
<doc id="8311" url="https://en.wikipedia.org/wiki?curid=8311" title="Dinosaur">
Dinosaur

Dinosaurs are a class of extinct reptiles and modern birds of the clade Dinosauria. They first appeared during the Triassic period, between 243 and 233.23 million years ago, although the exact origin and timing of the evolution of dinosaurs is the subject of active research. They became the dominant terrestrial vertebrates after the Triassic–Jurassic extinction event 201.3 million years ago; their dominance continued throughout the Jurassic and Cretaceous periods. The fossil record demonstrates that birds are modern feathered dinosaurs, having evolved from earlier theropods during the Late Jurassic epoch. As such, birds were the only dinosaur lineage to survive the Cretaceous–Paleogene extinction event approximately 66 million years ago. Dinosaurs can therefore be divided into avian dinosaurs, or birds; and non-avian dinosaurs, which are all dinosaurs other than birds.
Dinosaurs are a varied group of animals from taxonomic, morphological and ecological standpoints. Birds, at over 10,000 living species, are the most diverse group of vertebrates besides perciform fish. Using fossil evidence, paleontologists have identified over 500 distinct genera and more than 1,000 different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species (birds) and fossil remains. Through the first half of the 20th century, before birds were recognized to be dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that all dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction. Some were herbivorous, others carnivorous. Evidence suggests that all dinosaurs were egg-laying; and that nest-building was a trait shared by many dinosaurs, both avian and non-avian.
While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. While the dinosaurs' modern-day surviving avian lineage (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs (non-avian and avian) were large-bodied—the largest sauropod dinosaurs are estimated to have reached lengths of and heights of and were the largest land animals of all time. Still, the idea that non-avian dinosaurs were uniformly gigantic is a misconception based in part on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small: "Xixianykus", for example, was only about long.
Since the first dinosaur fossils were recognized in the early 19th century, mounted fossil dinosaur skeletons have been major attractions at museums around the world, and dinosaurs have become an enduring part of world culture. The large sizes of some dinosaur groups, as well as their seemingly monstrous and fantastic nature, have ensured dinosaurs' regular appearance in best-selling books and films, such as "Jurassic Park". Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.
The taxon 'Dinosauria' was formally named in 1841 by paleontologist Sir Richard Owen, who used it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived . Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it merely to evoke their size and majesty.
Other prehistoric animals, including pterosaurs, mosasaurs, ichthyosaurs, plesiosaurs, and "Dimetrodon", while often popularly conceived of as dinosaurs, are not taxonomically classified as dinosaurs. Pterosaurs are distantly related to dinosaurs, being members of the clade Ornithodira. The other groups mentioned are, like dinosaurs and pterosaurs, members of Sauropsida (the reptile and bird clade), except "Dimetrodon" (which is a synapsid).
Under phylogenetic nomenclature, dinosaurs are usually defined as the group consisting of the most recent common ancestor (MRCA) of "Triceratops" and modern birds (Neornithes), and all its descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of "Megalosaurus" and "Iguanodon", because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions result in the same set of animals being defined as dinosaurs: "Dinosauria = Ornithischia + Saurischia", encompassing ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (herbivorous quadrupeds with horns and frills), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), theropods (mostly bipedal carnivores and birds), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).
Birds are now recognized as being the sole surviving lineage of theropod dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, a majority of contemporary paleontologists concerned with dinosaurs reject the traditional style of classification in favor of phylogenetic taxonomy; this approach requires that, for a group to be natural, all descendants of members of the group must be included in the group as well. Birds are thus considered to be dinosaurs and dinosaurs are, therefore, not extinct. Birds are classified as belonging to the subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians, which are dinosaurs.
Research by Matthew G. Baron, David B. Norman, and Paul M. Barrett in 2017 suggested a radical revision of dinosaurian systematics. Phylogenetic analysis by Baron "et al." recovered the Ornithischia as being closer to the Theropoda than the Sauropodomorpha, as opposed to the traditional union of theropods with sauropodomorphs. They resurrected the clade Ornithoscelida to refer to the group containing Ornithischia and Theropoda. Dinosauria itself was re-defined as the last common ancestor of "Triceratops horridus", "Passer domesticus" and "Diplodocus carnegii", and all of its descendants, to ensure that sauropods and kin remain included as dinosaurs.
Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Many prehistoric animal groups are popularly conceived of as dinosaurs, such as ichthyosaurs, mosasaurs, plesiosaurs, pterosaurs, and pelycosaurs (especially "Dimetrodon"), but are not classified scientifically as dinosaurs, and none had the erect hind limb posture characteristic of true dinosaurs. Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic Era, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a domestic cat, and were generally rodent-sized carnivores of small prey.
Dinosaurs have always been an extremely varied group of animals; according to a 2006 study, over 500 non-avian dinosaur genera have been identified with certainty so far, and the total number of genera preserved in the fossil record has been estimated at around 1850, nearly 75% of which remain to be discovered. An earlier study predicted that about 3,400 dinosaur genera existed, including many that would not have been preserved in the fossil record. By September 17, 2008, 1,047 different species of dinosaurs had been named.
In 2016, the estimated number of dinosaur species that existed in the Mesozoic was estimated to be 1,543–2,468. Some are herbivorous, others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some prehistoric species were quadrupeds, and others, such as "Anchisaurus" and "Iguanodon", could walk just as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although known for large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by at least the Early Jurassic epoch. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avian dinosaurs (such as "Microraptor") could fly or at least glide, and others, such as spinosaurids, had semiaquatic habits.
While recent discoveries have made it more difficult to present a universally agreed-upon list of dinosaurs' distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clear descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the most recent common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.
A detailed assessment of archosaur interrelations by Sterling Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known:
Nesbitt found a number of further potential synapomorphies and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others.
A variety of other skeletal features are shared by dinosaurs. However, because they are either common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of Infratemporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in "Herrerasaurus"); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in "Saturnalia tupiniquim", for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the Late Triassic epoch are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature.
Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar-erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.
Dinosaurs diverged from their archosaur ancestors during the Middle to Late Triassic epochs, roughly 20 million years after the devastating Permian–Triassic extinction event wiped out an estimated 96% of all marine species and 70% of terrestrial vertebrate species approximately 252 million years ago. Radiometric dating of the rock formation that contained fossils from the early dinosaur genus "Eoraptor" at 231.4 million years old establishes its presence in the fossil record at this time. Paleontologists think that "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators. Dinosaurs may have appeared as early as 243 million years ago, as evidenced by remains of the genus "Nyasasaurus" from that period, though known fossils of these animals are too fragmentary to tell if they are dinosaurs or very close dinosaurian relatives. Paleontologist Max C. Langer "et al." (2018) determined that "Staurikosaurus" from the Santa Maria Formation dates to 233.23 million years ago, making it older in geologic age than "Eoraptor".
When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchia, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, a variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 201 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early-mid Norian and late Norian or earliest Rhaetian stages, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct. Also notably, there was a heightened rate of extinction during the Carnian Pluvial Event.
Dinosaur evolution after the Triassic follows changes in vegetation and the location of continents. In the Late Triassic and Early Jurassic, the continents were connected as the single landmass Pangaea, and there were a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the Late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the Middle and Late Jurassic, where most localities had predators consisting of ceratosaurians, spinosauroids, and carnosaurians, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized sinraptorid theropods and unusual, long-necked sauropods like "Mamenchisaurus". Ankylosaurians and ornithopods were also becoming more common, but prosauropods had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like the earlier prosauropods, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians.
By the Early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like "Psittacosaurus" became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late Early Cretaceous or early Late Cretaceous. A major change in the Early Cretaceous, which would be amplified in the Late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with dental batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid "Nigersaurus".
There were three general dinosaur faunas in the Late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became extremely diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.
The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, such as crocodilians, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.
The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of "Gastornis", eogruiids, bathornithids, ratites, geranoidids, mihirungs, and "terror birds"). It is often cited that mammals out-competed the neornithines for dominance of most terrestrial niches but many of these groups co-existed with rich mammalian faunas for most of the Cenozoic Era. Terror birds and bathornithids occupied carnivorous guilds alongside predatory mammals, and ratites are still fairly successful as mid-sized herbivores; eogruiids similarly lasted from the Eocene to Pliocene, only becoming extinct very recently after over 20 million years of co-existence with many mammal groups.
Dinosaurs belong to a group known as archosaurs, which also includes modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.
Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with "Triceratops" than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek "sauros" (σαῦρος) meaning "lizard" and "ischion" (ἰσχίον) meaning "hip joint"—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups ("Herrerasaurus", therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).
By contrast, ornithischians—"bird-hipped", from the Greek "ornitheios" (ὀρνίθειος) meaning "of a bird" and "ischion" (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubic bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species that were primarily herbivores. Despite the terms "bird hip" and "lizard hip", birds are not part of Ornithischia, but rather Saurischia—birds evolved from earlier dinosaurs with "lizard hips".
The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and organized based on the list of Mesozoic dinosaur species provided by Holtz (2007). A more detailed version can be found at Dinosaur classification.
The dagger (†) is used to signify groups with no living members.
Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics), chemistry, biology, and the Earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.
Current evidence suggests that dinosaur average size varied through the Triassic, Early Jurassic, Late Jurassic and Cretaceous. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the category. The mode of Mesozoic dinosaur body masses is between . This contrasts sharply with the average size of Cenozoic mammals, estimated by the National Museum of Natural History as about .
The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest was an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as "Paraceratherium" (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.
Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals were ever fossilized and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.
The tallest and heaviest dinosaur known from good skeletons is "Giraffatitan brancai" (previously classified as a species of "Brachiosaurus"). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde in Berlin; this mount is tall and long, and would have belonged to an animal that weighed between and  kilograms ( and  lb). The longest complete dinosaur is the long "Diplodocus", which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Museum of Natural History in 1907. The longest dinosaur known from good fossil material is the "Patagotitan": the skeleton mount in the American Museum of Natural History in New York is long. The Museo Municipal Carmen Funes in Plaza Huincul, Argentina, has an "Argentinosaurus" reconstructed skeleton mount long.
There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were discovered in the 1970s or later, and include the massive "Argentinosaurus", which may have weighed to  kilograms (90 to 110 short tons) and reached length of ; some of the longest were the long "Diplodocus hallorum" (formerly "Seismosaurus"), the long "Supersaurus" and long "Patagotitan"; and the tallest, the tall "Sauroposeidon", which could have reached a sixth-floor window. The heaviest and longest dinosaur may have been "Maraapunisaurus", known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been long and weighed kg ( lb). However, as no further evidence of sauropods of this size has been found, and the discoverer, Edward Drinker Cope, had made typographic errors before, it is likely to have been an extreme overestimation.
The largest carnivorous dinosaur was "Spinosaurus", reaching a length of , and weighing . Other large carnivorous theropods included "Giganotosaurus", "Carcharodontosaurus" and "Tyrannosaurus". "Therizinosaurus" and "Deinocheirus" were among the tallest of the theropods. The largest ornithischian dinosaur was probably the hadrosaurid "Shantungosaurus giganteus" which measured . The largest individuals may have weighed as much as .
The smallest dinosaur known is the bee hummingbird, with a length of only and mass of around . The smallest known non-avialan dinosaurs were about the size of pigeons and were those theropods most closely related to birds. For example, "Anchiornis huxleyi" is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of and a total skeletal length of . The smallest herbivorous non-avialan dinosaurs included "Microceratus" and "Wannanosaurus", at about long each.
Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors that are common in birds, as well as in crocodiles (birds' closest living relatives), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.
The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 "Iguanodon bernissartensis", ornithischians that were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been discovered subsequently. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-billed (hadrosaurids) may have moved in great herds, like the American bison or the African Springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is no evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded the remains of over 20 "Sinornithomimus", from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as "Deinonychus" and "Allosaurus" can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators.
The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.
From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a "Velociraptor" attacking a "Protoceratops", providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an "Edmontosaurus", a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod "Majungasaurus".
Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, "Juravenator", and "Megapnosaurus" were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian "Agilisaurus" was inferred to be diurnal.
Based on current fossil evidence from dinosaurs such as "Oryctodromeus", some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids such as "Microraptor") most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, pioneered by Robert McNeill Alexander, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.
Modern birds are known to communicate using visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups, such as horns, frills, crests, sails, and feathers, suggests that visual communication has always been important in dinosaur biology. Reconstruction of the plumage color of "Anchiornis huxleyi", suggest the importance of color in visual communication in non-avian dinosaurs. The evolution of dinosaur vocalization is less certain. Paleontologist Phil Senter suggests that non-avian dinosaurs relied mostly on visual displays and possibly non-vocal acoustic sounds like hissing, jaw grinding or clapping, splashing and wing beating (possible in winged maniraptoran dinosaurs). He states they were unlikely to have been capable of vocalizing since their closest relatives, crocodilians and birds, use different means to vocalize, the former via the larynx and the latter through the unique syrinx, suggesting they evolved independently and their common ancestor was mute.
The earliest remains of a syrinx, which has enough mineral content for fossilization, was found in a specimen of the duck-like "Vegavis iaai" dated 69-66 million years ago, and this organ is unlikely to have existed in non-avian dinosaurs. However, in contrast to Senter, the researchers have suggested that dinosaurs could vocalize and that the syrinx-based vocal system of birds evolved from a larynx-based one, rather than the two systems evolving independently. A 2016 study suggests that dinosaurs produced closed mouth vocalizations like cooing, which occur in both crocodilians and birds as well as other reptiles. Such vocalizations evolved independently in extant archosaurs numerous times, following increases in body size. The crests of the Lambeosaurini and nasal chambers of ankylosaurids have been suggested to function in vocal resonance, though Senter states that the presence of resonance chambers in some dinosaurs is not necessarily evidence of vocalization as modern snakes have such chambers which intensify their hisses.
All dinosaurs laid amniotic eggs with hard shells made mostly of calcium carbonate. Dinosaur eggs were usually laid in a nest. Most species create somewhat elaborate nests which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as "Troodon", exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.
When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a "Tyrannosaurus rex" skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur "Allosaurus" and the ornithopod "Tenontosaurus". Because the line of dinosaurs that includes "Allosaurus" and "Tyrannosaurus" diverged from the line that led to "Tenontosaurus" very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs.
Another widespread trait among modern birds (but see below in regards to fossil groups and extant megapodes) is parental care for young after hatching. Jack Horner's 1978 discovery of a "Maiasaura" ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods. A specimen of the Mongolian oviraptorid "Citipati osmolskae" was discovered in a chicken-like brooding position in 1993, which may indicate that they had begun using an insulating layer of feathers to keep the eggs warm. A dinosaur embryo (pertaining to the prosauropod "Massospondylus") was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland.
However, there is ample evidence of precociality or superprecociality among many dinosaur species, particularly theropods. For instance, non-ornithuromorph birds have been abundantly demonstrated to have had slow growth rates, megapode-like egg burying behavior and the ability to fly soon after birth. Both "Tyrannosaurus rex" and "Troodon formosus" display juveniles with clear superprecociality and likely occupying different ecological niches than the adults. Superprecociality has been inferred for sauropods.
Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are "warm-blooded" (endothermic), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extends. Scientists disagree as to whether non-avian dinosaurs were endothermic, ectothermic, or some combination of both.
After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This supposed "cold-bloodedness" was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic remained a prevalent view until Robert T. "Bob" Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968.
Modern evidence indicates that some non-avian dinosaurs thrived in cooler temperate climates and that some early species must have regulated their body temperature by internal biological means (aided by the animals' bulk in large species and feathers or other body coverings in smaller species). Evidence of endothermy in Mesozoic dinosaurs includes the discovery of polar dinosaurs in Australia and Antarctica as well as analysis of blood-vessel structures within fossil bones that are typical of endotherms. Scientific debate continues regarding the specific ways in which dinosaur temperature regulation evolved.
In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Early avian-style respiratory systems with air sacs may have been capable of sustaining higher activity levels than those of mammals of similar size and build. In addition to providing a very efficient supply of oxygen, the rapid airflow would have been an effective cooling mechanism, which is essential for animals that are active but too large to get rid of all the excess heat through their skin.
Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets that may have come from dinosaurs are known from as long ago as the Cretaceous.
The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of their being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in "Oviraptor", but misidentified as an interclavicle. In the 1970s, John Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Yixian Formation, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives. They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.
Feathers are one of the most recognizable characteristics of modern birds, and a trait that was shared by all other dinosaur groups. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs, suggesting the possibility that feather-like filaments may have been common in the bird lineage and evolved before the appearance of dinosaurs themselves. Research into the genetics of American alligators has also revealed that crocodylian scutes do possess feather-keratins during embryonic development, but these keratins are not expressed by the animals before hatching.
"Archaeopteryx" was the first fossil found that revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Charles Darwin's seminal "On the Origin of Species" (1859), its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for "Compsognathus". Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Most of these specimens were unearthed in the lagerstätte of the Yixian Formation, Liaoning, northeastern China, which was part of an island continent during the Cretaceous. Though feathers have been found in only a few locations, it is possible that non-avian dinosaurs elsewhere in the world were also feathered. The lack of widespread fossil evidence for feathered non-avian dinosaurs may be because delicate features like skin and feathers are not often preserved by fossilization and thus are absent from the fossil record.
The description of feathered dinosaurs has not been without controversy; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the scientific nature of Feduccia's proposals has been questioned.
In 2016, it was reported that a dinosaur tail with feathers had been found enclosed in amber. The fossil is about 99 million years old.
Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.
Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to a 2005 investigation led by Patrick M. O'Connor. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In 2008, scientists described "Aerosteon riocoloradensis", the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT scanning of "Aerosteon"'s fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.
Fossils of the troodonts "Mei" and "Sinornithoides" demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.
Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.
The discovery that birds are a type of dinosaur showed that dinosaurs in general are not, in fact, extinct as is commonly stated. However, all non-avian dinosaurs, estimated to have been 628-1078 species, as well as many groups of birds did suddenly become extinct approximately 66 million years ago. It has been suggested that because small mammals, squamata and birds occupied the ecological niches suited for small body size, non-avian dinosaurs never evolved a diverse fauna of small-bodied species, which led to their downfall when large-bodied terrestrial tetrapods were hit by the mass extinction event. Many other groups of animals also became extinct at this time, including ammonites (nautilus-like mollusks), mosasaurs, plesiosaurs, pterosaurs, and many groups of mammals. Significantly, the insects suffered no discernible population loss, which left them available as food for other survivors. This mass extinction is known as the Cretaceous–Paleogene extinction event. The nature of the event that caused this mass extinction has been extensively studied since the 1970s; at present, several related theories are supported by paleontologists. Though the consensus is that an impact event was the primary cause of dinosaur extinction, some scientists cite other possible causes, or support the idea that a confluence of several factors was responsible for the sudden disappearance of dinosaurs from the fossil record.
The asteroid impact hypothesis, which was brought to wide attention in 1980 by Walter Alvarez and colleagues, links the extinction event at the end of the Cretaceous to a bolide impact approximately 66 million years ago. Alvarez "et al." proposed that a sudden increase in iridium levels, recorded around the world in the period's rock stratum, was direct evidence of the impact. The bulk of the evidence now suggests that a bolide wide hit in the vicinity of the Yucatán Peninsula (in southeastern Mexico), creating the approximately Chicxulub crater and triggering the mass extinction.
Scientists are not certain whether dinosaurs were thriving or declining before the impact event. Some scientists propose that the meteorite impact caused a long and unnatural drop in Earth's atmospheric temperature, while others claim that it would have instead created an unusual heat wave. The consensus among scientists who support this hypothesis is that the impact caused extinctions both directly (by heat from the meteorite impact) and also indirectly (via a worldwide cooling brought about when matter ejected from the impact crater reflected thermal radiation from the sun). Although the speed of extinction cannot be deduced from the fossil record alone, various models suggest that the extinction was extremely rapid, being down to hours rather than years.
In 2019, scientists drilling into the seafloor off Mexico extracted a unique geologic record of what they believe to be the day a city-sized asteroid smashed into the planet.
Before 2000, arguments that the Deccan Traps flood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 million years ago and lasted for over 2 million years. However, there is evidence that two thirds of the Deccan Traps were created in only 1 million years about 66 million years ago, and so these eruptions would have caused a fairly rapid extinction, possibly over a period of thousands of years, but still longer than would be expected from a single impact event.
The Deccan Traps in India could have caused extinction through several mechanisms, including the release into the air of dust and sulfuric aerosols, which might have blocked sunlight and thereby reduced photosynthesis in plants. In addition, Deccan Trap volcanism might have resulted in carbon dioxide emissions, which would have increased the greenhouse effect when the dust and aerosols cleared from the atmosphere. Before the mass extinction of the dinosaurs, the release of volcanic gases during the formation of the Deccan Traps "contributed to an apparently massive global warming. Some data point to an average rise in temperature of [] in the last half-million years before the impact [at Chicxulub]."
In the years when the Deccan Traps hypothesis was linked to a slower extinction, Luis Alvarez (who died in 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. However, even Walter Alvarez has acknowledged that there were other major changes on Earth even before the impact, such as a drop in sea level and massive volcanic eruptions that produced the Indian Deccan Traps, and these may have contributed to the extinctions.
Non-avian dinosaur remains are occasionally found above the Cretaceous–Paleogene boundary. In 2000, paleontologist Spencer G. Lucas "et al." reported the discovery of a single hadrosaur right femur in the San Juan Basin, New Mexico, and described it as evidence of Paleocene dinosaurs. The formation in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.5 million years ago. If the bone was not re-deposited into that stratum by weathering action, it would provide evidence that some dinosaur populations may have survived at least a half-million years into the Cenozoic. Other evidence includes the finding of dinosaur remains in the Hell Creek Formation up to above the Cretaceous–Paleogene boundary, representing  years of elapsed time. Similar reports have come from other parts of the world, including China. Many scientists, however, dismissed the supposed Paleocene dinosaurs as re-worked, that is, washed out of their original locations and then re-buried in much later sediments. Direct dating of the bones themselves has supported the later date, with uranium–lead dating methods resulting in a precise age of 64.8 ± 0.9 million years ago. If correct, the presence of a handful of dinosaurs in the early Paleocene would not change the underlying facts of the extinction.
Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese considered them to be dragon bones and documented them as such. For example, "Huayang Guo Zhi" (), a gazetteer compiled by Chang Qu () during the Western Jin Dynasty (265–316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.
Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a "Megalosaurus", was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his "The Natural History of Oxford-shire" (1677). He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He, therefore, concluded it to be the femur of a huge human, perhaps a Titan or another type of giant featured in legends. Edward Lhuyd, a friend of Sir Isaac Newton, published "Lithophylacii Britannici ichnographia" (1699), the first scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, "Rutellum impicatum", that had been found in Caswell, near Witney, Oxfordshire.
Between 1815 and 1824, the Rev William Buckland, the first Reader of Geology at the University of Oxford, collected more fossilized bones of "Megalosaurus" and became the first person to describe a dinosaur in a scientific journal. The second dinosaur genus to be identified, "Iguanodon", was discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell. Gideon Mantell recognized similarities between his fossils and the bones of modern iguanas. He published his findings in 1825.
The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Richard Owen coined the term "dinosaur". He recognized that the remains that had been found so far, "Iguanodon", "Megalosaurus" and "Hylaeosaurus", shared a number of distinctive features, and so decided to present them as a distinct taxonomic group. With the backing of Prince Albert, the husband of Queen Victoria, Owen established the Natural History Museum, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.
In 1858, William Parker Foulke discovered the first known American dinosaur, in marl pits in the small town of Haddonfield, New Jersey. (Although fossils had been found before, their nature had not been correctly discerned.) The creature was named "Hadrosaurus foulkii". It was an extremely important find: "Hadrosaurus" was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of interests in dinosaurs in the United States, known as dinosaur mania.
Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. The feud probably originated when Marsh publicly pointed out that Cope's reconstruction of an "Elasmosaurus" skeleton was flawed: Cope had inadvertently placed the plesiosaur's head at what should have been the animal's tail end. The fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Unfortunately, many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones. Modern paleontologists would find such methods crude and unacceptable, since blasting easily destroys fossil and stratigraphic evidence. Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History, while Marsh's is on display at the Peabody Museum of Natural History at Yale University.
After 1897, the search for dinosaur fossils extended to every continent, including Antarctica. The first Antarctic dinosaur to be discovered, the ankylosaurid "Antarctopelta oliveroi", was found on James Ross Island in 1986, although it was 1994 before an Antarctic species, the theropod "Cryolophosaurus ellioti", was formally named and described in a scientific journal.
Current dinosaur "hot spots" include southern South America (especially Argentina) and China. China, in particular, has produced many exceptional feathered dinosaur specimens due to the unique geology of its dinosaur beds, as well as an ancient arid climate particularly conducive to fossilization.
The field of dinosaur research has enjoyed a surge in activity that began in the 1970s and is ongoing. This was triggered, in part, by John Ostrom's discovery of "Deinonychus", an active predator that may have been warm-blooded, in marked contrast to the then-prevailing image of dinosaurs as sluggish and cold-blooded. Vertebrate paleontology has become a global science. Major new dinosaur discoveries have been made by paleontologists working in previously unexploited regions, including India, South America, Madagascar, Antarctica, and most significantly China (the well-preserved feathered dinosaurs in China have further consolidated the link between dinosaurs and their living descendants, modern birds). The widespread application of cladistics, which rigorously analyzes the relationships between biological organisms, has also proved tremendously useful in classifying dinosaurs. Cladistic analysis, among other modern techniques, helps to compensate for an often incomplete and fragmentary fossil record.
One of the best examples of soft-tissue impressions in a fossil dinosaur was discovered in the Pietraroia Plattenkalk in southern Italy. The discovery was reported in 1998, and described the specimen of a small, very young coelurosaur, "Scipionyx samniticus". The fossil includes portions of the intestines, colon, liver, muscles, and windpipe of this immature dinosaur.
In the March 2005 issue of "Science", the paleontologist Mary Higby Schweitzer and her team announced the discovery of flexible material resembling actual soft tissue inside a 68-million-year-old "Tyrannosaurus rex" leg bone from the Hell Creek Formation in Montana. After recovery, the tissue was rehydrated by the science team. When the fossilized bone was treated over several weeks to remove mineral content from the fossilized bone-marrow cavity (a process called demineralization), Schweitzer found evidence of intact structures such as blood vessels, bone matrix, and connective tissue (bone fibers). Scrutiny under the microscope further revealed that the putative dinosaur soft tissue had retained fine structures (microstructures) even at the cellular level. The exact nature and composition of this material, and the implications of Schweitzer's discovery, are not yet clear.
In 2009, a team including Schweitzer announced that, using even more careful methodology, they had duplicated their results by finding similar soft tissue in a duck-billed dinosaur, "Brachylophosaurus canadensis", found in the Judith River Formation of Montana. This included even more detailed tissue, down to preserved bone cells that seem even to have visible remnants of nuclei and what seem to be red blood cells. Among other materials found in the bone was collagen, as in the "Tyrannosaurus" bone. The type of collagen an animal has in its bones varies according to its DNA and, in both cases, this collagen was of the same type found in modern chickens and ostriches.
The extraction of ancient DNA from dinosaur fossils has been reported on two separate occasions; upon further inspection and peer review, however, neither of these reports could be confirmed. However, a functional peptide involved in the vision of a theoretical dinosaur has been inferred using analytical phylogenetic reconstruction methods on gene sequences of related modern species such as reptiles and birds. In addition, several proteins, including hemoglobin, have putatively been detected in dinosaur fossils.
In 2015, researchers reported finding structures similar to blood cells and collagen fibers, preserved in the bone fossils of six Cretaceous dinosaur specimens, which are approximately 75 million years old.
By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. The entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.
Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures was unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. Dinosaurs' enduring popularity, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.
The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens "Bleak House", dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel "Journey to the Center of the Earth", Sir Arthur Conan Doyle's 1912 book "The Lost World", the iconic 1933 film "King Kong", the 1954 "Godzilla" and its many sequels, the best-selling 1990 novel "Jurassic Park" by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, who have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.
General
Images
Video
Popular
Technical

</doc>
<doc id="8315" url="https://en.wikipedia.org/wiki?curid=8315" title="Diamagnetism">
Diamagnetism

Diamagnetic materials are repelled by a magnetic field; an applied magnetic field creates an induced magnetic field in them in the opposite direction, causing a repulsive force. In contrast, paramagnetic and ferromagnetic materials are attracted by a magnetic field. Diamagnetism is a quantum mechanical effect that occurs in all materials; when it is the only contribution to the magnetism, the material is called diamagnetic. In paramagnetic and ferromagnetic substances, the weak diamagnetic force is overcome by the attractive force of magnetic dipoles in the material. The magnetic permeability of diamagnetic materials is less than the permeability of vacuum, "μ". In most materials, diamagnetism is a weak effect which can only be detected by sensitive laboratory instruments, but a superconductor acts as a strong diamagnet because it repels a magnetic field entirely from its interior.
Diamagnetism was first discovered when Anton Brugmans observed in 1778 that bismuth was repelled by magnetic fields. In 1845, Michael Faraday demonstrated that it was a property of matter and concluded that every material responded (in either a diamagnetic or paramagnetic way) to an applied magnetic field. On a suggestion by William Whewell, Faraday first referred to the phenomenon as "diamagnetic" (the prefix "dia-" meaning "through" or "across"), then later changed it to "diamagnetism".
A simple rule of thumb is used in chemistry to determine whether a particle (atom, ion, or molecule) is paramagnetic or diamagnetic: If all electrons in the particle are paired, then the substance made of this particle is diamagnetic; If it has unpaired electrons, then the substance is paramagnetic.
Diamagnetism is a property of all materials, and always makes a weak contribution to the material's response to a magnetic field. However, other forms of magnetism (such as ferromagnetism or paramagnetism) are so much stronger that, when multiple different forms of magnetism are present in a material, the diamagnetic contribution is usually negligible. Substances where the diamagnetic behaviour is the strongest effect are termed diamagnetic materials, or diamagnets. Diamagnetic materials are those that some people generally think of as "non-magnetic", and include water, wood, most organic compounds such as petroleum and some plastics, and many metals including copper, particularly the heavy ones with many core electrons, such as mercury, gold and bismuth. The magnetic susceptibility values of various molecular fragments are called Pascal's constants.
Diamagnetic materials, like water, or water-based materials, have a relative magnetic permeability that is less than or equal to 1, and therefore a magnetic susceptibility less than or equal to 0, since susceptibility is defined as . This means that diamagnetic materials are repelled by magnetic fields. However, since diamagnetism is such a weak property, its effects are not observable in everyday life. For example, the magnetic susceptibility of diamagnets such as water is . The most strongly diamagnetic material is bismuth, , although pyrolytic carbon may have a susceptibility of in one plane. Nevertheless, these values are orders of magnitude smaller than the magnetism exhibited by paramagnets and ferromagnets. Because "χ" is derived from the ratio of the internal magnetic field to the applied field, it is a dimensionless value.
In rare cases, the diamagnetic contribution can be stronger than paramagnetic contribution. This is the case for gold, which has a magnetic susceptibility less than 0 (and is thus by definition a diamagnetic material), but when measured carefully with X-ray magnetic circular dichroism, has an extremely weak paramagnetic contribution that is overcome by a stronger diamagnetic contribution.
Superconductors may be considered perfect diamagnets (), because they expel all magnetic fields (except in a thin surface layer) due to the Meissner effect.
If a powerful magnet (such as a supermagnet) is covered with a layer of water (that is thin compared to the diameter of the magnet) then the field of the magnet significantly repels the water. This causes a slight dimple in the water's surface that may be seen by a reflection in its surface.
Diamagnets may be levitated in stable equilibrium in a magnetic field, with no power consumption. Earnshaw's theorem seems to preclude the possibility of static magnetic levitation. However, Earnshaw's theorem applies only to objects with positive susceptibilities, such as ferromagnets (which have a permanent positive moment) and paramagnets (which induce a positive moment). These are attracted to field maxima, which do not exist in free space. Diamagnets (which induce a negative moment) are attracted to field minima, and there can be a field minimum in free space.
A thin slice of pyrolytic graphite, which is an unusually strong diamagnetic material, can be stably floated in a magnetic field, such as that from rare earth permanent magnets. This can be done with all components at room temperature, making a visually effective demonstration of diamagnetism.
The Radboud University Nijmegen, the Netherlands, has conducted experiments where water and other substances were successfully levitated. Most spectacularly, a live frog (see figure) was levitated.
In September 2009, NASA's Jet Propulsion Laboratory (JPL) in Pasadena, California announced it had successfully levitated mice using a superconducting magnet, an important step forward since mice are closer biologically to humans than frogs. JPL said it hopes to perform experiments regarding the effects of microgravity on bone and muscle mass.
Recent experiments studying the growth of protein crystals have led to a technique using powerful magnets to allow growth in ways that counteract Earth's gravity.
A simple homemade device for demonstration can be constructed out of bismuth plates and a few permanent magnets that levitate a permanent magnet.
The electrons in a material generally settle in orbitals, with effectively zero resistance and act like current loops. Thus it might be imagined that diamagnetism effects in general would be common, since any applied magnetic field would generate currents in these loops that would oppose the change, in a similar way to superconductors, which are essentially perfect diamagnets. However, since the electrons are rigidly held in orbitals by the charge of the protons and are further constrained by the Pauli exclusion principle, many materials exhibit diamagnetism, but typically respond very little to the applied field.
The Bohr–van Leeuwen theorem proves that there cannot be any diamagnetism or paramagnetism in a purely classical system. However, the classical theory of Langevin for diamagnetism gives the same prediction as the quantum theory. The classical theory is given below.
Paul Langevin's theory of diamagnetism (1905) applies to materials containing atoms with closed shells (see dielectrics). A field with intensity , applied to an electron with charge and mass , gives rise to Larmor precession with frequency . The number of revolutions per unit time is , so the current for an atom with electrons is (in SI units)
The magnetic moment of a current loop is equal to the current times the area of the loop. Suppose the field is aligned with the axis. The average loop area can be given as formula_2, where formula_3 is the mean square distance of the electrons perpendicular to the axis. The magnetic moment is therefore
If the distribution of charge is spherically symmetric, we can suppose that the distribution of coordinates are independent and identically distributed. Then formula_5, where formula_6 is the mean square distance of the electrons from the nucleus. Therefore, formula_7. If formula_8 is the number of atoms per unit volume, the volume diamagnetic susceptibility in SI units is
The Langevin theory is not the full picture for metals because there are also non-localized electrons. The theory that describes diamagnetism in a free electron gas is called Landau diamagnetism, named after Lev Landau, and instead considers the weak counteracting field that forms when the electrons' trajectories are curved due to the Lorentz force. Landau diamagnetism, however, should be contrasted with Pauli paramagnetism, an effect associated with the polarization of delocalized electrons' spins. For the bulk case of a 3D system and low magnetic fields, the (volume) diamagnetic susceptibility can be calculated using Landau quantization, which in SI units is
where formula_11 is the Fermi energy. This is equivalent to formula_12, exactly formula_13 times Pauli paramagnetic susceptibility, where formula_14 is the Bohr magneton and formula_15 is the density of states (number of states per energy per volume). This formula takes into account the spin degeneracy of the carriers (spin ½ electrons).
In doped semiconductors the ratio between Landau and Pauli susceptibilities may change due to the effective mass of the charge carriers differing from the electron mass in vacuum, increasing the diamagnetic contribution. The formula presented here only applies for the bulk; in confined systems like quantum dots, the description is altered due to quantum confinement. Additionally, for strong magnetic fields, the susceptibility of delocalized electrons oscillates as a function of the field strength, a phenomenon known as the de Haas–van Alphen effect, also first described theoretically by Landau.

</doc>
<doc id="8317" url="https://en.wikipedia.org/wiki?curid=8317" title="Duke of Marlborough (title)">
Duke of Marlborough (title)

Duke of Marlborough ( ) is a title in the Peerage of England. It was created by Queen Anne in 1702 for John Churchill, 1st Earl of Marlborough (1650–1722), the noted military leader. In historical texts, it is often to him that an unqualified use of the title refers. The name of the dukedom refers to Marlborough in Wiltshire. 
The earldom of Marlborough was held by the family of Ley from its creation 1626 until its extinction with the death of the 4th earl in 1679. The title was recreated 10 years later for John Churchill (in 1689).
Churchill had been made "Lord Churchill of Eyemouth" (1682) in the Peerage of Scotland, and "Baron Churchill" of Sandridge (1685) and "Earl of Marlborough" (1689) in the Peerage of England. Shortly after her accession to the throne in 1702, Queen Anne made Churchill the first "Duke of Marlborough" and granted him the subsidiary title "Marquess of Blandford".
In 1678, Churchill married Sarah Jennings (1660–1744), a courtier and influential favourite of the queen. They had seven children, of whom four daughters married into some of the most important families in Great Britain; one daughter and one son died in infancy. He was pre-deceased by his son, John Churchill, Marquess of Blandford, in 1703; so, to prevent the extinction of the titles, a special Act of Parliament was passed. When the 1st Duke of Marlborough died in 1722 his title as "Lord Churchill of Eyemouth" in the Peerage of Scotland became extinct and the Marlborough titles passed, according to the Act, to his eldest daughter Henrietta (1681–1733), the 2nd Duchess of Marlborough. She was married to the 2nd Earl of Godolphin and had a son who predeceased her.
When Henrietta died in 1733, the Marlborough titles passed to her nephew Charles Spencer (1706–1758), the third son of her late sister Anne (1683–1716), who had married the 3rd Earl of Sunderland in 1699. After his older brother's death in 1729, Charles Spencer had already inherited the Spencer family estates and the titles of "Earl of Sunderland" (1643) and "Baron Spencer" of Wormleighton (1603), all in the Peerage of England. Upon his maternal aunt Henrietta's death in 1733, Charles Spencer succeeded to the Marlborough family estates and titles and became the 3rd Duke. When he died in 1758, his titles passed to his eldest son George (1739–1817), who was succeeded by his eldest son George, the 5th Duke (1766–1840). In 1815, Francis Spencer (the younger son of the 4th Duke) was created "Baron Churchill" in the Peerage of the United Kingdom. In 1902, his grandson, the 3rd Baron Churchill, was created Viscount Churchill.
In 1817, the 5th Duke obtained permission to assume and bear the surname of Churchill in addition to his surname of Spencer, to perpetuate the name of his illustrious great-great-grandfather. At the same time he received Royal Licence to quarter the coat of arms of Churchill with his paternal arms of Spencer. The modern Dukes thus originally bore the surname "Spencer": the double-barrelled surname of "Spencer-Churchill" as used since 1817 remains in the family, although many members have preferred to style themselves simply as "Churchill".
The 7th Duke was the paternal grandfather of the British Prime Minister Sir Winston Churchill, born at Blenheim Palace on 30 November 1874.
The 11th Duke, John Spencer-Churchill died in 2014, having assumed the title in 1972. The 12th and present Duke is Charles James Spencer-Churchill.
The family seat is Blenheim Palace in Woodstock, Oxfordshire.
After his leadership in the victory against the French in the Battle of Blenheim on 13 August 1704, the 1st Duke was honoured by Queen Anne granting him the royal manor of Woodstock, and building him a house at her expense to be called Blenheim. Construction started in 1705 and the house was completed in 1722, the year of the 1st Duke's death. Blenheim Palace has since remained in the Churchill and Spencer-Churchill family.
With the exception of the 10th Duke and his first wife, the Dukes and Duchesses of Marlborough are buried in Blenheim Palace's chapel. Most other members of the Spencer-Churchill family are interred in St. Martin's parish churchyard at Bladon, a short distance from the palace.
The dukedom can theoretically pass through a female line. However, unlike the remainder to heirs general found in most other peerages that allow male-preference primogeniture, the grant does not allow for abeyance and follows a more restrictive Semi-Salic formula designed to keep succession wherever possible in the male line. The succession is as follows:
Succession to the title under the first and second contingencies have lapsed; holders of the title from the 3rd Duke trace their status from the third contingency.
It is now very unlikely that the dukedom will be passed to a woman or through a woman, since all the male-line descendants of the 1st Duke's second daughter Anne Spencer, Countess of Sunderland—including the lines of the Viscounts Churchill and Barons Churchill of Wychwood and of the Earl Spencer and of the entire Spencer-Churchill and Spencer family—would have to become extinct.
If that were to happen, the Churchill titles would pass to the Earl of Jersey (and merge with the earldom as long as it is extant), the heir-male of the 1st Duke's granddaughter Anne Villiers (born Egerton), Countess of Jersey, daughter of Elizabeth Egerton, Duchess of Bridgewater, the third daughter of the first Duke.
The next heir would be the Duke of Buccleuch, the heir-male of the 1st Duke's great-granddaughter Elizabeth Montagu, Duchess of Buccleuch, the daughter of Mary Montagu, Duchess of Montagu (1766 creation), the daughter of the 1st Duke's youngest daughter Mary, Duchess of Montagu (1705 creation).
The fourth surviving line is represented by the Earl of Chichester and his family, the heir-male of the 1st Duke's most senior great-great-granddaughter Mary Henrietta Osborne, Countess of Chichester, daughter of Francis Osborne, 5th Duke of Leeds, only child of Mary Godolphin, Duchess of Leeds, daughter of the 1st Duke's eldest daughter Henrietta Godolphin, 2nd Duchess of Marlborough, by her husband Francis Godolphin, 2nd Earl of Godolphin.
The Duke holds subsidiary titles: "Marquess of Blandford" (created in 1702 for John Churchill), "Earl of Sunderland" (created in 1643 for the Spencer family), "Earl of Marlborough" (created in 1689 for John Churchill), "Baron Spencer" of Wormleighton (created in 1603 for the Spencer family), and "Baron Churchill" of Sandridge (created in 1685 for John Churchill), all in the Peerage of England.
The title "Marquess of Blandford" is used as the courtesy title for the Duke's eldest son and heir. The Duke's eldest son's eldest son can use the courtesy title "Earl of Sunderland", and the duke's eldest son's eldest son's eldest son (not necessarily the eldest great-grandson) the title "Lord Spencer of Wormleighton" (not to be confused with Earl Spencer).
The title of "Earl of Marlborough", created for John Churchill in 1689, had previously been created for James Ley, in 1626, becoming extinct in 1679.
The 1st Duke was honoured with land and titles in the Holy Roman Empire: Emperor Leopold I created him a Prince in 1704, and in 1705, his successor Emperor Joseph I gave him the principality of Mindelheim (once the lordship of the noted soldier Georg von Frundsberg). He was obliged to surrender Mindelheim in 1714 by the Treaty of Utrecht, which returned it to Bavaria. He tried to obtain Nellenburg in Austria in exchange, which at that time was only a county ('Landgrafschaft'), but this failed, partially because Austrian law did not allow for Nellenburg to be converted into a sovereign principality. The 1st Duke's princely title of Mindelheim became extinct either on the return of the land to Bavaria or on his death, as the Empire operated Salic Law, which prevented female succession.
The original arms of Sir Winston Churchill (1620–1688), father of the 1st Duke of Marlborough, were simple and in use by his own father in 1619. The shield was Sable a lion rampant Argent, debruised by a bendlet Gules. The addition of a canton of Saint George (see below) rendered the distinguishing mark of the bendlet unnecessary.
The Churchill crest is blazoned as a lion couchant guardant Argent, supporting with its dexter forepaw a banner Gules, charged with a dexter hand appaumée of the first, staff Or.
In recognition of Sir Winston's services to King Charles I as Captain of the Horse, and his loyalty to King Charles II as a Member of Parliament, he was awarded an augmentation of honour to his arms around 1662. This rare mark of royal favour took the form of a canton of Saint George. At the same time, he was authorised to omit the bendlet, which had served the purpose of distinguishing this branch of the Churchill family from others which bore an undifferenced lion.
Sir Winston's shield and crest were inherited by his son John Churchill, 1st Duke of Marlborough. Minor modifications reflected the bearer's social rise: the helm was now shown in profile and had a closed grille to signify the bearer's rank as a peer, and there were now supporters placed on either side of the shield. They were the mythical Griffin (part lion, part eagle) and Wyvern (a dragon without hind legs). The supporters were derived from the arms of the family of the 1st Duke's mother, Drake of Ash (Argent, a wyvern gules; these arms can be seen on the monument in Musbury Church to Sir Bernard Drake, d.1586).
The motto was "Fiel pero desdichado" (Spanish for "Faithful but unfortunate"). The 1st Duke was also entitled to a coronet indicating his rank.
When the 1st Duke was made a Prince of the Holy Roman Empire in 1705, two unusual features were added: the Imperial Eagle and a Princely Coronet. His estates in Germany, such as Mindelheim, were represented in his arms by additional quarterings.
In 1817, the 5th Duke received Royal Licence to place the quarter of Churchill ahead of his paternal arms of Spencer. The shield of the Spencer family arms is: quarterly Argent and Gules, in the second and third quarters a fret Or, over all on a bend Sable three escallops of the first. The Spencer crest is: out of a ducal coronet Or, a griffin's head between two wings expanded Argent, gorged with a collar gemel and armed Gules. Paul Courtenay observes that "It would be normal in these circumstances for the paternal arms (Spencer) to take precedence over the maternal (Churchill), but because the Marlborough dukedom was senior to the Sunderland earldom, the procedure was reversed in this case."
Also in 1817, a further augmentation of honour was added to his armorial achievement. This incorporated the bearings from the standard of the Manor of Woodstock and was borne on an escutcheon, displayed over all in the centre chief point, as follows: Argent a cross of Saint George surmounted by an inescutcheon Azure, charged with three fleurs-de-lys Or, two over one. This inescutcheon represents the royal arms of France.
These quartered arms, incorporating the two augmentations of honour, have been the arms of all subsequent Dukes of Marlborough.
The motto "Fiel pero desdichado" is Spanish for "Faithful though Joyless". "Desdichado" means without happiness or without joy, alluding to the first Duke's father, Winston, who was a royalist and faithful supporter of the king during the English Civil War but was not compensated for his losses after the restoration. Charles II knighted Winston Churchill and other Civil War royalists but did not compensate them for their wartime losses, thereby inducing Winston to adopt the motto. It is unusual for the motto of an Englishman of the era to be in Spanish rather than Latin, and it is not known why this is the case.
The earldom of Marlborough was held by the family of Ley from 1626 to 1679. James Ley, the 1st Earl (c. 1550 – 1629), was lord chief justice of the King’s Bench in Ireland and then in England; he was an English member of parliament and was lord high treasurer from 1624 to 1628. In 1624 he was created Baron Ley and in 1626 Earl of Marlborough. The 3rd earl was his grandson James (1618–1665), a naval officer who was killed in action with the Dutch. James was succeeded by his uncle William, a younger son of the 1st earl, on whose death in 1679 the earldom became extinct.
The heir apparent to the dukedom is George John Godolphin Spencer-Churchill, Marquess of Blandford (b. 1992), eldest son of the 12th Duke.
<section begin=FamilyTree />
<section end="FamilyTree" />

</doc>
<doc id="8322" url="https://en.wikipedia.org/wiki?curid=8322" title="December 17">
December 17


</doc>
<doc id="8324" url="https://en.wikipedia.org/wiki?curid=8324" title="Difference engine">
Difference engine

A difference engine, first created by Charles Babbage, is an automatic mechanical calculator designed to tabulate polynomial functions. Its name is derived from the method of divided differences, a way to interpolate or tabulate functions by using a small set of polynomial co-efficients. Most mathematical functions commonly used by engineers, scientists and navigators, including logarithmic and trigonometric functions, can be approximated by polynomials, so a difference engine can compute many useful tables of numbers.
The notion of a mechanical calculator for mathematical functions can be traced back to the Antikythera mechanism of the 2nd century BC, while early modern examples are attributed to Pascal and Leibniz in the 17th century. 
In 1784 J. H. Müller, an engineer in the Hessian army, devised and built an adding machine and described the basic principles of a difference machine in a book published in 1786 (the first written reference to a difference machine is dated to 1784), but he was unable to obtain funding to progress with the idea.
Charles Babbage began to construct a small difference engine in c. 1819 and had completed it by 1822 (Difference Engine 0). He announced his invention on 14 June 1822, in a paper to the Royal Astronomical Society, entitled "Note on the application of machinery to the computation of astronomical and mathematical tables". This machine used the decimal number system and was powered by cranking a handle. The British government was interested, since producing tables was time-consuming and expensive and they hoped the difference engine would make the task more economical.
In 1823, the British government gave Babbage £1700 to start work on the project. Although Babbage's design was feasible, the metalworking techniques of the era could not economically make parts in the precision and quantity required. Thus the implementation proved to be much more expensive and doubtful of success than the government's initial estimate. In 1832, Babbage and Joseph Clement produced a small working model (one-seventh of the calculating section of Difference Engine No. 1, which was intended to operate on 20-digit numbers and sixth-order differences) which operated on 6-digit numbers and second-order differences. Lady Byron described seeing the working prototype in 1833: "We both went to see the thinking machine (or so it seems) last Monday. It raised several Nos. to the 2nd and 3rd powers, and extracted the root of a Quadratic equation." Work on the larger engine was suspended in 1833.
By the time the government abandoned the project in 1842, Babbage had received and spent over £17,000 on development, which still fell short of achieving a working engine. The government valued only the machine's output (economically produced tables), not the development (at unknown and unpredictable cost to complete) of the machine itself. Babbage did not, or was unwilling to, recognize that predicament. Meanwhile, Babbage's attention had moved on to developing an analytical engine, further undermining the government's confidence in the eventual success of the difference engine. By improving the concept as an analytical engine, Babbage had made the difference engine concept obsolete, and the project to implement it an utter failure in the view of the government.
The incomplete Difference Engine No. 1 was put on display to the public at the 1862 International Exhibition in South Kensington, London.
Babbage went on to design his much more general analytical engine, but later produced an improved "Difference Engine No. 2" design (31-digit numbers and seventh-order differences), between 1846 and 1849. Babbage was able to take advantage of ideas developed for the analytical engine to make the new difference engine calculate more quickly while using fewer parts.
Inspired by Babbage's difference engine in 1834, Per Georg Scheutz built several experimental models. In 1837 his son Edward proposed to construct a working model in metal, and in 1840 finished the calculating part, capable of calculating series with 5-digit numbers and first-order differences, which was later extended to third-order (1842). In 1843, after adding the printing part, the model was completed.
In 1851, funded by the government, construction of the larger and improved (15-digit numbers and fourth-order differences) machine began, and finished in 1853. The machine was demonstrated at the World's Fair in Paris, 1855 and then sold in 1856 to the Dudley Observatory in Albany, New York. Delivered in 1857, it was the first printing calculator sold. In 1857 the British government ordered the next Scheutz's difference machine, which was built in 1859. It had the same basic construction as the previous one, weighing about .
Martin Wiberg improved Scheutz's construction (c. 1859, his machine has the same capacity as Scheutz's - 15-digit and fourth-order) but used his device only for producing and publishing printed tables (interest tables in 1860, and logarithmic tables in 1875).
Alfred Deacon of London in c. 1862 produced a small difference engine (20-digit numbers and third-order differences).
American George B. Grant started working on his calculating machine in 1869, unaware of the works of Babbage and Scheutz (Schentz). One year later (1870) he learned about difference engines and proceed to design one himself, describing his construction in 1871. In 1874 the Boston Thursday Club raised a subscription for the construction of a large-scale model, which was built in 1876. It could be expanded to enhance precision and weighed about .
Christel Hamann built one machine (16-digit numbers and second-order differences) in 1909 for the "Tables of Bauschinger and Peters" ("Logarithmic-Trigonometrical Tables with eight decimal places"), which was first published in Leipzig in 1910. It weighed about .
Burroughs Corporation in about 1912 built a machine for the Nautical Almanac Office which was used as a difference engine of second-order. It was later replaced in 1929 by a Burroughs Class 11 (13-digit numbers and second-order differences, or 11-digit numbers and [at least up to] fifth-order differences).
Alexander John Thompson about 1927 built "integrating and differencing machine" (13-digit numbers and fifth-order differences) for his table of logarithms "Logarithmetica britannica". This machine was composed of four modified Triumphator calculators.
Leslie Comrie in 1928 described how to use the Brunsviga-Dupla calculating machine as a difference engine of second-order (15-digit numbers). He also noted in 1931 that National Accounting Machine Class 3000 could be used as a difference engine of sixth-order.
During the 1980s, Allan G. Bromley, an associate professor at the University of Sydney, Australia, studied Babbage's original drawings for the Difference and Analytical Engines at the Science Museum library in London. This work led the Science Museum to construct a working calculating section of difference engine No. 2 from 1985 to 1991, under Doron Swade, the then Curator of Computing. This was to celebrate the 200th anniversary of Babbage's birth in 1999. In 2002, the printer which Babbage originally designed for the difference engine was also completed. The conversion of the original design drawings into drawings suitable for engineering manufacturers' use revealed some minor errors in Babbage's design (possibly introduced as a protection in case the plans were stolen), which had to be corrected. Once completed, both the engine and its printer worked flawlessly, and still do. The difference engine and printer were constructed to tolerances achievable with 19th-century technology, resolving a long-standing debate as to whether Babbage's design would have worked. (One of the reasons formerly advanced for the non-completion of Babbage's engines had been that engineering methods were insufficiently developed in the Victorian era.)
The printer's primary purpose is to produce stereotype plates for use in printing presses, which it does by pressing type into soft plaster to create a flong. Babbage intended that the Engine's results be conveyed directly to mass printing, having recognized that many errors in previous tables were not the result of human calculating mistakes but from error in the manual typesetting process. The printer's paper output is mainly a means of checking the engine's performance.
In addition to funding the construction of the output mechanism for the Science Museum's difference engine, Nathan Myhrvold commissioned the construction of a second complete Difference Engine No. 2, which was on exhibit at the Computer History Museum in Mountain View, California from 10 May 2008 until 31 January 2016.
It has since been transferred to Intellectual Ventures in Seattle where it is on display just outside the main lobby.
The difference engine consists of a number of columns, numbered from 1 to N. The machine is able to store one decimal number in each column. The machine can only add the value of a column "n" + 1 to column "n" to produce the new value of "n". Column "N" can only store a constant, column 1 displays (and possibly prints) the value of the calculation on the current iteration.
The engine is programmed by setting initial values to the columns. Column 1 is set to the value of the polynomial at the start of computation. Column 2 is set to a value derived from the first and higher derivatives of the polynomial at the same value of X. Each of the columns from 3 to "N" is set to a value derived from the formula_1 first and higher derivatives of the polynomial.
In the Babbage design, one iteration (i.e., one full set of addition and carry operations) happens for each rotation of the main shaft. Odd and even columns alternately perform an addition in one cycle. The sequence of operations for column formula_2 is thus:
Steps 1,2,3,4 occur for every odd column, while steps 3,4,1,2 occur for every even column.
While Babbage's original design placed the crank directly on the main shaft, it was later realized that the force required to crank the machine would have been too great for a human to handle comfortably. Therefore, the two models that were built incorporate a 4:1 reduction gear at the crank, and four revolutions of the crank are required to perform one full cycle.
Each iteration creates a new result, and is accomplished in four steps corresponding to four complete turns of the handle shown at the far right in the picture below. The four steps are:
The engine represents negative numbers as ten's complements. Subtraction amounts to addition of a negative number. This works in the same manner that modern computers perform subtraction, known as two's complement.
The principle of a difference engine is Newton's method of divided differences. If the initial value of a polynomial (and of its finite differences) is calculated by some means for some value of X, the difference engine can calculate any number of nearby values, using the method generally known as the method of finite differences. For example, consider the quadratic polynomial
with the goal of tabulating the values "p"(0), "p"(1), "p"(2), "p"(3), "p"(4), and so forth. The table below is constructed as follows: the second column contains the values of the polynomial, the third column contains the differences of the two left neighbors in the second column, and the fourth column contains the differences of the two neighbors in the third column:
The numbers in the third values-column are constant. In fact, by starting with any polynomial of degree "n", the column number "n" + 1 will always be constant. This is the crucial fact behind the success of the method.
This table was built from left to right, but it is possible to continue building it from right to left down a diagonal in order to compute more values. To calculate "p"(4) use the values from the lowest diagonal. Start with the fourth column constant value of 4 and copy it down the column. Then continue the third column by adding 4 to 11 to get 15. Next continue the second column by taking its previous value, 22 and adding the 15 from the third column. Thus "p"(5) is 22 + 15 = 37. In order to compute "p"(6), we iterate the same algorithm on the "p"(5) values: take 4 from the fourth column, add that to the third column's value 15 to get 19, then add that to the second column's value 37 to get 56, which is "p"(6). This process may be continued ad infinitum. The values of the polynomial are produced without ever having to multiply. A difference engine only needs to be able to add. From one loop to the next, it needs to store 2 numbers—in this example (the last elements in the first and second columns). To tabulate polynomials of degree "n", one needs sufficient storage to hold "n" numbers.
Babbage's difference engine No. 2, finally built in 1991, could hold 8 numbers of 31 decimal digits each and could thus tabulate 7th degree polynomials to that precision. The best machines from Scheutz could store 4 numbers with 15 digits each.
The initial values of columns can be calculated by first manually calculating N consecutive values of the function and by backtracking, i.e. calculating the required differences.
Col formula_6 gets the value of the function at the start of computation formula_7. Col formula_8 is the difference between formula_9 and formula_7...
If the function to be calculated is a polynomial function, expressed as
the initial values can be calculated directly from the constant coefficients "a", "a","a", ..., "a" without calculating any data points. The initial values are thus:
Many commonly used functions are analytic functions, which can be expressed as power series, for example as a Taylor series. The initial values can be calculated to any degree of accuracy; if done correctly the engine will give exact results for first N steps. After that, the engine will only give an approximation of the function.
The Taylor series expresses the function as a sum obtained from its derivatives at one point. For many functions the higher derivatives are trivial to obtain; for instance, the sine function at 0 has values of 0 or formula_19 for all derivatives. Setting 0 as the start of computation we get the simplified Maclaurin series
The same method of calculating the initial values from the coefficients can be used as for polynomial functions. The polynomial constant coefficients will now have the value
The problem with the methods described above is that errors will accumulate and the series will tend to diverge from the true function. A solution which guarantees a constant maximum error is to use curve fitting. A minimum of "N" values are calculated evenly spaced along the range of the desired calculations. Using a curve fitting technique like Gaussian reduction an "N"−1th degree polynomial interpolation of the function is found. With the optimized polynomial, the initial values can be calculated as above.
William Gibson and Bruce Sterling's "The Difference Engine" is an alternate history novel that looks at how society would have progressed had the difference engine and his analytical engine worked as Babbage envisioned.
The story takes place in Victorian England in which technological advancement is on the rise because of the success of Babbage's analytical machine. The convention of steampunk in which Victorian fashion is combined with the technological elements of the Industrial Revolution is seen throughout the story since its technology is so advanced in the era.

</doc>
<doc id="8326" url="https://en.wikipedia.org/wiki?curid=8326" title="Draupnir">
Draupnir

In Norse mythology, Draupnir (Old Norse "the dripper") is a gold ring possessed by the god Odin with the ability to multiply itself: Every ninth night, eight new rings 'drip' from Draupnir, each one of the same size and weight as the original.
Draupnir was forged by the dwarven brothers Brokkr and Eitri (or Sindri). Brokkr and Eitri made this ring as one of a set of three gifts which included Mjöllnir and Gullinbursti. They made these gifts in accordance with a wager Loki made saying that Brokkr and Eitri could not make better gifts than the three made by the Sons of Ivaldi. In the end, Mjöllnir, Thor's hammer, won the contest for Brokkr and Eitri. Loki used a loophole to get out of the wager for his head (the wager was for Loki's head only, but he argued that, to remove his head, they would have to injure his neck, which was not in the bargain) and Brokkr punished him by sealing his lips shut with wire.
The ring was placed by Odin on the funeral pyre of his son Baldr:
Odin laid upon the pyre the gold ring called Draupnir; this quality attended it: that every ninth night there fell from it eight gold rings of equal weight. (from the "Gylfaginning").
The ring was subsequently retrieved by Hermóðr. It was offered as a gift by Freyr's servant Skírnir in the wooing of Gerðr, which is described in the poem "Skírnismál".
Draupnir is represented as a card in the Yu-Gi-Oh Trading Card Game. It has an effect that mimics the multiplication ability of the mythological version. If it is destroyed by another card's effect, you can add another "Nordic Relic" card to your hand. 
"DRAUPNIR" was revealed as the key to a website that Neal Caffrey and Mozzie used to view their stolen Nazi U-boat treasure in "Taking Account", the seventh episode of the third season of "White Collar".
It also appeared in episode 11 of "" as a tool to seal Loki's spirit.
The Draupnir is never called by name but is simply known as Odin's ring in the first three books of the Witches of East End novels. This ring allows the wearer to teleport to any place of the nine worlds, and a copy of equal power was once owned by Loki before it was destroyed by Freya.

</doc>
<doc id="8328" url="https://en.wikipedia.org/wiki?curid=8328" title="Divergence">
Divergence

In vector calculus, divergence is a vector operator that operates on a vector field, producing a scalar field giving the quantity of the vector field's source at each point. More technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point.
As an example, consider air as it is heated or cooled. The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region. The divergence of the velocity field in that region would thus have a positive value. While the air is cooled and thus contracting, the divergence of the velocity has a negative value.
In physical terms, the divergence of a vector field is the extent to which the vector field flux behaves like a source at a given point. It is a local measure of its "outgoingness" – the extent to which there is more of the field vectors exiting an infinitesimal region of space than entering it. A point at which the flux is outgoing has positive divergence, and is often called a "source" of the field. A point at which the flux is directed inward has negative divergence, and is often called a "sink" of the field. The greater the flux of field through a small surface enclosing a given point, the greater the value of divergence at that point. A point at which there is zero flux through an enclosing surface has zero divergence. 
The divergence of a vector field is often illustrated using the example of the velocity field of a fluid, a liquid or gas. A moving gas has a velocity, a speed and direction, at each point which can be represented by a vector, so the velocity of the gas forms a vector field. If a gas is heated, it will expand. This will cause a net motion of gas particles outward in all directions. Any closed surface in the gas will enclose gas which is expanding, so there will be an outward flux of gas through the surface. So the velocity field will have positive divergence everywhere. Similarly, if the gas is cooled, it will contract. There will be more room for gas particles in any volume, so the external pressure of the fluid will cause a net flow of gas volume inward through any closed surface. Therefore the velocity field has negative divergence everywhere. In contrast in an unheated gas with a constant density, the gas may be moving, but the volume rate of gas flowing into any closed surface must equal the volume rate flowing out, so the "net" flux of fluid through any closed surface is zero. Thus the gas velocity has zero divergence everywhere. A field which has zero divergence everywhere is called solenoidal. 
If the fluid is heated only at one point or small region, or a small tube is introduced which supplies a source of additional fluid at one point, the fluid there will expand, pushing fluid particles around it outward in all directions. This will cause an outward velocity field throughout the fluid, centered on the heated point. Any closed surface enclosing the heated point will have a flux of fluid particles passing out of it, so there is positive divergence at that point. However any closed surface "not" enclosing the point will have a constant density of fluid inside, so just as many fluid particles are entering as leaving the volume, thus the net flux out of the volume is zero. Therefore the divergence at any other point is zero.
The divergence of a vector field at a point is defined as the limit of the ratio of the surface integral of out of the surface of a closed volume enclosing to the volume of , as shrinks to zero
where is the volume of , is the boundary of , and is the outward unit normal to that surface. It can be shown that the above limit always converges to the same value for any sequence of volumes that contain and approach zero volume. The result, , is a scalar function of . 
Since this definition is coordinate-free, it shows that the divergence is the same in any coordinate system. However it is not often used practically to calculate divergence; when the vector field is given in a coordinate system the coordinate definitions below are much simpler to use.
A vector field with zero divergence everywhere is called "solenoidal" – in which case any closed surface has no net flux across it.
In three-dimensional Cartesian coordinates, the divergence of a continuously differentiable vector field formula_1 is defined as the scalar-valued function:
Although expressed in terms of coordinates, the result is invariant under rotations, as the physical interpretation suggests. This is because the trace of the Jacobian matrix of an -dimensional vector field in -dimensional space is invariant under any invertible linear transformation.
The common notation for the divergence is a convenient mnemonic, where the dot denotes an operation reminiscent of the dot product: take the components of the operator (see del), apply them to the corresponding components of , and sum the results. Because applying an operator is different from multiplying the components, this is considered an abuse of notation.
For a vector expressed in local unit cylindrical coordinates as
where is the unit vector in direction , the divergence is
The use of local coordinates is vital for the validity of the expression. If we consider the position vector and the functions formula_5, formula_6, and formula_7, which assign the corresponding global cylindrical coordinate to a vector, in general formula_8, formula_9, and formula_10. In particular, if we consider the identity function formula_11, we find that:
In spherical coordinates, with the angle with the axis and the rotation around the axis, and formula_13 again written in local unit coordinates, the divergence is
Let formula_15 be continuously differentiable second-order tensor field defined as follows:
the divergence in cartesian coordinate system is a first-order tensor field and can be defined in two ways:
and
We have
We should note that if tensor is symmetric formula_20 then formula_21 and this cause that often in literature this two definitions (and symbols formula_22 and formula_23) are switched and interchangeably used (especially in mechanics equations where tensor symmetry is assumed).
Expressions of formula_24 in cylindrical and spherical coordinates are given in the article del in cylindrical and spherical coordinates.
Using Einstein notation we can consider the divergence in general coordinates, which we write as , where is the number of dimensions of the domain. Here, the upper index refers to the number of the coordinate or component, so refers to the second component, and not the quantity squared. The index variable is used to refer to an arbitrary element, such as . The divergence can then be written via the Voss-Weyl formula, as:
where formula_26 is the local coefficient of the volume element and are the components of with respect to the local unnormalized covariant basis (sometimes written as formula_27). The Einstein notation implies summation over , since it appears as both an upper and lower index.
The volume coefficient formula_26 is a function of position which depends on the coordinate system. In Cartesian, cylindrical and spherical coordinates, using the same conventions as before, we have formula_29, formula_30 and formula_31, respectively. It can also be expressed as formula_32, where formula_33 is the metric tensor. Since the determinant is a scalar quantity which doesn't depend on the indices, we can suppress them and simply write formula_34. Another expression comes from computing the determinant of the Jacobian for transforming from Cartesian coordinates, which for gives formula_35
Some conventions expect all local basis elements to be normalized to unit length, as was done in the previous sections. If we write formula_36 for the normalized basis, and formula_37 for the components of with respect to it, we have that 
using one of the properties of the metric tensor. By dotting both sides of the last equality with the contravariant element formula_39, we can conclude that formula_40. After substituting, the formula becomes:
See "" for further discussion.
The following properties can all be derived from the ordinary differentiation rules of calculus. Most importantly, the divergence is a linear operator, i.e.,
for all vector fields and and all real numbers and .
There is a product rule of the following type: if is a scalar-valued function and is a vector field, then
or in more suggestive notation
Another product rule for the cross product of two vector fields and in three dimensions involves the curl and reads as follows:
or
The Laplacian of a scalar field is the divergence of the field's gradient:
The divergence of the curl of any vector field (in three dimensions) is equal to zero: 
If a vector field with zero divergence is defined on a ball in , then there exists some vector field on the ball with . For regions in more topologically complicated than this, the latter statement might be false (see Poincaré lemma). The degree of "failure" of the truth of the statement, measured by the homology of the chain complex
serves as a nice quantification of the complicatedness of the underlying region . These are the beginnings and main motivations of de Rham cohomology.
It can be shown that any stationary flux that is twice continuously differentiable in and vanishes sufficiently fast for can be decomposed uniquely into an "irrotational part" and a "source-free part" . Moreover, these parts are explicitly determined by the respective "source densities" (see above) and "circulation densities" (see the article Curl):
For the irrotational part one has
with
The source-free part, , can be similarly written: one only has to replace the "scalar potential" by a "vector potential" and the terms by , and the source density 
by the circulation density .
This "decomposition theorem" is a by-product of the stationary case of electrodynamics. It is a special case of the more general Helmholtz decomposition, which works in dimensions greater than three as well.
One can express the divergence as a particular case of the exterior derivative, which takes a 2-form to a 3-form in . Define the current two-form as
It measures the amount of "stuff" flowing through a surface per unit time in a "stuff fluid" of density moving with local velocity . Its exterior derivative is then given by
Thus, the divergence of the vector field can be expressed as:
Here the superscript is one of the two musical isomorphisms, and is the Hodge star operator. Working with the current two-form and the exterior derivative is usually easier than working with the vector field and divergence, because unlike the divergence, the exterior derivative commutes with a change of (curvilinear) coordinate system.
The divergence of a vector field can be defined in any number of dimensions. If 
in a Euclidean coordinate system with coordinates , define
The appropriate expression is more complicated in curvilinear coordinates.
In the case of one dimension, reduces to a regular function, and the divergence reduces to the derivative.
For any , the divergence is a linear operator, and it satisfies the "product rule"
for any scalar-valued function .
The divergence of a vector field extends naturally to any differentiable manifold of dimension that has a volume form (or density) , e.g. a Riemannian or Lorentzian manifold. Generalising the construction of a two-form for a vector field on , on such a manifold a vector field defines an -form obtained by contracting with . The divergence is then the function defined by
Standard formulas for the Lie derivative allow us to reformulate this as
This means that the divergence measures the rate of expansion of a volume element as we let it flow with the vector field.
On a pseudo-Riemannian manifold, the divergence with respect to the metric volume form can be computed in terms of the Levi-Civita connection :
where the second expression is the contraction of the vector field valued 1-form with itself and the last expression is the traditional coordinate expression from Ricci calculus.
An equivalent expression without using connection is
where is the metric and denotes the partial derivative with respect to coordinate .
Divergence can also be generalised to tensors. In Einstein notation, the divergence of a contravariant vector is given by
where denotes the covariant derivative.
Equivalently, some authors define the divergence of a mixed tensor by using the musical isomorphism : if is a -tensor ( for the contravariant vector and for the covariant one), then we define the "divergence of " to be the -tensor
that is, we take the trace over the "first two" covariant indices of the covariant derivative

</doc>
<doc id="8334" url="https://en.wikipedia.org/wiki?curid=8334" title="December 18">
December 18


</doc>
<doc id="8336" url="https://en.wikipedia.org/wiki?curid=8336" title="Decision problem">
Decision problem

In computability theory and computational complexity theory, a decision problem is a problem that can be posed as a yes-no question of the input values. An example of a decision problem is deciding whether a given natural number is prime. Another is the problem "given two numbers "x" and "y", does "x" evenly divide "y"?". The answer is either 'yes' or 'no' depending upon the values of "x" and "y". A method for solving a decision problem, given in the form of an algorithm, is called a decision procedure for that problem. A decision procedure for the decision problem "given two numbers "x" and "y", does "x" evenly divide "y"?" would give the steps for determining whether "x" evenly divides "y". One such algorithm is long division. If the remainder is zero the answer is 'yes', otherwise it is 'no'. A decision problem which can be solved by an algorithm is called "decidable".
Decision problems typically appear in mathematical questions of decidability, that is, the question of the existence of an effective method to determine the existence of some object or its membership in a set; some of the most important problems in mathematics are undecidable.
The field of computational complexity categorizes "decidable" decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem. The field of recursion theory, meanwhile, categorizes "undecidable" decision problems by Turing degree, which is a measure of the noncomputability inherent in any solution.
A "decision problem" is a yes-or-no question on an infinite set of inputs. It is traditional to define the decision problem as the set of possible inputs together with the set of inputs for which the answer is "yes".
These inputs can be natural numbers, but can also be values of some other kind, like binary strings or strings over some other alphabet. The subset of strings for which the problem returns "yes" is a formal language, and often decision problems are defined as formal languages.
Using an encoding such as Gödel numberings, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers.
A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor. Although much more efficient methods of primality testing are known, the existence of any effective method is enough to establish decidability.
A decision problem "A" is "decidable" or "effectively solvable" if "A" is a recursive set. A problem is "partially decidable", "semidecidable", "solvable", or "provable" if "A" is a recursively enumerable set. Problems that are not decidable are "undecidable". For those it is not possible to create an algorithm, efficient or otherwise, that solves them.
The halting problem is an important undecidable decision problem; for more examples, see list of undecidable problems.
Decision problems can be ordered according to many-one reducibility and related to feasible reductions such as polynomial-time reductions. A decision problem "P" is said to be "complete" for a set of decision problems "S" if "P" is a member of "S" and every problem in "S" can be reduced to "P". Complete decision problems are used in computational complexity theory to characterize complexity classes of decision problems. For example, the Boolean satisfiability problem is complete for the class NP of decision problems under polynomial-time reducibility.
Decision problems are closely related to function problems, which can have answers that are more complex than a simple 'yes' or 'no'. A corresponding function problem is "given two numbers "x" and "y", what is "x" divided by "y"?".
A function problem consists of a partial function "f"; the informal "problem" is to compute the values of "f" on the inputs for which it is defined.
Every function problem can be turned into a decision problem; the decision problem is just the graph of the associated function. (The graph of a function "f" is the set of pairs ("x","y") such that "f"("x") = "y".) If this decision problem were effectively solvable then the function problem would be as well. This reduction does not respect computational complexity, however. For example, it is possible for the graph of a function to be decidable in polynomial time (in which case running time is computed as a function of the pair ("x","y") ) when the function is not computable in polynomial time (in which case running time is computed as a function of "x" alone). The function "f"("x") = "2" has this property.
Every decision problem can be converted into the function problem of computing the characteristic function of the set associated to the decision problem. If this function is computable then the associated decision problem is decidable. However, this reduction is more liberal than the standard reduction used in computational complexity (sometimes called polynomial-time many-one reduction); for example, the complexity of the characteristic functions of an NP-complete problem and its co-NP-complete complement is exactly the same even though the underlying decision problems may not be considered equivalent in some typical models of computation.
Unlike decision problems, for which there is only one correct answer for each input, optimization problems are concerned with finding the "best" answer to a particular input. Optimization problems arise naturally in many applications, such as the traveling salesman problem and many questions in linear programming.
There are standard techniques for transforming function and optimization problems into decision problems. For example, in the traveling salesman problem, the optimization problem is to produce a tour with minimal weight. The associated decision problem is: for each "N", to decide whether the graph has any tour with weight less than "N". By repeatedly answering the decision problem, it is possible to find the minimal weight of a tour.
Because the theory of decision problems is very well developed, research in complexity theory has typically focused on decision problems. Optimization problems themselves are still of interest in computability theory, as well as in fields such as operations research.

</doc>
<doc id="8339" url="https://en.wikipedia.org/wiki?curid=8339" title="Domain Name System">
Domain Name System

The Domain Name System (DNS) is a hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.
The Domain Name System delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Network administrators may delegate authority over sub-domains of their allocated name space to other name servers. This mechanism provides distributed and fault-tolerant service and was designed to avoid a single large central database.
The Domain Name System also specifies the technical functionality of the database service that is at its core. It defines the DNS protocol, a detailed specification of the data structures and data communication exchanges used in the DNS, as part of the Internet Protocol Suite.
The Internet maintains two principal namespaces, the domain name hierarchy and the Internet Protocol (IP) address spaces. The Domain Name System maintains the domain name hierarchy and provides translation services between it and the address spaces. Internet name servers and a communication protocol implement the Domain Name System. A DNS name server is a server that stores the DNS records for a domain; a DNS name server responds with answers to queries against its database.
The most common types of records stored in the DNS database are for Start of Authority (SOA), IP addresses (A and AAAA), SMTP mail exchangers (MX), name servers (NS), pointers for reverse DNS lookups (PTR), and domain name aliases (CNAME). Although not intended to be a general purpose database, DNS has been expanded over time to store records for other types of data for either automatic lookups, such as DNSSEC records, or for human queries such as "responsible person" (RP) records. As a general purpose database, the DNS has also been used in combating unsolicited email (spam) by storing a real-time blackhole list (RBL). The DNS database is traditionally stored in a structured text file, the zone file, but other database systems are common.
An often-used analogy to explain the Domain Name System is that it serves as the phone book for the Internet by translating human-friendly computer hostnames into IP addresses. For example, the domain name www.example.com translates to the addresses (IPv4) and (IPv6). The DNS can be quickly and transparently updated, allowing a service's location on the network to change without affecting the end users, who continue to use the same hostname. Users take advantage of this when they use meaningful Uniform Resource Locators (URLs) and e-mail addresses without having to know how the computer actually locates the services.
An important and ubiquitous function of the DNS is its central role in distributed Internet services such as cloud services and content delivery networks. When a user accesses a distributed Internet service using a URL, the domain name of the URL is translated to the IP address of a server that is proximal to the user. The key functionality of the DNS exploited here is that different users can "simultaneously" receive different translations for the "same" domain name, a key point of divergence from a traditional phone-book view of the DNS. This process of using the DNS to assign proximal servers to users is key to providing faster and more reliable responses on the Internet and is widely used by most major Internet services.
The DNS reflects the structure of administrative responsibility in the Internet. Each subdomain is a zone of administrative autonomy delegated to a manager. For zones operated by a registry, administrative information is often complemented by the registry's RDAP and WHOIS services. That data can be used to gain insight on, and track responsibility for, a given host on the Internet.
Using a simpler, more memorable name in place of a host's numerical address dates back to the ARPANET era. The Stanford Research Institute (now SRI International) maintained a text file named HOSTS.TXT that mapped host names to the numerical addresses of computers on the ARPANET. Elizabeth Feinler developed and maintained the first ARPANET directory. Maintenance of numerical addresses, called the Assigned Numbers List, was handled by Jon Postel at the University of Southern California's Information Sciences Institute (ISI), whose team worked closely with SRI.
Addresses were assigned manually. Computers, including their hostnames and addresses, were added to the primary file by contacting the SRI's Network Information Center (NIC), directed by Elizabeth Feinler, by telephone during business hours. Later, Feinler set up a WHOIS directory on a server in the NIC for retrieval of information about resources, contacts, and entities. She and her team developed the concept of domains. Feinler suggested that domains should be based on the location of the physical address of the computer. Computers at educational institutions would have the domain "edu", for example. She and her team managed the Host Naming Registry from 1972 to 1989.
By the early 1980s, maintaining a single, centralized host table had become slow and unwieldy and the emerging network required an automated naming system to address technical and personnel issues. Postel directed the task of forging a compromise between five competing proposals of solutions to Paul Mockapetris. Mockapetris instead created the Domain Name System in 1983.
The Internet Engineering Task Force published the original specifications in RFC 882 and RFC 883 in November 1983.
In 1984, four UC Berkeley students, Douglas Terry, Mark Painter, David Riggle, and Songnian Zhou, wrote the first Unix name server implementation for the Berkeley Internet Name Domain, commonly referred to as BIND. In 1985, Kevin Dunlap of DEC substantially revised the DNS implementation. Mike Karels, Phil Almquist, and Paul Vixie have maintained BIND since then. In the early 1990s, BIND was ported to the Windows NT platform. It was widely distributed, especially on Unix systems, and is still the most widely used DNS software on the Internet.
In November 1987, RFC 1034 and RFC 1035 superseded the 1983 DNS specifications. Several additional Request for Comments have proposed extensions to the core DNS protocols.
The domain name space consists of a tree data structure. Each node or leaf in the tree has a "label" and zero or more "resource records" (RR), which hold information associated with the domain name. The domain name itself consists of the label, concatenated with the name of its parent node on the right, separated by a dot.
The tree sub-divides into "zones" beginning at the root zone. A DNS zone may consist of only one domain, or may consist of many domains and sub-domains, depending on the administrative choices of the zone manager. DNS can also be partitioned according to "class" where the separate classes can be thought of as an array of parallel namespace trees.
Administrative responsibility for any zone may be divided by creating additional zones. Authority over the new zone is said to be "delegated" to a designated name server. The parent zone ceases to be authoritative for the new zone.
The definitive descriptions of the rules for forming domain names appear in RFC 1035, RFC 1123, RFC 2181, and RFC 5892. A domain name consists of one or more parts, technically called "labels", that are conventionally concatenated, and delimited by dots, such as example.com.
The right-most label conveys the top-level domain; for example, the domain name www.example.com belongs to the top-level domain "com".
The hierarchy of domains descends from right to left; each label to the left specifies a subdivision, or subdomain of the domain to the right. For example, the label "example" specifies a subdomain of the "com" domain, and "www" is a subdomain of example.com. This tree of subdivisions may have up to 127 levels.
A label may contain zero to 63 characters. The null label, of length zero, is reserved for the root zone. The full domain name may not exceed the length of 253 characters in its textual representation. In the internal binary representation of the DNS the maximum length requires 255 octets of storage, as it also stores the length of the name.
Although no technical limitation exists to use any character in domain name labels which are representable by an octet, hostnames use a preferred format and character set. The characters allowed in labels are a subset of the ASCII character set, consisting of characters "a" through "z", "A" through "Z", digits "0" through "9", and hyphen. This rule is known as the "LDH rule" (letters, digits, hyphen). Domain names are interpreted in case-independent manner. Labels may not start or end with a hyphen. An additional rule requires that top-level domain names should not be all-numeric.
The limited set of ASCII characters permitted in the DNS prevented the representation of names and words of many languages in their native alphabets or scripts. To make this possible, ICANN approved the Internationalizing Domain Names in Applications (IDNA) system, by which user applications, such as web browsers, map Unicode strings into the valid DNS character set using Punycode. In 2009 ICANN approved the installation of internationalized domain name country code top-level domains ("ccTLD"s). In addition, many registries of the existing top-level domain names ("TLD"s) have adopted the IDNA system, guided by RFC 5890, RFC 5891, RFC 5892, RFC 5893.
The Domain Name System is maintained by a distributed database system, which uses the client–server model. The nodes of this database are the name servers. Each domain has at least one authoritative DNS server that publishes information about that domain and the name servers of any domains subordinate to it. The top of the hierarchy is served by the root name servers, the servers to query when looking up ("resolving") a TLD.
An "authoritative" name server is a name server that only gives answers to DNS queries from data that has been configured by an original source, for example, the domain administrator or by dynamic DNS methods, in contrast to answers obtained via a query to another name server that only maintains a cache of data.
An authoritative name server can either be a "primary" server or a "secondary" server. Historically the terms "master/slave" and "primary/secondary" were sometimes used interchangeably but the current practice is to use the latter form. A primary server is a server that stores the original copies of all zone records. A secondary server uses a special automatic updating mechanism in the DNS protocol in communication with its primary to maintain an identical copy of the primary records.
Every DNS zone must be assigned a set of authoritative name servers. This set of servers is stored in the parent domain zone with name server (NS) records.
An authoritative server indicates its status of supplying definitive answers, deemed "authoritative", by setting a protocol flag, called the "Authoritative Answer" ("AA") bit in its responses. This flag is usually reproduced prominently in the output of DNS administration query tools, such as dig, to indicate "that the responding name server is an authority for the domain name in question."
Domain name resolvers determine the domain name servers responsible for the domain name in question by a sequence of queries starting with the right-most (top-level) domain label.
For proper operation of its domain name resolver, a network host is configured with an initial cache ("hints") of the known addresses of the root name servers. The hints are updated periodically by an administrator by retrieving a dataset from a reliable source.
Assuming the resolver has no cached records to accelerate the process, the resolution process starts with a query to one of the root servers. In typical operation, the root servers do not answer directly, but respond with a referral to more authoritative servers, e.g., a query for "www.wikipedia.org" is referred to the "org" servers. The resolver now queries the servers referred to, and iteratively repeats this process until it receives an authoritative answer. The diagram illustrates this process for the host that is named by the fully qualified domain name "www.wikipedia.org".
This mechanism would place a large traffic burden on the root servers, if every resolution on the Internet required starting at the root. In practice caching is used in DNS servers to off-load the root servers, and as a result, root name servers actually are involved in only a relatively small fraction of all requests.
In theory, authoritative name servers are sufficient for the operation of the Internet. However, with only authoritative name servers operating, every DNS query must start with recursive queries at the root zone of the Domain Name System and each user system would have to implement resolver software capable of recursive operation.
To improve efficiency, reduce DNS traffic across the Internet, and increase performance in end-user applications, the Domain Name System supports DNS cache servers which store DNS query results for a period of time determined in the configuration ("time-to-live") of the domain name record in question.
Typically, such caching DNS servers also implement the recursive algorithm necessary to resolve a given name starting with the DNS root through to the authoritative name servers of the queried domain. With this function implemented in the name server, user applications gain efficiency in design and operation.
The combination of DNS caching and recursive functions in a name server is not mandatory; the functions can be implemented independently in servers for special purposes.
Internet service providers typically provide recursive and caching name servers for their customers. In addition, many home networking routers implement DNS caches and recursors to improve efficiency in the local network.
The client side of the DNS is called a DNS resolver. A resolver is responsible for initiating and sequencing the queries that ultimately lead to a full resolution (translation) of the resource sought, e.g., translation of a domain name into an IP address. DNS resolvers are classified by a variety of query methods, such as "recursive", "non-recursive", and "iterative". A resolution process may use a combination of these methods.
In a "non-recursive query", a DNS resolver queries a DNS server that provides a record either for which the server is authoritative, or it provides a partial result without querying other servers. In case of a caching DNS resolver, the non-recursive query of its local DNS cache delivers a result and reduces the load on upstream DNS servers by caching DNS resource records for a period of time after an initial response from upstream DNS servers.
In a "recursive query", a DNS resolver queries a single DNS server, which may in turn query other DNS servers on behalf of the requester. For example, a simple stub resolver running on a home router typically makes a recursive query to the DNS server run by the user's ISP. A recursive query is one for which the DNS server answers the query completely by querying other name servers as needed. In typical operation, a client issues a recursive query to a caching recursive DNS server, which subsequently issues non-recursive queries to determine the answer and send a single answer back to the client. The resolver, or another DNS server acting recursively on behalf of the resolver, negotiates use of recursive service using bits in the query headers. DNS servers are not required to support recursive queries.
The "iterative query" procedure is a process in which a DNS resolver queries a chain of one or more DNS servers. Each server refers the client to the next server in the chain, until the current server can fully resolve the request. For example, a possible resolution of www.example.com would query a global root server, then a "com" server, and finally an "example.com" server.
Name servers in delegations are identified by name, rather than by IP address. This means that a resolving name server must issue another DNS request to find out the IP address of the server to which it has been referred. If the name given in the delegation is a subdomain of the domain for which the delegation is being provided, there is a circular dependency.
In this case, the name server providing the delegation must also provide one or more IP addresses for the authoritative name server mentioned in the delegation. This information is called "glue". The delegating name server provides this glue in the form of records in the "additional section" of the DNS response, and provides the delegation in the "authority section" of the response. A glue record is a combination of the name server and IP address.
For example, if the authoritative name server for example.org is ns1.example.org, a computer trying to resolve www.example.org first resolves ns1.example.org. As ns1 is contained in example.org, this requires resolving example.org first, which presents a circular dependency. To break the dependency, the name server for the top level domain org includes glue along with the delegation for example.org. The glue records are address records that provide IP addresses for ns1.example.org. The resolver uses one or more of these IP addresses to query one of the domain's authoritative servers, which allows it to complete the DNS query.
A standard practice in implementing name resolution in applications is to reduce the load on the Domain Name System servers by caching results locally, or in intermediate resolver hosts. Results obtained from a DNS request are always associated with the time to live (TTL), an expiration time after which the results must be discarded or refreshed. The TTL is set by the administrator of the authoritative DNS server. The period of validity may vary from a few seconds to days or even weeks.
As a result of this distributed caching architecture, changes to DNS records do not propagate throughout the network immediately, but require all caches to expire and to be refreshed after the TTL. RFC 1912 conveys basic rules for determining appropriate TTL values.
Some resolvers may override TTL values, as the protocol supports caching for up to sixty-eight years or no caching at all. Negative caching, i.e. the caching of the fact of non-existence of a record, is determined by name servers authoritative for a zone which must include the Start of Authority (SOA) record when reporting no data of the requested type exists. The value of the "minimum" field of the SOA record and the TTL of the SOA itself is used to establish the TTL for the negative answer.
A reverse DNS lookup is a query of the DNS for domain names when the IP address is known. Multiple domain names may be associated with an IP address. The DNS stores IP addresses in the form of domain names as specially formatted names in pointer (PTR) records within the infrastructure top-level domain arpa. For IPv4, the domain is in-addr.arpa. For IPv6, the reverse lookup domain is ip6.arpa. The IP address is represented as a name in reverse-ordered octet representation for IPv4, and reverse-ordered nibble representation for IPv6.
When performing a reverse lookup, the DNS client converts the address into these formats before querying the name for a PTR record following the delegation chain as for any DNS query. For example, assuming the IPv4 address 208.80.152.2 is assigned to Wikimedia, it is represented as a DNS name in reverse order: 2.152.80.208.in-addr.arpa. When the DNS resolver gets a pointer (PTR) request, it begins by querying the root servers, which point to the servers of American Registry for Internet Numbers (ARIN) for the 208.in-addr.arpa zone. ARIN's servers delegate 152.80.208.in-addr.arpa to Wikimedia to which the resolver sends another query for 2.152.80.208.in-addr.arpa, which results in an authoritative response.
Users generally do not communicate directly with a DNS resolver. Instead DNS resolution takes place transparently in applications such as web browsers, e-mail clients, and other Internet applications. When an application makes a request that requires a domain name lookup, such programs send a resolution request to the DNS resolver in the local operating system, which in turn handles the communications required.
The DNS resolver will almost invariably have a cache (see above) containing recent lookups. If the cache can provide the answer to the request, the resolver will return the value in the cache to the program that made the request. If the cache does not contain the answer, the resolver will send the request to one or more designated DNS servers. In the case of most home users, the Internet service provider to which the machine connects will usually supply this DNS server: such a user will either have configured that server's address manually or allowed DHCP to set it; however, where systems administrators have configured systems to use their own DNS servers, their DNS resolvers point to separately maintained name servers of the organization. In any event, the name server thus queried will follow the process outlined above, until it either successfully finds a result or does not. It then returns its results to the DNS resolver; assuming it has found a result, the resolver duly caches that result for future use, and hands the result back to the software which initiated the request.
Some large ISPs have configured their DNS servers to violate rules, such as by disobeying TTLs, or by indicating that a domain name does not exist just because one of its name servers does not respond.
Some applications such as web browsers maintain an internal DNS cache to avoid repeated lookups via the network. This practice can add extra difficulty when debugging DNS issues as it obscures the history of such data. These caches typically use very short caching times on the order of one minute.
Internet Explorer represents a notable exception: versions up to IE 3.x cache DNS records for 24 hours by default. Internet Explorer 4.x and later versions (up to IE 8) decrease the default timeout value to half an hour, which may be changed by modifying the default configuration.
When Google Chrome detects issues with the DNS server it displays a specific error message.
The Domain Name System includes several other functions and features.
Hostnames and IP addresses are not required to match in a one-to-one relationship. Multiple hostnames may correspond to a single IP address, which is useful in virtual hosting, in which many web sites are served from a single host. Alternatively, a single hostname may resolve to many IP addresses to facilitate fault tolerance and load distribution to multiple server instances across an enterprise or the global Internet.
DNS serves other purposes in addition to translating names to IP addresses. For instance, mail transfer agents use DNS to find the best mail server to deliver e-mail: An MX record provides a mapping between a domain and a mail exchanger; this can provide an additional layer of fault tolerance and load distribution.
The DNS is used for efficient storage and distribution of IP addresses of blacklisted email hosts. A common method is to place the IP address of the subject host into the sub-domain of a higher level domain name, and to resolve that name to a record that indicates a positive or a negative indication.
For example:
E-mail servers can query blacklist.example to find out if a specific host connecting to them is in the blacklist. Many of such blacklists, either subscription-based or free of cost, are available for use by email administrators and anti-spam software.
To provide resilience in the event of computer or network failure, multiple DNS servers are usually provided for coverage of each domain. At the top level of global DNS, thirteen groups of root name servers exist, with additional "copies" of them distributed worldwide via anycast addressing.
Dynamic DNS (DDNS) updates a DNS server with a client IP address on-the-fly, for example, when moving between ISPs or mobile hot spots, or when the IP address changes administratively.
The DNS protocol uses two types of DNS messages, queries and replies; both have the same format. Each message consists of a header and four sections: question, answer, authority, and an additional space. A header field ("flags") controls the content of these four sections.
The header section consists of the following fields: "Identification", "Flags", "Number of questions", "Number of answers", "Number of authority resource records" (RRs), and "Number of additional RRs". Each field is 16 bits long, and appears in the order given. The identification field is used to match responses with queries. The flag field consists of sub-fields as follows:
After the flag, the header ends with four 16-bit integers which contain the number of records in each of the sections that follow, in the same order.
The question section has a simpler format than the resource record format used in the other sections. Each question record (there is usually just one in the section) contains the following fields:
The domain name is broken into discrete labels which are concatenated; each label is prefixed by the length of that label.
DNS primarily uses the User Datagram Protocol (UDP) on port number 53 to serve requests. DNS queries consist of a single UDP request from the client followed by a single UDP reply from the server. When the length of the answer exceeds 512 bytes and both client and server support EDNS, larger UDP packets are used. Otherwise, the query is sent again using the Transmission Control Protocol (TCP). TCP is also used for tasks such as zone transfers. Some resolver implementations use TCP for all queries.
The Domain Name System specifies a database of information elements for network resources. The types of information elements are categorized and organized with a list of DNS record types, the resource records (RRs). Each record has a type (name and number), an expiration time (time to live), a class, and type-specific data. Resource records of the same type are described as a "resource record set" (RRset), having no special ordering. DNS resolvers return the entire set upon query, but servers may implement round-robin ordering to achieve load balancing. In contrast, the Domain Name System Security Extensions (DNSSEC) work on the complete set of resource record in canonical order.
When sent over an Internet Protocol network, all records use the common format specified in RFC 1035:
"NAME" is the fully qualified domain name of the node in the tree . On the wire, the name may be shortened using label compression where ends of domain names mentioned earlier in the packet can be substituted for the end of the current domain name.
"TYPE" is the record type. It indicates the format of the data and it gives a hint of its intended use. For example, the "A" record is used to translate from a domain name to an IPv4 address, the "NS" record lists which name servers can answer lookups on a DNS zone, and the "MX" record specifies the mail server used to handle mail for a domain specified in an e-mail address.
"RDATA" is data of type-specific relevance, such as the IP address for address records, or the priority and hostname for MX records. Well known record types may use label compression in the RDATA field, but "unknown" record types must not (RFC 3597).
The "CLASS" of a record is set to IN (for "Internet") for common DNS records involving Internet hostnames, servers, or IP addresses. In addition, the classes Chaos (CH) and Hesiod (HS) exist. Each class is an independent name space with potentially different delegations of DNS zones.
In addition to resource records defined in a zone file, the domain name system also defines several request types that are used only in communication with other DNS nodes ("on the wire"), such as when performing zone transfers (AXFR/IXFR) or for EDNS (OPT).
The domain name system supports wildcard DNS records which specify names that start with the "asterisk label", '*', e.g., *.example. DNS records belonging to wildcard domain names specify rules for generating resource records within a single DNS zone by substituting whole labels with matching components of the query name, including any specified descendants. For example, in the following configuration, the DNS zone "x.example" specifies that all subdomains, including subdomains of subdomains, of "x.example" use the mail exchanger (MX) "a.x.example". The A record for "a.x.example" is needed to specify the mail exchanger IP address. As this has the result of excluding this domain name and its subdomains from the wildcard matches, an additional MX record for the subdomain "a.x.example", as well as a wildcarded MX record for all of its subdomains, must also be defined in the DNS zone.
The role of wildcard records was refined in RFC 4592, because the original definition in RFC 1034 was incomplete and resulted in misinterpretations by implementers.
The original DNS protocol had limited provisions for extension with new features. In 1999, Paul Vixie published in RFC 2671 (superseded by RFC 6891) an extension mechanism, called Extension mechanisms for DNS (EDNS) that introduced optional protocol elements without increasing overhead when not in use. This was accomplished through the OPT pseudo-resource record that only exists in wire transmissions of the protocol, but not in any zone files. Initial extensions were also suggested (EDNS0), such as increasing the DNS message size in UDP datagrams.
Dynamic DNS updates use the UPDATE DNS opcode to add or remove resource records dynamically from a zone database maintained on an authoritative DNS server. The feature is described in RFC 2136. This facility is useful to register network clients into the DNS when they boot or become otherwise available on the network. As a booting client may be assigned a different IP address each time from a DHCP server, it is not possible to provide static DNS assignments for such clients.
Originally, security concerns were not major design considerations for DNS software or any software for deployment on the early Internet, as the network was not open for participation by the general public. However, the expansion of the Internet into the commercial sector in the 1990s changed the requirements for security measures to protect data integrity and user authentication.
Several vulnerability issues were discovered and exploited by malicious users. One such issue is DNS cache poisoning, in which data is distributed to caching resolvers under the pretense of being an authoritative origin server, thereby polluting the data store with potentially false information and long expiration times (time-to-live). Subsequently, legitimate application requests may be redirected to network hosts operated with malicious intent.
DNS responses traditionally do not have a cryptographic signature, leading to many attack possibilities; the Domain Name System Security Extensions (DNSSEC) modify DNS to add support for cryptographically signed responses. DNSCurve has been proposed as an alternative to DNSSEC. Other extensions, such as TSIG, add support for cryptographic authentication between trusted peers and are commonly used to authorize zone transfer or dynamic update operations.
Some domain names may be used to achieve spoofing effects. For example, and paypa1.com are different names, yet users may be unable to distinguish them in a graphical user interface depending on the user's chosen typeface. In many fonts the letter "l" and the numeral "1" look very similar or even identical. This problem is acute in systems that support internationalized domain names, as many character codes in ISO 10646 may appear identical on typical computer screens. This vulnerability is occasionally exploited in phishing.
Techniques such as forward-confirmed reverse DNS can also be used to help validate DNS results.
DNS can also "leak" from otherwise secure or private connections, if attention is not paid to their configuration, and at times DNS has been used to bypass firewalls by malicious persons, and exfiltrate data, since it is often seen as innocuous.
Originally designed as a public, hierarchical, distributed and heavily cached database, DNS protocol has no confidentiality controls. User queries and nameserver responses are being sent unencrypted which enables network packet sniffing, DNS hijacking, DNS cache poisoning and man-in-the-middle attacks. This deficiency is commonly used by cybercriminals and network operators for marketing purposes, user authentication on captive portals and censorship.
User privacy is further exposed by proposals for increasing the level of client IP information in DNS queries (RFC 7871) for the benefit of Content Delivery Networks.
The main approaches that are in use to counter privacy issues with DNS:
Solutions preventing DNS inspection by local network operator are criticized for thwarting corporate network security policies and Internet censorship. They are also criticized from privacy point of view, as giving away the DNS resolution to the hands of a small number of companies known for monetizing user traffic and for centralizing DNS name resolution, which is generally perceived as harmful for the Internet.
The right to use a domain name is delegated by domain name registrars which are accredited by the Internet Corporation for Assigned Names and Numbers (ICANN) or other organizations such as OpenNIC, that are charged with overseeing the name and number systems of the Internet. In addition to ICANN, each top-level domain (TLD) is maintained and serviced technically by an administrative organization, operating a registry. A "registry" is responsible for operating the database of names within its authoritative zone, although the term is most often used for TLDs. A "registrant" is a person or organization who asked for domain registration. The registry receives registration information from each domain name "registrar", which is authorized (accredited) to assign names in the corresponding zone and publishes the information using the WHOIS protocol. As of 2015, usage of RDAP is being considered.
ICANN publishes the complete list of TLDs, TLD registries, and domain name registrars. Registrant information associated with domain names is maintained in an online database accessible with the WHOIS service. For most of the more than 290 country code top-level domains (ccTLDs), the domain registries maintain the WHOIS (Registrant, name servers, expiration dates, etc.) information. For instance, DENIC, Germany NIC, holds the DE domain data. From about 2001, most Generic top-level domain (gTLD) registries have adopted this so-called "thick" registry approach, i.e. keeping the WHOIS data in central registries instead of registrar databases.
For top-level domains on COM and NET, a "thin" registry model is used. The domain registry (e.g., GoDaddy, BigRock and PDR, VeriSign, etc., etc.) holds basic WHOIS data (i.e., registrar and name servers, etc.). Organizations, or registrants using ORG on the other hand, are on the Public Interest Registry exclusively.
Some domain name registries, often called "network information centers" (NIC), also function as registrars to end-users, in addition to providing access to the WHOIS datasets. The top-level domain registries, such as for the domains COM, NET, and ORG use a registry-registrar model consisting of many domain name registrars. In this method of management, the registry only manages the domain name database and the relationship with the registrars. The "registrants" (users of a domain name) are customers of the registrar, in some cases through additional subcontracting of resellers.
The Domain Name System is defined by Request for Comments (RFC) documents published by the Internet Engineering Task Force (Internet standards). The following is a list of RFCs that define the DNS protocol.
These RFCs are advisory in nature, but may provide useful information despite defining neither a standard or BCP. (RFC 1796)
These RFCs have an official status of Unknown, but due to their age are not clearly labeled as such.

</doc>
